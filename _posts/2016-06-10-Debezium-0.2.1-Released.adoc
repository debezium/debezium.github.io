---
layout: post
title: Debezium 0.2.1 Released
date:   2016-06-10 10:19:59 -0600
tags: [ releases, mysql, docker ]
author: rhauch
---

I'm happy to announce that **Debezium 0.2.1 is now available**. The link:/docs/connectors/mysql/[MySQL connector] has been significantly improved and is now able to monitor and produce change events for link:/docs/connectors/mysql/#ha-mysql-clusters#enabling-gtids[HA MySQL clusters] using link:/docs/connectors/mysql/[GTIDs], perform a link:/docs/connectors/mysql/#snapshots[consistent snapshot] when starting up the first time, and has a completely redesigned link:/docs/connectors/mysql/#events[event message structure] that provides a ton more information with every event. Our link:/docs/releases/[change log] has all the details about bugs, enhancements, new features, and backward compatibility notices. We've also updated our link:/docs/tutorial/[tutorial].

+++<!-- more -->+++

[NOTE]
====
What happened to 0.2.0? Well, we released it to Maven Central before we'd noticed a https://issues.redhat.com/browse/DBZ-71[few] https://issues.redhat.com/browse/DBZ-72[problems] that we thought it best to fix right away. Thus 0.2.1 was born.
====

== Installing the MySQL connector

If you've already installed https://zookeeper.apache.org[Zookeeper], http://kafka.apache.org/[Kafka], and http://kafka.apache.org/documentation.html#connect[Kafka Connect], then using Debezium's MySQL connector is easy. Simply download the https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/0.2.1/debezium-connector-mysql-0.2.1-plugin.tar.gz[connector's plugin archive], extract the JARs into your Kafka Connect environment, and add the directory with the JARs to http://docs.confluent.io/3.0.0/connect/userguide.html#installing-connector-plugins[Kafka Connect's classpath]. Restart your Kafka Connect process to pick up the new JARs.

If immutable containers are your thing, then check out https://hub.docker.com/r/debezium/[Debezium's Docker images] for Zookeeper, Kafka, and Kafka Connect with the MySQL connector already pre-installed and ready to go. Our link:/docs/tutorial/[tutorial] even walks you through using these images, and this is a great way to learn what Debezium is all about. You can even link:/blog/2016/05/31/Debezium-on-Kubernetes/[run Debezium on Kubernetes and OpenShift].

== Using the MySQL connector

To use the connector to produce change events for a particular MySQL server or cluster, simply create a link:/docs/connectors/mysql/#configuration[configuration file for the MySQL Connector] and use the link:http://docs.confluent.io/3.0.0/connect/userguide.html#rest-interface[Kafka Connect REST API] to add that connector to your Kafka Connect cluster. When the connector starts, it will grab a consistent snapshot of the databases in your MySQL server and start reading the MySQL binlog, producing events for every inserted, updated, and deleted row. The connector can optionally produce events with the DDL statements that were applied, and you can even choose to produce events for a subset of the databases and tables. Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed. See the link:/docs/connectors/mysql/[MySQL connector's documentation] for all the details.

== Using the libraries

Although Debezium is really intended to be used as turnkey services, all of Debezium's JARs and other artifacts are available in http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22io.debezium%22[Maven Central]. You might want to use our link:/blog/2016/04/15/parsing-ddl/[MySQL DDL parser] from our MySQL connector library to parse those DDL statments in your consumers. 

We do provide a small library so applications can link:/docs/embedded/[embed any Kafka Connect connector] and consume data change events read directly from the source system. This provides a much lighter weight system (since Zookeeper, Kafka, and Kafka Connect services are not needed), but as a consequence is not as fault tolerant or reliable since the application must manage and maintain all state normally kept inside Kafka's distributed and replicated logs. It's perfect for use in tests, and with careful consideration it may be useful in some applications.
