---
layout: post
title:  Image classification with Debezium and TensorFlow
date:   2023-04-21 10:20:00 -0100
tags: [ machine-learning, tensorflow, examples, apache-kafka ]
featured: true
author: vjuranek
---

With the recent success of ChatGTP we can observe another wave of interest in the AI field and in machine learning in general.
The previous wave of interest in this field was, at least to a certain extent, caused by the fact that awesome ML frameworks like https://www.tensorflow.org/[TensorFlow], https://pytorch.org/[PyTorch] or general data processing frameworks like https://spark.apache.org/[Spark] became available and made writing of ML models much more easy.
Since that time these frameworks have matured and writing models is even easier, as you will see later in this blog.
However, data set preparation and gathering data from various sources can be sometimes hard or even painful.
Creating a complete pipeline which would pull existing or newly created data, adjust the data and ingest it into selected ML libraries can be quite a challenging task.
Let's try to investigate if Debezium can help with this task and explore how we can leverage Debezium capabilities to make it more easy.

+++<!-- more -->+++

== Change data capture and Debezium in ML pipelines

In the machine learning area change data capture (CDC) can be a very powerful concept especially when it comes to online machine learning.
However, CDC can also be an important part of the pipeline using pre-trained models. 
We can use CDC to deliver new data immediately into a pre-trained model, which can evaluate it  and other parts of the pipeline can take any action based on the model output in the real time.

Besides these use cases, Debezium obviously seems to be a very good fit for any pipeline which includes loading data from databases.
Debezium can capture existing data as well as stream any newly created data.
Another important feature of Debezium is support for single message transforms.
We can adjust the data at the very beginning of the whole pipeline.
Being it either some data transformation or filtering, we can send over the wire only data we are really interested in and thus eventually save the bandwidth and speed up the whole pipeline.
Last, but not least, Debezium is able to deliver records into several message brokers and more brokers are being added (several new ones are available in recent 2.2.0 release).
This increases the chances to integrate Debezium with other tools and add it into most of the data pipelines.
In the future, Debezium potential can be increased further, e.g. by extending CDC not only to databases.

So, this is the theory.
Now let's explore how it works in reality.
In this blog post we will look at how to stream data into TensorFlow.
Based on the interest from the community, this may result in a series of blog posts where we try to explore possible integrations with other ML libraries and frameworks.

== Debezium and TensorFlow integration

TensorFlow is one of the most popular machine learning frameworks.
It provides a comprehensive platform for building, training, and deploying machine learning models across a wide range of applications.

To keep things simple, we will implement a model for recognition of handwritten digits, which is more or less the Hello World equivalent in the neural networks field.
The ultimate goal of this demo is to load MNIST data samples from Postgres using Debezium, pass it to our model implemented in Tensorflow for training and use this trained model for real time classification of images which are continuously stored in the database.

The complete pipeline is depicted in the diagram bellow:

++++
<div class="imageblock centered-image">
    <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/pipeline.png" class="responsive-image" alt="Debezium TensorFlow integration pipeline">
</div>
++++

All the code mentioned later in this blog is available as a Debezium example in http://TBD[Debezium example repository].

=== The data sample

We will use http://yann.lecun.com/exdb/mnist/[MNIST data sample].
The training sample contains 60,000 images with handwritten digits from 0 to 9 and the same amount of labels with corresponding digits.
Test sample contains 1,000 images.
The samples are available as gzip binaries.
As we assume a use case where the data of interest are in the database, we need to load the data into the database first.

We need to generate two SQL files, one for the train data set, `mnist_train.sql`, and one for test data sample, `mnist_test.sql`.
Each file would contain SQL commands for creating table with two columns: `pixels` column of type `BYTEA`, which would contain raw image bytes and `labels` column of type `SMALLINT`, which would contain digit corresponding to the image in given table row.
Rest of the file would contain commands for populating the table.
Image bytes can be decoded as a HEX string.

As we will show how to leverage Debezium also for data streaming later in this post, we will initially load into the database only training data set.
SQL file with training data will be used directly by Postgres container - when it starts, it will load this data into the training table.
The test data SQL file will be used later on.
However, the preparation of the data is the same for training as well as test samples and we can prepare both of them in one go.

To prepare the these SQL files, you can use `mnist2sql.py` script from http://TBD[Debezium tensorflow-mnist example]:

[source,bash]
----
$ ./mnist2sql.py --download
----

The script assumes MNIST data sets are available in `postgres` directory.
When the `--download` parameter is used, script  downloads MNIST data samples into `postgres` directory first.
The resulting SQL files are also located in the `postgres` directory.

=== Loading streamed data into Tensorflow

The most common Debezium usage is streaming the records to Kafka.
For loading data from various sources, TensorFlow provides https://www.tensorflow.org/io[TensorFlow I/O] module.
Besides other sources, it also provides capability to load the data from Kafka.
There are several ways to do it.
https://www.tensorflow.org/io/api_docs/python/tfio/IODataset#from_kafka[IODataset.from_kafka()] method loads only existing data from specified Kafka topics.
For streaming the data there are two classes, currently marked as experimental, https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaBatchIODataset[KafkaBatchIODataset] and https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaGroupIODataset[KafkaGroupIODataset].
Both are very similar and allow to work with streaming data, i.e. they not only read the existing data from Kafka topic, but also wait for new data and eventually pass new records into the TensorFlow.
If there are no new events within the specified time frame, the streaming finishes.

In all cases obtained records are loaded into TensorFlow as the https://www.tensorflow.org/api_docs/python/tf/data/Dataset[Dataset].
This is a convenient data structure provided by TensorFlow for building https://www.tensorflow.org/guide/data[data pipelines],  which may include further data transformations or preprocessing.

This sounds great.
However, the biggest caveat is how the records are represented within the Dataset.
It seems that these Kafka loaders completely ignore the schema of the records provided by Kafka.
Keys and values are always represented as the raw bytes.
This won't be that bad, but even worse, it seems that somewhere on the way the data are converted into the strings (i.e. `toString()` method on the object is called).
So if you pass e.g. raw image bytes via Kafka, using Kafka `BYTES_SCHEMA`, it would result into something like this:

[source]
----
<tf.Tensor: shape=(64,), dtype=string, numpy=
 array([b'[B@418b353d', b'[B@6aa28a4c', b'[B@b626485', b'[B@6d7491cd',
        b'[B@13fa86c5', b'[B@7c3bc352', b'[B@64e5d61c', b'[B@2dd6d9b4',
        b'[B@6addae65', b'[B@48ded13f', b'[B@2c1bb0e', b'[B@19c1d99b',
        b'[B@1ee8f240', b'[B@20019f8b', b'[B@2f17494e', b'[B@380d4036',
        b'[B@61aecf85', b'[B@4d7fe9fc', b'[B@58b79424', b'[B@ae963f4',
        b'[B@1dac57cb', b'[B@2fae7d8b', b'[B@4b5ccaee', b'[B@aebf6b2',
        b'[B@7506ea2b', b'[B@29989325', b'[B@43e2742', b'[B@51350f11',
        b'[B@13a0f0ae', b'[B@7e4c4844', b'[B@b3d64f8', b'[B@7209bf09',
        b'[B@66380466', b'[B@7aaa7e8d', b'[B@1ad0cf84', b'[B@259eca20',
        b'[B@3a3f1c1', b'[B@36e4ff1f', b'[B@6578fc29', b'[B@79c924be',
        b'[B@765b7f70', b'[B@67567aa3', b'[B@456d4bd4', b'[B@75317b13',
        b'[B@58bc3a3a', b'[B@c6bc0ec', b'[B@2377095e', b'[B@5de017c0',
        b'[B@64b48bac', b'[B@360a5b76', b'[B@2d2c9910', b'[B@70afd562',
        b'[B@3006c930', b'[B@54b3e5ad', b'[B@1d1e0232', b'[B@1394d036',
        b'[B@155dd43d', b'[B@5e88d5b6', b'[B@33ea53c7', b'[B@64a30ec',
        b'[B@7dcdf024', b'[B@6570bf4e', b'[B@4e5bc4c', b'[B@537f216c'],
       dtype=object)>,
----

Instead of getting a batch of raw image bytes which you can further transform in TensorFlow, you get only string representation of Java byte arrays, which is not very useful.

To mitigate this issue, the most easy and straightforward approach seems to be to convert raw image bytes into numbers before sending them into Kafka.
As TensorFlow provides methods for parsing CSV input, we can convert each image into one CSV line of numbers.
TensorFlow mostly works with numbers, so we would have to convert our images into numbers anyway.
The number which is on the image we can pass as a message key.
Now, a single message transform supported by Debezium comes really handy.
The transform can look like this:

[source,java]
----
    @Override
    public R apply(R r) {
        final Struct value = (Struct) r.value();
        String key = value.getInt16(labelFieldName).toString();

        StringBuilder builder = new StringBuilder();
        for (byte pixel : value.getBytes(pixlesFieldName)) {
            builder.append(pixel & 0xFF).append(",");
        }
        if (builder.length() > 0) {
            builder.deleteCharAt(builder.length() - 1);
        }
        String newValue = builder.toString();

        return r.newRecord(r.topic(), r.kafkaPartition(), Schema.STRING_SCHEMA, key, Schema.STRING_SCHEMA, newValue, r.timestamp());
    }
----

On the TensorFlow side we have to convert bytes obtained from Kafka messages into numbers. 
It can be easily done by creating a map function which can look like this:

[source,python]
----
def decode_kafka_record(record):
    img_int = tf.io.decode_csv(record.message, [[0.0] for i in range(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / 255.
    label_int = tf.strings.to_number(record.key, out_type=tf.dtypes.int32)
    return (img_norm, label_int)
----

Here we parse CSV lines, which can be provided as the raw bytes, and immediately scale the numbers to be in <0, 1> interval, which is convenient later on for training our model.
Loading of the data and creating data batches is very straightforward:

[source,python]
----
train_ds = tfio.IODataset.from_kafka(KAFKA_TRAIN_TOPIC, partition=0, offset=0, servers=KAFKA_SERVERS)
train_ds = train_ds.map(decode_kafka_record)
train_ds = train_ds.batch(BATCH_SIZE)
----

Here we use `IODataset.from_kafka()` for loading existing data from Kafka topic, use our map function to convert bytes into the numbers and scale the numbers and as a last step we create batches from the data set for more efficient processing.
Parameters of  `tfio.IODataset.from_kafka()` are self-explanatory and probably don't need any further comments.

As a result we have a data set formed by two-dimensional tensors.
The first dimension is a vector of floats representing the image, while the second dimension is formed by a single number (scalar) which describes what number is on the picture.
Once we have prepared our training data set, we are ready to define our neural network model.

=== Defining the model

To keep things simple, as the main goal of this post is not to show the best handwritten digit classifier, but to show how to create the data pipeline, let's use very simple model:

[source,python]
----
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10)
])
----

This model contains only two layers.
Although this model is really simple, it still does a pretty good job in recognition of handwritten digits.
Probably more interesting than the model itself is how easy it is to write a mode in TensorFlow (or actually https://keras.io/[Keras], but it's now part of TensorFlow).

Similarly easy is to define model optimizer and the loss function:

[source,python]
----
model.compile(
    optimizer=tf.keras.optimizers.Adam(0.001),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)
----

It's outside of the scope of this post to explain what these functions are and you can check almost any machine learning online course or textbook on this topic for a detailed explanation.

Once we have out model ready, we can train it on the train dataset prepared in previous section:

[source,python]
----
model.fit(train_ds,epochs=MAX_EPOCHS)
----

This step may take quite some time to finish.
However, once finished, our model is ready to recognize handwritten digits!

=== Streaming the data into the model

Let's see how good our model is in digit recognition.
But as our main goal here is to explore the means how to ingest data into TensorFlow, we will start model evaluation on empty (or more accurately even non-existing) Kafka topic and see if we will be able to evaluate the data on the fly as they will pop-up first in the database and then in the corresponding Kafka topic.
For this purpose we can use one of the above-mentioned streaming classes:

[source,python]
----
test_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=9000,
    configuration=[
        "session.timeout.ms=10000",
        "max.poll.interval.ms=10000",
        "auto.offset.reset=earliest"
    ],
)
----

Again, arguments are mostly self-explanatory.
Two things may need further explanation: `stream_timeout` and `configuration` parameters.
`stream_timeout` determines interval of inactivity (in milliseconds) after which the streaming would terminate.
`configuration` is https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md[librdkafka configuration].
It's basically a configuration of Kafka client and you should configure at least session timeout (`session.timeout.ms`) and its poll interval (`max.poll.interval.ms`).
The values of these parameters should be higher than the value of `stream_timeout`.

The dataset provided by this loader is slightly different - instead of providing a single record containing message and its key, we get key and message already split.
Therefore, we have to define slightly modified map function with two arguments:

[source,python]
----
def decode_kafka_stream_record(message, key):
    img_int = tf.io.decode_csv(message, [[0.0] for i in range(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / 255.
    label_int = tf.strings.to_number(key, out_type=tf.dtypes.int32)
    return (img_norm, label_int)
----

With this function, we can adjust the dataset and create batches as before:

[source,python]
----
test_ds = test_ds.map(decode_kafka_stream_record)
test_ds = test_ds.batch(BATCH_SIZE)
----

and evaluate the model:

[source,python]
----
model.evaluate(test_ds)
----

You can execute a cell with model evaluation in the Jupyter notebook.
As there is no such topic in Kafka and even no table with test data in the database, it will wait.
However, as the streaming timeout is set to 9 seconds, after launching the model evaluation you have to provide some data within this time frame.
You can use a prepared SQL file, which was created at the very beginning of this demo.
In `postgres` directory, there is `mnist_test.sql` file, which can be easily used for creating test data sample the database:

[source,bash]
----
$ export PGPASSWORD=postgres
$ psql -h localhost -U postgres -f postgres/mnist_test.sql
----

After a short while you should see in the Jupyter notebook output that some data arrived into the model and a few moments later final evaluation of the model.

To make the results more close to humans, let's define an image manually and serve it to the model.
We can also easily show the image in the Jypiter notebook.
The function for plotting the images and providing model predictions as a plot title can looks like this:

[source,python]
----
def plot_and_predict(pixels):
    test = tf.constant([pixels])
    tf.shape(test)
    test_norm = tf.cast(test, tf.float32) / 255.

    prediction = model.predict(test_norm)
    number = tf.nn.softmax(prediction).numpy().argmax()
    
    pixels_array = np.asarray(pixels)
    raw_img = np.split(pixels_array, 28)
    plt.imshow(raw_img)
    plt.title(number)
    plt.axis("off")
----

Probably the only cryptic line in this function is the one containing the `softmax()` function.
This function converts the resulting vector into a vector of probabilities.
Elements of this vector express the probability that the number on a given position is the one on the image.
Therefore, the position with the highest probability is the prediction of the model.
This is where `argmax()` comes from.

We can try it e.g. for this images, which contains hand written number 3:

[source]
----
pixels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,108,43,6,6,6,6,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,84,248,254,254,254,254,254,241,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,90,254,254,254,223,173,173,173,253,156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,79,157,228,245,251,188,63,17,0,0,54,252,132,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,254,254,254,244,131,0,0,0,0,13,220,254,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,254,225,160,47,0,0,0,0,59,211,254,206,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,21,14,0,0,0,2,17,146,245,250,194,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,140,140,171,254,254,254,203,55,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,254,254,254,179,211,254,254,202,171,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,167,233,193,69,16,3,9,16,107,231,248,195,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,73,229,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,99,252,254,146,0,0,0,0,0,0,0,0,79,142,0,0,0,0,0,0,0,0,0,26,28,116,147,247,254,239,150,22,0,0,0,0,0,0,0,0,175,230,174,155,66,66,132,174,174,174,174,250,255,254,192,189,99,36,0,0,0,0,0,0,0,0,0,0,106,226,254,254,254,254,254,254,254,254,217,151,80,43,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,7,114,114,114,46,5,5,5,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
plot_and_predict(pixels)
----

The result would be as follows:

++++
<div class="imageblock centered-image">
    <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_digit.png" class="responsive-image" alt="TensorFlow digit recognition">
</div>
++++

You can of course do the same by reading from Kafka stream.
We can re-use existing topics for this purpose.
As we already read all records from test stream, we need to change Kafka consumer group if we want to read it again using streaming `KafkaGroupIODataset`:

[source,python]
----
manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id="mnistcg2",
    servers=KAFKA_SERVERS,
    stream_timeout=9000,
    configuration=[
        "session.timeout.ms=10000",
        "max.poll.interval.ms=10000",
        "auto.offset.reset=earliest"
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)
----

If you want to create new stream and verify that our model can provide prediction as the new data arrives, you can easily do so:

[source,bash]
----
$ head -5 mnist_test.sql | sed s/test/manual/ > mnist_manual.sql
$ psql -h localhost -U postgres -f postgres/mnist_manual.sql
----

In such case you don't need to change Kafka consumer group, but you have to change Kafka topic:

[source,python]
----
manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=["tf.public.mnist_manual"],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=9000,
    configuration=[
        "session.timeout.ms=10000",
        "max.poll.interval.ms=10000",
        "auto.offset.reset=earliest"
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)
----

In either case the result should look like this:

++++
<div class="imageblock centered-image">
    <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_streaming.png" class="responsive-image" alt="TensorFlow digit recognition from streaming">
</div>
++++


== Conclusions

In this demo we have shown how to load existing data from the database, transform it on the fly, ingest the data into TensorFlow model via Kafka and use the data for model training.
Later on we ingested newly created data into this pre-trained model using CDC and data streaming and obtained meaningful results.
We can conclude that Debezium can provide valuable service not only for use cases like the one described in this post, but it can also play a key role in ingesting data to online machine learning pipelines.

While the whole pipeline is not very hard to implement, there are definitely areas which can be improved, make the user experience better and/or make the whole pipeline more smooth.
As our (Debezium developers) background is not primarily in machine learning and data science, we would appreciate any input from the community, how Debezium can be used in machine learning pipelines (or is already used, if there are any such cases) and where are the rooms for improvements.
We would also appreciate any new ideas how Debezium, or in general change data capture, can be useful in this area.
These ideas may reveal further Debezium potential to ingest data into machine learning pipelines and may contribute to better user experience in the whole process.
In case you have in input any this regard, don't hesitate to reach to use on the http://debezium.zulipchat.com/[Zulip chat], https://groups.google.com/forum/#!forum/debezium[mailing list] or you can transform your ideas directly into http://issues.redhat.com/projects/DBZ/issues[Jira feature requests].
