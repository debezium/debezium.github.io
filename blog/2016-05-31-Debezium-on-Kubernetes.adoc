= Debezium on Kubernetes
ceposta
:awestruct-tags: [ mysql, sql, kubernetes, docker, kafka ]
:awestruct-layout: blog-post

In the link:http://debezium.io/docs/tutorial/[Debezium Tutorial] (which walks you step by step how to use Debezium -- and you should check it out!), we assemble all of the pieces for using Debezium using Docker. Installing, starting, linking all of the individual pieces involves some deep attention to detail. You can use things like Docker Compose or your own scripts to help out with this, but in this blog I'd like to show how to get Debezium running on a container cluster manager from Red Hat and Google called link:http://kubernetes.io[Kubernetes].

If you've not heard of link:http://kubernetes.io[Kubernetes], it's a container (Docker/Rocket/Hyper.sh) cluster management tool. All of the popular cluster management and compute resource scheduling projects had their roots in Google one way or another as Google is no stranger to running containers at scale. They start/stop/cluster 2 Billion containers per week and they contributed a lot of the Linux kernel underpinnings that make containers possible. They have an internal cluster manager named Borg and even link:http://research.google.com/pubs/pub43438.html[wrote a paper on it]. Google got tired of everyone implementing their papers in Java so they decided to implement this one themselves :)

Kubernetes is written in Go-lang and is quickly becoming the de-facto API for scheduling, managing, and clustering containers at scale. This blog isn't intended to be a primer on Kubernetes, so we recommend heading over to the link:http://kubernetes.io/docs/getting-started-guides/[Getting Started] docs and learn about Kubernetes.

== Getting started

To get started, we need to have access to a Kubernetes cluster. If you follow the link:http://kubernetes.io/docs/getting-started-guides/[getting started] guides, you should be able to get going. FWIW, we recommend link:https://blog.openshift.com/one-vagrant-image-openshift-origin-v3/[OpenShift's all in one VM] or the link:http://developers.redhat.com/products/cdk/overview/[Red Hat Container Development Kit] which provides a hardened, production-ready distribution of Kubernetes. Once you've logged in, you should be able to run `kubectl get pod` to get a list of Kubernetes pods you may have running. You don't need anything running else inside Kubernetes to get started.

To get and build the Kubernetes manifest files (yaml descriptors), go clone the link:https://github.com/debezium/debezium-kubernetes[Debezium Kubernetes] repo and run the following command:

[source,bash,indent=0]
----
    $ mvn clean
    $ mvn install
----

This project uses the awesome link:http://fabric8.io/guide/mavenPlugin.html[Fabric8 maven plugin] to automatically generate the Kubernetes manifest files. Here's an example of what gets generated in `$PROJECT_ROOT/zk-standalone/target/classes/kubernetes.yml`

[listing,indent=0,options="nowrap"]
----
---
apiVersion: "v1"
items:
- apiVersion: "v1"
  kind: "Service"
  metadata:
    annotations: {}
    labels:
      project: "zookeeper"
      provider: "debezium"
      version: "0.1-SNAPSHOT"
      group: "io.debezium"
    name: "zookeeper"
  spec:
    deprecatedPublicIPs: []
    externalIPs: []
    ports:
    - port: 2181
      protocol: "TCP"
      targetPort: 2181
    selector:
      project: "zookeeper"
      provider: "debezium"
      group: "io.debezium"
- apiVersion: "v1"
  kind: "ReplicationController"
  metadata:
    annotations:
      fabric8.io/git-branch: "master"
      fabric8.io/git-commit: "004e222462749fbaf12c3ee33edca9b077ee9003"
    labels:
      project: "zookeeper"
      provider: "debezium"
      version: "0.1-SNAPSHOT"
      group: "io.debezium"
    name: "zk-standalone"
  spec:
    replicas: 1
    selector:
      project: "zookeeper"
      provider: "debezium"
      version: "0.1-SNAPSHOT"
      group: "io.debezium"
    template:
      metadata:
        annotations: {}
        labels:
          project: "zookeeper"
          provider: "debezium"
          version: "0.1-SNAPSHOT"
          group: "io.debezium"
      spec:
        containers:
        - args: []
          command: []
          env:
          - name: "KUBERNETES_NAMESPACE"
            valueFrom:
              fieldRef:
                fieldPath: "metadata.namespace"
          image: "docker.io/debezium/zookeeper:0.1"
          imagePullPolicy: "IfNotPresent"
          name: "zk-standalone"
          ports:
          - containerPort: 3888
            name: "election"
          - containerPort: 2888
            name: "peer"
          - containerPort: 2181
            name: "client"
          securityContext: {}
          volumeMounts: []
        imagePullSecrets: []
        nodeSelector: {}
        volumes: []
kind: "List"
----


== Starting Zookeeper and Kafka on Kubernetes

To start link:http://zookeeper.apache.org[Apache Zookeeper] or link:http://kafka.apache.org[Apache Kafka] inside Kubernetes you have two options. If you have the `kubectl` command line (or the `oc` tool from the OpenShift client distros) on your machine you can apply the newly generated Kubernetes manifest files like this:

[source,bash,indent=0]
----
    $ kubectl create -f <path_to_file>
----

Or you can use the `fabric8-maven-plugin`'s `fabric8:apply` goal to apply the manifest files. Note for either of these two options to work, you must be currently logged into a Kubernetes cluster (also note, `oc login <url>` makes this super easy... alternatively take a look at link:http://blog.christianposta.com/kubernetes/logging-into-a-kubernetes-cluster-with-kubectl/[Logging into a Kubernetes Cluster with kubectl]).

Let's deploy Zookeeper to our Kubernetes Cluster. We need to be in `$PROJECT_ROOT/zk-standalone` directory.  I'll show both the maven plugin commands as well as the `kubectl` commands:

[source,bash,indent=0]
----
    $ cd zk-standalone
    $ kubectl create -f target/classes/kubernetes.yml

    service "zookeeper" created
    replicationcontroller "zk-standalone" created
----

Or with fabric8 maven plugin:

[source,bash,indent=0]
----
    $ cd zk-standalone
    $ mvn fabric8:apply

    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
    [INFO] Scanning for projects...
    [INFO]
    [INFO] ------------------------------------------------------------------------
    [INFO] Building zk-standalone 0.1-SNAPSHOT
    [INFO] ------------------------------------------------------------------------
    [INFO]
    [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ zk-standalone ---
    [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
    [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/zk-standalone/target/classes/kubernetes.json
    [INFO] OpenShift platform detected
    [INFO] Using namespace: ticket
    [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
    [INFO] Creating a Service from kubernetes.json namespace ticket name zookeeper
    [INFO] Created Service: zk-standalone/target/fabric8/applyJson/ticket/service-zookeeper.json
    [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name zk-standalone
    [INFO] Created ReplicationController: zk-standalone/target/fabric8/applyJson/ticket/replicationcontroller-zk-standalone.json
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 2.661 s
    [INFO] Finished at: 2016-05-19T15:59:26-07:00
    [INFO] Final Memory: 26M/260M
    [INFO] ------------------------------------------------------------------------
----

Let's do the same thing for the Kafka deployment. Navigate to `$PROJECT_ROOT/zk-standalone/kafka`

[source,bash,indent=0]
----
    $ cd kafka
    $ kubectl create -f target/classes/kubernetes.yml

    service "kafka" created
    replicationcontroller "kafka" created
----

Or with fabric8 maven plugin:

[source,bash,indent=0]
----
    $ cd kafka
    $ mvn fabric8:apply

    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
    [INFO] Scanning for projects...
    [INFO]
    [INFO] ------------------------------------------------------------------------
    [INFO] Building kafka 0.1-SNAPSHOT
    [INFO] ------------------------------------------------------------------------
    [INFO]
    [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ kafka ---
    [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
    [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/kafka/target/classes/kubernetes.json
    [INFO] OpenShift platform detected
    [INFO] Using namespace: ticket
    [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
    [INFO] Creating a Service from kubernetes.json namespace ticket name kafka
    [INFO] Created Service: kafka/target/fabric8/applyJson/ticket/service-kafka.json
    [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name kafka
    [INFO] Created ReplicationController: kafka/target/fabric8/applyJson/ticket/replicationcontroller-kafka.json
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 2.563 s
    [INFO] Finished at: 2016-05-19T16:03:25-07:00
    [INFO] Final Memory: 26M/259M
    [INFO] ------------------------------------------------------------------------
----


Let's make sure we have things running:

[source,bash,indent=0]
----
    $ kubectl get pod

    NAME                  READY     STATUS    RESTARTS   AGE
    kafka-mqmxt           1/1       Running   0          46s
    zk-standalone-4mo02   1/1       Running   0          4m
----


One thing you'll notice that's missing is that we didn't manually specify "linking" between the containers when we start them. Kubernetes has a cluster service discovery feature called link:http://kubernetes.io/docs/user-guide/services/[Kubernetes Services] that load-balances against and lets us use internal DNS (or cluster IPs) to discover pods. For example, in our kafka deployment you'll see the following in the kubernetes.yml file:

[listing,indent=0,options="nowrap"]
----
        containers:
        - args: []
          command: []
          env:
          - name: "KAFKA_ADVERTISED_PORT"
            value: "9092"
          - name: "KAFKA_ADVERTISED_HOST_NAME"
            value: "kafka"
          - name: "KAFKA_ZOOKEEPER_CONNECT"
            value: "zookeeper:2181"
          - name: "KAFKA_PORT"
            value: "9092"
          - name: "KUBERNETES_NAMESPACE"
            valueFrom:
              fieldRef:
                fieldPath: "metadata.namespace"
          image: "docker.io/debezium/kafka:0.1"
          imagePullPolicy: "IfNotPresent"
          name: "kafka"
----

We end up passing in values for the `KAFKA_ZOOKEEPER_CONNECT` environment variable which Kafka can use to discover Zookeeper pods. The "DNS" name we use is just "zookeeper". If you were to log in to one of the pods and try to reach the "zookeeper" host name, it would resolve to one of the pods (if there are multiple) that runs zookeeper. Slick! This discovery mechanism is used for the rest of the components. Note, this cluster IP that the DNS resolves to *never* changes for the life of the Kubernetes Service regardless of how many (or zero) Pods for a given service. This means you can rely on this service discovery without all of the DNS caching issues you may otherwise run into.

Let's use the Kafka tools to create the `schema-changes` topic that we'll need for the Inventory database later on in the demo:

[source,bash,indent=0]
----

    $ KAFKA_POD_NAME=$(kubectl get pod | grep -i running | grep kafka | awk '{ print $1 }')

    $ kubectl exec $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic schema-changes.inventory
----



== Start up a MySQL Database on Kubernetes

Starting the MySQL database follows the same instructions as installing Zookeeper or Kafka. We will navigate to the `$PROJECT_ROOT/mysql56` directory. We're using the MySQL Docker image from here: link:https://github.com/openshift/mysql[MySQL 5.6] so that it runs on both vanilla Kubernetes and OpenShift v3.x


[source,bash,indent=0]
----
    $ cd mysql56
    $ kubectl create -f target/classes/kubernetes.yml

    service "mysql" created
    replicationcontroller "mysql56" created
----


Or mvn plugin:

[source,bash,indent=0]
----
    $ cd mysql56
    $ mvn fabric8:apply
----

Now when we do `kubectl get pod` we should see our MySQL database running:

[listing,indent=0,options="nowrap"]
----
NAME                  READY     STATUS    RESTARTS   AGE
kafka-mqmxt           1/1       Running   0          17m
mysql56-b4f36         1/1       Running   0          9m
zk-standalone-4mo02   1/1       Running   0          21m
----

Let's run a command to get client access to the database:

[source,bash,indent=0]
----
    $ MYSQL_POD_NAME=$(kubectl get pod | grep Running | grep ^mysql | awk '{ print $1 }')
    $ MYSQL_POD_IP=$(kubectl describe pod $MYSQL_POD_NAME | grep IP | awk '{ print $2 }')
----


[source,bash,indent=0]
----
    $ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin
    Warning: Using a password on the command line interface can be insecure.
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 1
    Server version: 5.6.26-log MySQL Community Server (GPL)

    Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql>
----


What we did here is login to the actual Kubernetes pod that's running the MySQL database. Using the `kubectl` command line, you can imagine getting access to a pod or docker container regardless of where it's running in the cluster and regardless which host. Let's exit out of the mysql shell (type `exit`) and run the following command to install some sample data:

[source,bash,indent=0]
----
kubectl exec  -it $MYSQL_POD_NAME -- bash -c "curl -s -L https://gist.github.com/christian-posta/e20ddb5c945845b4b9f6eba94a98af09/raw | /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin"

----

This will install a few sample tables into a database named `inventory`. If we log back into the MySQL pod we can show the databases and tables:

[source,bash,indent=0]
----
    $ kubectl exec -it $MYSQL_POD_NAME   -- /opt/rh/rh-mysql56/root/usr/bin/mysql -h$MYSQL_POD_IP -P3306 -uroot -padmin -e 'use inventory; show tables;'

    +---------------------+
    | Tables_in_inventory |
    +---------------------+
    | customers           |
    | orders              |
    | products            |
    | products_on_hand    |
    +---------------------+
    4 rows in set (0.00 sec)

----

== Start Kafka Connect and Debezium

Navigate to our `debezium-kubernetes` project and into directory `$PROJECT_ROOT/connect-mysql`. We will be starting a Kubernetes pod that runs Kafka connect with the Debezium MySQL connector on the classpath so that we can use Debezium to read the MySQL binlog and write those row events to Kafka. We start up Kafka Connect and Debezium on Kubernetes similar to the other components

[source,bash,indent=0]
----
    $ cd connect-mysql
    $ kubectl create -f target/classes/kubernetes.yml

    service "connect-mysql" created
    replicationcontroller "connect-mysql" created
----

Or with fabric8 maven plugin:

[source,bash,indent=0]
----
    $ cd connect-mysql
    $ mvn fabric8:apply
    Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=1512m; support was removed in 8.0
    [INFO] Scanning for projects...
    [INFO]
    [INFO] ------------------------------------------------------------------------
    [INFO] Building connect-mysql 0.1-SNAPSHOT
    [INFO] ------------------------------------------------------------------------
    [INFO]
    [INFO] --- fabric8-maven-plugin:2.2.115:apply (default-cli) @ connect-mysql ---
    [INFO] Using kubernetes at: https://172.28.128.4:8443/ in namespace ticket
    [INFO] Kubernetes JSON: /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/connect-mysql/target/classes/kubernetes.json
    [INFO] OpenShift platform detected
    [INFO] Using namespace: ticket
    [INFO] Looking at repo with directory /Users/ceposta/dev/idea-workspace/dbz/debezium-kubernetes/.git
    [INFO] Creating a Service from kubernetes.json namespace ticket name connect-mysql
    [INFO] Created Service: connect-mysql/target/fabric8/applyJson/ticket/service-connect-mysql.json
    [INFO] Creating a ReplicationController from kubernetes.json namespace ticket name connect-mysql
    [INFO] Created ReplicationController: connect-mysql/target/fabric8/applyJson/ticket/replicationcontroller-connect-mysql.json
    [INFO] ------------------------------------------------------------------------
    [INFO] BUILD SUCCESS
    [INFO] ------------------------------------------------------------------------
    [INFO] Total time: 2.255 s
    [INFO] Finished at: 2016-05-25T09:21:04-07:00
    [INFO] Final Memory: 27M/313M
    [INFO] ------------------------------------------------------------------------
----

Now, just like in the Docker tutorial for Debezium, we want to send a JSON object to the Kafka Connect API to start up our Debezium connector. We need to expose the API for the Kafka Connect cluster. You can do this however you want on Kubernetes (link:http://kubernetes.io/docs/user-guide/ingress/[Ingress definitions], link:http://kubernetes.io/docs/user-guide/services/[NodePort services], etc) or on OpenShift you can use link:https://docs.openshift.com/enterprise/3.2/architecture/core_concepts/routes.html[OpenShift Routes]. For this simple example, we'll use simple Pod port-forwarding to forward the `connect-mysql` pod's `8083` port to our local machine (again, regardless of where the Pod is actually running the cluster.. this is such an incredible feature of Kubernetes for doing development like this!)

Let's determine the pod name and then do port forwarding to our local machine:


[source,bash,indent=0]
----
    $ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
    $ kubectl port-forward $CONNECT_POD_NAME 8083:8083

    I0525 09:30:08.390491    6651 portforward.go:213] Forwarding from 127.0.0.1:8083 -> 8083
    I0525 09:30:08.390631    6651 portforward.go:213] Forwarding from [::1]:8083 -> 8083
----

We are forwarding the Pod's port `8083` to our local machine's `8083`. Now if we hit `http://localhost:8083` it will be directed to the pod which runs our Kafka Connect and Debezium services.

In another window, it may be useful to see the logs from the Pod to see whether or not there are any exceptions:


[source,bash,indent=0]
----
    $ CONNECT_POD_NAME=$(kubectl get pod | grep -i running | grep ^connect | awk '{ print $1 }')
    $ kubectl logs -f $CONNECT_POD_NAME

----

Now, let's use an HTTP client to post the Debezium Connector/Task to the endpoint we've just exposed locally:



[source,bash,indent=0]
----

curl -i -X POST -H "Accept:application/json" -H "Content-Type:application/json" http://localhost:8083/connectors/ -d '{ "name": "inventory-connector", "config": { "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "replicator", "database.password": "replpass", "database.server.id": "184054", "database.server.name": "mysql-server-1", "database.binlog": "mysql-bin.000001", "database.whitelist": "inventory", "database.history.kafka.bootstrap.servers": "kafka:9092", "database.history.kafka.topic": "schema-changes.inventory" } }'

----

If we're watching the log output for the `connect-mysql` pod, we'll see it eventually end up looking something like this:


[listing,indent=0,options="nowrap"]
----
    2016-05-27 18:50:14,580 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 2 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
    2016-05-27 18:50:14,690 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 3 : {mysql-server-1.inventory.products=LEADER_NOT_AVAILABLE}
    2016-05-27 18:50:14,911 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 7 : {mysql-server-1.inventory.products_on_hand=LEADER_NOT_AVAILABLE}
    2016-05-27 18:50:15,136 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 10 : {mysql-server-1.inventory.customers=LEADER_NOT_AVAILABLE}
    2016-05-27 18:50:15,362 - WARN  [kafka-producer-network-thread | producer-1:NetworkClient$DefaultMetadataUpdater@582] - Error while fetching metadata with correlation id 13 : {mysql-server-1.inventory.orders=LEADER_NOT_AVAILABLE}
----


If we now do a listing of our topics inside Kafka, we should see for each table in the mysql `inventory` database, we have a Kafka Topic:


[source,bash,indent=0]
----
    $ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-topics.sh --list --zookeeper zookeeper:2181
    __consumer_offsets
    my-connect-configs
    my-connect-offsets
    mysql-server-1.inventory.customers
    mysql-server-1.inventory.orders
    mysql-server-1.inventory.products
    mysql-server-1.inventory.products_on_hand
    schema-changes.inventory
----

We can take a look at what's in one of these topics:

[source,bash,indent=0]
----
    $ kubectl exec  $KAFKA_POD_NAME --  /kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --new-consumer --topic mysql-server-1.inventory.customers --from-beginning --property print.key=true
    {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"inventory.customers/pk"},"payload":{"id":1001}}   {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":false,"name":"inventory.customers"},"payload":{"id":1001,"first_name":"Sally","last_name":"Thomas","email":"sally.thomas@acme.com"}}
    {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"inventory.customers/pk"},"payload":{"id":1002}}   {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":false,"name":"inventory.customers"},"payload":{"id":1002,"first_name":"George","last_name":"Bailey","email":"gbailey@foobar.com"}}
    {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"inventory.customers/pk"},"payload":{"id":1003}}   {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":false,"name":"inventory.customers"},"payload":{"id":1003,"first_name":"Edward","last_name":"Walker","email":"ed@walker.com"}}
    {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"}],"optional":false,"name":"inventory.customers/pk"},"payload":{"id":1004}}   {"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id"},{"type":"string","optional":false,"field":"first_name"},{"type":"string","optional":false,"field":"last_name"},{"type":"string","optional":false,"field":"email"}],"optional":false,"name":"inventory.customers"},"payload":{"id":1004,"first_name":"Anne","last_name":"Kretchmar","email":"annek@noanswer.org"}}
----


Now we have the data for the MySQL database inside Kafka! Now we can use any tool/connector/integration to consume the events from Kafka and put them into a different database, into Hadoop, elasticsearch, data grid, etc.

If you want to delete the connector:

[source,bash,indent=0]
----
curl -i -X DELETE -H "Accept:application/json" http://localhost:8083/connectors/inventory-connector
----



