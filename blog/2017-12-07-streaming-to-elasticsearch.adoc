= Streaming Data Changes from Your Database to Elasticsearch
jpechane
:awestruct-tags: [ mysql, postgres, elasticsearch, smt, example ]
:awestruct-layout: blog-post

We have seen how to set up a change data stream to a downstream database link:/blog/2017/09/25/streaming-to-another-database[few weeks ago].

In this blog post we will follow the same approach to stream the data to an https://www.elastic.co/[Elasticsearch] server for potential further search processing.
But to make the matter a little bit more interesting, we will stream the data to both, a PostgreSQL database and Elasticsearch, so we will optimize access to the data via the SQL query language as well as via full-text search.

== Topology
Here's a diagram that shows how the data is flowing through our distributed system.
First, the Debezium MySQL connector is continuously capturing the changes from the MySQL database, and sending the changes for each table to separate Kafka topics.
Then, the Confluent JDBC sink connector is continuously reading those topics and writing the events into the PostgreSQL database.
And, at the same time, the Confluent Elasticsearch connector is continuously reading those same topics and writing the events into the Elasticsearch system.

.A General topology
[#img-general]
[caption="Figure 1: "]
image::dbz-to-multiple.svg[Scenario topology]

&nbsp; +

We are going to deploy these components into several different processes.
In this example, we'll deploy all three connectors to a single Kafka Connect instance that will write to and read from Kafka on behalf of all of the connectors.
(In production you might need to keep the connectors separated to achieve better performance.)a

&nbsp; +

.A Simplified topology
[#img-general]
[caption="Figure 2: "]
image::dbz-to-multiple-simplified.svg[Scenario topology]

== Configuration

We will use this https://github.com/debezium/debezium-examples/tree/master/unwrap-smt[Docker Compose file] for a fast deployment of the demo.
The deployment consists of following Docker images:

* https://hub.docker.com/r/debezium/zookeeper/[Apache ZooKeeper]
* https://hub.docker.com/r/debezium/kafka/[Apache Kafka]
* An https://github.com/debezium/debezium-examples/tree/master/unwrap-smt/debezium-jdbc[enriched] Kafka Connect / Debezium https://hub.docker.com/r/debezium/connect/[image] with changes
** PostgreSQL JDBC driver placed into `/kafka/libs` directory
** https://docs.confluent.io/current/connect/connect-jdbc/docs/index.html[Kafka Connect JDBC Connector] (developed by https://www.confluent.io/[Confluent]) placed into `/kafka/connect/kafka-connect-jdbc` directory
* Pre-populated MySQL used in our link:docs/tutorial[tutorial]
* Empty PostgreSQL
* Empty ElasticSearch

The message format is not same for the Debezium source connector and JDBC and Elasticsearch connectors as they are developed separately and focuses on slightly different objectives.
Debezium uses a more complex event structure so that it captures all of the information available.
Both sink connectors expect a simple message that represents a record to be written.

`UnwrapFromEnvelope` single message transformation (SMT) collapses the complex change event structure into the same row-based format expected by the two sink connectors and effectively acts as a http://www.enterpriseintegrationpatterns.com/patterns/messaging/MessageTranslator.html[message translator] between the two aforementioned formats.

== Example

Let's move directly to our example as that's where the changes are visible.

First of all we need to deploy all components:

[source,bash,indent=0]
----
export DEBEZIUM_VERSION=0.6
docker-compose up
----

When all components are started we are going to register the Elasticsearch Sink connector writing into the Elasticsearch instance.
We want to use the same key (primary id) in the source and both PostgreSQL and Elasticsearch:

[source,bash,indent=0]
----
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @es-sink.json
----

We're using this registration request:

[source,json,indent=0]
----
{
    {
        "name": "elastic-sink",
        "config": {
            "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
            "tasks.max": "1",
            "topics": "customers",
            "connection.url": "http://elastic:9200",
            "transforms": "unwrap,key",
            "transforms.unwrap.type": "io.debezium.transforms.UnwrapFromEnvelope",           (1)
            "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",   (2)
            "transforms.key.field": "id",                                                    (2)
            "key.ignore": "false",                                                           (3)
            "type.name": "customer"                                                          (4)
        }
    }
}
----

The request configures these options:

1. extracting only the new row's state from Debezium's change data message
2. extracting the `id` field from the key `struct`, then the same key is used for the source and both destinations.
This is to address the fact that the Elasticsearch connector only supports numeric types and `string` as keys. If we do not extract the `id` the messages will be filtered out by the connector because of unknown key type.
3. use key from the event instead of generating a synthetic one
4. type under which the events will be registered in Elasticsearch

Next we are going to register the JDBC Sink connector writing into PostgreSQL database:

[source,bash,indent=0]
----
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @jdbc-sink.json
----

Then the source connector must be set up:

[source,bash,indent=0]
----
curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @source.json
----

Let's check if the databases and the search server are synchronized.
All the rows of the `customers` table should be found in the source database (MySQL) as well as the target database (Postgres) and Elasticsearch:

[source,bash,indent=0]
----
docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory -e "select * from customers"'
+------+------------+-----------+-----------------------+
| id   | first_name | last_name | email                 |
+------+------------+-----------+-----------------------+
| 1001 | Sally      | Thomas    | sally.thomas@acme.com |
| 1002 | George     | Bailey    | gbailey@foobar.com    |
| 1003 | Edward     | Walker    | ed@walker.com         |
| 1004 | Anne       | Kretchmar | annek@noanswer.org    |
+------+------------+-----------+-----------------------+

docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c "select * from customers"'
 last_name |  id  | first_name |         email
-----------+------+------------+-----------------------
 Thomas    | 1001 | Sally      | sally.thomas@acme.com
 Bailey    | 1002 | George     | gbailey@foobar.com
 Walker    | 1003 | Edward     | ed@walker.com
 Kretchmar | 1004 | Anne       | annek@noanswer.org

curl 'http://localhost:9200/customers/_search?pretty'
{
  "took" : 42,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 4,
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "customers",
        "_type" : "customer",
        "_id" : "1001",
        "_score" : 1.0,
        "_source" : {
          "id" : 1001,
          "first_name" : "Sally",
          "last_name" : "Thomas",
          "email" : "sally.thomas@acme.com"
        }
      },
      {
        "_index" : "customers",
        "_type" : "customer",
        "_id" : "1004",
        "_score" : 1.0,
        "_source" : {
          "id" : 1004,
          "first_name" : "Anne",
          "last_name" : "Kretchmar",
          "email" : "annek@noanswer.org"
        }
      },
      {
        "_index" : "customers",
        "_type" : "customer",
        "_id" : "1002",
        "_score" : 1.0,
        "_source" : {
          "id" : 1002,
          "first_name" : "George",
          "last_name" : "Bailey",
          "email" : "gbailey@foobar.com"
        }
      },
      {
        "_index" : "customers",
        "_type" : "customer",
        "_id" : "1003",
        "_score" : 1.0,
        "_source" : {
          "id" : 1003,
          "first_name" : "Edward",
          "last_name" : "Walker",
          "email" : "ed@walker.com"
        }
      }
    ]
  }
}
----

With the connectors still running, we can add a new row to the MySQL database and then check that it was replicated into both the PostgreSQL database and Elasticsearch:

[source,bash,indent=0]
----
docker-compose exec mysql bash -c 'mysql -u $MYSQL_USER  -p$MYSQL_PASSWORD inventory'
mysql> insert into customers values(default, 'John', 'Doe', 'john.doe@example.com');
Query OK, 1 row affected (0.02 sec)

docker-compose exec -postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c "select * from customers"'
 last_name |  id  | first_name |         email
-----------+------+------------+-----------------------
...
Doe        | 1005 | John       | john.doe@example.com
(5 rows)

curl 'http://localhost:9200/customers/_search?pretty'
...
      {
        "_index" : "customers",
        "_type" : "customer",
        "_id" : "1005",
        "_score" : 1.0,
        "_source" : {
          "id" : 1005,
          "first_name" : "John",
          "last_name" : "Doe",
          "email" : "john.doe@example.com"
        }
      }
...
----

== Summary

We set up a complex streaming data pipeline to synchronize a MySQL database with another database and also with an Elasticsearch instance.
We managed to keep the same identifier across all systems which allows us to correlate records across the system as the whole.
If you'd like to try out this set-up yourself, just clone the project from our https://github.com/debezium/debezium-examples/tree/master/unwrap-smt[examples repo].
In case you need help, have feature requests or would like to share your experiences with this pipeline, please let us know in the comments below.

== About Debezium

Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of http://kafka.apache.org/[Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is link:/license[open source] under the http://www.apache.org/licenses/LICENSE-2.0.html[Apache License, Version 2.0].

== Get involved

We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter https://twitter.com/debezium[@debezium], https://gitter.im/debezium/user[chat with us on Gitter],
or join our https://groups.google.com/forum/#!forum/debezium[mailing list] to talk with the community.
All of the code is open source https://github.com/debezium/[on GitHub],
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or https://issues.jboss.org/projects/DBZ/issues/[log an issue].
