<!DOCTYPE html> <html lang="en"> <head> <title>Streaming Cassandra at WePay - Part 1 · Debezium</title> <meta charset="utf-8"> <meta content="width=device-width, initial-scale=1.0" name="viewport"> <meta content="" name="description"> <meta content="" name="author"> <link href="https://static.jboss.org/theme/css/bootstrap-community/3.2.0.2/bootstrap-community.min.css" media="screen" rel="stylesheet"> <!--[if lt IE 9]><script src="https://static.jboss.org/theme/js/libs/html5/pre3.6/html5.min.js"></script><![endif]--> <link href="https://static.jboss.org/example/images/favicon.ico" rel="shortcut icon"> <link href="https://static.jboss.org/example/images/apple-touch-icon-144x144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144"> <link href="https://static.jboss.org/example/images/apple-touch-icon-114x114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114"> <link href="https://static.jboss.org/example/images/apple-touch-icon-72x72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72"> <link href="https://static.jboss.org/example/images/apple-touch-icon-precomposed.png" rel="apple-touch-icon-precomposed"> <link href="/stylesheets/debezium.css" rel="stylesheet" type="text/css"> <link href="https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1" rel="canonical"> <style>
      @media (min-width: 980px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-banner-1180px.png); height: 110px;  }
      }
      @media (max-width: 979px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-logo.png); background-repeat:no-repeat; background-position: center bottom; height: 60px; }
      }
      @media (max-width: 650px) {
        .banner { width: 100%; margin: 0px auto; }
      }
      @media (max-width: 450px) {
        .banner { height: 90px; }
      }
    </style> <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"> <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script> <script>
      hljs.initHighlightingOnLoad();
    </script> <script src="https://static.jboss.org/theme/js/libs/jquery/jquery-1.9.1.min.js"></script> <style>
      /* adjusting the vertical spacing for when a stickynav is engaged */
      .breadcrumb-fixed > .active {
        color: #8c8f91;
      }
      .breadcrumb-fixed {
        margin: 70px 0 10px;
        padding: 8px 15px;
        margin-bottom: 20px;
        list-style: none;
        background-color: #f5f5f5;
        border-radius: 4px;
      }
      
      .breadcrumb-fixed > li {
        display: inline-block;
      }
    </style> </head> <body> <div id="rhbar"> <a class="jbdevlogo" href="https://www.jboss.org/projects/about"></a> <a class="rhlogo" href="https://www.redhat.com/"></a> </div> <div id=""> <ul class="visuallyhidden" id="top"> <li> <a accesskey="n" href="#nav" title="Skip to navigation">Skip to navigation</a> </li> <li> <a accesskey="c" href="#page" title="Skip to content">Skip to content</a> </li> </ul> <div class="container" id="content"> <div class="navbar navbar-inverse navbar-fix"> <div class="container-fluid"> <div class="navbar-header"> <button class="navbar-toggle collapsed" data-target="#navbar-1" data-toggle="collapse"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/"> Debezium </a> </div> <div class="collapse navbar-collapse" id="navbar-1"> <ul class="nav navbar-nav pull-right"> <li class=""><a href="/docs/faq/">FAQ</a></li> <li class=""><a href="/docs/">DOCS</a></li> <li class=""><a href="/community/">COMMUNITY</a></li> <li class="active"><a href="/blog/">BLOG</a></li> </ul> </div> </div> </div> <div id="equalHeightsLayout"> <div class="row row-no-expand"> <div class="hidden-xs col-sm-3 no-right-padding" id="leftdocnav"> <div class="panel-docnav"> <div class="panel-heading"> <h3 class="panel-title"> Latest posts </h3> </div> <div class="panel-body"> <ul class="list-group"> <li class="list-group-item"> <a href="/blog/2019/08/20/debezium-0-10-0-beta4-released/" rel="tooltip" title="Click to go to post">Debezium 0.10.0.Beta4 Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/07/25/debezium-0-10-0-beta3-released/" rel="tooltip" title="Click to go to post">Debezium 0.10.0.Beta3 Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/" rel="tooltip" title="Click to go to post">Streaming Cassandra at WePay - Part 2</a> </li> <li class="list-group-item"> <a href="/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/" rel="tooltip" title="Click to go to post">Streaming Cassandra at WePay - Part 1</a> </li> <li class="list-group-item"> <a href="/blog/2019/07/08/tutorial-sentry-debezium-container-images/" rel="tooltip" title="Click to go to post">Tutorial for Adding Sentry into Debezium Container Images</a> </li> <li class="list-group-item"> <a href="/blog/2019/06/28/debezium-0-10-0-beta2-released/" rel="tooltip" title="Click to go to post">Debezium 0.10.0.Beta2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/06/19/debezium-wears-fedora/" rel="tooltip" title="Click to go to post">Debezium Wears Fedora</a> </li> <li class="list-group-item"> <a href="/blog/2019/06/12/debezium-0-10-0-beta1-released/" rel="tooltip" title="Click to go to post">Debezium 0.10.0.Beta1 Released</a> </li> </ul> </div> </div> </div> <div class="col-xs-12 col-sm-9 post-text-padding" id="maincol"> <div class="text-right"> <h3> Subscribe <a class="rss" href="/blog.atom"> <i class="icon-rss"></i> </a> </h3> </div> <div class="post"> <h1 class="title"> <a href="/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/">Streaming Cassandra at WePay - Part 1</a> </h1> <div class="byline"> <p> <em> July 12, 2019 by Joy Gao </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/cassandra/">cassandra</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p><strong><em>This post originally appeared on the <a href="https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1">WePay Engineering blog</a>.</em></strong></p> </div> <div class="paragraph"> <p>Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. <a href="https://vitess.io">Vitess</a>) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.</p> </div> </div> </div> <div class="sect1"> <h2 id="batch_etl_options"><a class="anchor" href="#batch_etl_options"></a>Batch ETL Options</h2> <div class="sectionbody"> <div class="paragraph"> <p>After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose data in Cassandra to <a href="https://cloud.google.com/bigquery/">BigQuery</a>, our data warehouse, for analytics and reporting. We quickly built an Airflow <a href="https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/cassandra_hook.py">hook</a> and <a href="https://github.com/apache/airflow/blob/master/airflow/contrib/operators/cassandra_to_gcs.py">operator</a> to execute full loads. This obviously doesn’t scale, as it rewrites the entire database on each load. To scale the pipeline, we evaluated two incremental load approaches, but both have their shortcomings:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>Range query. This is a common ETL approach where data is extracted via a range query at regular intervals, such as hourly or daily. Anyone familiar with <a href="https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key">Cassandra data modelling</a> would quickly realize how unrealistic this approach is. Cassandra tables need to be modeled to optimize query patterns used in production. Adding this query pattern for analytics in most cases means cloning the table with different clustering keys. RDBMS folks might suggest secondary index to support this query pattern, but <a href="https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes">secondary index in Cassandra are local</a>, therefore this approach would pose performance and scaling issues of its own.</p> </li> <li> <p>Process unmerged SSTables. SSTables are Cassandra’s immutable storage files. Cassandra offers a <a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/tools/ToolsSSTabledump.html">sstabledump</a> CLI command that converts SSTable content into human-readable JSON. However, Cassandra is built on top of the concept of <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">Log-Structured Merge (LSM) Tree</a>, meaning SSTables merge periodically into new compacted files. Depending on the compaction strategy, detecting unmerged SSTable files out-of-band may be challenging (we later learned about the <a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupIncremental.html">incremental backup</a> feature in Cassandra which only backs up uncompacted SSTables; so this approach would have worked as well.)</p> </li> </ol> </div> <div class="paragraph"> <p>Given these challenges, and having built and operated a <a href="https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka">streaming data pipeline for MySQL</a>, we began to explore streaming options for Cassandra.</p> </div> </div> </div> <div class="sect1"> <h2 id="streaming_options"><a class="anchor" href="#streaming_options"></a>Streaming Options</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="double_writing"><a class="anchor" href="#double_writing"></a>Double-Writing</h3> <div class="imageblock centered-image"> <img src="/images/cassandra/double-write.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Image showing writer send two distinct writes"> </div> <div class="paragraph"> <p>The idea is to publish to Kafka every time a write is performed on Cassandra. This double-writing could be performed via the built-in trigger or a custom wrapper around the client. There are performance problems with this approach. First, due to the fact that we now need to write to two systems instead of one, write latency is increased. More importantly, when a write to one system fails due to a timeout, whether the write is successful or not is indeterministic. To guarantee data consistency on both systems, we would have to implement <a href="https://en.wikipedia.org/wiki/Distributed_transaction">distributed transactions</a>, but multiple roundtrips for consensus will increase latency and reduce throughput further. This defeats the purpose of a high write-throughput database.</p> </div> </div> <div class="sect2"> <h3 id="kafka_as_event_source"><a class="anchor" href="#kafka_as_event_source"></a>Kafka as Event Source</h3> <div class="imageblock centered-image"> <img src="/images/cassandra/event-source.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Image showing writes sent to Kafka and then downstream DB"> </div> <div class="paragraph"> <p>The idea is to write to Kafka rather than directly writing to Cassandra; and then apply the writes to Cassandra by consuming events from Kafka. Event sourcing is a pretty popular approach these days. However, if you already have existing services directly writing to Cassandra, it would require a change in application code and a nontrivial migration. This approach also violates <a href="https://docs.oracle.com/cd/E17076_05/html/gsg_db_rep/C/rywc.html">read-your-writes consistency</a>: the requirement that if a process performs a write, then the same process performing a subsequent read must observe the write’s effects. Since writes are routed through Kafka, there will be a lag between when the write is issued and when it is applied; during this time, reads to Cassandra will result in stale data. This may cause unforeseeable production issues.</p> </div> </div> <div class="sect2"> <h3 id="parsing_commit_logs"><a class="anchor" href="#parsing_commit_logs"></a>Parsing Commit Logs</h3> <div class="imageblock centered-image"> <img src="/images/cassandra/commit-log.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Image showing commit logs sent to Kafka"> </div> <div class="paragraph"> <p>Cassandra introduced a <a href="http://cassandra.apache.org/doc/3.11.3/operating/cdc.html">change data capture (CDC) feature</a> in 3.0 to expose its commit logs. Commit logs are write-ahead logs in Cassandra designed to provide durability in case of machine crashes. They are typically discarded upon flush. With CDC enabled, they are instead transferred to a local CDC directory upon flush, which is then readable by other processes on the Cassandra node. This allows us to use the same CDC mechanism as in our MySQL streaming pipeline. It decouples production operations from analytics, and thus does not require additional work from application engineers.</p> </div> <div class="paragraph"> <p>Ultimately, after considering throughput, consistency, and separation of concerns, the final option – parsing commit logs – became the top contender.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="commit_log_deep_dive"><a class="anchor" href="#commit_log_deep_dive"></a>Commit Log Deep Dive</h2> <div class="sectionbody"> <div class="paragraph"> <p>Aside from exposing commit logs, Cassandra also provides <code>CommitLogReader</code> and <code>CommitLogReadHandler</code> classes to help with the deserialization of logs. It seems like the hard work has been done, and what’s left is applying transformations – converting deserialized representations into Avro records and publish them to Kafka. However, as we dug further into the implementation of the CDC feature and of Cassandra itself, we realized that there are many new challenges.</p> </div> <div class="sect2"> <h3 id="delayed_processing"><a class="anchor" href="#delayed_processing"></a>Delayed Processing</h3> <div class="paragraph"> <p>Commit logs only arrive in the CDC directory when it is full, in which case it would be flushed/discarded. This implies there is a delay between when the event is logged and when the event is captured. If little to no writes are executed, then the delay in event capturing could be arbitrarily long.</p> </div> </div> <div class="sect2"> <h3 id="space_management"><a class="anchor" href="#space_management"></a>Space Management</h3> <div class="paragraph"> <p>In MySQL you can set binlog retention such that the logs will be automatically deleted after the configured retention period. However in Cassandra there is no such option. Once the commit logs are transferred to CDC directory, consumption must be in place to clean up commit logs after processing. If the available disk space for CDC directory exceeds a given threshold, further writes to the database will be rejected.</p> </div> </div> <div class="sect2"> <h3 id="duplicated_events"><a class="anchor" href="#duplicated_events"></a>Duplicated Events</h3> <div class="paragraph"> <p>Commit logs on an individual Cassandra node do not reflect all writes to the cluster; they only reflect writes to the node. This makes it necessary to process commit logs on all nodes. But with a replication factor of N, N copies of each event are sent downstream.</p> </div> </div> <div class="sect2"> <h3 id="out_of_order_events"><a class="anchor" href="#out_of_order_events"></a>Out-of-Order Events</h3> <div class="paragraph"> <p>Writes to an individual Cassandra node are logged serially as they arrive. However, these events may arrive out-of-order from when they are issued. Downstream consumers of these events must understand the event time and implement last write wins logic similar to <a href="https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/dml/dmlAboutReads.html">Cassandra’s read path</a> to get the correct result.</p> </div> </div> <div class="sect2"> <h3 id="out_of_band_schema_change"><a class="anchor" href="#out_of_band_schema_change"></a>Out-of-Band Schema Change</h3> <div class="paragraph"> <p>Schema changes of tables are communicated via a <a href="https://en.wikipedia.org/wiki/Gossip_protocol">gossip protocol</a> and are not recorded in commit logs. Therefore changes in schema could only be detected on a best-effort basis.</p> </div> </div> <div class="sect2"> <h3 id="incomplete_row_data"><a class="anchor" href="#incomplete_row_data"></a>Incomplete Row Data</h3> <div class="paragraph"> <p>Cassandra does not perform read before write, as a result change events do not capture the state of every column, they only capture the state of modified columns. This makes the change event less useful than if the full row is available.</p> </div> <div class="paragraph"> <p>Once we acquired a deep understanding of Cassandra commit logs, we re-assessed our requirements against the given constraints in order to design a <a href="https://riccomini.name/minimum-viable-infrastructure">minimum viable infrastructure</a>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="minimum_viable_infrastructure"><a class="anchor" href="#minimum_viable_infrastructure"></a>Minimum Viable Infrastructure</h2> <div class="sectionbody"> <div class="paragraph"> <p>Borrowing from the <a href="https://en.wikipedia.org/wiki/Minimum_viable_product">minimum viable product</a> philosophy, we want to design a data pipeline with a minimum set of features and requirements to satisfy our immediate customers. For Cassandra CDC, this means:</p> </div> <div class="ulist"> <ul> <li> <p>Production database’s health and performance should not be negatively impacted by introducing CDC; slowed operations and system downtimes are much costlier than a delay in the analytics pipeline</p> </li> <li> <p>Querying Cassandra tables in our data warehouse should match the results of querying the production database (barring delays); having duplicate and/or incomplete rows amplifies post-processing workload for every end user With these criteria in front of us, we began to brainstorm for solutions, and ultimately came up with three approaches:</p> </li> </ul> </div> <div class="sect2"> <h3 id="stateless_stream_processing"><a class="anchor" href="#stateless_stream_processing"></a>Stateless Stream Processing</h3> <div class="paragraph"> <p>This solution is inspired by Datastax’s <a href="https://www.datastax.com/dev/blog/advanced-replication-in-dse-5-1">advanced replication blog post</a>. The idea is to deploy an agent on each Cassandra node to process local commit logs. Each agent is considered as “primary” for a subset of writes based on partition keys, such that every event has exactly one primary agent. Then during CDC, in order to avoid duplicate events, each agent only sends an event to Kafka if it is the primary agent for the event. To handle eventual consistency, each agent would sort events into per-table time-sliced windows as they arrive (but doesn’t publish them right away); when a window expires, events in that window are hashed, and the hash is compared against other nodes. If they don’t match, data is fetched from the inconsistent node so the correct value could be resolved by last write wins. Finally the corrected events in that window will be sent to Kafka. Any out-of-order event beyond the time-sliced windows would have to be logged into an out-of-sequence file and handled separately. Since deduplication and ordering are done in-memory, concerns with agent failover causing data loss, OOM issues impacting production database, and the overall complexity of this implementation stopped us from exploring it further.</p> </div> </div> <div class="sect2"> <h3 id="stateful_stream_processing"><a class="anchor" href="#stateful_stream_processing"></a>Stateful Stream Processing</h3> <div class="paragraph"> <p>This solution is the most feature rich. The idea is that the agent on each Cassandra node will process commit logs and publish events to Kafka without deduplication and ordering. Then a stream processing engine will consume these raw events and do the heavy lifting (such as filtering out duplicate events with a cache, managing event orders with event-time windowing, and capturing state of unmodified columns by performing read before write on a state store), and then publish these derived events to a separate Kafka topic. Finally, <a href="https://github.com/wepay/kafka-connect-bigquery">KCBQ</a> will be used to consume events from this topic and upload them to BigQuery. This approach is appealing because it solves the problem generically – anyone can subscribe to the latter Kafka topic without needing to handle deduplication and ordering on their own. However, this approach introduces a nontrivial amount of operational overhead; we would have to maintain a stream processing engine, a database, and a cache.</p> </div> </div> <div class="sect2"> <h3 id="processing_on_read"><a class="anchor" href="#processing_on_read"></a>Processing-On-Read</h3> <div class="paragraph"> <p>Similar to the previous approach, the idea is to process commit logs on each Cassandra node and send events to Kafka without deduplication and ordering. Unlike the previous approach, the stream processing portion is completely eliminated. Instead the raw events will be directly uploaded to BigQuery via KCBQ. <a href="https://cloud.google.com/bigquery/docs/views-intro">Views</a> are created on top of the raw tables to handle deduplication, ordering, and merging of columns to form complete rows. Because BigQuery views are virtual tables, the processing is done lazily each time the view is queried. To prevent the view query from getting too expensive, the views would be materialized periodically. This approach removes both operational complexity and code complexity by leveraging BigQuery’s <a href="https://cloud.google.com/blog/products/gcp/bigquery-under-the-hood">massively parallel query engine</a>. However, the drawback is that non-KCBQ downstream consumers must do all the work on their own.</p> </div> <div class="paragraph"> <p>Given that our main purpose of streaming Cassandra is data warehousing, we ultimately decided to implement <em>processing-on-read</em>. It provides the essential features for our existing use case, and offers the flexibility to expand into the other two more generic solutions mentioned above in the future.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="open_source"><a class="anchor" href="#open_source"></a>Open Source</h2> <div class="sectionbody"> <div class="paragraph"> <p>During this process of building a real-time data pipeline for Cassandra, we have received a substantial amount of interest on this project. As a result, we have decided to open-source the Cassandra CDC agent under the <a href="https://debezium.io">Debezium</a> umbrella as an <a href="https://github.com/debezium/debezium-incubator">incubating connector</a>. If you would like to learn more or contribute, check out the work-in-progress pull request for <a href="https://github.com/debezium/debezium-incubator/pull/98">source code</a> and <a href="https://github.com/debezium/debezium.github.io/pull/325">documentation</a>.</p> </div> <div class="paragraph"> <p>In the second half of this blog post series, we will elaborate on the CDC implementation itself in more details. Stay tuned!</p> </div> </div> </div> </div> <div class="well"> <div class="row"> <div class="col-md-8"> <h2>Joy Gao</h2> <p> <span class="bio-size">Joy Gao is a software engineering at WePay, where she focuses on change data capture, data warehousing, and distributed systems.</span> </p> <p class="bio-size"> <a href="https://twitter.com/joygao"> <i class="icon-twitter"></i> </a> &nbsp; <a href="https://github.com/jgao54"> <i class="icon-github"></i> </a> &nbsp; </p> </div> <div class="col-md-4"> <img alt="" class="img-responsive pull-right portrait" src="/images/joygao.jpg"> </div> </div> </div> <div class="comments"> <div id="disqus_thread"></div> <script type="text/javascript">
                            var disqus_shortname = 'Debezium';
                            var disqus_url = "http://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/";
                            var disqus_developer = null;
                            var disqus_identifier = null;
                            (function() {
                              var dsq = document.createElement("script"); dsq.type = "text/javascript"; dsq.async = true;
                              dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                              (document.getElementsByTagName("head")[0] || document.getElementsByTagName("body")[0]).appendChild(dsq);
                            })();
                            </script> <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript=Debezium">comments powered by Disqus.</a></noscript> </div> </div> </div> </div> </div> <footer class="container"> <div class="row"> <div class="col-md-5 col-md-offset-1"> <h4>Debezium</h4> <p> &#169; 2019 Debezium Community <br> <br> <i class="icon-fire"></i> Mixed with <a href="http://twitter.github.com/bootstrap">Bootstrap</a>, baked by <a href="http://awestruct.org">Awestruct</a>. <br> <i class="icon-flag"></i> Website and docs licensed under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>. <br> <i class="icon-flag-alt"></i> Code released under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, v2.0</a>. <br> <i class="icon-file-alt"></i> <a href="https://www.redhat.com/legal/legal_statement.html" title="Terms">Terms</a> | <a href="https://www.redhat.com/legal/privacy_statement.html" title="Privacy Policy">Privacy</a> </p> </div> <div class="col-md-3"> <h4>Documentation</h4> <ul class="list-unstyled"> <li> <a href="/docs/features/" title="Features">Features</a> </li> <li> <a href="/docs/install/" title="Install">Install</a> </li> <li> <a href="/docs/manage/" title="Manage">Manage</a> </li> <li> <a href="/docs/architecture/" title="Architecture">Architecture</a> </li> <li> <a href="/docs/faq/" title="FAQ">FAQ</a> </li> <li> <a href="/docs/contribute/" title="Contribute">Contribute</a> </li> </ul> </div> <div class="col-md-3"> <h4>Connect</h4> <ul class="list-unstyled"> <li> <a href="/blog" title="Blog">Blog</a> </li> <li> <a href="http://twitter.com/debezium" title="Twitter">Twitter</a> </li> <li> <a href="http://github.com/debezium" title="GitHub">GitHub</a> </li> <li> <a href="https://gitter.im/debezium/dev" title="Chat">Chat</a> </li> <li> <a href="https://groups.google.com/forum/#!forum/debezium" title="Google Groups">Google Groups</a> </li> <li> <a href="http://stackoverflow.com/questions/tagged/debezium" title="StackOverflow">StackOverflow</a> </li> </ul> </div> </div> </footer> <div class="container" id="companyfooter"> <div class="redhatlogo"> <div id="logospacer"></div> <a href="https://www.redhat.com/"><img src="/images/Logo-Red_Hat-Sponsored_By-B-Standard-RGB.svg"></a> </div> </div> <span class="backToTop"> <a href="#top">back to top</a> </span> <script src="https://static.jboss.org/theme/js/libs/bootstrap-community/3.2.0.2/bootstrap-community.min.js"></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqCfg.js'></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqImg.js'></script> <div id="oTags"> <script type="text/javascript" src="//www.redhat.com/j/s_code.js"></script> <script type="text/javascript"><!--
        var coreUrl = encodeURI(document.URL.split("?")[0]).replace(/-/g," ");
        var urlSplit = coreUrl.toLowerCase().split(/\//);
        var urlLast = urlSplit[urlSplit.length-1];
        var pageNameString = "";
        var siteName = "";
        var minorSectionIndex = 3
        if (urlLast == "") {
            urlSplit.splice(-1,1);
        }
        if (urlLast.search(/\./) >= 0) {
            if (urlLast == "index.html") {
                urlSplit.splice(-1,1);
            }
            else {
                urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
            }
        }
        siteName = urlSplit[2].split(".")[1];
        s.prop14 = s.eVar27 = siteName || "";
        s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
        s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
        pageNameString = urlSplit.splice(3).join(" | ");
        s.pageName = "jboss | community | " + siteName + " | " + pageNameString;
        s.server = "jboss";
        s.channel = "jboss | community";
        s.prop4 = s.eVar23 = encodeURI(document.URL);
        s.prop21 = s.eVar18 = coreUrl;
        s.prop2 = s.eVar22 = "en";
        s.prop3 = s.eVar19 = "us";
        //--></script> <script type="text/javascript" src="//www.redhat.com/j/rh_omni_footer.js"></script> <script language="JavaScript" type="text/javascript"><!--
        if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
        //--></script> <noscript><a href="http://www.omniture.com" title="Web Analytics"><img src="https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]&cdp=3&[AQE]" height="1" width="1" border="0" alt=""/></a></noscript> </div> <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script> <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-10656779-1");
      pageTracker._trackPageview();
      } catch(err) {}</script> <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
      ga('create', 'UA-76464546-1', 'auto');
      ga('send', 'pageview');
      ga('set', 'anonymizeIp', true);
      ga('require', 'linkid', 'linkid.js');
      
      </script> </div> </body> </html>