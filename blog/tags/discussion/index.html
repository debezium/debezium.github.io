<!DOCTYPE html> <html lang="en"> <head> <title>Debezium Blog</title> <meta charset="utf-8"> <meta content="width=device-width, initial-scale=1.0" name="viewport"> <meta content="" name="description"> <meta content="" name="author"> <link href="https://static.jboss.org/theme/css/bootstrap-community/3.2.0.2/bootstrap-community.min.css" media="screen" rel="stylesheet"> <!--[if lt IE 9]><script src="https://static.jboss.org/theme/js/libs/html5/pre3.6/html5.min.js"></script><![endif]--> <link href="https://static.jboss.org/example/images/favicon.ico" rel="shortcut icon"> <link href="https://static.jboss.org/example/images/apple-touch-icon-144x144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144"> <link href="https://static.jboss.org/example/images/apple-touch-icon-114x114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114"> <link href="https://static.jboss.org/example/images/apple-touch-icon-72x72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72"> <link href="https://static.jboss.org/example/images/apple-touch-icon-precomposed.png" rel="apple-touch-icon-precomposed"> <link href="/stylesheets/debezium.css" rel="stylesheet" type="text/css"> <style>
      @media (min-width: 980px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-banner-1180px.png); height: 110px;  }
      }
      @media (max-width: 979px) {
        .banner { background-image: url(https://static.jboss.org/example/images/debezium-logo.png); background-repeat:no-repeat; background-position: center bottom; height: 60px; }
      }
      @media (max-width: 650px) {
        .banner { width: 100%; margin: 0px auto; }
      }
      @media (max-width: 450px) {
        .banner { height: 90px; }
      }
    </style> <link href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"> <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script> <script>
      hljs.initHighlightingOnLoad();
    </script> <script src="https://static.jboss.org/theme/js/libs/jquery/jquery-1.9.1.min.js"></script> <style>
      /* adjusting the vertical spacing for when a stickynav is engaged */
      .breadcrumb-fixed > .active {
        color: #8c8f91;
      }
      .breadcrumb-fixed {
        margin: 70px 0 10px;
        padding: 8px 15px;
        margin-bottom: 20px;
        list-style: none;
        background-color: #f5f5f5;
        border-radius: 4px;
      }
      
      .breadcrumb-fixed > li {
        display: inline-block;
      }
    </style> </head> <body> <div id="rhbar"> <a class="jbdevlogo" href="https://www.jboss.org/projects/about"></a> <a class="rhlogo" href="https://www.redhat.com/"></a> </div> <div id=""> <ul class="visuallyhidden" id="top"> <li> <a accesskey="n" href="#nav" title="Skip to navigation">Skip to navigation</a> </li> <li> <a accesskey="c" href="#page" title="Skip to content">Skip to content</a> </li> </ul> <div class="container" id="content"> <div class="navbar navbar-inverse navbar-fix"> <div class="container-fluid"> <div class="navbar-header"> <button class="navbar-toggle collapsed" data-target="#navbar-1" data-toggle="collapse"> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button> <a class="navbar-brand" href="/"> Debezium </a> </div> <div class="collapse navbar-collapse" id="navbar-1"> <ul class="nav navbar-nav pull-right"> <li class=""><a href="/docs/faq/">FAQ</a></li> <li class=""><a href="/docs/">DOCS</a></li> <li class=""><a href="/community/">COMMUNITY</a></li> <li class="active"><a href="/blog/">BLOG</a></li> </ul> </div> </div> </div> <div id="equalHeightsLayout"> <div class="row post-text-padding row-no-expand"> <div class="hidden-xs col-sm-3 no-right-padding" id="leftdocnav"> <div class="panel-docnav"> <div class="panel-heading"> <h3 class="panel-title"> Latest posts </h3> </div> <div class="panel-body"> <ul class="list-group"> <li class="list-group-item"> <a href="/blog/2019/02/25/debezium-0-9-2-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.2.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/" rel="tooltip" title="Click to go to post">Reliable Microservices Data Exchange With the Outbox Pattern</a> </li> <li class="list-group-item"> <a href="/blog/2019/02/13/debezium-webinar-at-devnation-live/" rel="tooltip" title="Click to go to post">Debezium at DevNation Live</a> </li> <li class="list-group-item"> <a href="/blog/2019/02/13/debezium-0-9-1-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.1.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/02/05/debezium-0-9-0-final-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.0.Final Released</a> </li> <li class="list-group-item"> <a href="/blog/2019/01/28/debezium-0-9-0-cr1-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.0.CR1 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/12/19/debezium-0-9-0-beta2-released/" rel="tooltip" title="Click to go to post">Debezium 0.9.0.Beta2 Released</a> </li> <li class="list-group-item"> <a href="/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/" rel="tooltip" title="Click to go to post">Automating Cache Invalidation With Change Data Capture</a> </li> </ul> </div> </div> </div> <div class="col-xs-12 col-sm-9" id="maincol"> <div class="text-right"> <h3> Subscribe <a class="rss" href="/blog.atom"> <i class="icon-rss"></i> </a> </h3> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">Reliable Microservices Data Exchange With the Outbox Pattern</a> </h1> <div class="byline"> <p> <em> February 19, 2019 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> <a class="label label-info" href="/blog/tags/microservices/">microservices</a> <a class="label label-info" href="/blog/tags/apache-kafka/">apache-kafka</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="openblock teaser"> <div class="content"> <div class="paragraph"> <p>As part of their business logic, microservices often do not only have to update their own local data store, but they also need to notify other services about data changes that happened. The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner; it provides source services with instant "read your own writes" semantics, while offering reliable, eventually consistent data exchange across service boundaries.</p> </div> </div> </div> <div class="paragraph"> <p>If you&#8217;ve built a couple of microservices, you&#8217;ll probably agree that the <a href="https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/">hardest part about them is data</a>: microservices don&#8217;t exist in isolation and very often they need to propagate data and data changes amongst each other.</p> </div> <div class="paragraph"> <p>For instance consider a microservice that manages purchase orders: when a new order is placed, information about that order may have to be relayed to a shipment service (so it can assemble shipments of one or more orders) and a customer service (so it can update things like the customer&#8217;s total credit balance based on the new order).</p> </div> <div class="paragraph"> <p>There are different approaches for letting the order service know the other two about new purchase orders; e.g. it could invoke some <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a>, <a href="https://grpc.io/">grpc</a> or other (synchronous) API provided by these services. This might create some undesired coupling, though: the sending service must know which other services to invoke and where to find them. It also must be prepared for these services temporarily not being available. Service meshes such as <a href="https://istio.io/">Istio</a> can come in helpful here, by providing capabilities like request routing, retries, circuit breakers and much more.</p> </div> <div class="paragraph"> <p>The general issue of any synchronous approach is that one service cannot really function without the other services which it invokes. While buffering and retrying might help in cases where other services only need to be <em>notified</em> of certain events, this is not the case if a service actually needs to <em>query</em> other services for information. For instance, when a purchase order is placed, the order service might need to obtain the information how many times the purchased item is on stock from an inventory service.</p> </div> <div class="paragraph"> <p>Another downside of such a synchronous approach is that it lacks re-playability, i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.</p> </div> <div class="paragraph"> <p>Both problems can be addressed by using an asynchronous data exchange approach instead: i.e having the order, inventory and other services propagate events through a durable message log such as <a href="http://kafka.apache.org/">Apache Kafka</a>. By subscribing to these event streams, each service will be notified about the data change of other services. It can react to these events and, if needed, create a local representation of that data in its own data store, using a representation tailored towards its own needs. For instance, such view might be denormalized to efficiently support specific access patterns, or it may only contain a subset of the original data that&#8217;s relevant to the consuming service.</p> </div> <div class="paragraph"> <p>Durable logs also support re-playability, i.e. new consumers can be added as needed, enabling use cases you might not have had in mind originally, and without touching the source service. E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on purchase orders based on <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>. Once the purchase order events are in a Kafka topic (Kafka&#8217;s topic&#8217;s retention policy settings can be used to ensure that events remain in a topic as long as its needed for the given use cases and business requirements), new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice&#8217;s database, search index, data warehouse etc.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Dealing with Topic Growth</div> <div class="paragraph"> <p>Depending on the amount of data (number and size of records, frequency of changes), it may or may not be feasible to keep events in topics for a long or even indefinite time. Very often, some or even all events pertaining to a given data item (e.g. a specific purchase order) might be eligible for deletion from a business point of view after a given point in time. See the box "Deletion of Events from Kafka Topics" further below for some more thoughts on the deletion of events from Kafka topics in order to keep their size within bounds.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="the_issue_of_dual_writes"><a class="anchor" href="#the_issue_of_dual_writes"></a>The Issue of Dual Writes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to provide their functionality, microservices will typically have their own local data store. For instance, the order service may use a relational database to persist the information about purchase orders. When a new order is placed, this may result in an <code>INSERT</code> operation in a table <code>PurchaseOrder</code> in the service&#8217;s database. At the same time, the service may wish to send an event about the new order to Apache Kafka, so to propagate that information to other interested services.</p> </div> <div class="paragraph"> <p>Simply issuing these two requests may lead to potential inconsistencies, though. The reason being that we cannot have one shared transaction that would span the service&#8217;s database as well as Apache Kafka, as the latter doesn&#8217;t support to be enlisted in distributed (XA) transactions. So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database, but not having sent the corresponding message to Kafka (e.g. due to some networking issue). Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the local database. Both situations are undesirable; this may cause no shipment to be created for a seemingly successfully placed order. Or a shipment gets created, but then there&#8217;d be no trace about the corresponding purchase order in the order service itself.</p> </div> <div class="paragraph"> <p>So how can this situation be avoided? The answer is to only modify <em>one</em> of the two resources (the database <em>or</em> Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner. Let&#8217;s first consider the case of only writing to Apache Kafka.</p> </div> <div class="paragraph"> <p>When receiving a new purchase order, the order service would not do the <code>INSERT</code> into its database synchronously; instead, it would only send an event describing the new order to a Kafka topic. So only one resource gets modified at a time, and if something goes wrong with that, we&#8217;ll find out about it instantly and report back to the caller of the order service that the request failed.</p> </div> <div class="paragraph"> <p>At the same time, the service itself would subscribe to that Kafka topic. That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database. There&#8217;s one subtle challenge here, though, and that is the lack of "read your own write" semantics. E.g. let&#8217;s assume the order service also has an API for searching for all the purchase orders of a given customer. When invoking that API right after placing a new order, due to the asynchronous nature of processing messages from the Kafka topic, it might happen that the purchase order has not yet been persisted in the service&#8217;s database and thus will not be returned by that query. That can lead to a very confusing user experience, as users for instance may miss newly placed orders in their shopping history. There are ways to deal with this situation, e.g. the service could keep newly placed purchase orders in memory and answer subsequent queries based on that. This gets quickly non-trivial though when implementing more complex queries or considering that the order service might also comprise multiple nodes in a clustered set-up, which would require propagation of that data within the cluster.</p> </div> <div class="paragraph"> <p>Now how would things look like when only writing to the database synchronously and driving the export of a message to Apache Kafka based on that? This is where the outbox pattern comes in.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_outbox_pattern"><a class="anchor" href="#the_outbox_pattern"></a>The Outbox Pattern</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea of this approach is to have an "outbox" table in the service&#8217;s database. When receiving a request for placing a purchase order, not only an <code>INSERT</code> into the <code>PurchaseOrder</code> table is done, but, as part of the same transaction, also a record representing the event to be sent is inserted into that outbox table.</p> </div> <div class="paragraph"> <p>The record describes an event that happened in the service, for instance it could be a JSON structure representing the fact that a new purchase order has been placed, comprising data on the order itself, its order lines as well as contextual information such as a use case identifier. By explicitly emitting events via records in the outbox table, it can be ensured that events are structured in a way suitable for external consumers. This also helps to make sure that event consumers won&#8217;t break when for instance altering the internal domain model or the <code>PurchaseOrder</code> table.</p> </div> <div class="paragraph"> <p>An asynchronous process monitors that table for new entries. If there are any, it propagates the events as messages to Apache Kafka. This gives us a very nice balance of characteristics: By synchronously writing to the <code>PurchaseOrder</code> table, the source service benefits from "read your own writes" semantics. A subsequent query for purchase orders will return the newly persisted order, as soon as that first transaction has been committed. At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.</p> </div> <div class="paragraph"> <p>Now, the outbox pattern isn&#8217;t actually a new idea. It has been in use for quite some time. In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions, it can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker. You can also find a description of the pattern on Chris Richardson&#8217;s excellent <a href="https://microservices.io/patterns/data/application-events.html">microservices.io</a> site.</p> </div> <div class="paragraph"> <p>Nevertheless, the pattern gets much less attention than it deserves and it is especially useful in the context of microservices. As we&#8217;ll see, the outbox pattern can be implemented in a very elegant and efficient way using change data capture and Debezium. In the following, let&#8217;s explore how.</p> </div> </div> </div> <div class="sect1"> <h2 id="an_implementation_based_on_change_data_capture"><a class="anchor" href="#an_implementation_based_on_change_data_capture"></a>An Implementation Based on Change Data Capture</h2> <div class="sectionbody"> <div class="paragraph"> <p><a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">Log-based Change Data Capture</a> (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka. As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime. Debezium comes with <a href="/docs/connectors/">CDC connectors</a> for several databases such as MySQL, Postgres and SQL Server. The following example will use the <a href="/docs/connectors/postgresql">Debezium connector for Postgres</a>.</p> </div> <div class="paragraph"> <p>You can find the complete <a href="https://github.com/debezium/debezium-examples/tree/master/outbox">source code of the example</a> on GitHub. Refer to the <a href="https://github.com/debezium/debezium-examples/blob/master/outbox/README.md">README.md</a> for details on building and running the example code. The example is centered around two microservices, <a href="https://github.com/debezium/debezium-examples/tree/master/outbox/order-service">order-service</a> and <a href="https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service">shipment-service</a>. Both are implemented in Java, using <a href="http://cdi-spec.org/">CDI</a> as the component model and JPA/Hibernate for accessing their respective databases. The order service runs on <a href="http://wildfly.org/">WildFly</a> and exposes a simple REST API for placing purchase orders and canceling specific order lines. It uses a Postgres database as its local data store. The shipment service is based on <a href="http://thorntail.io/">Thorntail</a>; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database. Debezium tails the transaction log ("write-ahead log", WAL) of the order service&#8217;s Postgres database in order to capture any new events in the outbox table and propagates them to Apache Kafka.</p> </div> <div class="paragraph"> <p>The overall architecture of the solution can be seen in the following picture:</p> </div> <div class="imageblock centered-image"> <img src="/images/outbox_pattern.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Outbox Pattern Overview"> </div> <div class="paragraph"> <p>Note that the pattern is in no way tied to these specific implementation choices. It could equally well be realized using alternative technologies such as Spring Boot (e.g. leveraging Spring Data&#8217;s <a href="https://docs.spring.io/spring-data/commons/docs/current/api/index.html?org/springframework/data/domain/DomainEvents.html">support for domain events</a>), plain JDBC or other programming languages than Java altogether.</p> </div> <div class="paragraph"> <p>Now let&#8217;s take a closer look at some of the relevant components of the solution.</p> </div> <div class="sect2"> <h3 id="the_outbox_table"><a class="anchor" href="#the_outbox_table"></a>The Outbox Table</h3> <div class="paragraph"> <p>The <code>outbox</code> table resides in the database of the order service and has the following structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>Column        |          Type          | Modifiers&#x000A;--------------+------------------------+-----------&#x000A;id            | uuid                   | not null&#x000A;aggregatetype | character varying(255) | not null&#x000A;aggregateid   | character varying(255) | not null&#x000A;type          | character varying(255) | not null&#x000A;payload       | jsonb                  | not null</code></pre> </div> </div> <div class="paragraph"> <p>Its columns are these:</p> </div> <div class="ulist"> <ul> <li> <p><code>id</code>: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure. Generated when creating a new event.</p> </li> <li> <p><code>aggregatetype</code>: the type of the <em>aggregate root</em> to which a given event is related; the idea being, leaning on the same concept of domain-driven design, that exported events should refer to an aggregate (<a href="https://martinfowler.com/bliki/DDD_Aggregate.html">"a cluster of domain objects that can be treated as a single unit"</a>), where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate. This could for instance be "purchase order" or "customer".</p> <div class="paragraph"> <p>This value will be used to route events to corresponding topics in Kafka, so there&#8217;d be a topic for all events related to purchase orders, one topic for all customer-related events etc. Note that also events pertaining to a child entity contained within one such aggregate should use that same type. So e.g. an event representing the cancelation of an individual order line (which is part of the purchase order aggregate) should also use the type of its aggregate root, "order", ensuring that also this event will go into the "order" Kafka topic.</p> </div> </li> <li> <p><code>aggregateid</code>: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id; Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root, e.g. the purchase order id for an order line cancelation event. This id will be used as the key for Kafka messages later on. That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic, which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.</p> </li> <li> <p><code>type</code>: the type of event, e.g. "Order Created" or "Order Line Canceled". Allows consumers to trigger suitable event handlers.</p> </li> <li> <p><code>payload</code>: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.</p> </li> </ul> </div> </div> <div class="sect2"> <h3 id="sending_events_to_the_outbox"><a class="anchor" href="#sending_events_to_the_outbox"></a>Sending Events to the Outbox</h3> <div class="paragraph"> <p>In order to "send" events to the outbox, code in the order service could in general just do an <code>INSERT</code> into the outbox table. However, it&#8217;s a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed. <a href="https://docs.jboss.org/weld/reference/latest/en-US/html/events.html">CDI events</a> come in very handy for this. They can be raised in the application code and will be processed <em>synchronously</em> by the outbox event sender, which will do the required <code>INSERT</code> into the outbox table.</p> </div> <div class="paragraph"> <p>All outbox event types should implement the following contract, resembling the structure of the outbox table shown before:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public interface ExportedEvent {&#x000A;&#x000A;    String getAggregateId();&#x000A;    String getAggregateType();&#x000A;    JsonNode getPayload();&#x000A;    String getType();&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To produce such event, application code uses an injected <code>Event</code> instance, as e.g. here in the <code>OrderService</code> class:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class OrderService {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    @Inject&#x000A;    private Event&lt;ExportedEvent&gt; event;&#x000A;&#x000A;    @Transactional&#x000A;    public PurchaseOrder addOrder(PurchaseOrder order) {&#x000A;        order = entityManager.merge(order);&#x000A;&#x000A;        event.fire(OrderCreatedEvent.of(order));&#x000A;        event.fire(InvoiceCreatedEvent.of(order));&#x000A;&#x000A;        return order;&#x000A;    }&#x000A;&#x000A;    @Transactional&#x000A;    public PurchaseOrder updateOrderLine(long orderId, long orderLineId,&#x000A;            OrderLineStatus newStatus) {&#x000A;        // ...&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>In the <code>addOrder()</code> method, the JPA entity manager is used to persist the incoming order in the database and the injected <code>event</code> is used to fire a corresponding <code>OrderCreatedEvent</code> and an <code>InvoiceCreatedEvent</code>. Again, keep in mind that, despite the notion of "event", these two things happen within one and the same transaction. i.e. within this transaction, three records will be inserted into the database: one in the table with purchase orders and two in the outbox table.</p> </div> <div class="paragraph"> <p>Actual event implementations are straight-forward; as an example, here&#8217;s the <code>OrderCreatedEvent</code> class:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public class OrderCreatedEvent implements ExportedEvent {&#x000A;&#x000A;    private static ObjectMapper mapper = new ObjectMapper();&#x000A;&#x000A;    private final long id;&#x000A;    private final JsonNode order;&#x000A;&#x000A;    private OrderCreatedEvent(long id, JsonNode order) {&#x000A;        this.id = id;&#x000A;        this.order = order;&#x000A;    }&#x000A;&#x000A;    public static OrderCreatedEvent of(PurchaseOrder order) {&#x000A;        ObjectNode asJson = mapper.createObjectNode()&#x000A;                .put("id", order.getId())&#x000A;                .put("customerId", order.getCustomerId())&#x000A;                .put("orderDate", order.getOrderDate().toString());&#x000A;&#x000A;        ArrayNode items = asJson.putArray("lineItems");&#x000A;&#x000A;        for (OrderLine orderLine : order.getLineItems()) {&#x000A;        items.add(&#x000A;                mapper.createObjectNode()&#x000A;                .put("id", orderLine.getId())&#x000A;                .put("item", orderLine.getItem())&#x000A;                .put("quantity", orderLine.getQuantity())&#x000A;                .put("totalPrice", orderLine.getTotalPrice())&#x000A;                .put("status", orderLine.getStatus().name())&#x000A;            );&#x000A;        }&#x000A;&#x000A;        return new OrderCreatedEvent(order.getId(), asJson);&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getAggregateId() {&#x000A;        return String.valueOf(id);&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getAggregateType() {&#x000A;        return "Order";&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public String getType() {&#x000A;        return "OrderCreated";&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public JsonNode getPayload() {&#x000A;        return order;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note how <a href="https://github.com/FasterXML/jackson">Jackson&#8217;s</a> <code>ObjectMapper</code> is used to create a JSON representation of the event&#8217;s payload.</p> </div> <div class="paragraph"> <p>Now let&#8217;s take a look at the code that consumes any fired <code>ExportedEvent</code> and does the corresponding write to the outbox table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class EventSender {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    public void onExportedEvent(@Observes ExportedEvent event) {&#x000A;        OutboxEvent outboxEvent = new OutboxEvent(&#x000A;                event.getAggregateType(),&#x000A;                event.getAggregateId(),&#x000A;                event.getType(),&#x000A;                event.getPayload()&#x000A;        );&#x000A;&#x000A;        entityManager.persist(outboxEvent);&#x000A;        entityManager.remove(outboxEvent);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>It&#8217;s rather simple: for each event the CDI runtime will invoke the <code>onExportedEvent()</code> method. An instance of the <code>OutboxEvent</code> entity is persisted in the database&#8201;&#8212;&#8201;and removed right away!</p> </div> <div class="paragraph"> <p>This might be surprising at first. But it makes sense when remembering how log-based CDC works: it doesn&#8217;t examine the actual contents of the table in the database, but instead it tails the append-only transaction log. The calls to <code>persist()</code> and <code>remove()</code> will create an <code>INSERT</code> and a <code>DELETE</code> entry in the log once the transaction commits. After that, Debezium will process these events: for any <code>INSERT</code>, a message with the event&#8217;s payload will be sent to Apache Kafka. <code>DELETE</code> events on the other hand can be ignored, as the removal from the outbox table is a mere technicality that doesn&#8217;t require any propagation to the message broker. So we are able to capture the event added to the outbox table by means of CDC, but when looking at the contents of the table itself, it will always be empty. This means that no additional disk space is needed for the table (apart from the log file elements which will automatically be discarded at some point) and also no separate house-keeping process is required to stop it from growing indefinitely.</p> </div> </div> <div class="sect2"> <h3 id="registering_the_debezium_connector"><a class="anchor" href="#registering_the_debezium_connector"></a>Registering the Debezium Connector</h3> <div class="paragraph"> <p>With the outbox implementation in place, it&#8217;s time to register the Debezium Postgres connector, so it can capture any new events in the outbox table and relay them to Apache Kafka. That can be done by POST-ing the following JSON request to the REST API of Kafka Connect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "name": "outbox-connector",&#x000A;    "config": {&#x000A;        "connector.class" : "io.debezium.connector.postgresql.PostgresConnector",&#x000A;        "tasks.max" : "1",&#x000A;        "database.hostname" : "order-db",&#x000A;        "database.port" : "5432",&#x000A;        "database.user" : "postgresuser",&#x000A;        "database.password" : "postgrespw",&#x000A;        "database.dbname" : "orderdb",&#x000A;        "database.server.name" : "dbserver1",&#x000A;        "schema.whitelist" : "inventory",&#x000A;        "table.whitelist" : "inventory.outboxevent",&#x000A;        "tombstones.on.delete" : "false",&#x000A;        "transforms" : "router",&#x000A;        "transforms.router.type" : "io.debezium.examples.outbox.routingsmt.EventRouter"&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>This sets up an instance of <code>io.debezium.connector.postgresql.PostgresConnector</code>, capturing changes from the specified Postgres instance. Note that by means of a table whitelist, only changes from the <code>outboxevent</code> table are captured. It also applies a single message transform (SMT) named <code>EventRouter</code>.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Deletion of Events from Kafka Topics</div> <div class="paragraph"> <p>By setting the <code>tombstones.on.delete</code> to <code>false</code>, no deletion markers ("tombstones") will be emitted by the connector when an event record gets deleted from the outbox table. That makes sense, as the deletion from the outbox table shouldn&#8217;t affect the retention of events in the corresponding Kafka topics. Instead, a specific retention time for the event topics may be configured in Kafka, e.g. to retain all purchase order events for 30 days.</p> </div> <div class="paragraph"> <p>Alternatively, one could work with <a href="https://kafka.apache.org/documentation/#compaction">compacted topics</a>. This would require some changes to the design of events in the outbox table:</p> </div> <div class="ulist"> <ul> <li> <p>they must describe the entire aggregate; so for instance also an event representing the cancelation of a single order line should describe the complete current state of the containing purchase order; that way consumers will be able to obtain the entire state of the purchase order also when only seeing the last event pertaining to a given order, after log compaction ran.</p> </li> <li> <p>they must have one more <code>boolean</code> attribute indicating whether a particular event represents the deletion of the event&#8217;s aggregate root. Such an event (e.g. of type <code>OrderDeleted</code>) could then be used by the event routing SMT described in the next section to produce a deletion marker for that aggregate root. Log compaction would then remove all events pertaining to the given purchase order when its <code>OrderDeleted</code> event has been written to the topic.</p> </li> </ul> </div> <div class="paragraph"> <p>Naturally, when deleting events, the event stream will not be re-playable from its very beginning any longer. Depending on the specific business requirements, it might be sufficient to just keep the final state of a given purchase order, customer etc. This could be achieved using compacted topics and a sufficiently value for the topic&#8217;s <code>delete.retention.ms</code> setting. Another option could be to move historic events to some sort of cold storage (e.g. an Amazon S3 bucket), from where they can be retrieved if needed, followed by reading the latest events from the Kafka topics. Which approach to follow depends on the specific requirements, expected amount of data and expertise in the team developing and operating the solution.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="topic_routing"><a class="anchor" href="#topic_routing"></a>Topic Routing</h3> <div class="paragraph"> <p>By default, the Debezium connectors will send all change events originating from one given table to the same topic, i.e. we&#8217;d end up with a single Kafka topic named <code>dbserver1.inventory.outboxevent</code> which would contain all events, be it order events, customer events etc.</p> </div> <div class="paragraph"> <p>To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though, to have multiple topics, e.g. <code>OrderEvents</code>, <code>CustomerEvents</code> and so on. For instance the shipment service might not be interested in any customer events. By only subscribing to the <code>OrderEvents</code> topic, it will be sure to never receive any customer events.</p> </div> <div class="paragraph"> <p>In order to route the change events captured from the outbox table to different topics, that custom SMT <code>EventRouter</code> is used. Here is the code of its <code>apply()</code> method, which will be invoked by Kafka Connect for each record emitted by the Debezium connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Override&#x000A;public R apply(R record) {&#x000A;    // Ignoring tombstones just in case&#x000A;    if (record.value() == null) {&#x000A;        return record;&#x000A;    }&#x000A;&#x000A;    Struct struct = (Struct) record.value();&#x000A;    String op = struct.getString("op");&#x000A;&#x000A;    // ignoring deletions in the outbox table&#x000A;    if (op.equals("d")) {&#x000A;        return null;&#x000A;    }&#x000A;    else if (op.equals("c")) {&#x000A;        Long timestamp = struct.getInt64("ts_ms");&#x000A;        Struct after = struct.getStruct("after");&#x000A;&#x000A;        String key = after.getString("aggregateid");&#x000A;        String topic = after.getString("aggregatetype") + "Events";&#x000A;&#x000A;        String eventId = after.getString("id");&#x000A;        String eventType = after.getString("type");&#x000A;        String payload = after.getString("payload");&#x000A;&#x000A;        Schema valueSchema = SchemaBuilder.struct()&#x000A;            .field("eventType", after.schema().field("type").schema())&#x000A;            .field("ts_ms", struct.schema().field("ts_ms").schema())&#x000A;            .field("payload", after.schema().field("payload").schema())&#x000A;            .build();&#x000A;&#x000A;        Struct value = new Struct(valueSchema)&#x000A;            .put("eventType", eventType)&#x000A;            .put("ts_ms", timestamp)&#x000A;            .put("payload", payload);&#x000A;&#x000A;        Headers headers = record.headers();&#x000A;        headers.addString("eventId", eventId);&#x000A;&#x000A;        return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,&#x000A;                record.timestamp(), headers);&#x000A;    }&#x000A;    // not expecting update events, as the outbox table is "append only",&#x000A;    // i.e. event records will never be updated&#x000A;    else {&#x000A;        throw new IllegalArgumentException("Record of unexpected op type: " + record);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>When receiving a delete event (<code>op</code> = <code>d</code>), it will discard that event, as that deletion of event records from the outbox table is not relevant to downstream consumers. Things get more interesting, when receiving a create event (<code>op</code> = <code>c</code>). Such record will be propagated to Apache Kafka.</p> </div> <div class="paragraph"> <p>Debezium&#8217;s change events have a complex structure, that contain the old (<code>before</code>) and new (<code>after</code>) state of the represented row. The event structure to propagate is obtained from the <code>after</code> state. The <code>aggregatetype</code> value from the captured event record is used to build the name of the topic to send the event to. For instance, events with <code>aggregatetype</code> set to <code>Order</code> will be sent to the <code>OrderEvents</code> topic. <code>aggregateid</code> is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic. The message value is a structure comprising the original event payload (encoded as JSON), the timestamp indicating when the event was produced and the event type. Finally, the event UUID is propagated as a Kafka header field. This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="events_in_apache_kafka"><a class="anchor" href="#events_in_apache_kafka"></a>Events in Apache Kafka</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now let&#8217;s take a look into the <code>OrderEvents</code> and <code>CustomerEvents</code> topics.</p> </div> <div class="paragraph"> <p>If you have checked out the example sources and started all the components via Docker Compose (see the <em>README.md</em> file in the example project for more details), you can place purchase orders via the order service&#8217;s REST API like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders</code></pre> </div> </div> <div class="paragraph"> <p>Similarly, specific order lines can be canceled:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2</code></pre> </div> </div> <div class="paragraph"> <p>When using a tool such as the very practical <a href="https://github.com/edenhill/kafkacat">kafkacat</a> utility, you should now see messages like these in the <code>OrderEvents</code> topic:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>kafkacat -b kafka:9092 -C -o beginning -f 'Headers: %h\nKey: %k\nValue: %s\n' -q -t OrderEvents</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>Headers: eventId=d03dfb18-8af8-464d-890b-09eb8b2dbbdd&#x000A;Key: "4"&#x000A;Value: {"eventType":"OrderCreated","ts_ms":1550307598558,"payload":"{\"id\": 4, \"lineItems\": [{\"id\": 7, \"item\": \"Debezium in Action\", \"status\": \"ENTERED\", \"quantity\": 2, \"totalPrice\": 39.98}, {\"id\": 8, \"item\": \"Debezium for Dummies\", \"status\": \"ENTERED\", \"quantity\": 1, \"totalPrice\": 29.99}], \"orderDate\": \"2019-01-31T12:13:01\", \"customerId\": 123}"}&#x000A;Headers: eventId=49f89ea0-b344-421f-b66f-c635d212f72c&#x000A;Key: "4"&#x000A;Value: {"eventType":"OrderLineUpdated","ts_ms":1550308226963,"payload":"{\"orderId\": 4, \"newStatus\": \"CANCELLED\", \"oldStatus\": \"ENTERED\", \"orderLineId\": 7}"}</code></pre> </div> </div> <div class="paragraph"> <p>The <code>payload</code> field with the message values is the string-ified JSON representation of the original events. The Debezium Postgres connector emits <code>JSONB</code> columns as a string (using the <code>io.debezium.data.Json</code> logical type name), which is why the quotes are escaped. The <a href="https://stedolan.github.io/jq/">jq</a> utility, and more specifically, its <code>fromjson</code> operator, come in handy for displaying the event payload in a more readable way:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>kafkacat -b kafka:9092 -C -o beginning -t Order | jq '.payload | fromjson'</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;  "id": 4,&#x000A;  "lineItems": [&#x000A;    {&#x000A;      "id": 7,&#x000A;      "item": "Debezium in Action",&#x000A;      "status": "ENTERED",&#x000A;      "quantity": 2,&#x000A;      "totalPrice": 39.98&#x000A;    },&#x000A;    {&#x000A;      "id": 8,&#x000A;      "item": "Debezium for Dummies",&#x000A;      "status": "ENTERED",&#x000A;      "quantity": 1,&#x000A;      "totalPrice": 29.99&#x000A;    }&#x000A;  ],&#x000A;  "orderDate": "2019-01-31T12:13:01",&#x000A;  "customerId": 123&#x000A;}&#x000A;{&#x000A;  "orderId": 4,&#x000A;  "newStatus": "CANCELLED",&#x000A;  "oldStatus": "ENTERED",&#x000A;  "orderLineId": 7&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>You can also take a look at the <code>CustomerEvents</code> topic to inspect the events representing the creation of an invoice when a purchase order is added.</p> </div> <div class="sect2"> <h3 id="duplicate_detection_in_the_consuming_service"><a class="anchor" href="#duplicate_detection_in_the_consuming_service"></a>Duplicate Detection in the Consuming Service</h3> <div class="paragraph"> <p>At this point, our implementation of the outbox pattern is fully functional; when the order service receives a request to place an order (or cancel an order line), it will persist the corresponding state in the <code>purchaseorder</code> and <code>orderline</code> tables of its database. At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database. The Debezium Postgres connector captures any insertions into that table and routes the events into the Kafka topic corresponding to the aggregate type represented by a given event.</p> </div> <div class="paragraph"> <p>To wrap things up, let&#8217;s explore how another microservice such as the shipment service can consume these messages. The entry point into that service is a regular Kafka consumer implementation, which is not too exciting and hence omitted here for the sake of brevity. You can find its <a href="https://github.com/debezium/debezium-examples/blob/master/outbox/shipment-service/src/main/java/io/debezium/examples/outbox/shipment/facade/KafkaEventConsumer.java">source code</a> in the example repository. For each incoming message on the <code>Order</code> topic, the consumer calls the <code>OrderEventHandler</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class OrderEventHandler {&#x000A;&#x000A;    private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);&#x000A;&#x000A;    @Inject&#x000A;    private MessageLog log;&#x000A;&#x000A;    @Inject&#x000A;    private ShipmentService shipmentService;&#x000A;&#x000A;    @Transactional&#x000A;    public void onOrderEvent(UUID eventId, String key, String event) {&#x000A;        if (log.alreadyProcessed(eventId)) {&#x000A;            LOGGER.info("Event with UUID {} was already retrieved, ignoring it", eventId);&#x000A;            return;&#x000A;        }&#x000A;&#x000A;        JsonObject json = Json.createReader(new StringReader(event)).readObject();&#x000A;        JsonObject payload = json.containsKey("schema") ? json.getJsonObject("payload") :json;&#x000A;&#x000A;        String eventType = payload.getString("eventType");&#x000A;        Long ts = payload.getJsonNumber("ts_ms").longValue();&#x000A;        String eventPayload = payload.getString("payload");&#x000A;&#x000A;        JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));&#x000A;        JsonObject payloadObject = payloadReader.readObject();&#x000A;&#x000A;        if (eventType.equals("OrderCreated")) {&#x000A;            shipmentService.orderCreated(payloadObject);&#x000A;        }&#x000A;        else if (eventType.equals("OrderLineUpdated")) {&#x000A;            shipmentService.orderLineUpdated(payloadObject);&#x000A;        }&#x000A;        else {&#x000A;            LOGGER.warn("Unkown event type");&#x000A;        }&#x000A;&#x000A;        log.processed(eventId);&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The first thing done by <code>onOrderEvent()</code> is to check whether the event with the given UUID has been processed before. If so, any further calls for that same event will be ignored. This is to prevent any duplicate processing of events caused by the "at least once" semantics of this data pipeline. For instance it could happen that the Debezium connector or the consuming service fail before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively. In that case, after a restart of Debezium or the consuming service, a few events may be processed a second time. Propagating the event UUID as a Kafka message header allows for an efficient detection and exclusion of duplicates in the consumer.</p> </div> <div class="paragraph"> <p>If a message is received for the first time, the message value is parsed and the business method of the <code>ShippingService</code> method corresponding to the specific event type is invoked with the event payload. Finally, the message is marked as processed with the message log.</p> </div> <div class="paragraph"> <p>This <code>MessageLog</code> simply keeps track of all consumed events in a table within the service&#8217;s local database:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class MessageLog {&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager entityManager;&#x000A;&#x000A;    @Transactional(value=TxType.MANDATORY)&#x000A;    public void processed(UUID eventId) {&#x000A;        entityManager.persist(new ConsumedMessage(eventId, Instant.now()));&#x000A;    }&#x000A;&#x000A;    @Transactional(value=TxType.MANDATORY)&#x000A;    public boolean alreadyProcessed(UUID eventId) {&#x000A;        return entityManager.find(ConsumedMessage.class, eventId) != null;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop. This allows for re-trying to process the message later on.</p> </div> <div class="paragraph"> <p>Note that a more complete implementation should take care of re-trying given messages only for a certain number of times, before re-routing any unprocessable messages to a dead-letter queue or similar. Also there should be some house-keeping on the message log table; periodically, all events older than the consumer&#8217;s current offset committed with the broker may be deleted, as it&#8217;s ensured that such messages won&#8217;t be propagated to the consumer another time.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>The outbox pattern is a great way for propagating data amongst different microservices.</p> </div> <div class="paragraph"> <p>By only modifying a single resource - the source service&#8217;s own database - it avoids any potential inconsistencies of altering multiple resources at the same time which don&#8217;t share one common transactional context (the database and Apache Kafka). By writing to the database first, the source service has instant "read your own writes" semantics, which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.</p> </div> <div class="paragraph"> <p>At the same time, the pattern enables asynchronous event propagation to other microservices. Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services. Given the right topic retention settings, new consumers may come up long after an event has been originally produced, and build up their own local state based on the event history.</p> </div> <div class="paragraph"> <p>Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services. If for instance single components of the solution fail or are not available for some time, e.g. during an update, events will simply be processed later on: after a restart, the Debezium connector will continue to tail the outbox table from the point where it left off before. Similarly, any consumer will continue to process topics from its previous offset. By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.</p> </div> <div class="paragraph"> <p>Naturally, such event pipeline between different services is eventually consistent, i.e. consumers such as the shipping service may lag a bit behind producers such as the order service. Usually, that&#8217;s just fine, though, and can be handled in terms of the application&#8217;s business logic. For instance there&#8217;ll typically be no need to create a shipment within the very same second as an order has been placed. Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range), thanks to log-based change data capture which allows for emission of events in near-realtime.</p> </div> <div class="paragraph"> <p>One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service&#8217;s API. I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind. This is to ensure to not accidentally break any consumers when upgrading the producing service. At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.</p> </div> <div class="paragraph"> <p><em>Many thanks to Hans-Peter Grahsl, Jiri Pechanec, Justin Holmes and Ren Kerner for their feedback while writing this post!</em></p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/">Automating Cache Invalidation With Change Data Capture</a> </h1> <div class="byline"> <p> <em> December 05, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>The <a href="https://docs.jboss.org/hibernate/stable/orm/userguide/html_single/Hibernate_User_Guide.html#caching-config">second-level cache</a> of Hibernate ORM / JPA is a proven and efficient way to increase application performance: caching read-only or rarely modified entities avoids roundtrips to the database, resulting in improved response times of the application.</p> </div> <div class="paragraph"> <p>Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms), so its contents are shared across transactions and concurrent sessions. Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too. As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM will update the cache automatically.</p> </div> <div class="paragraph"> <p>Things get tricky, though, when bypassing the application, e.g. when modifying records directly in the database. Hibernate ORM then has no way of knowing that the cached data has become stale, and it&#8217;s necessary to invalidate the affected items explicitly. A common way for doing so is to foresee some admin functionality that allows to clear an application&#8217;s caches. For this to work, it&#8217;s vital to not forget about calling that invalidation functionality, or the application will keep working with outdated cached data.</p> </div> <div class="paragraph"> <p>In the following we&#8217;re going to explore an alternative approach for cache invalidation, which works in a reliable and fully automated way: by employing Debezium and its <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">change data capture</a> (CDC) capabilities, you can track data changes in the database itself and react to any applied change. This allows to invalidate affected cache entries in near-realtime, without the risk of stale data due to missed changes. If an entry has been evicted from the cache, Hibernate ORM will load the latest version of the entity from the database the next time is requested.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_example_application"><a class="anchor" href="#the_example_application"></a>The Example Application</h2> <div class="sectionbody"> <div class="paragraph"> <p>As an example, consider this simple model of two entities, <code>PurchaseOrder</code> and <code>Item</code>:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_class_diagram.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Example domain model"> </div> <div class="paragraph"> <p>A purchase order represents the order of an item, where its total price is the ordered quantity times the item&#8217;s base price.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Source Code</div> <div class="paragraph"> <p>The <a href="https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/">source code</a> of this example is provided on GitHub. If you want to follow along and try out all the steps described in the following, clone the repo and follow the instructions in <a href="https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/_README.md">README.md</a> for building the project.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Modelling order and item as JPA entities is straight-forward:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class PurchaseOrder {&#x000A;&#x000A;    @Id&#x000A;    @GeneratedValue(generator = "sequence")&#x000A;    @SequenceGenerator(&#x000A;        name = "sequence", sequenceName = "seq_po", initialValue = 1001, allocationSize = 50&#x000A;    )&#x000A;    private long id;&#x000A;    private String customer;&#x000A;    @ManyToOne private Item item;&#x000A;    private int quantity;&#x000A;    private BigDecimal totalPrice;&#x000A;&#x000A;    // ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>As changes to items are rare, the <code>Item</code> entity should be cached. This can be done by simply specifying JPA&#8217;s <a href="https://docs.oracle.com/javaee/7/api/javax/persistence/Cacheable.html">@Cacheable</a> annotation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@Cacheable&#x000A;public class Item {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;    private String description;&#x000A;    private BigDecimal price;&#x000A;&#x000A;    // ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>You also need to enable the second-level cache in the <em>META-INF/persistence.xml</em> file. The property <code>hibernate.cache.use_second_level_cache</code> activates the cache itself, and the <code>ENABLE_SELECTIVE</code> cache mode causes only those entities to be put into the cache which are annotated with <code>@Cacheable</code>. It&#8217;s also a good idea to enable SQL query logging and cache access statistics. That way you&#8217;ll be able to verify whether things work as expected by examining the application log:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">&lt;?xml version="1.0" encoding="utf-8"?&gt;&#x000A;&lt;persistence xmlns="http://xmlns.jcp.org/xml/ns/persistence"&#x000A;    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&#x000A;    xsi:schemaLocation="..."&#x000A;    version="2.2"&gt;&#x000A;&#x000A;    &lt;persistence-unit name="orders-PU-JTA" transaction-type="JTA"&gt;&#x000A;        &lt;jta-data-source&gt;java:jboss/datasources/OrderDS&lt;/jta-data-source&gt;&#x000A;        &lt;shared-cache-mode&gt;ENABLE_SELECTIVE&lt;/shared-cache-mode&gt;&#x000A;        &lt;properties&gt;&#x000A;            &lt;property name="hibernate.cache.use_second_level_cache" value="true" /&gt;&#x000A;&#x000A;            &lt;property name="hibernate.show_sql" value="true" /&gt;&#x000A;            &lt;property name="hibernate.format_sql" value="true" /&gt;&#x000A;            &lt;property name="hibernate.generate_statistics" value="true" /&gt;&#x000A;&#x000A;            &lt;!-- dialect etc. ... --&gt;&#x000A;        &lt;/properties&gt;&#x000A;    &lt;/persistence-unit&gt;&#x000A;&lt;/persistence&gt;</code></pre> </div> </div> <div class="paragraph"> <p>When running on a <a href="https://www.oracle.com/technetwork/java/javaee/overview/index.html">Java EE</a> application server (or <a href="https://jakarta.ee/">Jakarta EE</a> how the stack is called after it has been donated to the Eclipse Foundation), that&#8217;s all you need to enable second-level caching. In the case of <a href="http://wildfly.org/">WildFly</a> (which is what&#8217;s used in the example project), the <a href="http://infinispan.org/">Infinispan</a> key/value store is used as the cache provider by default.</p> </div> <div class="paragraph"> <p>Now try and see what happens when modifying an item&#8217;s price by running some SQL in the database, bypassing the application layer. If you&#8217;ve checked out the example source code, comment out the <code>DatabaseChangeEventListener</code> class and start the application as described in the <em>README.md</em>. You then can place purchase orders using curl like this (a couple of example items have been persisted at application start-up):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">&gt; curl -H "Content-Type: application/json" \&#x000A;  -X POST \&#x000A;  --data '{ "customer" : "Billy-Bob", "itemId" : 10003, "quantity" : 2 }' \&#x000A;  http://localhost:8080/cache-invalidation/rest/orders</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">{&#x000A;    "id" : 1002,&#x000A;    "customer" : "Billy-Bob",&#x000A;    "item" : {&#x000A;        "id" :10003,&#x000A;        "description" : "North By Northwest",&#x000A;        "price" : 14.99&#x000A;    },&#x000A;    "quantity" : 2,&#x000A;    "totalPrice" : 29.98&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The response is the expected one, as the item price is 14.99. Now update the item&#8217;s price directly in the database. The example uses Postgres, so you can use the <em>psql</em> CLI utility to do so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">docker-compose exec postgres bash -c 'psql -U $POSTGRES_USER $POSTGRES_DB -c "UPDATE item SET price = 20.99 where id = 10003"'</code></pre> </div> </div> <div class="paragraph"> <p>Placing another purchase order for the same item using curl, you&#8217;ll see that the calculated total price doesn&#8217;t reflect the update. Not good! But it&#8217;s not too surprising, given that the price update was applied completely bypassing the application layer and Hibernate ORM.</p> </div> </div> </div> <div class="sect1"> <h2 id="the_change_event_handler"><a class="anchor" href="#the_change_event_handler"></a>The Change Event Handler</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now let&#8217;s explore how to use Debezium and CDC to react to changes in the <code>item</code> table and invalidate corresponding cache entries.</p> </div> <div class="paragraph"> <p>While Debezium most of the times is deployed into <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a> (thus streaming change events into Apache Kafka topics), it has another mode of operation that comes in very handy for the use case at hand. Using the <a href="/docs/embedded/">embedded engine</a>, you can run the Debezium connectors as a library directly within your application. For each change event received from the database, a configured callback method will be invoked, which in the case at hand will evict the affected item from the second-level cache.</p> </div> <div class="paragraph"> <p>The following picture shows the design of this approach:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_architecture.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Architecture Overview"> </div> <div class="paragraph"> <p>While this doesn&#8217;t come with the scalability and fault tolerance provided by Apache Kafka, it nicely fits the given requirements. As the second-level cache is bound to the application lifecycle, there is for instance no need for the offset management and restarting capabilities provided by the Kafka Connect framework. For the given use case it is enough to receive data change events while the application is running, and using the embedded engine enables exactly that.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Clustered Applications</div> <div class="paragraph"> <p>Note that it still might make sense to use Apache Kafka and the regular deployment of Debezium into Kafka Connect when running a clustered application where each node has a local cache. Instead of registering a connector on each node, Kafka and Connect would allow you to deploy a single connector instance and have the application nodes listen to the topic(s) with the change events. This would result in less resource utilization in the database.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Having added the dependencies of the Debezium embedded engine (<em>io.debezium:debezium-embedded:0.9.0.Beta1</em>) and the Debezium Postgres connector (<em>io.debezium:debezium-connector-postgres:0.9.0.Beta1</em>) to your project, a class <code>DatabaseChangeEventListener</code> for listening to any changes in the database can be implemented like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class DatabaseChangeEventListener {&#x000A;&#x000A;    @Resource&#x000A;    private ManagedExecutorService executorService;&#x000A;&#x000A;    @PersistenceUnit private EntityManagerFactory emf;&#x000A;&#x000A;    @PersistenceContext&#x000A;    private EntityManager em;&#x000A;&#x000A;    private EmbeddedEngine engine;&#x000A;&#x000A;    public void startEmbeddedEngine(@Observes @Initialized(ApplicationScoped.class) Object init) {&#x000A;        Configuration config = Configuration.empty()&#x000A;                .withSystemProperties(Function.identity()).edit()&#x000A;                .with(EmbeddedEngine.CONNECTOR_CLASS, PostgresConnector.class)&#x000A;                .with(EmbeddedEngine.ENGINE_NAME, "cache-invalidation-engine")&#x000A;                .with(EmbeddedEngine.OFFSET_STORAGE, MemoryOffsetBackingStore.class)&#x000A;                .with("name", "cache-invalidation-connector")&#x000A;                .with("database.hostname", "postgres")&#x000A;                .with("database.port", 5432)&#x000A;                .with("database.user", "postgresuser")&#x000A;                .with("database.password", "postgrespw")&#x000A;                .with("database.server.name", "dbserver1")&#x000A;                .with("database.dbname", "inventory")&#x000A;                .with("database.whitelist", "public")&#x000A;                .with("snapshot.mode", "never")&#x000A;                .build();&#x000A;&#x000A;        this.engine = EmbeddedEngine.create()&#x000A;                .using(config)&#x000A;                .notifying(this::handleDbChangeEvent)&#x000A;                .build();&#x000A;&#x000A;        executorService.execute(engine);&#x000A;    }&#x000A;&#x000A;    @PreDestroy&#x000A;    public void shutdownEngine() {&#x000A;        engine.stop();&#x000A;    }&#x000A;&#x000A;    private void handleDbChangeEvent(SourceRecord record) {&#x000A;        if (record.topic().equals("dbserver1.public.item")) {&#x000A;            Long itemId = ((Struct) record.key()).getInt64("id");&#x000A;            Struct payload = (Struct) record.value();&#x000A;            Operation op = Operation.forCode(payload.getString("op"));&#x000A;&#x000A;            if (op == Operation.UPDATE || op == Operation.DELETE) {&#x000A;                emf.getCache().evict(Item.class, itemId);&#x000A;            }&#x000A;        }&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Upon application start-up, this configures an instance of the <a href="/docs/connectors/postgresql/">Debezium Postgres connector</a> and sets up the embedded engine for running the connector. The <a href="/docs/connectors/postgresql/#connector-properties">connector options</a> (host name, credentials etc.) are mostly the same as when deploying the connector into Kafka Connect. There is no need for doing an initial snapshot of the existing data, hence the <a href="/docs/connectors/postgresql/#snapshots">snapshot mode</a> is set to "never".</p> </div> <div class="paragraph"> <p>The offset storage option is used for controlling how connector offsets should be persisted. As it&#8217;s not necessary to process any change events occurring while the connector is not running (instead you&#8217;d just begin to read the log from the current location after the restart), the in-memory implementation provided by Kafka Connect is used.</p> </div> <div class="paragraph"> <p>Once configured, the embedded engine must be run via an <code>Executor</code> instance. As the example runs in WildFly, a managed executor can simply be obtained through <code>@Resource</code> injection for that purpose (see <a href="https://www.jcp.org/en/jsr/detail?id=236">JSR 236</a>).</p> </div> <div class="paragraph"> <p>The embedded engine is configured to invoke the <code>handleDbChangeEvent()</code> method for each received data change event. In this method it first is checked whether the incoming event originates from the <code>item</code> table. If that&#8217;s the case, and if the change event represents an <code>UPDATE</code> or <code>DELETE</code> statement, the affected <code>Item</code> instance is evicted from the second-level cache. JPA 2.0 provides a <a href="https://javaee.github.io/javaee-spec/javadocs/index.html?javax/persistence/Cache.html">simple API</a> for this purpose which is accessible via the <code>EntityManagerFactory</code>.</p> </div> <div class="paragraph"> <p>With the <code>DatabaseChangeEventListener</code> class in place, the cache entry will now automatically be evicted when doing another item update via <em>psql</em>. When placing the first purchase order for that item after the update, you&#8217;ll see in the application log how Hibernate ORM executes a query <code>SELECT ... FROM item ...</code> in order to load the item referenced by the order. Also the cache statistics will report one "L2C miss". Upon subsequent orders of that same item it will be obtained from the cache again.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Eventual Consistency</div> <div class="paragraph"> <p>While the event handling happens in near-realtime, it&#8217;s important to point out that it still applies eventual consistency semantics. This means that there is a very short time window between the point in time where a transaction is committed and the point in time where the change event is streamed from the log to the event handler and the cache entry is invalidated.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="avoiding_cache_invalidations_after_application_triggered_data_changes"><a class="anchor" href="#avoiding_cache_invalidations_after_application_triggered_data_changes"></a>Avoiding Cache Invalidations After Application-triggered Data Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>The change event listener shown above satisfies the requirement of invalidating cached items after external data changes. But in its current form it is evicting cache items a bit too aggressively: cached items will also be purged when updating an <code>Item</code> instance through the application itself. This is not only not needed (as the cached item already is the current version), but it&#8217;s even counter-productive: the superfluous cache evictions will cause additional database roundtrips, resulting in longer response times.</p> </div> <div class="paragraph"> <p>It is therefore necessary to distinguish between data changes performed by the application itself and external data changes. Only in the latter case the affected items should be evicted from the cache. In order to do so, you can leverage the fact that each Debezium data change event contains the id of the originating transaction. Keeping track of all transactions run by the application itself allows to trigger the cache eviction only for those items altered by external transactions.</p> </div> <div class="paragraph"> <p>Accounting for this change, the overall architecture looks like so:</p> </div> <div class="imageblock centered-image"> <img src="/images/cache_invalidation_architecture_tx_registry.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Architecture Overview with Transaction Registry"> </div> <div class="paragraph"> <p>The first thing to implement is the transaction registry, i.e. a class for the transaction book keeping:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class KnownTransactions {&#x000A;&#x000A;    private final DefaultCacheManager cacheManager;&#x000A;    private final Cache&lt;Long, Boolean&gt; applicationTransactions;&#x000A;&#x000A;    public KnownTransactions() {&#x000A;        cacheManager = new DefaultCacheManager();&#x000A;        cacheManager.defineConfiguration(&#x000A;                "tx-id-cache",&#x000A;                new ConfigurationBuilder()&#x000A;                    .expiration()&#x000A;                        .lifespan(60, TimeUnit.SECONDS)&#x000A;                    .build()&#x000A;                );&#x000A;&#x000A;        applicationTransactions = cacheManager.getCache("tx-id-cache");&#x000A;    }&#x000A;&#x000A;    @PreDestroy&#x000A;    public void stopCacheManager() {&#x000A;        cacheManager.stop();&#x000A;    }&#x000A;&#x000A;    public void register(long txId) {&#x000A;        applicationTransactions.put(txId, true);&#x000A;    }&#x000A;&#x000A;    public boolean isKnown(long txId) {&#x000A;        return Boolean.TRUE.equals(applicationTransactions.get(txId));&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>This uses the Infinispan <code>DefaultCacheManager</code> for creating and maintaining an in-memory cache of transaction ids encountered by the application. As data change events arrive in near-realtime, the TTL of the cache entries can be rather short (in fact, the value of one minute shown in the example is chosen very conservatively, usually events should be received within seconds).</p> </div> <div class="paragraph"> <p>The next step is to retrieve the current transaction id whenever a request is processed by the application and register it within <code>KnownTransactions</code>. This should happen once per transaction. There are multiple ways for implementing this logic; in the following a Hibernate ORM <code>FlushEventListener</code> is used for this purpose:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">class TransactionRegistrationListener implements FlushEventListener {&#x000A;&#x000A;    private volatile KnownTransactions knownTransactions;&#x000A;&#x000A;    public TransactionRegistrationListener() {&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public void onFlush(FlushEvent event) throws HibernateException {&#x000A;        event.getSession().getActionQueue().registerProcess( session -&gt; {&#x000A;            Number txId = (Number) event.getSession().createNativeQuery("SELECT txid_current()")&#x000A;                    .setFlushMode(FlushMode.MANUAL)&#x000A;                    .getSingleResult();&#x000A;&#x000A;            getKnownTransactions().register(txId.longValue());&#x000A;        } );&#x000A;    }&#x000A;&#x000A;    private  KnownTransactions getKnownTransactions() {&#x000A;        KnownTransactions value = knownTransactions;&#x000A;&#x000A;        if (value == null) {&#x000A;            knownTransactions = value = CDI.current().select(KnownTransactions.class).get();&#x000A;        }&#x000A;&#x000A;        return value;&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>As there&#8217;s no portable way to obtain the transaction id, this is done using a native SQL query. In the case of Postgres, the <code>txid_current()</code> function can be called for that. Hibernate ORM event listeners are not subject to dependency injection via CDI. Hence the static <code>current()</code> method is used to obtain a handle to the application&#8217;s CDI container and get a reference to the <code>KnownTransactions</code> bean.</p> </div> <div class="paragraph"> <p>This listener will be invoked whenever Hibernate ORM is synchronizing its persistence context with the database ("flushing"), which usually happens exactly once when the transaction is committed.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="title">Manual Flushes</div> <div class="paragraph"> <p>The session / entity manager can also be flushed manually, in which case the <code>txid_current()</code> function would be invoked multiple times. That&#8217;s neglected here for the sake of simplicity. The actual code in the example repo contains a slightly extended version of this class which makes sure that the transaction id is obtained only once.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>To register the flush listener with Hibernate ORM, an <code>Integrator</code> implementation must be created and declared in the <em>META-INF/services/org.hibernate.integrator.spi.Integrator</em> file:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">public class TransactionRegistrationIntegrator implements Integrator {&#x000A;&#x000A;    @Override&#x000A;    public void integrate(Metadata metadata, SessionFactoryImplementor sessionFactory,&#x000A;            SessionFactoryServiceRegistry serviceRegistry) {&#x000A;        serviceRegistry.getService(EventListenerRegistry.class)&#x000A;            .appendListeners(EventType.FLUSH, new TransactionRegistrationListener());&#x000A;    }&#x000A;&#x000A;    @Override&#x000A;    public void disintegrate(SessionFactoryImplementor sessionFactory,&#x000A;            SessionFactoryServiceRegistry serviceRegistry) {&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code>io.debezium.examples.cacheinvalidation.persistence.TransactionRegistrationIntegrator</code></pre> </div> </div> <div class="paragraph"> <p>During bootstrap, Hibernate ORM will detect the integrator class (by means of the <a href="https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html">Java service loader</a>), invoke its <code>integrate()</code> method which in turn will register the listener class for the <code>FLUSH</code> event.</p> </div> <div class="paragraph"> <p>The last step is to exclude any events stemming from transactions run by the application itself in the database change event handler:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@ApplicationScoped&#x000A;public class DatabaseChangeEventListener {&#x000A;&#x000A;    // ...&#x000A;&#x000A;    @Inject&#x000A;    private KnownTransactions knownTransactions;&#x000A;&#x000A;    private void handleDbChangeEvent(SourceRecord record) {&#x000A;        if (record.topic().equals("dbserver1.public.item")) {&#x000A;            Long itemId = ((Struct) record.key()).getInt64("id");&#x000A;            Struct payload = (Struct) record.value();&#x000A;            Operation op = Operation.forCode(payload.getString("op"));&#x000A;            Long txId = ((Struct) payload.get("source")).getInt64("txId");&#x000A;&#x000A;            if (!knownTransactions.isKnown(txId) &amp;&amp;&#x000A;                    (op == Operation.UPDATE || op == Operation.DELETE)) {&#x000A;                emf.getCache().evict(Item.class, itemId);&#x000A;            }&#x000A;        }&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>And with that, you got all the pieces in place: cached <code>Item</code>s will only be evicted after external data changes, but not after changes done by the application itself. To confirm, you can invoke the example&#8217;s <code>items</code> resource using curl:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-bash" data-lang="bash">&gt; curl -H "Content-Type: application/json" \&#x000A;  -X PUT \&#x000A;  --data '{ "description" : "North by Northwest", "price" : 20.99}' \&#x000A;  http://localhost:8080/cache-invalidation/rest/items/10003</code></pre> </div> </div> <div class="paragraph"> <p>When placing the next order for the item after this update, you should see that the <code>Item</code> entity is obtained from the cache, i.e. the change event will not have caused the item&#8217;s cache entry to be evicted. In contrast, if you update the item&#8217;s price via <em>psql</em> another time, the item should be removed from the cache and the order request will produce a cache miss, followed by a <code>SELECT</code> against the <code>item</code> table in the database.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we&#8217;ve explored how Debezium and change data capture can be employed to invalidate application-level caches after external data changes. Compared to manual cache invalidation, this approach works very reliably (by capturing changes directly from the database log, no events will be missed) and fast (cache eviction happens in near-realtime after the data changes).</p> </div> <div class="paragraph"> <p>As you have seen, not too much glue code is needed in order to implement this. While the shown implementation is somewhat specific to the entities of the example, it should be possible to implement the change event handler in a more generic fashion, so that it can handle a set of configured entity types (essentially, the database change listener would have to convert the primary key field(s) from the change events into the primary key type of the corresponding entities in a generic way). Also such generic implementation would have to provide the logic for obtaining the current transaction id for the most commonly used databases.</p> </div> <div class="paragraph"> <p>Please let us know whether you think this would be an interesting extension to have for Debezium and Hibernate ORM. For instance this could be a new module under the Debezium umbrella, and it could also be a very great project to work on, should you be interested in contributing to Debezium. If you got any thoughts on this idea, please post a comment below or come to our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Many thanks to Guillaume Smet, Hans-Peter Grahsl and Jiri Pechanec for their feedback while writing this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/">Materializing Aggregate Views With Hibernate and Debezium</a> </h1> <div class="byline"> <p> <em> September 20, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Updating external full text search indexes (e.g. <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>) after data changes is a very popular use case for change data capture (CDC).</p> </div> <div class="paragraph"> <p>As we&#8217;ve discussed in a <a href="/blog/2018/01/17/streaming-to-elasticsearch/">blog post</a> a while ago, the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s <a href="https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html">sink connector for Elasticsearch</a> makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time. This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch, which is perfectly fine for many use cases.</p> </div> <div class="paragraph"> <p>It gets more challenging though if you&#8217;d like to put entire aggregates into a single index. An example could be a customer and all their addresses; those would typically be stored in two separate tables in an RDBMS, linked by a foreign key, whereas you&#8217;d like to have just one index in Elasticsearch, containing documents of customers with their addresses embedded, allowing you to efficiently search for customers based on their address.</p> </div> <div class="paragraph"> <p>Following up to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based solution</a> to this we described recently, we&#8217;d like to present in this post an alternative for materializing such aggregate views driven by the application layer.</p> </div> </div> </div> <div class="sect1"> <h2 id="overview"><a class="anchor" href="#overview"></a>Overview</h2> <div class="sectionbody"> <div class="paragraph"> <p>The idea is to materialize views in a separate table in the source database, right in the moment the original data is altered.</p> </div> <div class="paragraph"> <p>Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table. This is done within the actual transaction altering the data, which means the aggregate view is always consistent with the primary data. In particular this approach isn&#8217;t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.</p> </div> <div class="paragraph"> <p>The following picture shows the overall architecture:</p> </div> <img src="/images/jpa_aggregations.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Streaming Materialized Aggregate Views to Elastisearch"> <div class="paragraph"> <p>Here the aggregate views are materialized by means of a small extension to <a href="http://hibernate.org/orm/">Hibernate ORM</a>, which stores the JSON aggregates within the source database (note "aggregate views" can be considered conceptually the same as "materialized views" as known from different RDBMS, as in that they materialize the result of a "join" operation, but technically we&#8217;re not using the latter to store aggregate views, but a regular table). Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type. The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.</p> </div> <div class="paragraph"> <p>You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. Of course the general idea isn&#8217;t limited to Hibernate ORM or JPA, you could implement something similar with any other API you&#8217;re using to access your data.</p> </div> </div> </div> <div class="sect1"> <h2 id="creating_aggregate_views_via_hibernate_orm"><a class="anchor" href="#creating_aggregate_views_via_hibernate_orm"></a>Creating Aggregate Views via Hibernate ORM</h2> <div class="sectionbody"> <div class="paragraph"> <p>For the following let&#8217;s assume we&#8217;re persisting a simple domain model (comprising a <code>Customer</code> entity and a few related ones like <code>Address</code>, (customer) <code>Category</code> etc.) in a database. Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a <a href="http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events">Hibernate event listener</a>. Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath, from where it will be picked up automatically when bootstrapping the entity manager / session factory.</p> </div> <div class="paragraph"> <p>Our example listener reacts to an annotation, <code>@MaterializeAggregate</code>, which marks those entity types that should be the roots of materialized aggregates.</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;@MaterializeAggregate(aggregateName="customers-complete")&#x000A;public class Customer {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    private String firstName;&#x000A;&#x000A;    @OneToMany(mappedBy = "customer", fetch = FetchType.EAGER, cascade = CascadeType.ALL)&#x000A;    private Set&lt;Address&gt; addresses;&#x000A;&#x000A;    @ManyToOne&#x000A;    private Category category;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Now if any entity annotated with <code>@MaterializeAggregate</code> is inserted, updated or deleted via Hibernate, the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).</p> </div> <div class="paragraph"> <p>Under the hood the <a href="https://github.com/FasterXML/jackson">Jackson API</a> is used for serializing the model into JSON. This means you can use any of its annotations to customize the JSON output, e.g. <code>@JsonIgnore</code> to exclude the inverse relationship from <code>Address</code> to <code>Customer</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">@Entity&#x000A;public class Address {&#x000A;&#x000A;    @Id&#x000A;    private long id;&#x000A;&#x000A;    @ManyToOne&#x000A;    @JoinColumn(name = "customer_id")&#x000A;    @JsonIgnore&#x000A;    private Customer customer;&#x000A;&#x000A;    private String street;&#x000A;&#x000A;    private String city;&#x000A;&#x000A;    ...&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Note that <code>Address</code> itself isn&#8217;t marked with <code>@MaterializeAggregate</code>, i.e. it won&#8217;t be materialized into an aggregate view by itself.</p> </div> <div class="paragraph"> <p>After using JPA&#8217;s <code>EntityManager</code> to insert or update a few customers, let&#8217;s take a look at the <code>aggregates</code> table which has been populated by the listener (value schema omitted for the sake of brevity):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-sql" data-lang="sql">&gt; select * from aggregates;&#x000A;&#x000A;| rootType | keySchema | rootId | materialization | valueSchema |&#x000A;&#x000A;| customers-complete&#x000A;&#x000A;| {&#x000A;  "schema" : {&#x000A;    "type" : "struct",&#x000A;    "fields" : [ {&#x000A;      "type" : "int64",&#x000A;      "optional" : false,&#x000A;      "field" : "id"&#x000A;    } ],&#x000A;    "optional" : false,&#x000A;    "name" : "customers-complete.Key"&#x000A;  }&#x000A;}&#x000A;&#x000A;| { "id" : 1004 }&#x000A;&#x000A;| { "schema" : { ... } }&#x000A;&#x000A;| {&#x000A;  "id" : 1004,&#x000A;  "firstName" : "Anne",&#x000A;  "lastName" : "Kretchmar",&#x000A;  "email" : "annek@noanswer.org",&#x000A;  "tags" : [ "long-term", "vip" ],&#x000A;  "birthday" : 5098,&#x000A;  "category" : {&#x000A;    "id" : 100001,&#x000A;    "name" : "Retail"&#x000A;  },&#x000A;  "addresses" : [ {&#x000A;    "id" : 16,&#x000A;    "street" : "1289 University Hill Road",&#x000A;    "city" : "Canehill",&#x000A;    "state" : "Arkansas",&#x000A;    "zip" : "72717",&#x000A;    "type" : "SHIPPING"&#x000A;  } ]&#x000A;} |</code></pre> </div> </div> <div class="paragraph"> <p>The table contains these columns:</p> </div> <div class="ulist"> <ul> <li> <p><code>rootType</code>: The name of the aggregate as given in the <code>@MaterializeAggregate</code> annotation</p> </li> <li> <p><code>rootId</code>: The aggregate&#8217;s id as serialized JSON</p> </li> <li> <p><code>materialization</code>: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.</p> </li> <li> <p><code>keySchema</code>: The Kafka Connect schema of the row&#8217;s key</p> </li> <li> <p><code>valueSchema</code>: The Kafka Connect schema of the materialization</p> </li> </ul> </div> <div class="paragraph"> <p>Let&#8217;s talk about the two schema columns for a bit. JSON itself is quite limited as far as its supported data types are concerned. So for instance we&#8217;d loose information about a numeric field&#8217;s value range (int vs. long etc.) without any additional information. Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.</p> </div> <div class="paragraph"> <p>Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes. Hence the example implementation provides custom serializers for Jackson&#8217;s schema system, which allow us to emit Kafka Connect&#8217;s schema representation (with more precise type information) instead of plain JSON Schema. This will come in handy in the following when we&#8217;d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.</p> </div> </div> </div> <div class="sect1"> <h2 id="capturing_changes_to_the_aggregate_table"><a class="anchor" href="#capturing_changes_to_the_aggregate_table"></a>Capturing Changes to the Aggregate Table</h2> <div class="sectionbody"> <div class="paragraph"> <p>We now have a mechanism in place which transparently persists aggregates into a separate table within the source database, whenever the application data is changed through Hibernate. Note that this happens within the boundaries of the source transaction, so if the same would be rolled back for some reason, also the aggregate view would not be updated.</p> </div> <div class="paragraph"> <p>The Hibernate listener uses insert-or-update semantics when writing an aggregate view, i.e. for a given aggregate root there&#8217;ll always be exactly one corresponding entry in the aggregate table which reflects its current state. If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.</p> </div> <div class="paragraph"> <p>So let&#8217;s set up Debezium now to capture any changes to the <code>aggregates</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "inventory-connector",&#x000A;      "config": {&#x000A;          "connector.class": "io.debezium.connector.mysql.MySqlConnector",&#x000A;          "tasks.max": "1",&#x000A;          "database.hostname": "mysql",&#x000A;          "database.port": "3306",&#x000A;          "database.user": "debezium",&#x000A;          "database.password": "dbz",&#x000A;          "database.server.id": "184054",&#x000A;          "database.server.name": "dbserver1",&#x000A;          "database.whitelist": "inventory",&#x000A;          "table.whitelist": ".*aggregates",&#x000A;          "database.history.kafka.bootstrap.servers": "kafka:9092",&#x000A;          "database.history.kafka.topic": "schema-changes.inventory"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This registers the MySQL connector with the "inventory" database (we&#8217;re using an expanded version of the schema from the <a href="/docs/tutorial/">Debezium tutorial</a>), capturing any changes to the "aggregates" table.</p> </div> </div> </div> <div class="sect1"> <h2 id="expanding_json"><a class="anchor" href="#expanding_json"></a>Expanding JSON</h2> <div class="sectionbody"> <div class="paragraph"> <p>If we now were to browse the corresponding Kafka topic, we&#8217;d see data change events in the known Debezium format for all the changes to the <code>aggregates</code> table.</p> </div> <div class="paragraph"> <p>The "materialization" field with the records' "after" state still is a single field containing a JSON string, though. What we&#8217;d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields. For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding <code>valueSchema</code> and converts this into a full-blown Kafka Connect record. The same is done for keys. DELETE events are rewritten into tombstone events. Finally, the SMT re-routes every record to a topic named after the aggregate root, allowing consumers to subscribe just to changes to specific aggregate types.</p> </div> <div class="paragraph"> <p>So let&#8217;s add that SMT when registering the Debezium CDC connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;"transforms":"expandjson",&#x000A;"transforms.expandjson.type":"io.debezium.aggregation.smt.ExpandJsonSmt",&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>When now browsing the "customers-complete" topic, we&#8217;ll see the strongly typed Kafka Connect records we&#8217;d expect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [&#x000A;            {&#x000A;                "type": "int64",&#x000A;                "optional": false,&#x000A;                "field": "id"&#x000A;            }&#x000A;        ],&#x000A;        "optional": false,&#x000A;        "name": "customers-complete.Key"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004&#x000A;    }&#x000A;}&#x000A;{&#x000A;    "schema": {&#x000A;        "type": "struct",&#x000A;        "fields": [ ... ],&#x000A;        "optional": true,&#x000A;        "name": "urn:jsonschema:com:example:domain:Customer"&#x000A;    },&#x000A;    "payload": {&#x000A;        "id": 1004,&#x000A;        "firstName": "Anne",&#x000A;        "lastName": "Kretchmar",&#x000A;        "email": "annek@noanswer.org",&#x000A;        "active": true,&#x000A;        "tags" : [ "long-term", "vip" ],&#x000A;        "birthday" : 5098,&#x000A;        "category": {&#x000A;            "id": 100001,&#x000A;            "name": "Retail"&#x000A;        },&#x000A;        "addresses": [&#x000A;            {&#x000A;                "id": 16,&#x000A;                "street": "1289 University Hill Road",&#x000A;                "city": "Canehill",&#x000A;                "state": "Arkansas",&#x000A;                "zip": "72717",&#x000A;                "type": "LIVING"&#x000A;            }&#x000A;        ]&#x000A;    }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>To confirm that these are actual typed Kafka Connect records and not just a single JSON string field, you could for instance use the <a href="/docs/configuration/avro/">Avro message converter</a> and examine the message schemas in the schema registry.</p> </div> </div> </div> <div class="sect1"> <h2 id="sinking_aggregate_messages_into_elasticsearch"><a class="anchor" href="#sinking_aggregate_messages_into_elasticsearch"></a>Sinking Aggregate Messages Into Elasticsearch</h2> <div class="sectionbody"> <div class="paragraph"> <p>The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the "customers-complete" topic and letting it push any changes to the corresponding index:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">curl -i -X POST \&#x000A;  -H "Accept:application/json" \&#x000A;  -H "Content-Type:application/json" \&#x000A;  http://localhost:8083/connectors/ -d @- &lt;&lt;-EOF&#x000A;  {&#x000A;      "name": "es-customers",&#x000A;      "config": {&#x000A;          "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",&#x000A;          "tasks.max": "1",&#x000A;          "topics": "customers-complete",&#x000A;          "connection.url": "http://elastic:9200",&#x000A;          "key.ignore": "false",&#x000A;          "schema.ignore" : "false",&#x000A;          "behavior.on.null.values" : "delete",&#x000A;          "type.name": "customer-with-addresses",&#x000A;          "transforms" : "key",&#x000A;          "transforms.key.type": "org.apache.kafka.connect.transforms.ExtractField$Key",&#x000A;          "transforms.key.field": "id"&#x000A;      }&#x000A;  }&#x000A;EOF</code></pre> </div> </div> <div class="paragraph"> <p>This uses Connect&#8217;s <code>ExtractField</code> transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents. Specifying the "behavior.on.null.values" option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).</p> </div> <div class="paragraph"> <p>Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">&gt; curl -X GET -H "Accept:application/json" \&#x000A;  http://localhost:9200/customers-complete/_search?pretty&#x000A;&#x000A;  {&#x000A;      "_shards": {&#x000A;          "failed": 0,&#x000A;          "successful": 5,&#x000A;          "total": 5&#x000A;      },&#x000A;      "hits": {&#x000A;          "hits": [&#x000A;              {&#x000A;                  "_id": "1004",&#x000A;                  "_index": "customers-complete",&#x000A;                  "_score": 1.0,&#x000A;                  "_source": {&#x000A;                      "active": true,&#x000A;                      "addresses": [&#x000A;                          {&#x000A;                              "city": "Canehill",&#x000A;                              "id": 16,&#x000A;                              "state": "Arkansas",&#x000A;                              "street": "1289 University Hill Road",&#x000A;                              "type": "LIVING",&#x000A;                              "zip": "72717"&#x000A;                          }&#x000A;                      ],&#x000A;                      "tags" : [ "long-term", "vip" ],&#x000A;                      "birthday" : 5098,&#x000A;                      "category": {&#x000A;                          "id": 100001,&#x000A;                          "name": "Retail"&#x000A;                      },&#x000A;                      "email": "annek@noanswer.org",&#x000A;                      "firstName": "Anne",&#x000A;                      "id": 1004,&#x000A;                      "lastName": "Kretchmar",&#x000A;                      "scores": [],&#x000A;                      "someBlob": null,&#x000A;                      "tags": []&#x000A;                  },&#x000A;                  "_type": "customer-with-addresses"&#x000A;              }&#x000A;          ],&#x000A;          "max_score": 1.0,&#x000A;          "total": 1&#x000A;      },&#x000A;      "timed_out": false,&#x000A;      "took": 11&#x000A;  }</code></pre> </div> </div> <div class="paragraph"> <p>And there you have it: a customer&#8217;s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch. If you&#8217;re using JPA to update the customer, you&#8217;ll see the data in the index being updated accordingly in near-realtime.</p> </div> </div> </div> <div class="sect1"> <h2 id="pros_and_cons"><a class="anchor" href="#pros_and_cons"></a>Pros and Cons</h2> <div class="sectionbody"> <div class="paragraph"> <p>So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the <a href="/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/">KStreams-based approach</a>?</p> </div> <div class="paragraph"> <p>The big advantage is consistency and awareness of transactional boundaries, whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates. For instance, if you&#8217;re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses. This not the case for the approach discussed here, as you&#8217;ll only ever stream complete aggregates to Kafka. Also this approach feels a bit more "light-weight", i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model, whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.</p> </div> <div class="paragraph"> <p>The downside of driving aggregations through the application layer is that it&#8217;s not fully agnostic to the way you access the primary data. If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates. Although this again could be done through change data capture and Debezium: change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes. You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead. This often may be acceptable, though.</p> </div> <div class="paragraph"> <p>Another question to ask is what&#8217;s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch. The answer is the highly increased robustness and fault tolerance. If the Elasticsearch cluster can&#8217;t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again. Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.</p> </div> <div class="paragraph"> <p>Note that while we&#8217;ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records. One example would be MongoDB and the <a href="https://github.com/hpgrahsl/kafka-connect-mongodb">sink connector</a> maintained by Hans-Peter Grahsl, which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook"><a class="anchor" href="#outlook"></a>Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Hibernate ORM extension as well as the SMT discussed in this post can be found in our <a href="https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations">examples repository</a>. They should be considered to be at "proof-of-concept" level currently.</p> </div> <div class="paragraph"> <p>That being said, we&#8217;re considering to make this a Debezium component proper, allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component. For that we&#8217;d have to improve a few things first, though. Most importantly, an API is needed which will let you (re-)create aggregates on demand, e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners). Also aggregates should be re-created automatically, if any of the referenced entities change (with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).</p> </div> <div class="paragraph"> <p>If you like this idea, then let us know about it, so we can gauge the general interest in this. Also, this would be a great item to work on, if you&#8217;re interested in contributing to the Debezium project. Looking forward to hearing from you, e.g. in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> <div class="paragraph"> <p>Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/">Streaming MySQL Data Changes to Amazon Kinesis</a> </h1> <div class="byline"> <p> <em> August 30, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> <a class="label label-info" href="/blog/tags/examples/">examples</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Most of the times Debezium is used to stream data changes into <a href="http://kafka.apache.org/">Apache Kafka</a>. What though if you&#8217;re using another streaming platform such as <a href="https://pulsar.incubator.apache.org/">Apache Pulsar</a> or a cloud-based solution such as <a href="https://aws.amazon.com/kinesis/">Amazon Kinesis</a>, <a href="https://azure.microsoft.com/services/event-hubs/">Azure Event Hubs</a> and the like? Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?</p> </div> <div class="paragraph"> <p>Turns out, with just a bit of glue code, you can! In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis, a fully-managed data streaming service available on the Amazon cloud.</p> </div> </div> </div> <div class="sect1"> <h2 id="introducing_the_debezium_embedded_engine"><a class="anchor" href="#introducing_the_debezium_embedded_engine"></a>Introducing the Debezium Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is implemented as a set of connectors for Kafka and thus usually is run via <a href="https://kafka.apache.org/documentation/#connect">Kafka Connect</a>. But there&#8217;s one little gem in Debezium which isn&#8217;t as widely known yet, which is the <a href="/docs/embedded/">embedded engine</a>.</p> </div> <div class="paragraph"> <p>When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application. For this purpose, the <em>debezium-embedded</em> module provides a small runtime environment which performs the tasks that&#8217;d otherwise be handled by the Kafka Connect framework: requesting change records from the connector, committing offsets etc. Each change record produced by the connector is passed to a configured event handler method, which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.</p> </div> <div class="paragraph"> <p>The overall architecture looks like so:</p> </div> <img src="/images/debezium-embedded.png" style="max-width:100%; margin-bottom:10px;" class="responsive-image" alt="Debezium Embedded Engine Streaming to Amazon Kinesis"> <div class="paragraph"> <p>Now let&#8217;s walk through the relevant parts of the code required for that. A complete executable example can be found in the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis">debezium-examples</a> repo on GitHub.</p> </div> </div> </div> <div class="sect1"> <h2 id="set_up"><a class="anchor" href="#set_up"></a>Set-Up</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to use Debezium&#8217;s embedded engine, add the <em>debezium-embedded</em> dependency as well as the Debezium connector of your choice to your project&#8217;s <em>pom.xml</em>. In the following we&#8217;re going to use the connector for MySQL. We also need to add a dependency to the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html">Kinesis Client API</a>, so these are the dependencies needed:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-xml" data-lang="xml">...&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-embedded&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;io.debezium&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;debezium-connector-mysql&lt;/artifactId&gt;&#x000A;    &lt;version&gt;0.8.3.Final&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;&lt;dependency&gt;&#x000A;    &lt;groupId&gt;com.amazonaws&lt;/groupId&gt;&#x000A;    &lt;artifactId&gt;amazon-kinesis-client&lt;/artifactId&gt;&#x000A;    &lt;version&gt;1.9.0&lt;/version&gt;&#x000A;&lt;/dependency&gt;&#x000A;...</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="configuring_the_embedded_engine"><a class="anchor" href="#configuring_the_embedded_engine"></a>Configuring the Embedded Engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium embedded engine is configured through an instance of <code>io.debezium.config.Configuration</code>. This class can obtain values from system properties or from a given config file, but for the sake of the example we&#8217;ll simply pass all required values via its fluent builder API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">Configuration config = Configuration.create()&#x000A;    .with(EmbeddedEngine.CONNECTOR_CLASS, "io.debezium.connector.mysql.MySqlConnector")&#x000A;    .with(EmbeddedEngine.ENGINE_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_NAME, "kinesis")&#x000A;    .with(MySqlConnectorConfig.SERVER_ID, 8192)&#x000A;    .with(MySqlConnectorConfig.HOSTNAME, "localhost")&#x000A;    .with(MySqlConnectorConfig.PORT, 3306)&#x000A;    .with(MySqlConnectorConfig.USER, "debezium")&#x000A;    .with(MySqlConnectorConfig.PASSWORD, "dbz")&#x000A;    .with(MySqlConnectorConfig.DATABASE_WHITELIST, "inventory")&#x000A;    .with(MySqlConnectorConfig.TABLE_WHITELIST, "inventory.customers")&#x000A;    .with(EmbeddedEngine.OFFSET_STORAGE,&#x000A;        "org.apache.kafka.connect.storage.MemoryOffsetBackingStore")&#x000A;    .with(MySqlConnectorConfig.DATABASE_HISTORY,&#x000A;        MemoryDatabaseHistory.class.getName())&#x000A;    .with("schemas.enable", false)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>If you&#8217;ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.</p> </div> <div class="paragraph"> <p>But let&#8217;s talk about the <code>OFFSET_STORAGE</code> and <code>DATABASE_HISTORY</code> options in a bit more detail. They deal with how connector offsets and the database history should be persisted. When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics. But that&#8217;s not an option here, so an alternative is needed. For this example we&#8217;re simply going to keep the offsets and database history in memory. I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.</p> </div> <div class="paragraph"> <p>While out of scope for this blog post, it wouldn&#8217;t be too difficult to create alternative implementations of the <code>OffsetBackingStore</code> and <code>DatabaseHistory</code> contracts, respectively. For instance if you&#8217;re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store. Note that, different from Kafka, a Kinesis stream wouldn&#8217;t be suitable for storing the database history. The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector. Another alternative could be to use the existing filesystem based implementations <code>FileOffsetBackingStore</code> and <code>FileDatabaseHistory</code>, respectively.</p> </div> <div class="paragraph"> <p>The next step is to build an <code>EmbeddedEngine</code> instance from the configuration. Again this is done using a fluent API:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">EmbeddedEngine engine = EmbeddedEngine.create()&#x000A;    .using(config)&#x000A;    .using(this.getClass().getClassLoader())&#x000A;    .using(Clock.SYSTEM)&#x000A;    .notifying(this::sendRecord)&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>The most interesting part here is the <code>notifying</code> call. The method passed here is the one which will be invoked by the engine for each emitted data change record. So let&#8217;s take a look at the implementation of this method.</p> </div> </div> </div> <div class="sect1"> <h2 id="sending_change_records_to_kinesis"><a class="anchor" href="#sending_change_records_to_kinesis"></a>Sending Change Records to Kinesis</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>sendRecord()</code> method is where the magic happens. We&#8217;ll convert the incoming <code>SourceRecord</code> into an equivalent JSON representation and propagate it to a Kinesis stream.</p> </div> <div class="paragraph"> <p>For that, it&#8217;s important to understand some conceptual differences between Apache Kafka and Kinesis. Specifically, messages in Kafka have a <em>key</em> and a <em>value</em> (which both are arbitrary byte arrays). In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.</p> </div> <div class="paragraph"> <p>In Kinesis on the other hand a message contains a <em>data blob</em> (again an arbitrary byte sequence) and a <em>partition key</em>. Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.</p> </div> <div class="paragraph"> <p>Now one could think of mapping the key from Debezium&#8217;s change data events to the Kinesis partition key, but partition keys are limited to a length of 256 bytes. Depending on the length of primary key column(s) in the captured tables, this might not be enough. So a safer option is to create a hash value from the change message key and use that as the partition key. This in turn means that the change message key structure should be added next to the actual value to the Kinesis message&#8217;s data blob. While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn&#8217;t know which column(s) make up the primary key.</p> </div> <div class="paragraph"> <p>With that in mind, let&#8217;s take a look at the <code>sendRecord()</code> implementation:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">private void sendRecord(SourceRecord record) {&#x000A;    // We are interested only in data events not schema change events&#x000A;    if (record.topic().equals("kinesis")) {&#x000A;        return;&#x000A;    }&#x000A;&#x000A;    // create schema for container with key *and* value&#x000A;    Schema schema = SchemaBuilder.struct()&#x000A;        .field("key", record.keySchema())&#x000A;        .field("value", record.valueSchema())&#x000A;        .build();&#x000A;&#x000A;    Struct message = new Struct(schema);&#x000A;    message.put("key", record.key());&#x000A;    message.put("value", record.value());&#x000A;&#x000A;    // create partition key by hashing the record's key&#x000A;    String partitionKey = String.valueOf(&#x000A;        record.key() != null ? record.key().hashCode() : -1);&#x000A;&#x000A;    // create data blob representing the container by using Kafka Connect's&#x000A;    // JSON converter&#x000A;    final byte[] payload = valueConverter.fromConnectData(&#x000A;        "dummy", schema, message);&#x000A;&#x000A;    // Assemble the put-record request ...&#x000A;    PutRecordRequest putRecord = new PutRecordRequest();&#x000A;&#x000A;    putRecord.setStreamName(record.topic());&#x000A;    putRecord.setPartitionKey(partitionKey);&#x000A;    putRecord.setData(ByteBuffer.wrap(payload));&#x000A;&#x000A;    // ... and execute it&#x000A;    kinesisClient.putRecord(putRecord);&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The code is quite straight-forward; as discussed above it&#8217;s first creating a container structure containing key <em>and</em> value of the incoming source record. This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of <code>JsonConverter</code>). Then a <code>PutRecordRequest</code> is assembled from that blob, the partition key and the change record&#8217;s topic name, which finally is sent to Kinesis.</p> </div> <div class="paragraph"> <p>The Kinesis client object can be re-used and is set up once like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">// Uses the credentials from the local "default" AWS profile&#x000A;AWSCredentialsProvider credentialsProvider =&#x000A;    new ProfileCredentialsProvider("default");&#x000A;&#x000A;this.kinesisClient = AmazonKinesisClientBuilder.standard()&#x000A;    .withCredentials(credentialsProvider)&#x000A;    .withRegion("eu-central-1") // use your AWS region here&#x000A;    .build();</code></pre> </div> </div> <div class="paragraph"> <p>With that, we&#8217;ve set up an instance of Debezium&#8217;s <code>EmbeddedEngine</code> which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis. The last missing step is to actually run the engine. This is done on a separate thread using an <code>Executor</code>, e.g. like so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-java" data-lang="java">ExecutorService executor = Executors.newSingleThreadExecutor();&#x000A;executor.execute(engine);</code></pre> </div> </div> <div class="paragraph"> <p>Note you also should make sure to properly shut down the engine eventually. How that can be done <a href="https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88">is shown</a> in the accompanying example in the <em>debezium-examples</em> repo.</p> </div> </div> </div> <div class="sect1"> <h2 id="running_the_example"><a class="anchor" href="#running_the_example"></a>Running the Example</h2> <div class="sectionbody"> <div class="paragraph"> <p>Finally let&#8217;s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream. Start by cloning the examples repository and go to the <em>kinesis</em> directory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">git clone https://github.com/debezium/debezium-examples.git&#x000A;cd debezium-examples/kinesis</code></pre> </div> </div> <div class="paragraph"> <p>Make sure you&#8217;ve met the <a href="https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites">prerequisites</a> described in the example&#8217;s <em>README.md</em>; most notably you should have a local Docker installation and you&#8217;ll need to have set up an AWS account as well as have the AWS client tools installed. Note that Kinesis isn&#8217;t part of the free tier when registering with AWS, i.e. you&#8217;ll pay a (small) amount of money when executing the example. Don&#8217;t forget to delete the streams you&#8217;ve set up once done, we won&#8217;t pay your AWS bills :)</p> </div> <div class="paragraph"> <p>Now run Debezium&#8217;s MySQL example database to have some data to play with:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">docker run -it --rm --name mysql -p 3306:3306 \&#x000A;  -e MYSQL_ROOT_PASSWORD=debezium \&#x000A;  -e MYSQL_USER=mysqluser \&#x000A;  -e MYSQL_PASSWORD=mysqlpw \&#x000A;  debezium/example-mysql:0.8</code></pre> </div> </div> <div class="paragraph"> <p>Create a Kinesis stream for change events from the <code>customers</code> table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis create-stream --stream-name kinesis.inventory.customers \&#x000A;  --shard-count 1</code></pre> </div> </div> <div class="paragraph"> <p>Execute the Java application that runs the Debezium embedded engine (if needed, adjust the value of the <code>kinesis.region</code> property in <em>pom.xml</em> to your own region first):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">mvn exec:java</code></pre> </div> </div> <div class="paragraph"> <p>This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.</p> </div> <div class="paragraph"> <p>In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used (usually, you&#8217;d implement a Kinesis Streams application for consuming the events). To do so, set up a <a href="https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators">shard iterator</a> first:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq '.ShardIterator')</code></pre> </div> </div> <div class="paragraph"> <p>Note how the <a href="https://stedolan.github.io/jq/">jq</a> utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API. Next that iterator can be used to examine the stream:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR</code></pre> </div> </div> <div class="paragraph"> <p>You should receive an array of records like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;    "Records": [&#x000A;        {&#x000A;            "SequenceNumber":&#x000A;                "49587760482547027816046765529422807492446419903410339842",&#x000A;            "ApproximateArrivalTimestamp": 1535551896.475,&#x000A;            "Data": "eyJiZWZvcm...4OTI3MzN9",&#x000A;            "PartitionKey": "eyJpZCI6MTAwMX0="&#x000A;        },&#x000A;        ...&#x000A;    ]&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>The <code>Data</code> element is a Base64-encoded representation of the message&#8217;s data blob. Again <em>jq</em> comes in handy: we can use it to just extract the <code>Data</code> part of each record and decode the Base64 representation (make sure to use jq 1.6 or newer):</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell">aws kinesis get-records --shard-iterator $ITERATOR | \&#x000A;  jq -r '.Records[].Data | @base64d' | jq .</code></pre> </div> </div> <div class="paragraph"> <p>Now you should see the change events as JSON, each one with key and value:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": null,&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 0,&#x000A;      "ts_sec": 0,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 154,&#x000A;      "row": 0,&#x000A;      "snapshot": true,&#x000A;      "thread": null,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "c",&#x000A;    "ts_ms": 1535555325628&#x000A;  }&#x000A;}&#x000A;...</code></pre> </div> </div> <div class="paragraph"> <p>Next let&#8217;s try and update a record in MySQL:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-shell" data-lang="shell"># Start MySQL CLI client&#x000A;docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \&#x000A;  sh -c 'exec mysql -h"$MYSQL_PORT_3306_TCP_ADDR" \&#x000A;  -P"$MYSQL_PORT_3306_TCP_PORT" -uroot -p"$MYSQL_ENV_MYSQL_ROOT_PASSWORD"'&#x000A;&#x000A;# In the MySQL client&#x000A;use inventory;&#x000A;update customers set first_name = 'Trudy' where id = 1001;</code></pre> </div> </div> <div class="paragraph"> <p>If you now fetch the iterator again, you should see one more data change event representing that update:</p> </div> <div class="listingblock"> <div class="content"> <pre class="highlight"><code class="language-json" data-lang="json">...&#x000A;&#x000A;{&#x000A;  "key": {&#x000A;    "id": 1001&#x000A;  },&#x000A;  "value": {&#x000A;    "before": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Sally",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "after": {&#x000A;      "id": 1001,&#x000A;      "first_name": "Trudy",&#x000A;      "last_name": "Thomas",&#x000A;      "email": "sally.thomas@acme.com"&#x000A;    },&#x000A;    "source": {&#x000A;      "version": "0.8.1.Final",&#x000A;      "name": "kinesis",&#x000A;      "server_id": 223344,&#x000A;      "ts_sec": 1535627629,&#x000A;      "gtid": null,&#x000A;      "file": "mysql-bin.000003",&#x000A;      "pos": 364,&#x000A;      "row": 0,&#x000A;      "snapshot": false,&#x000A;      "thread": 10,&#x000A;      "db": "inventory",&#x000A;      "table": "customers",&#x000A;      "query": null&#x000A;    },&#x000A;    "op": "u",&#x000A;    "ts_ms": 1535627622546&#x000A;  }&#x000A;}</code></pre> </div> </div> <div class="paragraph"> <p>Once you&#8217;re done, stop the embedded engine application by hitting Ctrl + C, stop the MySQL server by running <code>docker stop mysql</code> and delete the <em>kinesis.inventory.customers</em> stream in Kinesis.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary_and_outlook"><a class="anchor" href="#summary_and_outlook"></a>Summary and Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we&#8217;ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis. Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from <a href="/docs/connectors/">all the CDC connectors</a> provided by Debezium and their capabilities and connect them to the streaming solution of your choice.</p> </div> <div class="paragraph"> <p>And we&#8217;re thinking about even further simplifying this usage of Debezium. Instead of requiring you to implement your own application that invokes the embedded engine API, we&#8217;re considering to provide a small self-contained Debezium runtime which you can simply execute. It&#8217;d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others. Of course such runtime would also provide suitable implementations for safely persisting offsets and database history, and it&#8217;d offer means of monitoring, health checks etc. Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!</p> </div> <div class="paragraph"> <p>If you like this idea, then please check out JIRA issue <a href="https://issues.jboss.org/browse/DBZ-651">DBZ-651</a> and let us know about your thoughts, e.g. by leaving a comment on the issue, in the comment section below or on our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <div class="post"> <h1 class="title"> <a href="/blog/2018/07/19/advantages-of-log-based-change-data-capture/">Five Advantages of Log-Based Change Data Capture</a> </h1> <div class="byline"> <p> <em> July 19, 2018 by Gunnar Morling </em> <em> under&nbsp; </em> <a class="label label-info" href="/blog/tags/discussion/">discussion</a> </p> </div> <div id="preamble"> <div class="sectionbody"> <div class="paragraph"> <p>Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the <a href="https://twitter.com/JUG_DA/status/1019634941020332032">Darmstadt Java User Group</a>. It was a great evening with lots of interesting discussions and questions. One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?</p> </div> <div class="paragraph"> <p>So first of all, what&#8217;s the difference between the two approaches? With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured. Log-based CDC in contrast works by reacting to any changes to the database&#8217;s log files (e.g. MySQL&#8217;s binlog or MongoDB&#8217;s op log).</p> </div> <div class="paragraph"> <p>As this wasn&#8217;t the first time this question came up, I thought I could provide a more extensive answer also here on the blog. That way I&#8217;ll be able to refer to this post in the future, should the question come up again :)</p> </div> <div class="paragraph"> <p>So without further ado, here&#8217;s my list of five advantages of log-based CDC over polling-based approaches.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">All Data Changes Are Captured</dt> <dd> <p>By reading the database&#8217;s log, you get the complete list of all data changes in their exact order of application. This is vital for many use cases where you are interested in the complete history of record changes. In contrast, with a polling-based approach you might miss intermediary data changes that happen between two runs of the poll loop. For instance it could happen that a record is inserted and deleted between two polls, in which case this record would never be captured by poll-based CDC.</p> <div class="paragraph"> <p>Related to this is the aspect of downtimes, e.g. when updating the CDC tool. With poll-based CDC, only the latest state of a given record would be captured once the CDC tool is back online, missing any earlier changes to the record that occurred during the downtime. A log-based CDC tool will be able to resume reading the database log from the point where it left off before it was shut down, causing the complete history of data changes to be captured.</p> </div> </dd> <dt class="hdlist1">Low Delays of Events While Avoiding Increased CPU Load</dt> <dd> <p>With polling, you might be tempted to increase the frequency of polling attempts in order to reduce the chances of missing intermediary updates. While this works to some degree, polling too frequently may cause performance issues (as the queries used for polling cause load on the source database). On the other hand, expanding the polling interval will reduce the CPU load but may not only result in missed change events but also in a longer delay for propagating data changes. Log-based CDC allows you to react to data changes in near real-time without paying the price of spending CPU time on running polling queries repeatedly.</p> </dd> <dt class="hdlist1">No Impact on Data Model</dt> <dd> <p>Polling requires some indicator to identify those records that have been changed since the last poll. So all the captured tables need to have some column like <code>LAST_UPDATE_TIMESTAMP</code> which can be used to find changed rows. This can be fine in some cases, but in others such requirement might not be desirable. Specifically, you&#8217;ll need to make sure that the update timestamps are maintained correctly on all tables to be captured by the writing applications or e.g. through triggers.</p> </dd> <dt class="hdlist1">Can Capture Deletes</dt> <dd> <p>Naturally, polling will not allow you to identify any records that have been deleted since the last poll. Often times that&#8217;s a problem for replication-like use cases where you&#8217;d like to have an identical data set on the source database and the replication targets, meaning you&#8217;d also like to delete records on the sink side if they have been removed in the source database.</p> </dd> <dt class="hdlist1">Can Capture Old Record State And Further Meta Data</dt> <dd> <p>Depending on the source database&#8217;s capabilities, log-based CDC can provide the old record state for update and delete events. Whereas with polling, you&#8217;ll only get the current row state. Having the old row state handy in a single change event can be interesting for many use cases, e.g. if you&#8217;d like to display the complete data change with old and new column values to an application user for auditing purposes.</p> <div class="paragraph"> <p>In addition, log-based approaches often can provide streams of schema changes (e.g. in form of applied DDL statements) and expose additional metadata such as transaction ids or the user applying a certain change. These things may generally be doable with query-based approaches, too (depending on the capabilities of the database), I haven&#8217;t really seen it being done in practice, though.</p> </div> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="summary"><a class="anchor" href="#summary"></a>Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>And that&#8217;s it, five advantages of log-based change data capture. Note that this is not to say that polling-based CDC doesn&#8217;t have its applications. If for instance your use case can be satisfied by propagating changes once per hour and it&#8217;s not a problem to miss intermediary versions of records that were valid in between, it can be perfectly fine.</p> </div> <div class="paragraph"> <p>But if you&#8217;re interested in capturing data changes in near real-time, making sure you don&#8217;t miss any change events (including deletions), then I&#8217;d recommend very much to explore the possibilities of log-based CDC as enabled by Debezium. The Debezium connectors do all the heavy-lifting for you, i.e. you don&#8217;t have to deal with all the low-level specifics of the individual databases and the means of getting changes from their logs. Instead, you can consume the generic and largely unified change data events produced by Debezium.</p> </div> </div> </div> <div class="sect1"> <h2 id="about_debezium"><a class="anchor" href="#about_debezium"></a>About Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is an open source distributed platform that turns your existing databases into event streams, so applications can see and respond almost instantly to each committed row-level change in the databases. Debezium is built on top of <a href="http://kafka.apache.org/">Kafka</a> and provides <a href="http://kafka.apache.org/documentation.html#connect">Kafka Connect</a> compatible connectors that monitor specific database management systems. Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running, ensuring that all events are processed correctly and completely. Debezium is <a href="/license/">open source</a> under the <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, Version 2.0</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="get_involved"><a class="anchor" href="#get_involved"></a>Get involved</h2> <div class="sectionbody"> <div class="paragraph"> <p>We hope you find Debezium interesting and useful, and want to give it a try. Follow us on Twitter <a href="https://twitter.com/debezium">@debezium</a>, <a href="https://gitter.im/debezium/user">chat with us on Gitter</a>, or join our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> to talk with the community. All of the code is open source <a href="https://github.com/debezium/">on GitHub</a>, so build the code locally and help us improve ours existing connectors and add even more connectors. If you find problems or have ideas how we can improve Debezium, please let us know or <a href="https://issues.jboss.org/projects/DBZ/issues/">log an issue</a>.</p> </div> </div> </div> </div> <hr> <ul class="pager"> <li class="previous"> <a href="/blog/tags/discussion/page/2/">&laquo; Older</a> </li> <li class="pages">Page 1 of 2</li> <li class="disabled next"> <a href="#">Newer &raquo;</a> </li> </ul> </div> </div> </div> </div> <footer class="container"> <div class="row"> <div class="col-md-5 col-md-offset-1"> <h4>Debezium</h4> <p> &#169; 2019 Debezium Community <br> <br> <i class="icon-fire"></i> Mixed with <a href="http://twitter.github.com/bootstrap">Bootstrap</a>, baked by <a href="http://awestruct.org">Awestruct</a>. <br> <i class="icon-flag"></i> Website and docs licensed under <a href="http://creativecommons.org/licenses/by/3.0/">CC BY 3.0</a>. <br> <i class="icon-flag-alt"></i> Code released under <a href="http://www.apache.org/licenses/LICENSE-2.0.html">Apache License, v2.0</a>. <br> <i class="icon-file-alt"></i> <a href="https://www.redhat.com/legal/legal_statement.html" title="Terms">Terms</a> | <a href="https://www.redhat.com/legal/privacy_statement.html" title="Privacy Policy">Privacy</a> </p> </div> <div class="col-md-3"> <h4>Documentation</h4> <ul class="list-unstyled"> <li> <a href="/docs/features/" title="Features">Features</a> </li> <li> <a href="/docs/install/" title="Install">Install</a> </li> <li> <a href="/docs/manage/" title="Manage">Manage</a> </li> <li> <a href="/docs/architecture/" title="Architecture">Architecture</a> </li> <li> <a href="/docs/faq/" title="FAQ">FAQ</a> </li> <li> <a href="/docs/contribute/" title="Contribute">Contribute</a> </li> </ul> </div> <div class="col-md-3"> <h4>Connect</h4> <ul class="list-unstyled"> <li> <a href="/blog" title="Blog">Blog</a> </li> <li> <a href="http://twitter.com/debezium" title="Twitter">Twitter</a> </li> <li> <a href="http://github.com/debezium" title="GitHub">GitHub</a> </li> <li> <a href="https://gitter.im/debezium/dev" title="Chat">Chat</a> </li> <li> <a href="https://groups.google.com/forum/#!forum/debezium" title="Google Groups">Google Groups</a> </li> <li> <a href="http://stackoverflow.com/questions/tagged/debezium" title="StackOverflow">StackOverflow</a> </li> </ul> </div> </div> </footer> <div class="container" id="companyfooter"> <div class="redhatlogo"> <div id="logospacer"></div> <a href="https://www.redhat.com/"><img src="https://static.jboss.org/theme/images/common/redhat_logo.png"></a> </div> </div> <span class="backToTop"> <a href="#top">back to top</a> </span> <script src="https://static.jboss.org/theme/js/libs/bootstrap-community/3.2.0.2/bootstrap-community.min.js"></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqCfg.js'></script> <script type='text/javascript' language='JavaScript' src='https://www.redhat.com/j/elqNow/elqImg.js'></script> <div id="oTags"> <script type="text/javascript" src="//www.redhat.com/j/s_code.js"></script> <script type="text/javascript"><!--
        var coreUrl = encodeURI(document.URL.split("?")[0]).replace(/-/g," ");
        var urlSplit = coreUrl.toLowerCase().split(/\//);
        var urlLast = urlSplit[urlSplit.length-1];
        var pageNameString = "";
        var siteName = "";
        var minorSectionIndex = 3
        if (urlLast == "") {
            urlSplit.splice(-1,1);
        }
        if (urlLast.search(/\./) >= 0) {
            if (urlLast == "index.html") {
                urlSplit.splice(-1,1);
            }
            else {
                urlSplit[urlSplit.length-1] = urlLast.split(".").splice(0,1);
            }
        }
        siteName = urlSplit[2].split(".")[1];
        s.prop14 = s.eVar27 = siteName || "";
        s.prop15 = s.eVar28 = urlSplit[minorSectionIndex] || "";
        s.prop16 = s.eVar29 = urlSplit[minorSectionIndex+1] || "";
        pageNameString = urlSplit.splice(3).join(" | ");
        s.pageName = "jboss | community | " + siteName + " | " + pageNameString;
        s.server = "jboss";
        s.channel = "jboss | community";
        s.prop4 = s.eVar23 = encodeURI(document.URL);
        s.prop21 = s.eVar18 = coreUrl;
        s.prop2 = s.eVar22 = "en";
        s.prop3 = s.eVar19 = "us";
        //--></script> <script type="text/javascript" src="//www.redhat.com/j/rh_omni_footer.js"></script> <script language="JavaScript" type="text/javascript"><!--
        if(navigator.appVersion.indexOf('MSIE')>=0)document.write(unescape('%3C')+'\!-'+'-')
        //--></script> <noscript><a href="http://www.omniture.com" title="Web Analytics"><img src="https://smtrcs.redhat.com/b/ss/redhatcom,redhatglobal/1/H.25.4--NS/0?[AQB]&cdp=3&[AQE]" height="1" width="1" border="0" alt=""/></a></noscript> </div> <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
      </script> <script type="text/javascript">
      try {
      var pageTracker = _gat._getTracker("UA-10656779-1");
      pageTracker._trackPageview();
      } catch(err) {}</script> <script>
       (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                          })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
      ga('create', 'UA-76464546-1', 'auto');
      ga('send', 'pageview');
      ga('set', 'anonymizeIp', true);
      ga('require', 'linkid', 'linkid.js');
      
      </script> </div> </body> </html>