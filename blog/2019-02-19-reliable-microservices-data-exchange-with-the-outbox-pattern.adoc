= Reliable Microservices Data Exchange With the Outbox Pattern
gmorling
:awestruct-tags: [ discussion, examples ]
:awestruct-layout: blog-post

If you've built a couple of microservices, you'll know that the https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/[hardest part about them is data]:
microservices don't exist in isolation and very often they'll need to propagate data and data changes amongst each other.

For instance consider a microservices that manages purchase orders:
when a new order is placed, information about that order may have to be relayed to a "shipment" service
(so it can assemble shipments of one or more orders) and a "customer" service
(so it can update things like the customer's total credit balance based on the new order).

There are different approaches for letting the "order" service know the other two about new purchase orders;
e.g. it could invoke some REST, grpc or other (synchronous) API provided by these services.
This might create some undesired coupling, though: the sending service must know which other services to invoke,
and it also must be prepared for these services temporarily not being available.
Service meshes such as Istio come in helpful here, by providing capabilities like circuit breakers,
request routing and much more.

One general issue of such a synchronous approach is that it lacks re-playability,
i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.

This can be achieved by using an asynchronous data exchange approach:
by having the order service send events into a durable message log such as Apache Kafka,
new consumers can be added as needed, enabling use cases you might not have had in mind originally.
E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on orders based on Elasticsearch.
Once the purchase order events are in a Kafka topic
(and depending on the topic's retention policy, they could remain their for an arbitrary long time or even indefinitely),
new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice's database, search index, data warehouse etc.

== The Issue of Dual Writes

In order to provide their functionality, data-centered microservices will typically have their own local database.
E.g. the "order" service may use a relational database to persist the information about purchase orders.
So when a new order is placed, this may result in an `INSERT` operation in a table `PurchaseOrder` in the service's own database.
At the same time, the service may wish to send an event about the new order to Apache Kafka,
so to propagate that information to the other interested services.

Simply issuing these two requests may lead to potential inconsistencies, though.
The reason being that we cannot have one shared transaction that would span the service's database as well as Apache Kafka.
So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database
but without having sent the corresponding message to Kafka
(e.g. due to some networking issue).
Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the database.
Both situations are undesirable, as it may cause no shipment to be created for a seemingly successfully placed order.
Or a shipment *would* be created but there'd be no trace about the corresponding purchase order in the "order" service itself.

So how could this situation be avoided?
The answer is to only modify *one* of the two resources (database and Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner.
Let's first consider the case of only writing to Apache Kafka.

When receiving a new purchase order, the "order" service would not do the `INSERT` into its database synchronously,
instead it would only send an event describing the new order to a Kafka topic.
So only one resource gets modified at a time, and if something goes wrong with that,
we'll find out about it instantly and report back to the caller of the "order" service that request failed.

At the same time, the service itself would subscribe to that Kafka topic.
That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database.
There's one subtle challenge here, though, and that is the lack of "read your own write" semantics.
E.g. let's assume the "order" service also has an API for querying for all the purchase orders of a given customer.
When invoking that API right after placing an order, due to the asynchronous nature of processing messages from the Kafka topic,
it might happen that the purchase order has not yet been persisted in the service's database and thus will not be returned by that query.
That can lead to a very confusing user experience, and users may miss newly placed orders in their shopping history.
There are ways to deal with this situation, e.g. the service could keep the newly placed purchase order in memory and answer subsequent queries based on that.
This gets quickly non-trivial though when implementing more complex queries or considering that the "order" service might also comprise multiple nodes in a clustered set-up.

Now how would things look like when only writing to the database synchronously and driving the sending of a message to Kafka based on that?
This is where the outbox pattern comes in.

== The Outbox Pattern

The idea of this approach is to have an `Events` table in the service's database.
When receiving a request for placing a purchase order, not only an `INSERT` into the `PurchaseOrder` table is done,
but at the same time, as part of the same transaction,
also an event record is inserted into that `Events` table.
An asynchronous process monitors that table for new entries and if there are any,
propagates the represented events to Apache Kafka.
This gives us a nice balance of characteristics:
By synchronously writing to the `PurchaseOrder` table, "read your own writes" semantics are ensured.
A subsequent query for purchase orders will return the newly persisted order, as soon as its transaction has been committed.
At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.

Now, the outbox pattern isn't any particular new idea.
It has been in use for quite some time.
In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions,
in can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker.
You can also find a description of the pattern on Chris Richardson's excellent https://microservices.io/patterns/data/application-events.html[microservices.io] site.

As we'll see, though, the pattern can be implemented in a very elegant way using change data capture and Debezium.
In the following, let's explore how.

== An Implementation Based on Change Data Capture

Log-based Change Data Capture (CDC) is a great fit for capturing new entries in the events table and stream them to Apache Kafka.
As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime.
Debezium comes with CDC connectors for several databases such as MySQL, Postgres and SQL Server.
The following example will use the Debezium connector for Postgres.

You can find the complete https://github.com/debezium/debezium-examples/tree/master/outbox[source code of the example] on GitHub.
It is centered around two microservices, _order-service_ and _shipment-service_.
Both are implemented in Java, using CDI as the component model and JPA/Hibernate for accessing their respective databases.
The order service runs on WildFly and exposes a simple REST API for receiving requests to place purchase orders and cancel specific order lines.
It uses a Postgres database as its local data store.
The shipment service is based on Thorntail; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database.
Debezium is used to capture new events in the outbox table of the order service's Postgres database and propagate them to Apache Kafka.

The overall architecture can be seen in the following picture:

++++
<div class="imageblock centered-image">
    <img src="/images/outbox_pattern.png" style="max-width:100%; margin-bottom:20px; margin-top:20px;" class="responsive-image" alt="Outbox Pattern Overview">
</div>
++++

TODO: Refine image

Now let's take a closer look at some of the relevant components of the solution.

=== The Outbox Table

The `outbox` table resides in the Postgres database of the order service and has the following structure:

[source]
----
Column        |          Type          | Modifiers
--------------+------------------------+-----------
id            | uuid                   | not null
aggregatetype | character varying(255) | not null
aggregateid   | character varying(255) | not null
type          | character varying(255) | not null
payload       | jsonb                  | not null
----

Its columns are these:

* `id`: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure, without committing the id of the last successfully processed event
* `aggregatetype`: the type of the root aggregate that is affected by a given event; this could for instance be "purchase order" or "customer".
This value will be used as the name of the corresponding topic in Kafka, so there'd be a topic for all events related to purchase orders, one topic for all customer-related events etc.
Note that also events pertaining to a sub-entity contained within an aggregate should use that same type, so e.g. an event representing the cancelation of an individual order line should use the aggregate type "order",
ensuring that also this event will go into the "order" Kafka topic.
* `aggregateid`: the id of the aggregate root that is affected by a given event; this could for instance be a purchase order id or a customer id;
Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing root aggregate, e.g. the purchase order id for an order line cancelation event.
This id will be used as the message key for the Kafka topic later on.
That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic,
which ensures that consumers of that topic will consume all the events related to one and the same aggregate root in the exact order as they were produced.
* `type`: the type of event, e.g. "Order Created" or "Order Line Canceled". Allows consumers to trigger suitable event handlers.
* `payload`: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.

=== Sending Events to the Outbox

In order to "send" events to the outbox, code in the order service could in general just do an `INSERT` into the outbox table.
But it's a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed.
CDI events come in very handy for this.
They can be raised in the application code and will be processed _synchronously_ by the outbox event sender,
which will do the required `INSERT` into the outbox table.
All outbox events should implement the following contract, resembling the structure of the outbox table shown before:

[source,java]
----
public interface ExportedEvent {

    String getAggregateId();
    String getAggregateType();
    JsonNode getPayload();
    String getType();
}
----

To produce such event, application code uses an injected `Event` instance, as e.g. here in the `OrderService` class:

[source,java]
----
@ApplicationScoped
public class OrderService {

    @PersistenceContext
    private EntityManager entityManager;

    @Inject
    private Event<ExportedEvent> event;

    @Transactional
    public PurchaseOrder addOrder(PurchaseOrder order) {
        order = entityManager.merge(order);

        event.fire(OrderCreatedEvent.of(order));
        event.fire(InvoiceCreatedEvent.of(order));

        return order;
    }

    // ...
}
----

In the `addOrder()` method, the JPA entity manager is used to persist the incoming order in the database
and the `event` instance is used to fire a corresponding `OrderCreatedEvent` and a `InvoiceCreatedEvent`.
Again, keep in mind that, despite the notion of "event", these two things happen within one and the same transaction.
i.e. within this transaction, three records will be inserted: one in the table with purchase orders and two in the events table.

Now let's take a look at the code that consumes any `ExportedEvent` and does the corresponding write to the events table:

[source,java]
----
@ApplicationScoped
public class EventSender {

    @PersistenceContext
    private EntityManager entityManager;

    public void onExportedEvent(@Observes ExportedEvent event) {
        OutboxEvent outboxEvent = new OutboxEvent(
                event.getAggregateType(), event.getAggregateId(), event.getType(), event.getPayload()
        );

        entityManager.persist(outboxEvent);
        entityManager.remove(outboxEvent);
    }
}
----

It's rather simple: for each event the CDI runtime will invoke the `onExportedEvent()` method.
An instance of the `OutboxEvent` entity is persisted in the database -- and removed right away!

This might be surprising at first.
But it makes sense when remembering how log-based CDC works:
it doesn't examine the actual contents of the table in the database, but instead it tails the append-only transaction log.
The calls to `persist()` and `remove()` will create an `INSERT` and a `DELETE` event in the log once the transaction commits.
After that, Debezium will process these events (handling the `INSERT` and ignoring the `DELETE`).
So we are able to capture the event added to the outbox by means of CDC,
but when looking at the contents of the events table itself, it will always be empty.
This means that no additional disk space is needed for the events table
(apart from the log file elements which will automatically be discarded at some point)
and also no separate house-keeping process is required to stop from growing indefinitely.

=== Registering the Debezium Connector

With the outbox in place, it's time to register the Debezium Postgres connector,
so it can capture any new events in the outbox table and relay them to Apache Kafka.
That can be done by sending the following JSON request to the REST API of Kafka Connect:

[source,json]
----
{
    "name": "outbox-connector",
    "config": {
        "connector.class" : "io.debezium.connector.postgresql.PostgresConnector",
        "tasks.max" : "1",
        "database.hostname" : "order-db",
        "database.port" : "5432",
        "database.user" : "postgresuser",
        "database.password" : "postgrespw",
        "database.dbname" : "orderdb",
        "database.server.name" : "dbserver1",
        "schema.whitelist" : "inventory",
        "table.whitelist" : "inventory.outboxevent",
        "tombstones.on.delete" : "false",
        "transforms" : "router",
        "transforms.router.type" : "io.debezium.examples.outbox.routingsmt.EventRouter"
    }
}
----

This sets up an instance of `io.debezium.connector.postgresql.PostgresConnector`,
capturing changes from the specified Postgres instances.
Note that by means of a table whitelist only changes from the `outboxevent` table are captured.
It also applies a single message transform (SMT) named `EventRouter`.

=== Topic Routing

By default, the Debezium connectors will send all change events originating from one given table to the same topic,
i.e. we'd end up with a single Kafka topic named `dbserver1.inventory.outboxevent` which would contain all events,
be it order events, customer events etc.

To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though,
to have multiple topics, e.g. `OrderEvents`, `CustomerEvents` and so on.
For instance the shipment service might not be interested in any customer events.
By only subscribing to the `OrderEvents` topic, it will be sure to never receive any customer events.

In order to route the change events captured from the outbox table to different topics, that custom SMT `EventRouter` is used.
Here is the code of its `apply()` method, which will be invoked by Kafka Connect for reach record emitted by the Debezium connector:

[source,java]
----
public R apply(R record) {
    // Ignoring tombstones just in case
    if (record.value() == null) {
        return record;
    }

    Struct struct = (Struct) record.value();
    String op = struct.getString("op");

    // ignoring deletions in the events table
    if (op.equals("d")) {
        return null;
    }
    else if (op.equals("c")) {
        Long timestamp = struct.getInt64("ts_ms");
        Struct after = struct.getStruct("after");

        String key = after.getString("aggregateid");
        String topic = after.getString("aggregatetype");

        String eventId = after.getString("id");
        String eventType = after.getString("type");
        String payload = after.getString("payload");

        Schema valueSchema = SchemaBuilder.struct()
            .field("eventType", after.schema().field("type").schema())
            .field("ts_ms", struct.schema().field("ts_ms").schema())
            .field("payload", after.schema().field("payload").schema())
            .build();

        Struct value = new Struct(valueSchema)
            .put("eventType", eventType)
            .put("ts_ms", timestamp)
            .put("payload", payload);

        Headers headers = record.headers();
        headers.addString("eventId", eventId);

        return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,
                record.timestamp(), headers);
    }
    else {
        throw new IllegalArgumentException("Record of unexpected op type: " + record);
    }
}
----

When receiving a delete event (`op` = `d`), it will discard that event,
as that deletion of event records from the outbox table is only a technicality not relevant to downstream consumers.
Things get more interesting, when receiving a create event (`op` = `c`).
Such record will be propagated to Apache Kafka.
Debezium's change events have a complex structure, that contain the old (`before`) and new (`after`) state of the represented row.
The event structure to propagate is obtained from the `after` state.
The `aggregatetype` value from the captured event record is used as the name of the topic to send the event to.
`aggregateid` is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic.
The message value is structure comprising the original event payload (encoded as JSON),
the timestamp indicating when the event was produced and the event type.
Finally, the event UUID is propagated as a Kafka header field.
This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.

== Events in Apache Kafka

Now let's take a look into the `Order` and `Customer` topics.

If you have checked out the example sources and started all the components via Docker Compose
(see the _README.md_ file in the example project for more details),
you can place purchase orders via the order service's REST API like so:

[source]
----
cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders
----

Similarly, specific order lines can be canceled:

[source]
----
cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2
----

When using a tool such as the very practical _kafkacat_ utility, you should now see messages like these in the `Order` topic:

[source]
----
kafkacat -b kafka:9092 -C -o beginning -f 'Headers: %h\nKey: %k\nValue: %s\n' -t Order
----

[source]
----
Headers: eventId=b2afe4be-6c1a-46fb-b60a-6b232ce70cad
Key: "1"
Value: {"eventType":"OrderCreated","ts_ms":1550247797707,"payload":"{\"id\": 1, \"customer\": 123, \"lineItems\": [{\"id\": 1, \"item\": \"Debezium in Action\", \"status\": \"ENTERED\", \"quantity\": 2, \"totalPrice\": 39.98}, {\"id\": 2, \"item\": \"Debezium for Dummies\", \"status\": \"ENTERED\", \"quantity\": 1, \"totalPrice\": 29.99}], \"orderDate\": \"2019-01-31T12:13:01\"}"}

Headers: eventId=cbc6817b-2b3a-42c7-8b33-83b71fa0cbe6
Key: "1"
Value: {"eventType":"OrderLineUpdated","ts_ms":1550248411631,"payload":"{\"orderId\": 1, \"newStatus\": \"CANCELLED\", \"oldStatus\": \"ENTERED\", \"orderLineId\": 2}"}
----

The `payload` field with the message values is the string-ified JSON representation of the original events.
The _jq_ utility, and more specifically, its `fromjson` operator comes in handy for displaying the event payload:

[source]
----
kafkacat -b kafka:9092 -C -o beginning -t Order | jq '.payload | fromjson'
----

[source,json]
----
{
  "id": 2,
  "customer": 0,
  "lineItems": [
    {
      "id": 3,
      "item": "Debezium in Action",
      "status": "ENTERED",
      "quantity": 2,
      "totalPrice": 39.98
    },
    {
      "id": 4,
      "item": "Debezium for Dummies",
      "status": "ENTERED",
      "quantity": 1,
      "totalPrice": 29.99
    }
  ],
  "orderDate": "2019-01-31T12:13:01"
}
{
  "orderId": 1,
  "newStatus": "CANCELLED",
  "oldStatus": "CANCELLED",
  "orderLineId": 2
}
----

You can also take a look at the `Customer` topic to inspect the events representing the creation of an invoice when a purchase order is added.

=== Duplicate Detection in the Consumer

At this point, our implementation of the outbox pattern is fully functional;
when the order service receives a request to place an order
(or cancel an order line),
it will persist the corresponding state in the `purchaseorder` and `orderline` tables of its database.
At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database.
The Debezium Postgres connector captures any insertions into that table
and routes the events into the Kafka topic corresponding to the aggregate type represented by a given message.

To wrap things up, let's explore how another microservice such as the shipment service can consume these messages.
The entry point into that service is a regular Kafka consumer implementation,
which is not too exciting and hence omitted here for the sake of brevity.
You can find its source code here in the example repository.
For each incoming message on the `Order` topic, the consumer calls the `OrderEventHandler`:

[source,java]
----
@ApplicationScoped
public class OrderEventHandler {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);

    @Inject
    private MessageLog log;

    @Inject
    private ShipmentService shipmentService;

    @Transactional
    public void onOrderEvent(UUID eventId, String key, String event) {
        if (log.alreadyProcessed(eventId)) {
            LOGGER.info("Event with UUID {} was already retrieved, ignoring it", eventId);
            return;
        }

        JsonObject json = Json.createReader(new StringReader(event)).readObject();
        JsonObject payload = json.containsKey("schema") ? json.getJsonObject("payload") :json;

        String eventType = payload.getString("eventType");
        Long ts = payload.getJsonNumber("ts_ms").longValue();
        String eventPayload = payload.getString("payload");

        JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));
        JsonObject payloadObject = payloadReader.readObject();

        if (eventType.equals("OrderCreated")) {
            shipmentService.orderCreated(payloadObject);
        }
        else if (eventType.equals("OrderLineUpdated")) {
            shipmentService.orderLineUpdated(payloadObject);
        }
        else {
            LOGGER.warn("Unkown event type");
        }

        log.processed(eventId);
    }
}
----

The first thing done by `onOrderEvent()` is to check whether the event with the given UUID has been processed before.
If so, any further calls for that same event will be ignored.
This is to prevent any duplicate processing of events caused by the "at least once" semantics of this data pipeline.
For instance it could happen that the Debezium connector or the consuming service fail
before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively.
In that case, after a restart of Debezium or the consuming service,
a few events may be processed a second time.
Propagating the event UUID as a Kafka message header allows for an efficient duplicate detection in the consumer.

If a message is received for the first time, the message value is parsed and the business method of the `ShippingService` method the specific event type is invoked with the event payload.
Finally, the message is marked as processed with the message log.

This `MessageLog` simply keeps track of all consumed events in a table within the service's local database:

[source,java]
----
@ApplicationScoped
public class MessageLog {

    @PersistenceContext
    private EntityManager entityManager;

    @Transactional(value=TxType.MANDATORY)
    public void processed(UUID eventId) {
        entityManager.persist(new ConsumedMessage(eventId, Instant.now()));
    }

    @Transactional(value=TxType.MANDATORY)
    public boolean alreadyProcessed(UUID eventId) {
        return entityManager.find(ConsumedMessage.class, eventId) != null;
    }
}
----

That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop.
This allows for re-trying to process the message later on.
Note that a more complete implementation should take care of re-trying given messages only for a certain number of times,
before re-routing any unprocessable messages to a dead-letter queue or similar.
Also there should be some house-keeping on the message log table;
periodically, all events older than the consumer's current offset committed with the broker may be deleted,
as it's ensured that such messages won't be propagated to the consumer another time.

== Summary

The outbox pattern is a great way for propagating data amongst different microservices.

By only modifying a single resource - the source service's own database -
it avoids any potential inconsistencies of altering multiple resources (the database and Kafka) at the same time which don't share one common transactional context.
By writing to the database first, the source service has instant "read your own writes" semantics,
which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.

At the same time, the pattern ensures asynchronous event propagation to other microservices.
Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services.
Given the right topic retention settings, new consumers may come up long after an event has been originally produced,
and build up their own local state based on the event history.

Putting Apache Kafka into the middle of the overall architecture also ensures a decoupling of involved services.
If for instance single components of the solution fail or are not available for some time, e.g. during an update,
events will simply be processed later on: after a restart,
the Debezium connector will continue to tail the events table from the point where it left off before.
Similarly, any consumer will continue to process topics from its previous offset.
By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.

Naturally, such event pipeline between different services is eventually consistent,
i.e. consumers such as the shipping service may lag a bit behind producers such as the order service.
Usually, that's just fine, though, and can be handled in terms of the application's business requirements.
For instance there'll typically be just no need to create a shipment within the very same second as an order has been placed.
Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range),
thanks to log-based change data capture which allows for emission of events in near-realtime.

One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service's API.
I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind.
This is to ensure to not accidentally break any consumers when upgrading the producing service.
At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.

== About Debezium

Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of http://kafka.apache.org/[Kafka] and provides http://kafka.apache.org/documentation.html#connect[Kafka Connect] compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is link:/license/[open source] under the http://www.apache.org/licenses/LICENSE-2.0.html[Apache License, Version 2.0].

== Get involved

We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter https://twitter.com/debezium[@debezium], https://gitter.im/debezium/user[chat with us on Gitter],
or join our https://groups.google.com/forum/#!forum/debezium[mailing list] to talk with the community.
All of the code is open source https://github.com/debezium/[on GitHub],
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or https://issues.jboss.org/projects/DBZ/issues/[log an issue].
