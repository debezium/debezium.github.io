= Approaches to Running Change Data Capture with Db2 
Luis Garces-Erice, Sean Rooney, Peter Urbanetz
:awestruct-tags: [ db2, featured ]
:awestruct-layout: blog-post


Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. link:https://vitess.io[Vitess]) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.

== Batch ETL Options

After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose data in Cassandra to link:https://cloud.google.com/bigquery/[BigQuery], our data warehouse, for analytics and reporting. We quickly built an Airflow link:https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/cassandra_hook.py[hook] and link:https://github.com/apache/airflow/blob/master/airflow/contrib/operators/cassandra_to_gcs.py[operator] to execute full loads. This obviously doesnâ€™t scale, as it rewrites the entire database on each load. To scale the pipeline, we evaluated two incremental load approaches, but both have their shortcomings:

