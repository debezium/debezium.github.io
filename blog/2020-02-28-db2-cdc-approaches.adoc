= Approaches to Running Change Data Capture for Db2 
Luis Garces-Erice, Sean Rooney, Peter Urbanetz
:awestruct-tags: [ db2, featured ]
:awestruct-layout: blog-post

== Back-Ground: Bringing Data to a Datalake

In 2016  IBM's Chief Data  Office (CDO) started  an effort to  build a
single  platform on  which IBM's  Enterprise Data  could be  ingested,
managed                                                            and
processed. link:https://www.slideshare.net/Chief_Data_Officer_Forum/ibm-chief-data-officer-summit-spring-2018-seth-dobrin-ed-walsh[CEDP].
IBM Research was one of the major contributors to this project. One of
the  fundamental  activities  was bringing  data  from  geographically
distributed       data       centers       to       the       platform
link:https://www.researchgate.net/publication/338037501_Experiences_with_Managing_Data_Ingestion_into_a_Corporate_Datalake

A  significant  fraction  of  this  enterprise  data  is  gathered  in
relational   databases  present   in   existing  data-warehouses   and
datamarts.  These are generally  production system whose primary usage
is as "systems  of record" for marketing, sales,  human resources etc.
As these are systems run by IBM for IBM unsurprisingly they are mainly
some variant of IBM's Db2.


== Getting Data From Db2 Efficiently

Data is ingested  into an immutable Landing Zone  within the Datalake.
This Landing Zone is implemented  as a HDFS instance.  Streaming data,
e.g. news, is moved from the  source using Kafka and then written into
HDFS using the appropriate connector.


One of  our key design objective  is automation. There are  over 5,000
relational database  tables from  over 200 different  sources ingested
everyday. In  order to scale  the data processing platform  aside from
the governance processes  that allow data owners to bring  data to the
platform the ingestion itself must be self-service.

Initially relational data was always bulk loaded from the source using
Sqoop.  A REST  Interface is made available such that  the data owners
can  configure when  the  data should  be  moved, e.g.   periodically,
trigger on  event etc.   A Sqoop  ingestion is  a distributed  sets of
tasks each of which use a JDBC connection to read part of a relational
database  table, generate  a file  based representation  of the  data,
e.g. parquet and  then store it on HDFS. With  Sqoop we can completely
refresh the  data, or append  to it, we *cannot*  modify incrementally
the data.



From a practical point of view this limited the periodicity with which
data could  be updated. Some of  the larger tables represented  10s of
GBytes of compressed parquet. While Sqoop  allows many tasks to be run
in  parallel for  the  same  table the  bottleneck  typically was  the
network  latency across  the WAN  and/or rate  controlling the  source
database system itself.  Often only a  small fraction of the table was
modified on any particular day, meaning that a huge amount of data was
sent unnecessarily.


We started introducing the use of Change Data Capture for the movement
of data  across the WAN.  Using a system  designed for files  that are
never modified  as a target for  the CDC system is  problematic. While
some           recent          work           like          Deltalakes
link:https://databricks.com/product/delta-lake-on-databricks[Deltalakes]
or                               Hive                              3.0
link:https://www.slideshare.net/Hadoop_Summit/what-is-new-in-apache-hive-30[Hive]
have  started introducing  delta  changes into  the Hadoop  ecosystem,
these were not mature enough for our needs.

We introduced the concept of a Relational Database Drop Zone in which
data owners could instantiate shadows of their database and from which
we could ingest  into HDFS. As the  Drop Zone and Landing  Zone are in
the  same  data  center  and  the   ingesting  of  data  is  a  highly
parallelizable  task,  the  actually  ingestion of  large  tables  was
typically order of magnitude faster  than the transferring of the data
from the source.

Data owners  could move data  using whatever tool they  preferred into
their Drop Zone. In particular they could use CDC systems.

CDC  system are  almost  as old  as  relational databases  themselves.
Typically  these were  designed  for purposes  of  back-up or  failure
recovery and were designed for use by a system administrator.

Db2 has a long pedigree being over  40 years old and running on a wide
set of operating systems including zOS, AIX, Linux and Windows. It has
evolved a  large set of distinct  tools for CDC for  uses in different
contexts.     We    started    exploring     the    use    of    IBM's
link:https://www.ibm.com/support/pages/q-replication-and-sql-replication-product-documentation-pdf-format-version-101-linux-unix-and-windows[SQL
Replication].  First tables that are put into CDC mode by the admin, a
capture agent is started which reads changes made to those tables from
the transaction log. The changes are  stored in special CDC Tables. At
the remote database an apply agent periodically reads the changes from
these CDC tables and updates the shadow tables.

While conceptually this is quite simple in practice it is difficult to
automate  for the  following reasons: 

 - source  and sink  are tightlycoupled  the same  table cannot  be replicated  to multiple  different target  database system  
-  if  the source  system  was already  using replication on a  table e.g. for back-up purposes, we  cannot use this method to replicate to the datalake 
- elevated privileges are required on the source. For Sqoop Data owners give read access to their system, but  giving admin  access  to  a 3rd  party  system  was a  compliance nightmare.   
-  elevated privileges  are  required  on the  sink.  For simplicity  our  Drop Zone  was  a  single  Db2 system  with  database instances for each  of the data sources.  Allowing the  data owners to set up  SQL Replication to  the Drop Zone  would allow them  access to each other  systems which is  a compliance violation and  a management nightmare.  
- the  tools were designed for system admins,  there are a huge  number  of gotchas  for  the  unwary,  e.g.  the mode  that  the transaction log has  to be in to  allow CDC, the time  the last backup was taken, whether the  database is row or column oriented  
- it is a Db2 specific  solution, although the  majority of the  relational data
sources were Db2, we also had Netezza, MySQL and SQL Server sources.


We found in  practice that the combination of the  above meant that it
was impractical to  allow data owners to use IBM  SQL Replication as a
CDC mechanism for the datalake.


IBM  Offers another  set  of  tools for  data  replication called  IBM
InfoSphere Data Replication (IIDR). This is sold as a distinct product
from Db2.   IIDR is not  a Db2 specific  solution, working for  a wide
range of relational  databases as well as  non-relational data storage
systems, e.g. file systems.  In essence IIDR has source agent and sink
agents.   The source  and sink  agent run  at or  close to  the target
system. Source agents  read the changes and propagate them  via a wide
range of protocols  including TCP sockets,MQ,Shared Files  etc. to the
sink agent.  The  source and sink agents are configured  via an entity
called the access  server through which sources are  connected to sink
and the tables  to capture are specified. The access  server is itself
typically  controlled via  a  Graphical User  Interface  by a  systems
administrator.

So for example  we can have a  Db2 source agent and  a Kafka Confluent
sink agent.  Through configuration of the access server a table can be
captured at the source and a Kafka Table (or KTable) representation of
the table can  be put into Kafka,  with the initial rows  of the table
being recorded  as a set  of Upsert messages (REFRESH)  and subsequent
changes to  the table  being recorded as  a sequence  of Upsert/Delete
Messages (MIRROR).

The KTable can  itself be streamed into a target  database in the Drop
Zone or used to make a snapshot that is loaded directly in the Landing
Zone.

IIDR   made   the  system   more   loosely   coupled  and   less   Db2
specific. However, it still was not  simple to automate. In essence we
need to be able  to allow a data owner to  specify the source database
system and the  tables to replicate via a REST  call and automatically
configure and  deploy the  necessary agents and  access server  on our
Kubernetes cluster. As we could not run on the source system itself we
federated the remote Db2 system to look  like it was local and ran the
agent on that.

IIDR assumes the  agent runs on the same hardware  architecture as the
relational database system. The IIDR agent uses a low level Db2 API to
read            the           transaction            log           the
link:https://www.ibm.com/support/knowledgecenter/SSEPGG_11.5.0/com.ibm.db2.luw.apdv.api.doc/doc/r0001673.html[db2ApiRead]
Many  of our  sources system  were  running on  AIX/PowerPC while  the
Kubernetes ran on Linux/Intel. This led to endian problems.

There  were two  fundamental problems  with  our approach:  - IIDR  is
designed to be monitored by  a system administrator. Trying to capture
the actions and responses of a system admin via scripts that parse the
logs and attempt  to react to failure was brittle,  as long as nothing
mishaved  the  system  runs  fine, but  if  something  failed  network
interruption, Kubernetes proxy failure, LDAP  being down etc. etc.  it
was almost  impossible to  automate the  appropriate response  - while
touching  the source  system as  little as  possible was  an admirable
objective, from a pratical point of  view it is almost impossible on a
real system to run the CDC  system independently of the source system.
If a system admin reloads from back-up  an older version of a table or
radically changes the  DDL of that table the CDC  system must be aware
that this has occurred and take the appropriate action. In the case of
changing the  DDL, a  new version  of the table  has been  created and
consequently a new version of the Ktable in turn must be created.

We  saw these  and many  more  problem when  trying to  use the  above
approach for using  CDC against real production  system.  We concluded
that the administration of the CDC system and the source system cannot
be done independently.


== Approaches to Implementing a Debezium Db2 Connector

When  Debezium  became available  we  started  evaluating it  for  our
purposes.  As it ran on a wide range of relational database system and
was open source we could imagine that database admintrator would allow
it  to  be  used  to  generate a  representation  of  their  data  for
downstream  applications.   Essentially,  the  Debezium  system  would
become an  extension of  the database source  system. Debezium  is not
required to  give an *identical*  copy of the database  tables (unlike
IIDR or SQL Replication). Typically the downstream application are for
auxillary tasks, i.e.  analytics, not for fail  over, meaning problems
such as preserving precise types  were less pressing.  For example, if
a timestamp  field was represented  as a  string in elastic  search it
would not cause the world to come to an end.

The biggest  concern we had  with Debezium was  that it didn't  have a
connector for Db2.

Two approaches  presented themselves: 

- Use  the low level Db2  API to read directly the transaction log like  IIDR 
- Use the SQL Replication CDC capture tables to read cpature tables using SQL


An investigation  of the code  identified that  the model used  by the
already  existing  connector  for  Microsoft  SQL  Server  was  almost
identical for that which would be  required for Db2. In essence the: 

- SQL queries to poll the changes would be different ;
- the structure and nature of  the LSN would be  different; 
- the fact  that Db2 distinghes between a  database system and  a database  while SQL Server  does not would have to be accounted for.

Otherwise everything else could be reused.

== Future Work/Extensions

=== Benchmarking
The Db2 Connector and SQL Server uses a polling model i.e. the connectors
periodically query the CDC table to determine what has changed since
the last time they polled. This is in contrast to MySQL which has a notification model.

We are interested in building a general purpose framework for benchmarking
systems in order to get a better understanding of where the trade-off
are in terms of latency, throughput of the CDC system and load on the
source system.


=== Db2 Notification System

An other approach is to build a notification system on Db2

...







