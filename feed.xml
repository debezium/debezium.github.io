<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://debezium.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debezium.io/" rel="alternate" type="text/html"/><updated>2023-02-13T19:39:53+00:00</updated><id>https://debezium.io/feed.xml</id><title type="html">Debezium</title><subtitle>Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.</subtitle><entry><title type="html">DDD Aggregates via CDC-CQRS Pipeline using Kafka &amp;amp; Debezium</title><link href="https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/" rel="alternate" type="text/html" title="DDD Aggregates via CDC-CQRS Pipeline using Kafka &amp;amp; Debezium"/><published>2023-02-04T00:00:00+00:00</published><updated>2023-02-04T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium</id><content type="html" xml:base="https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/"><![CDATA[<div class="paragraph"> <p>In this post, we are going to talk about a CDC-CQRS pipeline between a normalized relational database, MySQL, as the command database and a de-normalized NoSQL database, MongoDB, as the query database resulting in the creation of DDD Aggregates via Debezium &amp; Kafka-Streams.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>You can find the complete source code of the example <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline">here</a>. Refer to the <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline/blob/master/README.md">README.md</a> for details on building and running the example code.</p> </div> <div class="paragraph"> <p>The example is centered around three microservices: <code>order-write-service</code>, <code>order-aggregation-service</code> and <code>order-read-service</code>. These services are implemented as Spring-Boot applications in Java.</p> </div> <div class="paragraph"> <p>The <code>order-write-service</code> exposes two REST endpoints which persist shipping-details and item-details in their respective tables on MySQL database. Debezium tails the MySQL bin logs to capture any events in both these tables and publishes messages to Kafka topics. These topics are consumed by <code>order-aggregation-service</code> which is a Kafka-Streams application that joins data from both of these topics to create an Order-Aggregate object which is then published to a third topic. This topic is consumed by MongoDB Sink Connector and the data is persisted in MongoDB which is served by <code>order-read-service</code>.</p> </div> <div class="paragraph"> <p>The overall architecture of the solution can be seen in the following diagram:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/design_overview.png" style="max-width:90%;" class="responsive-image"> </div> </div> <div class="sect1"> <h2 id="rest_application_order_write_service">REST Application: order-write-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The first component that triggers the workflow starts is the <code>order-write-service</code>. This has been implemented as a Spring-Boot application and exposes two REST end-points:</p> </div> <div class="ulist"> <ul> <li> <p>POST: <code>api/shipping-details</code> to persist shipping details in the MySQL database</p> </li> <li> <p>POST: <code>api/item-details</code> to persist item details in the MySQL database</p> </li> </ul> </div> <div class="paragraph"> <p>Both of these endpoints persist their data in their respective tables in the MySQL database.</p> </div> </div> </div> <div class="sect1"> <h2 id="command_database_mysql">Command Database: MySQL</h2> <div class="sectionbody"> <div class="paragraph"> <p>The backend processing of the above-mentioned REST endpoints culminates in persisting the data in their respective tables in MySQL.</p> </div> <div class="paragraph"> <p>Shipping details are stored in a table called <code>SHIPPING_DETAILS</code>. And Item details are stored in a table called <code>ITEM_DETAILS</code>.</p> </div> <div class="paragraph"> <p>Here is the data-model of <code>SHIPPING_DETAILS</code> table, the column <code>ORDER_ID</code> is its primary key:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/shipping_details_data_model.png" style="max-width:100%;" class="responsive-image"> </div> </div> <div class="paragraph"> <p>Here is the data-model of <code>ITEM_DETAILS</code> table, the column <code>ORDER_ID</code> + <code>ITEM_ID</code> is its primary key:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/item_details_data_model.png" style="max-width:100%;" class="responsive-image"> </div> </div> </div> </div> <div class="sect1"> <h2 id="kafka_connect_source_connector_mysql_cdc_debezium">Kafka-Connect Source Connector: MySQL CDC Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Change Data Capture (CDC) is a solution that captures change events from a database transaction log (called BinLogs in the case of MySQL) and forwards those events to downstream consumers ex. Kafka topic.</p> </div> <div class="paragraph"> <p>Debezium is a platform that provides a low latency data streaming platform for change data capture (CDC) and is built on top of Apache Kafka. It allows database row-level changes to be captured as events and published to Apache Kafka topics. We setup and configure Debezium to monitor our databases, and then our applications consume events for each row-level change made to the database.</p> </div> <div class="paragraph"> <p>In our case, we will be using Debezium MySQL Source connector to capture any new events in the aforementioned tables and relay them to Apache Kafka. To achieve this, we will be registering our connecter by POST-ing the following JSON request to the REST API of Kafka Connect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db-connector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.mysql.MySqlConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mysql_db_server</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">3306</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">custom_mysql_user</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">custom_mysql_user_password</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">184054</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-server</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.whitelist</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">table.whitelist</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db.shipping_details,app-mysql-db.item_details</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.bootstrap.servers</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">kafka_server:29092</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.topic</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbhistory.app-mysql-db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">include.schema.changes</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">unwrap</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.unwrap.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.ExtractNewRecordState</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The above configuration is based on Debezium 1.9.5.Final. Be aware that if you attempt to use the demo with Debezium 2.0+, a number of the above configuration properties have new names and the configuration will require some adjustments.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>This sets up an instance of <code>io.debezium.connector.mysql.MySqlConnector</code>, capturing changes from the specified MySQL instance. Note that by means of a table include list, only changes from the <code>SHIPPING_DETAILS</code> and <code>ITEM_DETAILS</code> tables are captured. It also applies a single message transform (SMT) named <code>ExtractNewRecordState</code> which extracts the <code>after</code> field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its <code>after</code> field to create a simple Kafka record.</p> </div> <div class="paragraph"> <p>By default, the Kafka topic name is “serverName.schemaName.tableName” which as per our connector configuration translates to:</p> </div> <div class="ulist"> <ul> <li> <p><code>app-mysql-server.app-mysql-db.item_details</code></p> </li> <li> <p><code>app-mysql-server.app-mysql-db.shipping_details</code></p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="kafka_streams_application_order_aggregation_service">Kafka-Streams Application: order-aggregation-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Kafka-Streams application, namely <code>order-aggregation-service</code>, is going to process data from the two Kafka cdc-topics. These topics receive CDC events based on the shipping-details and item-details relations found in MySQL.</p> </div> <div class="paragraph"> <p>With that in place, the KStreams topology to create and maintain DDD order-aggregates on-the-fly can be built as follows.</p> </div> <div class="paragraph"> <p>The application reads the data from the shipping-details-cdc-topic. Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes we need to parse the order-id and the shipping-details from it to create a KTable with order-id as the key and shipping-details as the value.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Shipping Details Read</span>
KStream&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; shippingDetailsSourceInputKStream = streamsBuilder.stream(shippingDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE));

<span class="comment">// Change the Json value of the message to ShippingDetailsDto</span>
KStream&lt;<span class="predefined-type">String</span>, ShippingDetailsDto&gt; shippingDetailsDtoWithKeyAsOrderIdKStream = shippingDetailsSourceInputKStream
                        .map((orderIdJson, shippingDetailsJson) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(parseOrderId(orderIdJson), parseShippingDetails(shippingDetailsJson)));

<span class="comment">// Convert KStream to KTable</span>
KTable&lt;<span class="predefined-type">String</span>, ShippingDetailsDto&gt; shippingDetailsDtoWithKeyAsOrderIdKTable = shippingDetailsDtoWithKeyAsOrderIdKStream.toTable(
                        Materialized.&lt;<span class="predefined-type">String</span>, ShippingDetailsDto, KeyValueStore&lt;Bytes, <span class="type">byte</span><span class="type">[]</span>&gt;&gt;as(SHIPPING_DETAILS_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(SHIPPING_DETAILS_DTO_SERDE));</code></pre> </div> </div> <div class="paragraph"> <p>Similarly, the application reads the data from the item-details-cdc-topic and parses the order-id and the item from each individual message to group-by all the items pertaining to the same order-id in one list which is then aggregated to a KTable with order-id as key and the list of items pertaining to that specific order-id as value.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Item Details Read</span>
KStream&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; itemDetailsSourceInputKStream = streamsBuilder.stream(itemDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE));

<span class="comment">// Change the Key of the message from ItemId + OrderId to only OrderId and parse the Json value to ItemDto</span>
KStream&lt;<span class="predefined-type">String</span>, ItemDto&gt; itemDtoWithKeyAsOrderIdKStream = itemDetailsSourceInputKStream
                        .map((itemIdOrderIdJson, itemDetailsJson) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(parseOrderId(itemIdOrderIdJson), parseItemDetails(itemDetailsJson)));

<span class="comment">// Group all the ItemDtos for each OrderId</span>
KGroupedStream&lt;<span class="predefined-type">String</span>, ItemDto&gt; itemDtoWithKeyAsOrderIdKGroupedStream = itemDtoWithKeyAsOrderIdKStream.groupByKey(Grouped.with(STRING_SERDE, ITEM_DTO_SERDE));

<span class="comment">// Aggregate all the ItemDtos pertaining to each OrderId in a list</span>
KTable&lt;<span class="predefined-type">String</span>, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;&gt; itemDtoListWithKeyAsOrderIdKTable = itemDtoWithKeyAsOrderIdKGroupedStream.aggregate(
                (Initializer&lt;<span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;&gt;) <span class="predefined-type">ArrayList</span>::<span class="keyword">new</span>,
                (orderId, itemDto, itemDtoList) -&gt; addItemToList(itemDtoList, itemDto),
                Materialized.&lt;<span class="predefined-type">String</span>, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;, KeyValueStore&lt;Bytes, <span class="type">byte</span><span class="type">[]</span>&gt;&gt;as(ITEM_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(ITEM_DTO_ARRAYLIST_SERDE));</code></pre> </div> </div> <div class="paragraph"> <p>With both the KTables having order-id as the key, it’s easy enough to join them using order-id to create an aggregate called Order-Aggregate. Order-Aggregate is a composite object created by assimilating data from both the shipping-details as well as the item-details. This Order-Aggregate is then written to an order-aggregate Kafka topic.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Joining the two tables: shippingDetailsDtoWithKeyAsOrderIdKTable and itemDtoListWithKeyAsOrderIdKTable</span>
ValueJoiner&lt;ShippingDetailsDto, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;, OrderAggregate&gt; shippingDetailsAndItemListJoiner = (shippingDetailsDto, itemDtoList) -&gt; instantiateOrderAggregate(shippingDetailsDto, itemDtoList);
KTable&lt;<span class="predefined-type">String</span>, OrderAggregate&gt; orderAggregateKTable = shippingDetailsDtoWithKeyAsOrderIdKTable.join(itemDtoListWithKeyAsOrderIdKTable, shippingDetailsAndItemListJoiner);

<span class="comment">// Outputting to Kafka Topic</span>
orderAggregateKTable.toStream().to(orderAggregateTopicName, Produced.with(STRING_SERDE, ORDER_AGGREGATE_SERDE));</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="kafka_connect_sink_connector_mongodb_connector">Kafka-Connect Sink Connector: MongoDB Connector</h2> <div class="sectionbody"> <div class="paragraph"> <p>The sink connector is a Kafka Connect connector that reads data from Apache Kafka and writes data to some data-store. Using a MongoDB sink connector, it is easy to have the DDD aggregates written into MongoDB. All it needs is a configuration which can be posted to the REST API of Kafka Connect in order to run the connector.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mongo-sink-connector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">com.mongodb.kafka.connect.MongoSinkConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">topics</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order_aggregate</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.uri</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mongodb://root_mongo_user:root_mongo_user_password@mongodb_server:27017</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.storage.StringConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="value">false</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order_db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">document.id.strategy.overwrite.existing</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">document.id.strategy</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">hk,hv</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hk.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.transforms.HoistField$Key</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hk.field</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">_id</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hv.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.transforms.HoistField$Value</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hv.field</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="query_database_mongodb">Query Database: MongoDB</h2> <div class="sectionbody"> <div class="paragraph"> <p>The DDD aggregate is written to the database <code>order_db</code> in the collection <code>order</code> on MongoDB. The order-id becomes the <code>_id</code> of the table and the <code>order</code> column stores the order-aggregate as JSON.</p> </div> </div> </div> <div class="sect1"> <h2 id="rest_application_order_read_service">REST Application: order-read-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Order Aggregate persisted in MongoDB is served via a REST endpoint in <code>order-read-service</code>.</p> </div> <div class="ulist"> <ul> <li> <p>GET: <code>api/order/{order-id}</code> to retrieve the order from the MongoDB database</p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="execution_instructions">Execution Instructions</h2> <div class="sectionbody"> <div class="paragraph"> <p>The complete source code for this blog post is provided <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline">here</a> in Github. Begin by cloning this repository and changing into the <code>cdc-cqrs-pipeline</code> directory. The project provides a Docker Compose file with services for all the components:</p> </div> <div class="ulist"> <ul> <li> <p>MySQL</p> </li> <li> <p>Adminer (formerly known as phpMinAdmin), to manage MySQL via browser</p> </li> <li> <p>MongoDB</p> </li> <li> <p>Mongo Express, to manage MongoDB via browser</p> </li> <li> <p>Zookeeper</p> </li> <li> <p>Confluent Kafka</p> </li> <li> <p>Kafka Connect</p> </li> </ul> </div> <div class="paragraph"> <p>Once all services have started, register an instance of the Debezium MySQL connector &amp; MongoDB Connector by executing the <code>Create-MySQL-Debezium-Connector</code> and <code>Create-MongoDB-Sink-Connector</code> request respectively from <code>cdc-cqrs-pipeline.postman_collection.json</code>. Execute the request <code>Get-All-Connectors</code> to verify that the connectors have been properly created.</p> </div> <div class="paragraph"> <p>Change into the individual directories and spin-up the three Spring-Boot applications:</p> </div> <div class="ulist"> <ul> <li> <p><code>order-write-service</code>: runs on port no <code>8070</code></p> </li> <li> <p><code>order-aggregation-service</code>: runs on port no <code>8071</code></p> </li> <li> <p><code>order-read-service</code>: runs on port no <code>8072</code></p> </li> </ul> </div> <div class="paragraph"> <p>With this, our setup is complete.</p> </div> <div class="paragraph"> <p>To test the application, execute the request <code>Post-Shipping-Details</code> from the postman collection to insert shipping-details and <code>Post-Item-Details</code> to insert item-details for a particular order id.</p> </div> <div class="paragraph"> <p>Finally, execute the <code>Get-Order-By-Order-Id</code> request in the postman collection to retrieve the complete Order Aggregate.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary">Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services. Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services. If for instance single components of the solution fail or are not available for some time, events will simply be processed later on: after a restart, the Debezium connector will continue to tail the relevant tables from the point where it left off before. Similarly, any consumer will continue to process topics from its previous offset. By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.</p> </div> <div class="paragraph"> <p>Naturally, such event pipeline between different services is eventually consistent, i.e. consumers such as the order-read-service may lag a bit behind producers such as the order-write-service. Usually, that’s just fine, though, and can be handled in terms of the application’s business logic. Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range), thanks to log-based change data capture which allows for emission of events in near-realtime.</p> </div> </div> </div>]]></content><author><name>Purnima Jain</name></author><category term="ddd"/><category term="cdc"/><category term="cqrs"/><category term="debezium"/><category term="kafka"/><summary type="html"><![CDATA[In this post, we are going to talk about a CDC-CQRS pipeline between a normalized relational database, MySQL, as the command database and a de-normalized NoSQL database, MongoDB, as the query database resulting in the creation of DDD Aggregates via Debezium &amp; Kafka-Streams.]]></summary></entry><entry><title type="html">Debezium 2.1.2.Final Released</title><link href="https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/" rel="alternate" type="text/html" title="Debezium 2.1.2.Final Released"/><published>2023-01-26T00:00:00+00:00</published><updated>2023-01-26T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/"><![CDATA[<div class="paragraph"> <p>The Debezium release cadence is in full swing as I&#8217;m excited to announce Debezium <strong>2.1.2.Final</strong>!</p> </div> <div class="paragraph"> <p>This release focuses primarily on bug fixes and stability; and it is the recommended update for all users from earlier versions. This release contains <a href="https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+2.1.2.Final">28 resolved issues</a>, so let&#8217;s take a moment and discuss a critical breaking change.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_change">Breaking Change</h2> <div class="sectionbody"> <div class="paragraph"> <p>An edge case was reported in <a href="https://issues.redhat.com/browse/issues/DBZ-5996">DBZ-5996</a> where if a temporal column used <code>ZonedTimestamp</code> and if the column&#8217;s value had <code>0</code> micro or nanoseconds, rather than emitting the value as <code>2023-01-19T12:30:00.123000Z</code>, the value would be emitted in a truncated way as <code>2023-01-19T12:30:00.123Z</code>. This could lead to other issues with converters used in the event pipeline when the output from that column could be formatted inconsistently.</p> </div> <div class="paragraph"> <p>In order to remedy the edge case, the <code>ZonedTimestamp</code> implementation will now pad the fraction-based seconds value of the column&#8217;s value to the length/scale of the source database column. Using the example above of a <code>TIMESTAMP(6)</code> MySQL column type, the emitted value will now properly reflect a value of <code>2023-01-19T12:30:00.123000Z</code>.</p> </div> <div class="paragraph"> <p>While this change in behavior is likely to have minimal impact to most users, we wanted to bring attention to it in the event that you&#8217;ve perhaps used other means to handle this edge case in your pipelines. If you have, you should be able to rely on Debezium to emit the value consistently, even when the fraction-based seconds is <code>0</code>.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_changes">Other changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>A few noteworthy bug fixes and stability improvements include:</p> </div> <div class="ulist"> <ul> <li> <p>Data type conversion failed for mysql bigint <a href="https://issues.redhat.com/browse/DBZ-5798">DBZ-5798</a></p> </li> <li> <p>Oracle cannot undo change <a href="https://issues.redhat.com/browse/DBZ-5907">DBZ-5907</a></p> </li> <li> <p>Truncate records incompatible with ExtractNewRecordState <a href="https://issues.redhat.com/browse/DBZ-5966">DBZ-5966</a></p> </li> <li> <p>Computed partition must not be negative <a href="https://issues.redhat.com/browse/DBZ-5967">DBZ-5967</a></p> </li> <li> <p>NPE in execute snapshot signal with exclude.tables config on giving wrong table name <a href="https://issues.redhat.com/browse/DBZ-5988">DBZ-5988</a></p> </li> <li> <p>There is a problem with postgresql connector parsing the boundary value of money type <a href="https://issues.redhat.com/browse/DBZ-5991">DBZ-5991</a></p> </li> <li> <p>Nullable columns marked with "optional: false" in DDL events <a href="https://issues.redhat.com/browse/DBZ-6003">DBZ-6003</a></p> </li> <li> <p>Vitess: Handle the shard list difference between current db shards and persisted shards <a href="https://issues.redhat.com/browse/DBZ-6011">DBZ-6011</a></p> </li> <li> <p>Postgres LSN check should honor event.processing.failure.handling.mode <a href="https://issues.redhat.com/browse/DBZ-6012">DBZ-6012</a></p> </li> <li> <p>Enhance the Spanner connector by adding features and/or solving bugs <a href="https://issues.redhat.com/browse/DBZ-6014">DBZ-6014</a></p> </li> <li> <p>DDL statement with TokuDB engine specific "CLUSTERING KEY" couldn&#8217;t be parsed <a href="https://issues.redhat.com/browse/DBZ-6016">DBZ-6016</a></p> </li> <li> <p>DDL parse fail for role revoke with "user-like" role name <a href="https://issues.redhat.com/browse/DBZ-6019">DBZ-6019</a></p> </li> <li> <p>DDL parse fail for ALTER USER x DEFAULT ROLE y; <a href="https://issues.redhat.com/browse/DBZ-6020">DBZ-6020</a></p> </li> <li> <p>Offsets are not flushed on connect offsets topic when encountering an error on Postgres connector <a href="https://issues.redhat.com/browse/DBZ-6026">DBZ-6026</a></p> </li> <li> <p>Unexpected format for TIME column: 8:00 <a href="https://issues.redhat.com/browse/DBZ-6029">DBZ-6029</a></p> </li> <li> <p>Oracle does not support compression/logging clauses after an LOB storage clause <a href="https://issues.redhat.com/browse/DBZ-6031">DBZ-6031</a></p> </li> <li> <p>Debezium is logging the full message along with the error <a href="https://issues.redhat.com/browse/DBZ-6037">DBZ-6037</a></p> </li> <li> <p>Improve resilience during internal schema history recovery from Kafka <a href="https://issues.redhat.com/browse/DBZ-6039">DBZ-6039</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/2.1/release-notes#release-2.1.2-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 2.1.2.Final: Akshansh Jain, <a href="https://github.com/akanimesh7">Animesh Kumar</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/Lucascanna">Luca Scannapieco</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, and <a href="https://github.com/yoheimuta">yohei yoshimuta</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook, What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.1 will continue to receive bug fix and maintenance changes throughout this quarter. I expect there will be at least one additional release likely toward the middle or late February or March timeframe as we begin to wrap up up the work on Debezium 2.2.</p> </div> <div class="paragraph"> <p>Regarding Debezium 2.2, we intend to deliver another Alpha build in the coming weeks. We have lots of features still in the works, including the JDBC Sink Connector, configurable signal channels, new message transformations, and much more.</p> </div> <div class="paragraph"> <p>Stay tuned, and until then &#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[The Debezium release cadence is in full swing as I&#8217;m excited to announce Debezium 2.1.2.Final! This release focuses primarily on bug fixes and stability; and it is the recommended update for all users from earlier versions. This release contains 28 resolved issues, so let&#8217;s take a moment and discuss a critical breaking change.]]></summary></entry><entry><title type="html">We Are Hiring (Saga continues)</title><link href="https://debezium.io/blog/2023/01/24/we-are-hiring-2/" rel="alternate" type="text/html" title="We Are Hiring (Saga continues)"/><published>2023-01-24T00:00:00+00:00</published><updated>2023-01-24T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/24/we-are-hiring-2</id><content type="html" xml:base="https://debezium.io/blog/2023/01/24/we-are-hiring-2/"><![CDATA[<div class="paragraph"> <p>In November last year, we <a href="/blog/2022/11/15/filling-the-ranks/">announced</a> we were looking for reinforcements for the team. And I have two pieces of news for you today: a good one and an even better one.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>The good news is that we will have a new pair of hands joining the team soon. It is still early to share more details, but we are happy to see the new guy with us!</p> </div> <div class="paragraph"> <p>How about the better news? Well, we can have one more person! The process has changed a bit since the last time, but everything else holds! So if you are interested then please apply via the Red Hat <a href="https://global-redhat.icims.com/jobs/97420/principal-software-engineer---openshift-connectors/job">job portal</a>.</p> </div> <div class="paragraph"> <p>Don’t be shy, and don’t underestimate yourself. We would rather speak to more people than miss you!</p> </div>]]></content><author><name>Jiri Pechanec</name></author><category term="community"/><category term="hiring"/><summary type="html"><![CDATA[In November last year, we announced we were looking for reinforcements for the team. And I have two pieces of news for you today: a good one and an even better one.]]></summary></entry><entry><title type="html">Debezium 2.2.0.Alpha1 Released</title><link href="https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Alpha1 Released"/><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce not only the first release of the Debezium 2.2 series, but also the first release of Debezium in 2023, <strong>2.2.0.Alpha</strong>!</p> </div> <div class="paragraph"> <p>The Debezium 2.2.0.Alpha1 release includes some breaking changes, a number of bug fixes, and some noteworthy improvements and features, including but not limited to:</p> </div> <div class="ulist"> <ul> <li> <p>[Breaking Change] - <code>ZonedTimestamp</code> values will no longer truncate fractional seconds.</p> </li> <li> <p>[New] - Support ingesting changes from an Oracle logical stand-by database</p> </li> <li> <p>[New] - Support Amazon S3 buckets using the Debezium Storage API</p> </li> <li> <p>[New] - Support retrying database connections during connector start-up</p> </li> <li> <p>[New] - Debezium Server sink connector support for Apache RocketMQ and Infinispan</p> </li> </ul> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and dive into some of the features in more detail!</p> </div> <div class="sect1"> <h2 id="breaking_change">Breaking Change</h2> <div class="sectionbody"> <div class="paragraph"> <p>An edge case was reported in <a href="https://issues.redhat.com/browse/issues/DBZ-5996">DBZ-5996</a> where if a temporal column used <code>ZonedTimestamp</code> and if the column&#8217;s value had <code>0</code> micro or nanoseconds, rather than emitting the value as <code>2023-01-19T12:30:00.123000Z</code>, the value would be emitted in a truncated way as <code>2023-01-19T12:30:00.123Z</code>. This could lead to other issues with converters used in the event pipeline when the output from that column could be formatted inconsistently.</p> </div> <div class="paragraph"> <p>In order to remedy the edge case, the <code>ZonedTimestamp</code> implementation will now pad the fraction-based seconds value of the column&#8217;s value to the length/scale of the source database column. Using the example above of a <code>TIMESTAMP(6)</code> MySQL column type, the emitted value will now properly reflect a value of <code>2023-01-19T12:30:00.123000Z</code>.</p> </div> <div class="paragraph"> <p>While this change in behavior is likely to have minimal impact to most users, we wanted to bring attention to it in the event that you&#8217;ve perhaps used other means to handle this edge case in your pipelines. If you have, you should be able to rely on Debezium to emit the value consistently, even when the fraction-based seconds is <code>0</code>.</p> </div> </div> </div> <div class="sect1"> <h2 id="ingesting_changes_from_oracle_logical_stand_bys">Ingesting changes from Oracle logical stand-bys</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium for Oracle connector normally manages what is called a <em>flush table</em>, which is an internal table used to manage the flush cycles used by the Oracle Log Writer Buffer (LGWR) process. This flushing process requires that the user account the connector uses to have permission to create and write to this table. Logical stand-by databases often have more restrictive rules about data manipulation and may even be read-only, therefore, writing to the database is unfavorable or even not permissible.</p> </div> <div class="paragraph"> <p>To support an Oracle read-only logical stand-by database, we introduced a flag to disable the creation and management of this <em>flush table</em>. This feature can be used with both Oracle Standalone and Oracle RAC installations, and is currently considered incubating, meaning its subject to change in the future.</p> </div> <div class="paragraph"> <p>In order to enable Oracle read-only logical stand-by support, add the following connector option:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">internal.log.mining.read.only=true</code></pre> </div> </div> <div class="paragraph"> <p>In a future version, we plan to add support for an Oracle read-only physical stand-by database.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This configuration option is prefixed with <code>internal.</code>, meaning that it&#8217;s considered an undocumented and experimental feature. The semantics and behavior of this option are subject to change in future versions that may not be guaranteed forward or backward compatible.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="using_amazon_s3_buckets_with_storage_api">Using Amazon S3 buckets with Storage API</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium provides a Storage API framework that enables connectors to store offset and schema history state in a variety of persistence datastores. Moreover, the framework enables contributors to extend the API by adding new storage implementations with ease. Currently, the Storage API framework supports the local FileSystem, a Kafka Topic, or Redis datastores.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, we&#8217;re pleased to add Amazon S3 buckets as part of that framework, allowing the schema history to be persisted to an S3 bucket. An example connector configuration using S3 might look like the following:</p> </div> <div class="listingblock properties"> <div class="content"> <pre class="CodeRay highlight"><code>...
schema.history.internal=io.debezium.storage.s3.history
schema.history.internal.s3.access.key.id=aa
schema.history.internal.s3.secret.access.key=bb
schema.history.internal.s3.region.name=aws-global
schema.history.internal.s3.bucket.name=debezium
schema.history.internal.s3.object.name=db-history.log
schema.history.internal.s3.endpoint=http://&lt;server&gt;:&lt;port&gt;</code></pre> </div> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.history.internal.s3.access.key.id</code></dt> <dd> <p>Specifies the access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.secret.access.key</code></dt> <dd> <p>Specifies the secret access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.region.name</code></dt> <dd> <p>Specifies the region where the S3 bucket is available.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.bucket.name</code></dt> <dd> <p>Specifies the name of the S3 bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.object.name</code></dt> <dd> <p>Specifies the object name in the bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.endpoint</code></dt> <dd> <p>Specifies the S3 endpoint with the format of <code><a href="http://&lt;server&gt;:&lt;port&gt" class="bare">http://&lt;server&gt;:&lt;port&gt</a>;</code>.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="retry_database_connections_on_start_up">Retry database connections on start-up</h2> <div class="sectionbody"> <div class="paragraph"> <p>In previous releases of Debezium, the connector start-up phase used a fail-fast strategy. Simply put, this meant that if we couldn&#8217;t connect, authenticate, or performs any of the start-up phase steps required by the connector, the connector would enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>One specific problem area for users is if the connector gracefully starts, runs for a period of time, and then eventually encounters some fatal error. If the error is related to a resource that wasn&#8217;t accessed during the connector&#8217;s start-up lifecycle, the connector would typically gracefully restart just fine. However, the situation is different if the problem was related to the database&#8217;s availability and the database was still unavailable during the connector&#8217;s start-up phase. In this situation, the connector would fail-fast, and would enter a <code>FAILED</code> state, requiring manual intervention.</p> </div> <div class="paragraph"> <p>The fail-fast approach served Debezium well over the years, but in a world where a resource can come and go without warning, it became clear that changes were needed to improve Debezium&#8217;s reliability and resiliency. While the Kafka Connect&#8217;s retry/back-off framework has helped in this regard, that doesn&#8217;t address the concerns with start-up resources being unavailable with how the code is currently written.</p> </div> <div class="paragraph"> <p>Debezium 2.2 changes this landscape, shifting how we integrate with Kafka Connect&#8217;s source connector API slightly. Instead of accessing potentially unavailable resources during the start-up lifecycle, we moved that access to a later phase in the connector&#8217;s lifecycle. In effect, the Debezium start-up code is executed lazily that accesses potentially unavailable resources, which allows us to take advantage of the Kafka Connect retry/back-off framework even during our start-up code. In short, if the database is still unavailable during the connector&#8217;s start-up, the connector will continue to retry/back-off if Kafka Connect retries are enabled. Only once the maximum number of retry attempts has been reached or a non-retriable error occurs will the connector task enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>We hope this brings more reliability and resiliency for the Debezium experience, improving how errors are handled in an ever-changing landscape, and provides a solid foundation to manage connector lifecycles.</p> </div> </div> </div> <div class="sect1"> <h2 id="rocketmq_and_infinispan_support_in_debezium_server">RocketMQ and Infinispan support in Debezium Server</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium Server is a Quarkus-based framework that allows executing a Debezium connector from the command line, without Kafka or Kafka Connect, allowing the delivery of Debezium change events to any destination framework. With Debezium 2.2, two new sink connectors have been added to Debezium Server to support sending change events to Apache RocketMQ and to Infinispan.</p> </div> <div class="sect2"> <h3 id="rocketmq">RocketMQ</h3> <div class="paragraph"> <p><a href="https://rocketmq.apache.org">Apache RocketMQ</a> is a cloud-native messaging, eventing, and streaming real-time data processing platform that covers cloud-edge-device collaboration scenarios. In order to integrate Debezium Server with RocketMQ, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=rocketmq
debezium.sink.rocketmq.producer.name.srv.addr=&lt;hostname&gt;:&lt;port&gt;
debezium.sink.rocketmq.producer.group=debezuim-group
debezium.sink.rocketmq.producer.max.message.size=4194304
debezium.sink.rocketmq.producer.send.msg.timeout=3000
debezium.sink.rocketmq.producer.acl.enabled=false
debezium.sink.rocketmq.producer.access.key=&lt;access-key&gt;
debezium.sink.rocketmq.producer.secret.key=&lt;secret-key&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>rocketmq</code>, which enables the use of the RocketMQ module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.name.srv.addr</code></dt> <dd> <p>Specifies the host and port where Apache RocketMQ is available.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.group</code></dt> <dd> <p>Specifies the name associated with the Apache RocketMQ producer group.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.max.message.size</code></dt> <dd> <p>(Optional) Specifies the maximum number of bytes a message can be. Defaults to <code>4193404</code> (4MB).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.send.msg.timeout</code></dt> <dd> <p>(Optional) Specifies the timeout in milliseconds when sending messages. Defaults to <code>3000</code> (3 seconds).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.acl.enabled</code></dt> <dd> <p>(Optional) Controls whether access control lists are enabled. Defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.access.key</code></dt> <dd> <p>(Optional) The access key used for connecting to the Apache RocketMQ cluster.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.secret.key</code></dt> <dd> <p>(Optional) The access secret used for connecting to the Apache RocketMQ cluster.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with RocketMQ, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_apache_rocketmq">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="infinispan">Infinispan</h3> <div class="paragraph"> <p><a href="https://infinispan.org">Infinispan</a> is an in-memory, distributed data store that offers flexible deployment options with robust capabilities to store, manage, and process data. Infinispan is based on the notion of a key-value store that allows storing any data type. In order to integrate Debezium Server with Infinispan, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=infinispan
debezium.sink.infinispan.server.host=&lt;hostname&gt;
debezium.sink.infinispan.server.port=&lt;port&gt;
debezium.sink.infinispan.cache=&lt;cache-name&gt;
debezium.sink.infinispan.user=&lt;user&gt;
debezium.sink.infinispan.password=&lt;password&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>infinispan</code>, which enables the use of the Infinispan module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.server.host</code></dt> <dd> <p>Specifies the host name of one of the servers in the Infinispan cluster. This configuration option can also supply a comma-separated list of hostnames as well, such as <code>hostname1,hostname2</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.server.port</code></dt> <dd> <p>Specifies the port of the Infinispan cluster. Defaults to <code>11222</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.cache</code></dt> <dd> <p>Specifies the name of the Infinispan cache to write change events.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The Infinispan sink requires that the cache be created manually ahead of time. This enables the ability to create the cache with any variable configuration needed to fit your requirements.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.user</code></dt> <dd> <p>An optional configuration to specify the user to authenticate with, if authentication is required.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.password</code></dt> <dd> <p>An optional configuration to specify the password for the authenticating user, if authentication is required.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with Infinispan, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_infinispan">documentation</a>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Remove option for specifying driver class from MySQL Connector <a href="https://issues.redhat.com/browse/DBZ-4663">DBZ-4663</a></p> </li> <li> <p>Debezium is not working with Apicurio and custom truststores <a href="https://issues.redhat.com/browse/DBZ-5282">DBZ-5282</a></p> </li> <li> <p>Show/Hide password does not work on Connectors View details screen <a href="https://issues.redhat.com/browse/DBZ-5322">DBZ-5322</a></p> </li> <li> <p>Oracle cannot undo change <a href="https://issues.redhat.com/browse/DBZ-5907">DBZ-5907</a></p> </li> <li> <p>Postgresql Data Loss on restarts <a href="https://issues.redhat.com/browse/DBZ-5915">DBZ-5915</a></p> </li> <li> <p>Add support for Connect Headers to Debezium Server <a href="https://issues.redhat.com/browse/DBZ-5926">DBZ-5926</a></p> </li> <li> <p>Oracle Multithreading lost data <a href="https://issues.redhat.com/browse/DBZ-5945">DBZ-5945</a></p> </li> <li> <p>Spanner connector is missing JSR-310 dependency <a href="https://issues.redhat.com/browse/DBZ-5959">DBZ-5959</a></p> </li> <li> <p>Truncate records incompatible with ExtractNewRecordState <a href="https://issues.redhat.com/browse/DBZ-5966">DBZ-5966</a></p> </li> <li> <p>Computed partition must not be negative <a href="https://issues.redhat.com/browse/DBZ-5967">DBZ-5967</a></p> </li> <li> <p>Table size log message for snapshot.select.statement.overrides tables not correct <a href="https://issues.redhat.com/browse/DBZ-5985">DBZ-5985</a></p> </li> <li> <p>NPE in execute snapshot signal with exclude.tables config on giving wrong table name <a href="https://issues.redhat.com/browse/DBZ-5988">DBZ-5988</a></p> </li> <li> <p>There is a problem with postgresql connector parsing the boundary value of money type <a href="https://issues.redhat.com/browse/DBZ-5991">DBZ-5991</a></p> </li> <li> <p>Log statement for unparseable DDL statement in MySqlDatabaseSchema contains placeholder <a href="https://issues.redhat.com/browse/DBZ-5993">DBZ-5993</a></p> </li> <li> <p>Postgresql connector parses the null of the money type into 0 <a href="https://issues.redhat.com/browse/DBZ-6001">DBZ-6001</a></p> </li> <li> <p>Postgres LSN check should honor event.processing.failure.handling.mode <a href="https://issues.redhat.com/browse/DBZ-6012">DBZ-6012</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.Alpha1%20ORDER%20BY%20component%20ASC">42 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Akshansh Jain, Gabor, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/akanimesh7">Animesh Kumar</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/erdinctaskin">Erdinç Taşkın</a>, <a href="https://github.com/govi20">Govinda Sakhare</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jakzal">Jakub Zalas</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/Lucascanna">Luca Scannapieco</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/sunxiaojian">Sun Xiao Jian</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, and <a href="https://github.com/yoheimuta">yohei yoshimuta</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As the road to Debezium 2.2 is just starting, this initial release covers quite a lot of the features we&#8217;ve outlined our recent 2023 <a href="#/roadmap">road map</a> update. However, there are still a number of features that are still in active development, which include:</p> </div> <div class="ulist"> <ul> <li> <p>Configurable signal channels, enabling users to send signals not only from a database table or a Kafka topic, but also from other means such as an HTTP endpoint, the file system, etc.</p> </li> <li> <p>The Debezium JDBC sink connector that supports native Debezium change events out-of-the-box, without requiring the use of the Event Flattening transformation.</p> </li> <li> <p>A new single message transformation, <code>ExtractChangedRecordState</code>, that supports adding headers to the emitted event that describes that fields were changed or unchanged by the source event.</p> </li> <li> <p>And a plethora of enhancements to Debezium&#8217;s UI</p> </li> </ul> </div> <div class="paragraph"> <p>As we continue development on Debezium 2.2 and bugfixes to Debezium 2.1, we would love to hear your feedback or suggestions, whether it&#8217;s regarding our road map, the changes in this release, or something you&#8217;d like to see that we haven&#8217;t mentioned. Be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if there is. Or if you just want to stop by and give us a "Hello", we&#8217;d wecome that too.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce not only the first release of the Debezium 2.2 series, but also the first release of Debezium in 2023, 2.2.0.Alpha! The Debezium 2.2.0.Alpha1 release includes some breaking changes, a number of bug fixes, and some noteworthy improvements and features, including but not limited to: [Breaking Change] - ZonedTimestamp values will no longer truncate fractional seconds. [New] - Support ingesting changes from an Oracle logical stand-by database [New] - Support Amazon S3 buckets using the Debezium Storage API [New] - Support retrying database connections during connector start-up [New] - Debezium Server sink connector support for Apache RocketMQ and Infinispan]]></summary></entry><entry><title type="html">Change Data Capture with QuestDB and Debezium</title><link href="https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/" rel="alternate" type="text/html" title="Change Data Capture with QuestDB and Debezium"/><published>2023-01-06T00:00:00+00:00</published><updated>2023-01-06T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium</id><content type="html" xml:base="https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/"><![CDATA[<div class="paragraph"> <p>This tutorial was originally published by <a href="https://questdb.io/">QuestDB</a>, where guest contributor, <a href="https://yitaek.medium.com/">Yitaek Hwang</a>, shows us how to stream data into QuestDB with change data capture via Debezium and Kafka Connect.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Modern data architecture has largely shifted away from the <strong>ETL</strong> (Extract-Transform-Load) paradigm to <strong>ELT</strong> (Extract-Load-Transform) where raw data is first loaded into a data lake before transformations are applied (e.g., aggregations, joins) for further analysis. Traditional ETL pipelines were hard to maintain and relatively inflexible with changing business needs. As new cloud technologies promised cheaper storage and better scalability, data pipelines could move away from pre-built extractions and batch uploads to a more streaming architecture.</p> </div> <div class="paragraph"> <p><a href="https://en.wikipedia.org/wiki/Change_data_capture">Change data capture</a>(CDC) fits nicely into this paradigm shift where changes to data from one source can be streamed to other destinations. As the name implies, CDC tracks changes in data (usually a database) and provides plugins to act on those changes. For event-driven architectures, CDC is especially useful as a consistent data delivery mechanism between service boundaries (e.g., <a href="https://microservices.io/patterns/data/transactional-outbox.html">Outbox Pattern</a>). In a complex microservice environment, CDC helps to simplify data delivery logic by offloading the burden to the CDC systems.</p> </div> <div class="paragraph"> <p>To illustrate, let&#8217;s take a reference architecture to stream stock updates from PostgreSQL into QuestDB. A simple Java Spring App polls stock prices by ticker symbol and updates the current price to a PostgreSQL database. Then the updates are detected by Debezium and fed to a Kafka topic. Finally, the Kafka Connect QuestDB connector listens to that topic and streams changes into QuestDB for analysis.</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/overview.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Design overview</p> </div> </div> </div> <div class="paragraph"> <p>Structuring the data pipeline this way allows the application to be simple. The Java Spring App only needs to fetch the latest stock data and commit to PostgreSQL. Since PostgreSQL is an excellent OLTP (transactional) database, the app can rely on the ACID compliance to ensure that only the committed data will be seen by downstream services. The app developer does not need to worry about complicated retry logic or out-of-sync datasets. From the database standpoint, PostgreSQL can be optimized to do what it does best — transactional queries. Kafka can be leveraged to reliably feed data to other endpoints, and QuestDB can be used to store historical data to run analytical queries and visualization.</p> </div> <div class="paragraph"> <p>So without further ado, let&#8217;s get to the example:</p> </div> <div class="sect1"> <h2 id="prerequisites">Prerequisites</h2> <div class="sectionbody"> <div class="ulist"> <ul> <li> <p>Git</p> </li> <li> <p>Docker Engine: 20.10+</p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="setup">Setup</h2> <div class="sectionbody"> <div class="paragraph"> <p>To run the example locally, first clone the <a href="https://github.com/questdb/kafka-questdb-connector.git">QuestDG Kafka connector repo</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">$ git clone https://github.com/questdb/kafka-questdb-connector.git</code></pre> </div> </div> <div class="paragraph"> <p>Then, navigate to the stocks sample to build and run the Docker compose files:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">$ cd kafka-questdb-connector/kafka-questdb-connector-samples/stocks/
$ docker compose build
$ docker compose up</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>In Linux or older versions of Docker, the <code>compose</code> subcommand might not be available. You can try to execute <code>docker-compose</code> instead of <code>docker compose</code>. If <code>docker-compose</code> is unavailable in your distribution, you can <a href="https://docs.docker.com/compose/install/other/">install it</a> manually.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>This will build the Dockerfile for the Java Spring App/Kafka Connector for QuestDB and pull down PostgreSQL (preconfigured with Debezium), Kafka/Zookeeper, QuestDB, and Grafana containers. Kafka and Kafka Connect take a bit to initialize. Wait for the logs to stop by inspecting the connect container.</p> </div> <div class="sect2"> <h3 id="start_the_debezium_connector">Start the Debezium connector</h3> <div class="paragraph"> <p>At this point, the Java App is continuously updating the stock table in PostgreSQL, but the connections have not been setup. Create the Debezium connector (i.e., PostgreSQL → Debezium → Kafka) by executing the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d  '{&quot;name&quot;:&quot;debezium_source&quot;,&quot;config&quot;:{&quot;tasks.max&quot;:1,&quot;database.hostname&quot;:&quot;postgres&quot;,&quot;database.port&quot;:5432,&quot;database.user&quot;:&quot;postgres&quot;,&quot;database.password&quot;:&quot;postgres&quot;,&quot;connector.class&quot;:&quot;io.debezium.connector.postgresql.PostgresConnector&quot;,&quot;database.dbname&quot;:&quot;postgres&quot;,&quot;database.server.name&quot;:&quot;dbserver1&quot;}} ' localhost:8083/connectors</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="start-the-questdb-kafka-connect-sink">Start the QuestDB Kafka Connect sink</h3> <div class="paragraph"> <p>Finish the plumbing by creating the Kafka Connect side (i.e., Kafka → QuestDB sink):</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;name&quot;:&quot;questdb-connect&quot;,&quot;config&quot;:{&quot;topics&quot;:&quot;dbserver1.public.stock&quot;,&quot;table&quot;:&quot;stock&quot;, &quot;connector.class&quot;:&quot;io.questdb.kafka.QuestDBSinkConnector&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;key.converter&quot;:&quot;org.apache.kafka.connect.storage.StringConverter&quot;,&quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,&quot;host&quot;:&quot;questdb&quot;, &quot;transforms&quot;:&quot;unwrap&quot;, &quot;transforms.unwrap.type&quot;:&quot;io.debezium.transforms.ExtractNewRecordState&quot;, &quot;include.key&quot;: &quot;false&quot;, &quot;symbols&quot;: &quot;symbol&quot;, &quot;timestamp.field.name&quot;: &quot;last_update&quot;}}' localhost:8083/connectors</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="final_result">Final result</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now all the updates written to the PostgreSQL table will also be reflected in QuestDB. To validate, navigate to <a href="http://localhost:19000">http://localhost:19000</a> and select from the stock table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span> * <span class="keyword">FROM</span> stock;</code></pre> </div> </div> <div class="paragraph"> <p>You can also run aggregations for a more complex analysis:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"> <span class="class">SELECT</span>
   <span class="predefined-type">timestamp</span>,
   symbol,
   <span class="predefined">avg</span>(price),
   <span class="predefined">min</span>(price),
   <span class="predefined">max</span>(price)
 <span class="keyword">FROM</span> stock
   <span class="keyword">WHERE</span> symbol = <span class="string"><span class="delimiter">'</span><span class="content">IBM</span><span class="delimiter">'</span></span>
 SAMPLE <span class="keyword">BY</span> <span class="integer">1</span>m ALIGN <span class="keyword">TO</span> CALENDAR;</code></pre> </div> </div> <div class="paragraph"> <p>Finally, you can interact with a Grafana dashboard for visualization at <a href="http://localhost:3000/d/stocks/stocks?orgId=1&amp;refresh=5s&amp;viewPanel=2">http://localhost:3000/d/stocks/stocks?orgId=1&amp;refresh=5s&amp;viewPanel=2</a>.</p> </div> <div class="paragraph"> <p>The visualization is a candle chart composed of changes captured by Debezium; each candle shows the opening, closing, high, and low price, in a given time interval. The time interval can be changed by selecting the top-left 'Interval' option:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/screenshot.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Grafana candle chart</p> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="deep_dive">Deep dive</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now that we have the sample application up and running, let&#8217;s take a deeper dive into each component in the <a href="https://github.com/questdb/kafka-questdb-connector/tree/main/kafka-questdb-connector-samples/stocks">stocks</a> example.</p> </div> <div class="paragraph"> <p>We will look at the following files:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>├── kafka-questdb-connector/kafka-questdb-connector-samples/stocks/
│   ├── Dockerfile-App
|   |    -- The Dockerfile to package our Java App
|   ├── Dockerfile-Connect
|   |    -- The Dockerfile to combine the Debezium container
|   |    -- image the with QuestDB Kafka connector
│   ├── src/main/resources/schema.sql
|   |    -- The SQL which creates the stock table in PostgreSQL
|   |    -- and populates it with initial data
│   ├── src/main/java/com/questdb/kafka/connector/samples/StocksApplication.java
|   |    -- The Java Spring App which updates the stock table in PostgreSQL
|   |    -- in regular intervals
...</code></pre> </div> </div> <div class="sect2"> <h3 id="producer_java_app">Producer (Java App)</h3> <div class="paragraph"> <p>The producer is a simple Java Spring Boot App. It has two components:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>The <a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/src/main/resources/schema.sql">schema.sql</a> file. This file is used to create the stock table in PostgreSQL and populate it with initial data. It&#8217;s picked up by the Spring Boot App and executed on startup.</p> <div class="literalblock"> <div class="content"> <pre>[source,sql]
----
CREATE TABLE IF NOT EXISTS stock (
    id serial primary key,
    symbol varchar(10) unique,
    price float8,
    last_update timestamp
);
INSERT INTO stock (symbol, price, last_update) VALUES ('AAPL', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('IBM', 50.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('MSFT', 100.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('GOOG', 1000.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('FB', 200.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('AMZN', 1000.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('TSLA', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('NFLX', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('TWTR', 50.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('SNAP', 10.0, now()) ON CONFLICT DO NOTHING;
----</pre> </div> </div> <div class="literalblock"> <div class="content"> <pre>The `ON CONFLICT DO NOTHING` clause is used to avoid duplicate entries in the
table when the App is restarted.</pre> </div> </div> </li> <li> <p><a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/src/main/java/io/questdb/kafka/samples/StockService.java">Java code</a> to update prices and timestamps with a random value. The updates are not perfectly random, the application uses a very simple algorithm to generate updates which very roughly resembles stock price movements. In a real-life scenario, the application would fetch the price from some external source.</p> </li> </ol> </div> <div class="paragraph"> <p>The producer is packaged into a minimal Dockerfile, <a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/Dockerfile-App">Dockerfile-App</a>, and linked to PostgreSQL:</p> </div> <div class="listingblock"> <div class="content"> <pre>FROM maven:3.8-jdk-11-slim AS builder
COPY ./pom.xml /opt/stocks/pom.xml
COPY ./src ./opt/stocks/src
WORKDIR /opt/stocks
RUN mvn clean install -DskipTest
FROM azul/zulu-openjdk:11-latest
COPY --from=builder /opt/stocks/target/kafka-samples-stocks-*.jar /stocks.jar
CMD ["java", "-jar", "/stocks.jar"]</pre> </div> </div> </div> <div class="sect2"> <h3 id="kafka_connect_debezium_and_questdb_kafka_connector">Kafka Connect, Debezium, and QuestDB Kafka Connector</h3> <div class="paragraph"> <p>Before we dive into the Kafka Connect, Debezium, and the QuestDB Kafka connector configurations, let&#8217;s take a look at their relation with each other.</p> </div> <div class="paragraph"> <p>Kafka Connect is a framework for building connectors to move data between Kafka and other systems. It supports 2 classes of connectors:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>Source connectors - read data from a source system and write it to Kafka</p> </li> <li> <p>Sink connectors - read data from Kafka and write it to a sink system</p> </li> </ol> </div> <div class="paragraph"> <p>Debezium is a Source connector for Kafka Connect that can monitor and capture the row-level changes in the databases. What does it mean? Whenever a row is inserted, updated, or deleted in a database, Debezium will capture the change and write it as an event to Kafka.</p> </div> <div class="paragraph"> <p>On a technical level, Debezium is a Kafka Connect connector running inside the Kafka Connect framework. This is reflected in the <a href="https://hub.docker.com/r/debezium/connect">Debezium container image</a>, which packages the Kafka Connect with Debezium connectors pre-installed.</p> </div> <div class="paragraph"> <p>QuestDB Kafka connector is also a Kafka Connect connector. It&#8217;s a Sink connector that reads data from Kafka and writes it to QuestDB. We add the QuestDB Kafka connector to the Debezium container image, and we get a Kafka Connect image that has both Debezium and QuestDB Kafka connector installed!</p> </div> <div class="paragraph"> <p>This is the Dockerfile we use to build the image:</p> </div> <div class="paragraph"> <p>(<a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/Dockerfile-Connect">Dockerfile-Connect</a>)</p> </div> <div class="listingblock"> <div class="content"> <pre>FROM ubuntu:latest AS builder
WORKDIR /opt
RUN apt-get update &amp;&amp; apt-get install -y curl wget unzip jq
RUN curl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest | jq -r '.assets[]|select(.content_type == "application/zip")|.browser_download_url'|wget -qi -
RUN unzip kafka-questdb-connector-*-bin.zip

FROM debezium/connect:1.9.6.Final
COPY --from=builder /opt/kafka-questdb-connector/*.jar /kafka/connect/questdb-connector/</pre> </div> </div> <div class="paragraph"> <p>The Dockerfile downloads the latest release of the QuestDB Kafka connector, unzip it copies it to the Debezium container image. The resulting image has both Debezium and QuestDB Kafka connector installed:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/dockerfile-connect.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Dockerfile-Coonnect adding the QuestDB Kafka Connector layer</p> </div> </div> </div> <div class="paragraph"> <p>The overall Kafka connector is completed with a Source connector and a Sink connector:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/kafka-cluster.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>How the Source and Sink connector work with the Kafka cluster and the databases</p> </div> </div> </div> <div class="sect3"> <h4 id="debezium_connector">Debezium Connector</h4> <div class="paragraph"> <p>We already know that Debezium is a Kafka Connect connector that can monitor and capture the row-level changes in the databases. We also have a Docker image that has both Debezium and QuestDB Kafka connectors installed. However, at this point neither of the connectors is running. We need to configure and start them. This is done via CURL command that sends a POST request to the Kafka Connect REST API.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d  '{&quot;name&quot;:&quot;debezium_source&quot;,&quot;config&quot;:{&quot;tasks.max&quot;:1,&quot;database.hostname&quot;:&quot;postgres&quot;,&quot;database.port&quot;:5432,&quot;database.user&quot;:&quot;postgres&quot;,&quot;database.password&quot;:&quot;postgres&quot;,&quot;connector.class&quot;:&quot;io.debezium.connector.postgresql.PostgresConnector&quot;,&quot;database.dbname&quot;:&quot;postgres&quot;,&quot;database.server.name&quot;:&quot;dbserver1&quot;}} ' localhost:8083/connectors</code></pre> </div> </div> <div class="paragraph"> <p>The request body contains the configuration for the Debezium connector, let&#8217;s break it down:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_source</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="integer">5432</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbserver1</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>It listens to changes in the PostgreSQL database and publishes to Kafka with the above configuration. The topic name defaults to <code>&lt;server-name&gt;.&lt;schema&gt;.&lt;table&gt;.</code> In our example, it is <code>dbserver1.public.stock</code>. Why? Because the database server name is <code>dbserver1</code>, the schema is <code>public</code> and the only table we have is <code>stock</code>.</p> </div> <div class="paragraph"> <p>So after we send the request, Debezium will start listening to changes in the <code>stock</code> table and publish them to the <code>dbserver1.public.stock</code> topic.</p> </div> </div> <div class="sect3"> <h4 id="questdb_kafka_connector">QuestDB Kafka Connector</h4> <div class="paragraph"> <p>At this point, we have a PostgreSQL table <code>stock</code> being populated with random stock prices and a Kafka topic <code>dbserver1.public.stock</code> that contains the changes. The next step is to configure the QuestDB Kafka connector to read from the <code>dbserver1.public.stock</code> topic and write the data to QuestDB.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a deeper look at the configuration in the <a href="#start-the-questdb-kafka-connect-sink">start the QuestDB Kafka Connect sink</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="jason">{
  &quot;name&quot;: &quot;questdb-connect&quot;,
  &quot;config&quot;: {
    &quot;topics&quot;: &quot;dbserver1.public.stock&quot;,
    &quot;table&quot;: &quot;stock&quot;,
    &quot;connector.class&quot;: &quot;io.questdb.kafka.QuestDBSinkConnector&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,
    &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
    &quot;host&quot;: &quot;questdb&quot;,
    &quot;transforms&quot;: &quot;unwrap&quot;,
    &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.ExtractNewRecordState&quot;,
    &quot;include.key&quot;: &quot;false&quot;,
    &quot;symbols&quot;: &quot;symbol&quot;,
    &quot;timestamp.field.name&quot;: &quot;last_update&quot;
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>The important things to note here are:</p> </div> <div class="ulist"> <ul> <li> <p><code>table</code> and <code>topics</code>: The QuestDB Kafka connector will create a QuestDB table with the name <code>stock</code> and write the data from the <code>dbserver1.public.stock</code> topic to it.</p> </li> <li> <p><code>host</code>: The QuestDB Kafka connector will connect to QuestDB running on the <code>questdb</code> host. This is the name of the QuestDB container.</p> </li> <li> <p><code>connector.class</code>: The QuestDB Kafka connector class name. This tells Kafka Connect to use the QuestDB Kafka connector.</p> </li> <li> <p><code>value.converter</code>: The Debezium connector produces the data in JSON format. This is why we need to configure the QuestDB connector to use the JSON converter to read the data: <code>org.apache.kafka.connect.json.JsonConverter</code>.</p> </li> <li> <p><code>symbols</code>: Stock symbols are translated to <a href="https://questdb.io/docs/concept/symbol/">QuestDB symbol type</a>, used for string values with low cardinality (e.g., enums).</p> </li> <li> <p><code>timestamp.field.name</code>: Since QuestDB has great support for timestamp and partitioning based on that, we can specify the designated timestamp column.</p> </li> <li> <p><code>transforms</code>: unwrap field uses <code>io.debezium.transforms.ExtractNewRecordState</code> type to extract just the new data and not the metadata that Debezium emits. In other words, this is a filter to basically take the <code>payload.after</code> portion of the Debezium data on the Kafka topics. See its <a href="https://debezium.io/documentation/reference/1.9/transformations/event-flattening.html">documentation</a> for more details.</p> </li> </ul> </div> <div class="paragraph"> <p>The <code>ExtractNewRecordState</code> transform is probably the least intuitive part of the configuration. Let&#8217;s have a closer look at it: In short, for every change in the PostgreSQL table, the Debezium emits a JSON message to a Kafka topic such as the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">This JSON key contains Debezium message schema. It's not very relevant for this sample. Omitted for brevity.</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">payload</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">before</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">after</span><span class="delimiter">&quot;</span></span>: {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="integer">8</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">symbol</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">NFLX</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">price</span><span class="delimiter">&quot;</span></span>: <span class="float">1544.3357414199545</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">last_update</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978269856</span>
    }
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">version</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1.9.6.Final</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgresql</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbserver1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978272</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">snapshot</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">db</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">sequence</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">[</span><span class="char">\&quot;</span><span class="content">87397208</span><span class="char">\&quot;</span><span class="content">,</span><span class="char">\&quot;</span><span class="content">87397208</span><span class="char">\&quot;</span><span class="content">]</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">table</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">stock</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="integer">402087</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">lsn</span><span class="delimiter">&quot;</span></span>: <span class="integer">87397208</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">xmin</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">op</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">u</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978637</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transaction</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Don&#8217;t get scared if you feel overwhelmed by the sheer size of this message. Most of the fields are metadata, and they are not relevant to this sample. See <a href="https://debezium.io/documentation/reference/1.9/connectors/postgresql.html#postgresql-events">Debezium documentation</a>, for more details. The important point is that we cannot push the whole JSON message to QuestDB and we do not want all the metadata in QuestDB. We need to extract the <code>payload.after</code> portion of the message and only then push it to QuestDB. This is exactly what the <code>ExtractNewRecordState</code> transform does: It transforms the big message into a smaller one that contains only the <code>payload.after</code> portion of the message. Hence, it is as if the message looked like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="integer">8</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">symbol</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">NFLX</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">price</span><span class="delimiter">&quot;</span></span>: <span class="float">1544.3357414199545</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">last_update</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978269856</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>This is the message that we can push to QuestDB. The QuestDB Kafka connector will read this message and write it to the QuestDB table. The QuestDB Kafka connector will also create the QuestDB table if it does not exist. The QuestDB table will have the same schema as the JSON message - where each JSON field will be a column in the QuestDB table.</p> </div> </div> </div> <div class="sect2"> <h3 id="questdb_and_grafana">QuestDB and Grafana</h3> <div class="paragraph"> <p>Once the data is written to QuestDB tables, we can work with the time-series data easier. Since QuestDB is compatible with the PostgreSQL wire protocol, we can use the PostgreSQL data source on Grafana to visualize the data. The preconfigured dashboard is using the following query:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span>
  <span class="error">$</span>__time(<span class="predefined-type">timestamp</span>),
  <span class="predefined">min</span>(price) <span class="keyword">as</span> low,
  <span class="predefined">max</span>(price) <span class="keyword">as</span> high,
  first(price) <span class="keyword">as</span> open,
  last(price) <span class="keyword">as</span> close
<span class="keyword">FROM</span>
  stock
<span class="keyword">WHERE</span>
  <span class="error">$</span>__timeFilter(<span class="predefined-type">timestamp</span>)
  <span class="keyword">and</span> symbol = <span class="string"><span class="delimiter">'</span><span class="content">$Symbol</span><span class="delimiter">'</span></span>
SAMPLE <span class="keyword">BY</span> <span class="error">$</span>Interval ALIGN <span class="keyword">TO</span> CALENDAR;</code></pre> </div> </div> <div class="paragraph"> <p>We have created a system that continuously tracks and stores the latest prices for multiple stocks in a PostgreSQL table. These prices are then fed as events to Kafka through Debezium, which captures every price change. The QuestDB Kafka connector reads these events from Kafka and stores each change as a new row in QuestDB, allowing us to retain a comprehensive history of stock prices. This history can then be analyzed and visualized using tools such as Grafana, as demonstrated by the candle chart.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="next_steps">Next steps</h2> <div class="sectionbody"> <div class="paragraph"> <p>This sample project is a foundational reference architecture to stream data from a relational database into an optimized time series database. For existing projects that are using PostgreSQL, Debezium can be configured to start streaming data to QuestDB and take advantage of time series queries and partitioning. For databases that are also storing raw historical data, adopting Debezium may need some architectural changes. However, this is beneficial as it is an opportunity to improve performance and establish service boundaries between a transactional database and an analytical, time-series database.</p> </div> <div class="paragraph"> <p>This reference architecture can also be extended to configure Kafka Connect to also stream to other data warehouses for long-term storage. After inspecting the data, QuestDB can also be configured to downsample the data for longer term storage or even <a href="https://questdb.io/blog/2022/11/02/data-lifecycle-questdb/">detach partitions to save space</a>.</p> </div> <div class="paragraph"> <p>Give this <a href="https://github.com/questdb/kafka-questdb-connector/issues/new">sample application</a> a try and join the <a href="https://slack.questdb.io/">QuestDB Slack community</a> if you have any questions.</p> </div> </div> </div>]]></content><author><name>Yitaek Hwang</name></author><category term="questdb"/><category term="kafka"/><category term="debezium"/><category term="time series"/><summary type="html"><![CDATA[This tutorial was originally published by QuestDB, where guest contributor, Yitaek Hwang, shows us how to stream data into QuestDB with change data capture via Debezium and Kafka Connect.]]></summary></entry><entry><title type="html">Debezium 2.1.0.Final/Debezium 2.1.1.Final Released</title><link href="https://debezium.io/blog/2022/12/22/debezium-2-1-final-released/" rel="alternate" type="text/html" title="Debezium 2.1.0.Final/Debezium 2.1.1.Final Released"/><published>2022-12-22T00:00:00+00:00</published><updated>2022-12-22T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/12/22/debezium-2-1-final-released</id><content type="html" xml:base="https://debezium.io/blog/2022/12/22/debezium-2-1-final-released/"><![CDATA[<div class="paragraph"> <p>Today it&#8217;s my great pleasure to announce the availability of Debezium <strong>2.1.0.Final</strong>!</p> </div> <div class="paragraph"> <p>You might recently noticed that Debezium went a bit silent for the last few weeks. No, we are not going away. In fact the elves in Google worked furiously to bring you a present under a Christmas tree - Debezium Spanner connector.</p> </div> <div class="paragraph"> <p></p> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>Release 2.1.0.Final did not contain a mandatory dependency. This is fixed in 2.1.1.Final hot update.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>After a plenty of intensive effort we woud like to present <a href="https://github.com/nancyxu123">Nancy Xu</a> as the leading engineer behind the implementation of the <a href="https://github.com/debezium/debezium-connector-spanner">Debezium Spanner connector</a> for Google&#8217;s <a href="https://cloud.google.com/spanner">Cloud Spanner</a> distributed database. The connector itself is now in incubating state and still not fully feature complete (for example initial snapshots are not supported yet). Yet it is ready for a general use in scenarios where a robust Spanner-to-Kafka streaming implementation is required.</p> </div> <div class="paragraph"> <p>The initial release provides</p> </div> <div class="ulist"> <ul> <li> <p><a href="https://repo1.maven.org/maven2/io/debezium/debezium-connector-spanner/2.1.1.Final/">Installation packages</a></p> </li> <li> <p><a href="https://hub.docker.com/r/debezium/connect">Container image</a></p> </li> <li> <p><a href="/documentation/reference/2.1/connectors/spanner.html">Documentation</a></p> </li> </ul> </div> <div class="paragraph"> <p>As exciting these news are, this is not the only new feature available for Debezium. The release brings a nice pack of additional improvements.</p> </div> <div class="ulist"> <ul> <li> <p>Vitess connector supports initial snapshotting. This is a completely new feature and means that now the default behaviour for a new Vitess connector instance is to snapshot the current table content and then switch to streaming.</p> </li> <li> <p>Starting Debezium 2.0 we extracted a set of interfaces to provide additional pluggable persistent stores. Redis offset and internal schema history store was converted into module and is now available for generic use.</p> </li> <li> <p>MySQL connector processes <code>TRUNCATE TABLE</code> commands. When detected a <code>t</code> message is emitted into the table topic. This feature is optional and is diabled by default.</p> </li> <li> <p>Kafka Connect provides so-called <code>predicates</code> that enable user to apply transfromations conditionally. Debezium Engine and Debezium Server supports the same functionality too and is configured in the same way as kafka Connect.</p> </li> <li> <p>PostgreSQL connector is compatible with PostgreSQL 15.</p> </li> <li> <p>Cassandra connector is from the very start a bit odd duckling in the way how the codebase is written and connector deployed. This meant that only Kafka was suported as a destination. The connector was rewritten so it now can run inside Debezium Server and so any supported sink can be used as the destination.</p> </li> <li> <p>Nats JetStream is the new sink provided by Debezium Server.</p> </li> <li> <p>Kafka Connect by default calculates the topic partition number based on the message primary key. With the new <code>ComputePartition</code> it is possible to define a list of per-table columns to explicitly calculate and set the partition number.</p> </li> <li> <p>PostgreSQL flushes LSN (and thus truncates the WAL) when the message is recorded in Kafka. For scenarios that prefer manual WAL management it is possible to disable this behaviour.</p> </li> <li> <p>MongoDB connector always connected and streamed from primary node in the cluster. This is no longer necessary and non-primary nodes are preferred.</p> </li> </ul> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were many bugfixes, stability changes, and improvements throughout the development of Debezium 2.1. Altogether, a total of <a href="https://issues.redhat.com/browse/DBZ-5824?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(2.1.0.Alpha1%2C%202.1.0.Alpha2%2C%202.1.0.Beta1%2C%202.1.0.Final)%20ORDER%20BY%20component%20ASC">149 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this major release: Masazumi Kobayashi, <a href="https://github.com/jchipmunk">Andrey Pustovetov</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/Skezzowski">Balázs Sipos</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/btiernay">Bobby Tiernay</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/egyedt">Egyed Tamas</a>, <a href="https://github.com/enzo-cappa">Enzo Cappa</a>, <a href="https://github.com/erdinctaskin">Erdinç Taşkın</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/keriharris">Keri Harris</a>, <a href="https://github.com/marceloavan">Marcelo Avancini</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/mikekamornikov">Mike Kamornikov</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/poonam-meghnani">Poonam Meghnani</a>, <a href="https://github.com/prburgu">Praveen Burgu</a>, <a href="https://github.com/uurl">Raúl Estrada</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/sahapasci">Sahap Asci</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/subodh1810">Subodh Kant Chaturvedi</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/wuzhenhua01">Wu Zhenhua</a>, Xuan Shen, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, <a href="https://github.com/tooptoop4">tooptoop4</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>So what are our current plans for the first quarter of the next year? Some potential features you can expect include:</p> </div> <div class="ulist"> <ul> <li> <p>Initial work on JDBC sink connector</p> </li> <li> <p>Configurable signalling channels</p> </li> <li> <p>JDBC and S3 history and offset storage support</p> </li> </ul> </div> <div class="paragraph"> <p>As always, this roadmap is heavily influenced by the community, i.e. you. So if you would like to see any particular items here, please let us know.</p> </div> <div class="paragraph"> <p>Merry Christmas and Happy New Year 2023!</p> </div> <div class="paragraph"> <p>Onwards and Upwards!</p> </div> </div> </div>]]></content><author><name>Jiri Pechanec</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="mongodb"/><category term="spanner"/><summary type="html"><![CDATA[Today it&#8217;s my great pleasure to announce the availability of Debezium 2.1.0.Final! You might recently noticed that Debezium went a bit silent for the last few weeks. No, we are not going away. In fact the elves in Google worked furiously to bring you a present under a Christmas tree - Debezium Spanner connector.]]></summary></entry><entry><title type="html">Filling the Ranks</title><link href="https://debezium.io/blog/2022/11/15/filling-the-ranks/" rel="alternate" type="text/html" title="Filling the Ranks"/><published>2022-11-15T00:00:00+00:00</published><updated>2022-11-15T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/11/15/filling-the-ranks</id><content type="html" xml:base="https://debezium.io/blog/2022/11/15/filling-the-ranks/"><![CDATA[<div class="paragraph"> <p>As you are probably well aware, Gunnar Morling has stepped down from his position as Debezium project lead and is now pursuing new exciting adventures. It is sad, but every cloud has a silver lining!</p> </div> <div class="paragraph"> <p>What can it be? We (the Debezium team and Red Hat) are hiring! Are you a community contributor? Do you have any pull requests under your belt? Are you a happy Debezium user and eager to do more, or are you a seasoned Java developer looking for work in an exciting and inclusive open-source environment?</p> </div> <div class="paragraph"> <p> If any of that describes you, don’t hesitate to contact me (Jiri Pechanec &lt;<a href="mailto:jpechane@redhat.com">jpechane@redhat.com</a>&gt;) via email or our Zulip chat. I cannot promise you will be selected, but I can promise an open and fair process.</p> </div> <div class="paragraph"> <p>The following are a guideline, things we’d love to see but aren’t all required. If there are specific expectations, those are indicated inline.</p> </div> <div class="ulist"> <ul> <li> <p>Multiple years of Java development experience</p> <div class="ulist"> <ul> <li> <p>JDBC knowledge expected</p> </li> </ul> </div> </li> <li> <p>Enterprise Java is not required but the knowledge of integration patterns like message bus (JMS), routing, etc. is welcome</p> </li> <li> <p>Kafka or Kafka Connect experience is welcome</p> </li> <li> <p>At least user knowledge of some of the databases supported by Debezium is preferred</p> <div class="ulist"> <ul> <li> <p>Basic database concepts like transactions (ACID), and transaction logs are expected</p> </li> </ul> </div> </li> <li> <p>Open-source contributions are a plus</p> </li> <li> <p>Debezium contributions are a huge plus</p> </li> </ul> </div> <div class="paragraph"> <p>We can promise engaging and interesting work, an excellent and inclusive team that treats everyone with respect, and a wonderful community that is vibrant and growing daily.</p> </div> <div class="paragraph"> <p>Don’t be shy, and don’t underestimate yourself. We would rather speak to more people than miss you!</p> </div>]]></content><author><name>Jiri Pechanec</name></author><category term="community"/><category term="hiring"/><summary type="html"><![CDATA[As you are probably well aware, Gunnar Morling has stepped down from his position as Debezium project lead and is now pursuing new exciting adventures. It is sad, but every cloud has a silver lining! What can it be? We (the Debezium team and Red Hat) are hiring! Are you a community contributor? Do you have any pull requests under your belt? Are you a happy Debezium user and eager to do more, or are you a seasoned Java developer looking for work in an exciting and inclusive open-source environment?]]></summary></entry><entry><title type="html">Debezium 2.1.0.Alpha1 Released</title><link href="https://debezium.io/blog/2022/11/10/debezium-2-1-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.1.0.Alpha1 Released"/><published>2022-11-10T00:00:00+00:00</published><updated>2022-11-10T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/11/10/debezium-2-1-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2022/11/10/debezium-2-1-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce the first release of the Debezium 2.1 series, <strong>2.1.0.Alpha1</strong>!</p> </div> <div class="paragraph"> <p>The Debezium 2.1.0.Alpha1 release includes quite a number of bug fixes but also some noteworthy improvements and new features including but not limited to:</p> </div> <div class="ulist"> <ul> <li> <p>Support for PostgreSQL 15</p> </li> <li> <p>Single Message Transformation (SMT) predicate support in Debezium engine</p> </li> <li> <p>Capturing TRUNCATE as change event in MySQL table topics</p> </li> <li> <p>Oracle LogMiner performance improvements</p> </li> <li> <p>New Redis-based storage module</p> </li> </ul> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and dive into some of these in more detail!</p> </div> <div class="sect1"> <h2 id="smt_predicate_support_in_debezium_engine">SMT predicate support in Debezium engine</h2> <div class="sectionbody"> <div class="paragraph"> <p>Single Message Transformations (SMTs) are a critical part of a change event&#8217;s lifecycle and they can apply any number of messaging patterns to the emitted change event. For example, a database table may have a specific column that gets emitted as a part of Debezium&#8217;s change events, but you want this field to be excluded so that the field isn&#8217;t present in the persisted event inside Kafka. This can be done using a single message transformation (SMT).</p> </div> <div class="paragraph"> <p>However, Debezium emits a number of different event types such as heartbeat, schema change, and data change events. Each of these events have their own event structure and there may come a point where a specific SMT should only be applied to a specific event type or if a specific event has a certain criteria. One way to evaluate whether the SMT should be applied was to do this evaluation inside the SMT itself, checking its event type or all the criteria to see whether the SMT should be applied or if the SMT should return the event unchanged.</p> </div> <div class="paragraph"> <p>Kafka Connect later introduced a concept called <em>predicates</em>, which is where a set of external rules can be specified in the connector configuration and must be evaluated to determine whether the SMT should be fired for an event or whether the SMT is skipped. This has enormous benefits because it allows developers to write very specific transformations that focus on a singular mutation and its entirely up to the user to determine whether that SMT should be applied or not using <em>predicates</em>.</p> </div> <div class="paragraph"> <p>Starting in Debezium 2.1, the power of Single Message Transformation (SMT) predicates can be harnessed when using the Debezium Engine or Debezium Server. An example configuration might like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties"># Define the filter transformation, linking it to the IsFoo predicate/rule
debezium.transforms=Filter
debezium.transforms.Filter.type=org.apache.kafka.connect.transforms.Filter
debezium.transforms.Filter.predicate=IsFoo

# Define the IsFoo predicate/rule
debezium.predicates=IsFoo
debezium.predicates.IsFoo.type=org.apache.kafka.connect.transforms.predicates.TopicNameMatches
debezium.predicates.IsFoo.pattern=foo</code></pre> </div> </div> <div class="paragraph"> <p>With these additional <code>debezium.predicates.*</code> configuration properties, it is possible to define a set of rules that must be evaluated to determine whether the <code>Filter</code> SMT will be fired or skipped in the transformation chain. In the example above, the predicate checks to see whether the event&#8217;s topic name matches <code>foo</code> and if it does, the <code>Filter</code> transformation will be fired. If the topic name does not match <code>foo</code>, the <code>Filter</code> transformation is skipped.</p> </div> <div class="paragraph"> <p>To read more about applying Single Message Transformations (SMTs) selectively using predicates, see:</p> </div> <div class="ulist"> <ul> <li> <p><a href="https://debezium.io/documentation/reference/2.1/transformations/applying-transformations-selectively.html">Using SMT predicates to selectively apply transformations</a></p> </li> <li> <p><a href="https://debezium.io/documentation/reference/2.1/operations/debezium-server.html#debezium-predicates-configuration-options">Debezium Server predicates configuration and set up</a></p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="capture_mysql_truncate_as_change_event">Capture MySQL TRUNCATE as change event</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium has supported the concept of emitting a change event to signal a <code>TRUNCATE TABLE</code> scenario for PostgreSQL and Oracle for quite a while. Starting with Debezium 2.1, this behavior has been extended to the MySQL connector.</p> </div> <div class="paragraph"> <p>By default, the connector configuration option, <code>skipped.operations</code>, automatically skips <code>TRUNCATE</code> events if they&#8217;re detected. This means that by default, there will not be anything emitted when the connector detects this pattern. In order to support emission of such events, the <code>skipped.operations</code> configuration property must be specified with a value of <code>none</code> or other operation types that do not include the <code>t</code> (truncate) type.</p> </div> <div class="paragraph"> <p>Once the connector is configured to emit events for <code>TRUNCATE</code> operations, a new data change event type will be emitted to the table topics. These event types signal that the table or collection has been truncated. The event&#8217;s payload will looking like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json"><span class="key"><span class="delimiter">&quot;</span><span class="content">payload</span><span class="delimiter">&quot;</span></span>: {
  <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: {
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">op</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">t</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1465581029523</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>The most notable point here is that truncate events do not contain a <code>before</code> or <code>after</code> state.</p> </div> </div> </div> <div class="sect1"> <h2 id="new_redis_based_storage_module">New Redis-based storage module</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium recently modularized parts of its codebase around persisting offsets and schema history into a set of modules supporting File and Kafka -based implementation. In Debezium 2.1, a new module was introduced to support persisting to Redis data stores.</p> </div> <div class="paragraph"> <p>The following fully-qualified class names can be used to persist offsets or schema history to Redis data stores:</p> </div> <div class="ulist"> <ul> <li> <p><code>io.debezium.storage.redis.offset.RedisOffsetBackingStore</code></p> </li> <li> <p><code>io.debezium.storage.redis.history.RedisSchemaHistory</code></p> </li> </ul> </div> <div class="paragraph"> <p>If you have manually installed Debezium, be sure to include the <code>debezium-storage-redis</code> artifact on your classpath if it does not exist in order to gain access to these new implementations.</p> </div> <div class="paragraph"> <p>For information about what options can be configured with this new implementation, please see the <a href="https://debezium.io/documentation/reference/2.1/operations/debezium-server.html#debezium-source-configuration-properties">source configuration</a> section of the Debezium Server documentation and look for configuration options prefixed with:</p> </div> <div class="ulist"> <ul> <li> <p><code>debezium.source.offset.storage.redis.*</code></p> </li> <li> <p><code>debezium.source.schema.history.internal.redis.*</code></p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Missing snapshot pending transactions <a href="https://issues.redhat.com/browse/DBZ-5482">DBZ-5482</a></p> </li> <li> <p>Using snapshot.mode ALWAYS uses SCN from offsets <a href="https://issues.redhat.com/browse/DBZ-5626">DBZ-5626</a></p> </li> <li> <p>MongoDB multiple tasks monitor misalignment <a href="https://issues.redhat.com/browse/DBZ-5629">DBZ-5629</a></p> </li> <li> <p>UNIQUE INDEX with NULL value throws exception when lob.enabled is true <a href="https://issues.redhat.com/browse/DBZ-5682">DBZ-5682</a></p> </li> <li> <p>Columns are not excluded when doing incremental snapshots <a href="https://issues.redhat.com/browse/DBZ-5727">DBZ-5727</a></p> </li> <li> <p>NullPointerException thrown during snapshot of tables in Oracle source connector <a href="https://issues.redhat.com/browse/DBZ-5738">DBZ-5738</a></p> </li> <li> <p>Hostname not available for load balanced ocp services in ARO <a href="https://issues.redhat.com/browse/DBZ-5753">DBZ-5753</a></p> </li> <li> <p>Exclude Oracle Compression Advisor tables from capture to avoid infinite loop <a href="https://issues.redhat.com/browse/DBZ-5756">DBZ-5756</a></p> </li> <li> <p>Message with LSN 'LSN{XYZ}' not present among LSNs seen in the location phase <a href="https://issues.redhat.com/browse/DBZ-5792">DBZ-5792</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.1.0.Alpha1%20ORDER%20BY%20component%20ASC">55 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/enzo-cappa">Enzo Cappa</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/BetaCat0">Helong Zhang</a>, <a href="https://github.com/hdulay">Hubert Dulay</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/janjwerner-confluent">Jan Werner</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/chtitux">Théophile Helleboid</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/ywu-stripe">Yang Wu</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>So as we continue to work on Debezium 2.1, we&#8217;ve been able to include a number of the expected changes in today&#8217;s release, but we still do intend to deliver on a new Single Message Transformation (SMT) for generating change event deltas before the end of the year. There is also some much anticipated changes for Debezium UI, such as supporting editing of connector configurations and much more.</p> </div> <div class="paragraph"> <p>You can find this information and what else to expect as a part of Debezium in 2023 in our recently updated <a href="/roadmap/">road map</a>. We have quite a lot of new features planned for next year, and we would love to hear your feedback or suggestions on things that may not be on the roadmap you&#8217;d like to see. Be sure to get in touch with us on the mailing list if there is.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce the first release of the Debezium 2.1 series, 2.1.0.Alpha1! The Debezium 2.1.0.Alpha1 release includes quite a number of bug fixes but also some noteworthy improvements and new features including but not limited to: Support for PostgreSQL 15 Single Message Transformation (SMT) predicate support in Debezium engine Capturing TRUNCATE as change event in MySQL table topics Oracle LogMiner performance improvements New Redis-based storage module]]></summary></entry><entry><title type="html">Debezium Evolving</title><link href="https://debezium.io/blog/2022/10/26/debezium-evolving/" rel="alternate" type="text/html" title="Debezium Evolving"/><published>2022-10-26T00:00:00+00:00</published><updated>2022-10-26T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/26/debezium-evolving</id><content type="html" xml:base="https://debezium.io/blog/2022/10/26/debezium-evolving/"><![CDATA[<div class="paragraph"> <p>Some time in early 2017, I got a meeting invite from Debezium&#8217;s founder, <a href="https://twitter.com/rhauch">Randall Hauch</a>. He was about to begin a new chapter in his professional career and was looking for someone to take over as the project lead for Debezium. So we hopped on a call to talk things through, and I was immediately sold on the concept of change data capture, its large number of potential use cases and applications, and the idea of making this available to the community as open-source. After some short consideration I decided to take up this opportunity, and without a doubt this has been one of the best decisions I&#8217;ve ever made in my job.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Today, five years and two major releases (<a href="/blog/2019/12/18/debezium-1-0-0-final-released/">1.0</a>, <a href="/blog/2022/10/17/debezium-2-0-final-released/">2.0</a>) later, I am feeling really proud of what the Debezium community has accomplished, having established itself as <em>the</em> leading open-source platform for change data capture. The number of officially supported databases has grown from three to eight. Further Debezium-based CDC connectors are developed externally by database vendors like <a href="https://docs.scylladb.com/stable/using-scylla/integrations/scylla-cdc-source-connector.html">ScyllaDB</a> and <a href="https://docs.yugabyte.com/preview/explore/change-data-capture/debezium-connector-yugabytedb/">Yugabyte</a>, making Debezium&#8217;s change event format kind of a de-facto standard for CDC. The project is used in production by companies such as Reddit, Shopify, Ubisoft, and Zalando. Debezium became part of Red Hat&#8217;s commercially supported product offerings (<a href="https://access.redhat.com/documentation/en-us/red_hat_integration/2022.q3/html/getting_started_with_debezium/index">on-prem</a>, as well as <a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/connectors">fully managed</a> in the cloud), with multiple other vendors providing Debezium-backed offers as well. During the keynote of this year&#8217;s Current conference, Debezium was <a href="https://twitter.com/gunnarmorling/status/1577318988836642816">recognized</a> as one of the most impactful open-source projects in the Apache Kafka space.</p> </div> <div class="paragraph"> <p>The most important part to me though is the tremendous growth of the Debezium community itself. To this day, more than <a href="https://github.com/debezium/debezium/blob/main/COPYRIGHT.txt">450 individuals</a> have contributed to the code base. A big thank you to all the people and organizations who&#8217;ve worked tirelessly to make the vision of open-source change data capture a reality and continue to improve it every day: Red Hat&#8201;&#8212;&#8201;as the project&#8217;s main sponsor&#8201;&#8212;&#8201;Stripe, Instaclustr, SugarCRM, Redis, and many other companies and individual contributors!</p> </div> <div class="paragraph"> <p>After ten amazing years at Red Hat, I felt that it was about time for a change for me and start some new adventure, and I am going to join a start-up in the data streaming space next month. As part of this transition, I am also stepping down from the role as the project lead for Debezium. While I&#8217;ll be less active in the project on a daily basis, I definitely plan to stay involved and hopefully still send the one or other pull request.</p> </div> <div class="paragraph"> <p>My partner in crime <a href="https://github.com/jpechane">Jiri Pechanec</a> will take over as the acting engineering lead. Or, I should say, has taken over, since in fact he has had that role since earlier this year already. Jiri has been a member of the project for many years, working on several key features such as <a href="/blog/2021/10/07/incremental-snapshots/">incremental snapshots</a> and MongoDB change streams support. He&#8217;s an outstanding software engineer, with a unique insight into the problem space of CDC and decades of experience working in open source, and he will be an amazing lead for the Debezium project and community.</p> </div> <div class="paragraph"> <p>With the Debezium 2.0 release just through the door, addressing several consistency issues and getting rid of a fair chunk of technical debt, the project is in an excellent position for its future evolution. There are plans for another community-led connector which should be announced very soon, there&#8217;ll be support for exactly-once semantics as recently introduced in Kafka Connect (<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors">KIP -618</a>), a Kubernetes operator for Debezium Server, a JDBC sink connector, and much more.</p> </div> <div class="paragraph"> <p>The best is yet to come, and I can&#8217;t wait to see what this amazing community will build next!</p> </div>]]></content><author><name>Gunnar Morling</name></author><category term="community"/><category term="news"/><summary type="html"><![CDATA[Some time in early 2017, I got a meeting invite from Debezium&#8217;s founder, Randall Hauch. He was about to begin a new chapter in his professional career and was looking for someone to take over as the project lead for Debezium. So we hopped on a call to talk things through, and I was immediately sold on the concept of change data capture, its large number of potential use cases and applications, and the idea of making this available to the community as open-source. After some short consideration I decided to take up this opportunity, and without a doubt this has been one of the best decisions I&#8217;ve ever made in my job.]]></summary></entry><entry><title type="html">Debezium 1.9.7.Final Released</title><link href="https://debezium.io/blog/2022/10/26/debezium-1-9-7-final-released/" rel="alternate" type="text/html" title="Debezium 1.9.7.Final Released"/><published>2022-10-26T00:00:00+00:00</published><updated>2022-10-26T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/26/debezium-1-9-7-final-released</id><content type="html" xml:base="https://debezium.io/blog/2022/10/26/debezium-1-9-7-final-released/"><![CDATA[<div class="paragraph"> <p>I&#8217;m excited to announce the release of Debezium <strong>1.9.7.Final</strong>!</p> </div> <div class="paragraph"> <p>This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains <a href="https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.7.Final">22 resolved issues</a> overall.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="changes">Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>A few noteworthy bug fixes and stability improvements include:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium connectors ship with an old version of google-protobuf vulnerable to CVE-2022-3171 <a href="https://issues.redhat.com/browse/DBZ-5747">DBZ-5747</a></p> </li> <li> <p>ORA-01289: cannot add duplicate logfile <a href="https://issues.redhat.com/browse/DBZ-5276">DBZ-5276</a></p> </li> <li> <p>Using snapshot boundary mode "all" causes DebeziumException on Oracle RAC <a href="https://issues.redhat.com/browse/DBZ-5302">DBZ-5302</a></p> </li> <li> <p>Missing snapshot pending transactions <a href="https://issues.redhat.com/browse/DBZ-5482">DBZ-5482</a></p> </li> <li> <p>Outbox pattern nested payload leads to connector crash <a href="https://issues.redhat.com/browse/DBZ-5654">DBZ-5654</a></p> </li> <li> <p>Keyword virtual can be used as an identifier <a href="https://issues.redhat.com/browse/DBZ-5674">DBZ-5674</a></p> </li> <li> <p>MongoDB Connector with DocumentDB errors with "{$natural: -1} is not supported" <a href="https://issues.redhat.com/browse/DBZ-5677">DBZ-5677</a></p> </li> <li> <p>Function DATE_ADD can be used as an identifier <a href="https://issues.redhat.com/browse/DBZ-5679">DBZ-5679</a></p> </li> <li> <p>UNIQUE INDEX with NULL value throws exception when lob.enabled is true <a href="https://issues.redhat.com/browse/DBZ-5682">DBZ-5682</a></p> </li> <li> <p>MySqlConnector parse create view statement failed <a href="https://issues.redhat.com/browse/DBZ-5708">DBZ-5708</a></p> </li> <li> <p>Debezium Server 1.9.6 is using MSSQL JDBC 7.2.2 instead of 9.4.1 <a href="https://issues.redhat.com/browse/DBZ-5711">DBZ-5711</a></p> </li> <li> <p>Vitess: Handle Vstream error: unexpected server EOF <a href="https://issues.redhat.com/browse/DBZ-5722">DBZ-5722</a></p> </li> <li> <p>ParsingException: DDL statement couldn&#8217;t be parsed (index hints) <a href="https://issues.redhat.com/browse/DBZ-5724">DBZ-5724</a></p> </li> <li> <p>Oracle SQL parsing error when collation used <a href="https://issues.redhat.com/browse/DBZ-5726">DBZ-5726</a></p> </li> <li> <p>Unparseable DDL statement <a href="https://issues.redhat.com/browse/DBZ-5734">DBZ-5734</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/1.9/release-notes#release-1.9.7-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 1.9.7.Final: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/xinbinhuang">Bin Huang</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/janjwerner-confluent">Jan Werner</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/nilshartmann">Nils Hartmann</a>, <a href="https://github.com/thangdc94">Phạm Ngọc Thắng</a>, <a href="https://github.com/Sage-Pierce">Sage Pierce</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook, What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>This past year has been packed full of tons of changes. This makes the eighth and likely final stable release for Debezium 1.9 as we begin to turn our attention fully to Debezium 2.0 moving forward.</p> </div> <div class="paragraph"> <p>With Debezium 2.0 released on October 17th, just last week, the team is now hard at work addressing your feedback, so keep that coming. We&#8217;re also actively working on the next installment of Debezium, 2.1, which will be released later this year. Be sure to keep an eye on our <a href="/roadmap">road map</a> in the coming week as we intend to debut what is planned for Debezium 2.1 and what&#8217;s to come in 2023!</p> </div> <div class="paragraph"> <p>Until then, stay safe!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[I&#8217;m excited to announce the release of Debezium 1.9.7.Final! This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains 22 resolved issues overall.]]></summary></entry><entry><title type="html">Debugging flaky tests</title><link href="https://debezium.io/blog/2022/10/20/flaky-tests/" rel="alternate" type="text/html" title="Debugging flaky tests"/><published>2022-10-20T00:00:00+00:00</published><updated>2022-10-20T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/20/flaky-tests</id><content type="html" xml:base="https://debezium.io/blog/2022/10/20/flaky-tests/"><![CDATA[<div class="paragraph"> <p>When developing the tests for your project, sooner or later you will probably get into the situation when some of the tests fail randomly. These tests, also known as flaky tests, are very unpleasant as you never know if the failure was random or there is a regression in your code. In the worst case you just ignore these tests because you know they are flaky. Most of the testing frameworks even have a dedicated annotation or other means to express that the test is flaky and if it fails, the failure should be ignored. The value of such a test is very questionable. The best thing you can do with such a test is of course to fix it so that it doesn&#8217;t fail randomly. That&#8217;s easy to say, but harder to do. The hardest part is usually to make the test fail in your development environment so that you can debug it and understand why it fails and what is the root cause of the failure. In this blog post I&#8217;ll try to show a few techniques which may help you to simulate random test failures on you local machine.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>From my experience the most common reason for randomly failing tests is either not properly cleaned environment or slow environment. Both such situations are quite common in CI environments. Failures due to interference with other tests were more common in the past. Nowadays, when usage of virtual machines and containers is quite common, this is usually not an issue. Also, tests isolation implemented by various CI as a services offerings is done well. The downside of using a CI as a service is that you usually cannot log into the machine and debug the tests there. Therefore, you have to either enable debug logs and wait for the next failure or guess what the reason was and try to simulate it on your local machine.</p> </div> <div class="paragraph"> <p>The most common root cause of random failures in CI environments is slowness of various kinds. This is the result of overcommitting the resources in virtual environments or resource limits put on the VMs/containers. Therefore one of the most powerful ways to simulate random test failure locally is to restrict test resources on your local environment. Let&#8217;s see what the common options are and how to do it.</p> </div> <div class="sect1"> <h2 id="running_tests_in_one_thread">Running tests in one thread</h2> <div class="sectionbody"> <div class="paragraph"> <p>One possible way to slow down your tests, especially when you have a multi-threaded application, is to execute the tests on a single thread or limited number of threads. On the Linux operating system, it’s pretty easy with the <code>taskset</code> command. <a href="https://man7.org/linux/man-pages/man1/taskset.1.html">taskset</a> tell the Linux scheduler to attach given process to specified CPU core. To run e.g. Debezium MySQL <a href="https://github.com/debezium/debezium/blob/main/debezium-connector-mysql/src/test/java/io/debezium/connector/mysql/TransactionMetadataIT.java">TransactionMetadataIT</a> on a single CPU core, you just need to run</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>taskset -c 0 mvn verify -DskipTests -Dit.test=DebeziumEngineIT</code></pre> </div> </div> <div class="paragraph"> <p>which would execute the tests on CPU #0.</p> </div> </div> </div> <div class="sect1"> <h2 id="limiting_container_resources">Limiting container resources</h2> <div class="sectionbody"> <div class="paragraph"> <p>On the Debezium project we use containers for tests heavily. We run the databases against the tests run in the containers. What we often need is not to slow down the tests itself, but the database. Docker provides quite a lot of <a href="https://docs.docker.com/config/containers/resource_constraints/">options</a> how to limit container resources. The most useful is usually limiting the CPU using <code>--cpus</code> parameter. This allows us to limit the amount of CPU Docker can use for running the container. The nice thing here is that it can be a float number, so you can e.g. limit containers to use only half of CPU time by setting <code>--cpus=0.5</code>. In a similar way you can also limit other resource, like e.g. RAM.</p> </div> <div class="paragraph"> <p>The common Debezium workflow is to run the containers from Maven, using <a href="https://dmp.fabric8.io/">Docker Maven plugin</a>. The plugin provides <a href="https://dmp.fabric8.io/#property-configuration">long list of properties</a> which you can configure, including properties for limiting container resources. However, there is one caveat with this option. With current release, <code>docker.cpus</code> expect <a href="https://github.com/fabric8io/docker-maven-plugin/issues/1608">long number</a> instead of float and have a meaning, roughly saying, how many CPU nano seconds from one second cycle the container can take. E.g. equivalent of <code>--cpus=0.5</code> would be:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>mvn docker:start -Ddocker.cpus=500000000</code></pre> </div> </div> <div class="paragraph"> <p>This issue was <a href="https://github.com/fabric8io/docker-maven-plugin/pull/1609">fixed</a> recently and should be in the next Docker Maven plugin release, so once Debezium upgrade to the next version, you should be able use <code>docker.cpus</code> in the same way as you would use when running the container from the command line. Other Docker Maven plugin properties, e.g. <code>docker.memory</code> should work as expected.</p> </div> </div> </div> <div class="sect1"> <h2 id="imposing_network_latency">Imposing network latency</h2> <div class="sectionbody"> <div class="paragraph"> <p>Another common source of random test failures is network latency. There&#8217;s probably not any easy way to simulate it on a local machine and one has to use some kind of proxy. Fortunately, there is a proxy exactly for this purpose - <a href="https://github.com/Shopify/toxiproxy">Toxiproxy</a>. It&#8217;s a dedicated proxy to simulate various network failures and latencies. It has a rich feature set and moreover it&#8217;s pretty easy to set it up, so it&#8217;s a pleasure to work with it. Let&#8217;s see how to set it up with Debezium tests on a local machine.</p> </div> <div class="paragraph"> <p>You can install Toxiproxy locally (on Fedora by running <code>sudo dnf install toxiproxy</code>) or download it in a container:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker pull ghcr.io/shopify/toxiproxy</code></pre> </div> </div> <div class="paragraph"> <p>We are going to run Toxiproxy in a container, but it&#8217;s also convenient to install it locally as it contains a CLI utility to send commands to the Toxyproxy. Otherwise we would have to run the commands from the container. For simplicity, we will use a CLI tool installed locally. Toxyproxi allows us to send commands over HTTP and listens on port 8474. Therefore, when we start Toxyproxy, we need to expose this port. Another port we need to expose is the one for the database for which the Toxiproxy will serve as a proxy. In our example we will use MySQL, therefore we need to expose port 3306. We can of course use any other port, but in such a case we would need to pass additional parameter to the Debezium test, namely <code>database.port</code> pointing to the port exposed by Toxiproxy. Again, for simplicity, let&#8217;s stick with the default port 3306. Also, as we are going to run the Debezium tests from the local machine (not from a container), we need to attach Toxiproxy to the localhost network, which is by default named <code>host</code>. Putting everything together, we can run Toxiproxy container as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker run --rm -p 8474:8474 -p 3306:3306 --net=host -it ghcr.io/shopify/toxiproxy</code></pre> </div> </div> <div class="paragraph"> <p>Now we also have to start our database. As the port 3306 is already occupied by Toxiproxy, we have to choose another one, let&#8217;s say 3307:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>mvn docker:start -Dmysql.port=3307</code></pre> </div> </div> <div class="paragraph"> <p>The last missing piece is to tell Toxiproxy for which ports it should create the proxy. In our case it&#8217;s from port 3306 (listen port <code>-l</code>) to 3307 (upstream port <code>-u</code>):</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli create mysql -l 0.0.0.0:3306 -u 0.0.0.0:3307</code></pre> </div> </div> <div class="paragraph"> <p>This command creates a new proxy within Toxiproxy, called <code>mysql</code>. There can be multiple proxies. We can list all the proxies by running</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli list</code></pre> </div> </div> <div class="paragraph"> <p>which gives you output like this:</p> </div> <div class="listingblock"> <div class="content"> <pre>$ toxiproxy-cli list
Name                    Listen          Upstream                Enabled         Toxics
======================================================================================
mysql                   [::]:3306       0.0.0.0:3307            enabled         None</pre> </div> </div> <div class="paragraph"> <p>Now let&#8217;s try if everything works and run some test:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>mvn verify -DskipTests -Ddatabase.hostname=localhost -Pskip-integration-tests -Dit.test=TransactionMetadataIT</code></pre> </div> </div> <div class="paragraph"> <p>Everything should run as normal as we haven&#8217;t created any toxics (latencies or failure) yet. It&#8217;s just a check that the proxy works correctly. If everything works, let&#8217;s create a toxic now:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli toxic add mysql --type latency --attribute latency=500 -n mysql_latency</code></pre> </div> </div> <div class="paragraph"> <p>This will add a network latency of 500 ms on the mysql proxy. The toxic is named "mysql_latency".</p> </div> <div class="paragraph"> <p>You can get more details about specified proxy by running <code>inspect</code> command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli inspect mysql</code></pre> </div> </div> <div class="paragraph"> <p>with output like this:</p> </div> <div class="listingblock"> <div class="content"> <pre>$ toxiproxy-cli inspect mysql
Name: mysql     Listen: [::]:3306       Upstream: 0.0.0.0:3307
======================================================================
Upstream toxics:
Proxy has no Upstream toxics enabled.

Downstream toxics:
mysql_latency:  type=latency    stream=downstream       toxicity=1.00   attributes=[    jitter=0        latency=500     ]</pre> </div> </div> <div class="paragraph"> <p>Now, run the test again. Did you observe that the test ran substantially longer? If yes, everything works as expected, as we added latency to every call to the database.</p> </div> <div class="paragraph"> <p>This is a simple example of adding toxic to the Toxiproxy. Toxiproxy provides many more options and ways to configure the toxics. See <a href="https://github.com/Shopify/toxiproxy">Toxiproxy</a> for more details.</p> </div> <div class="paragraph"> <p>Once we are done, we can remove toxic</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli toxic remove mysql -n mysql_latency</code></pre> </div> </div> <div class="paragraph"> <p>as well as proxy itself:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>toxiproxy-cli delete mysql</code></pre> </div> </div> <div class="paragraph"> <p>or simply stop and delete the container.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary">Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post I tried to show a couple of techniques which may help you to simulate flaky test failures locally. All of them try to make the test environment less responsive, namely by limiting CPU or imposing network latencies using Toxiproxy. There are many other reasons why the tests can be flaky, in many parts of your application stack, and also there are many other tools which can inject various kinds of failures (e.g. disk failures). So this post is not by far exhaustive. But I hope it will help you to debug at least some of the flaky tests, if not in the Debezium project, then at least in your own project.</p> </div> <div class="paragraph"> <p>All these things, especially Toxiproxy, can be also used on a regular basis, even in the CI, to spot various hidden issues in the project which appears only when the environment where it runs doesn&#8217;t behave nicely.</p> </div> <div class="paragraph"> <p>Feel free to share in the discussion any other tips on how to debug flaky tests and what kind of tools you find handy.</p> </div> </div> </div>]]></content><author><name>Vojtěch Juránek</name></author><category term="community"/><category term="tests"/><summary type="html"><![CDATA[When developing the tests for your project, sooner or later you will probably get into the situation when some of the tests fail randomly. These tests, also known as flaky tests, are very unpleasant as you never know if the failure was random or there is a regression in your code. In the worst case you just ignore these tests because you know they are flaky. Most of the testing frameworks even have a dedicated annotation or other means to express that the test is flaky and if it fails, the failure should be ignored. The value of such a test is very questionable. The best thing you can do with such a test is of course to fix it so that it doesn&#8217;t fail randomly. That&#8217;s easy to say, but harder to do. The hardest part is usually to make the test fail in your development environment so that you can debug it and understand why it fails and what is the root cause of the failure. In this blog post I&#8217;ll try to show a few techniques which may help you to simulate random test failures on you local machine.]]></summary></entry><entry><title type="html">Debezium 2.0.0.Final Released</title><link href="https://debezium.io/blog/2022/10/17/debezium-2-0-final-released/" rel="alternate" type="text/html" title="Debezium 2.0.0.Final Released"/><published>2022-10-17T00:00:00+00:00</published><updated>2022-10-17T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/17/debezium-2-0-final-released</id><content type="html" xml:base="https://debezium.io/blog/2022/10/17/debezium-2-0-final-released/"><![CDATA[<div class="paragraph"> <p>Today it&#8217;s my great pleasure to announce the availability of Debezium <strong>2.0.0.Final</strong>!</p> </div> <div class="paragraph"> <p>Since our 1.0 release in December 2019, the community has worked vigorously to build a comprehensive open-source low-latency platform for change data capture (CDC). Over the past three years, we have extended Debezium&#8217;s portfolio to include a stable connector for Oracle, a community led connector for Vitess, the introduction of incremental snapshots, multi-partition support, and so much more. With the help of our active community of contributors and committers, Debezium is the de facto leader in the CDC space, deployed to production within lots of organizations from across multiple industries, using hundreds of connectors to stream data changes out of thousands of database platforms.</p> </div> <div class="paragraph"> <p>The 2.0 release marks a new milestone for Debezium, one that we are proud to share with each of you.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>In this post, we&#8217;re going to take a deep dive into all changes in Debezium 2.0, discussing the new features and explaining all the possible breaking changes that could have an impact during the upgrade process. As always we highly recommend that you take a look at the <a href="/releases/2.0/release-notes#release-2.0.0-final">release notes</a> to learn more about all fixed bugs, update procedures, etc. [release notes], especially when upgrading from an older release.</p> </div> <div class="ulist"> <ul> <li> <p><a href="#core-changes">Changes to core Debezium</a></p> </li> <li> <p><a href="#cassandra-changes">Changes to Cassandra connector</a></p> </li> <li> <p><a href="#mongodb-changes">Changes to MongoDB connector</a></p> </li> <li> <p><a href="#mysql-changes">Changes to MySQL connector</a></p> </li> <li> <p><a href="#oracle-changes">Changes to Oracle connector</a></p> </li> <li> <p><a href="#postgres-changes">Changes to PostgreSQL connector</a></p> </li> <li> <p><a href="#vitess-changes">Changes to Vitess connector</a></p> </li> <li> <p><a href="#container-changes">Changes for Debezium container images</a></p> </li> <li> <p><a href="#community-spaces">Community spaces</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="core-changes">Changes to core Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>The fundamental core of Debezium has changed quite a bit in Debezium 2.0. In this section, we&#8217;re going to dive into the changes with Debezium&#8217;s core, and discuss how those changes impact all users of Debezium.</p> </div> <div class="sect2"> <h3 id="java_11_is_required">Java 11 is required</h3> <div class="paragraph"> <p>We have wanted to make the leap to Java 11 for quite some time, and we felt that with Debezium 2.0 this was the right moment. With Java 11, this enables us to take advantage of new language features, such as the new <code>String</code> API and <code>Predicate</code> support changes within the code base, while also benefiting from many of the Java peformance improvements.</p> </div> <div class="paragraph"> <p>Our very own Vojtech Juranek published <a href="/blog/2022/05/04/switch-to-java-11/">this blog</a> where he discusses the switch to Java 11 in detail. The Java 11 runtime will be required moving forward to use Debezium, so be sure that Java 11 is available prior to upgrading.</p> </div> </div> <div class="sect2"> <h3 id="improved_incremental_snapshots">Improved Incremental Snapshots</h3> <div class="sect3"> <h4 id="stopping">Stopping</h4> <div class="paragraph"> <p>Since we first introduced incremental snapshots, users have asked for a way to stop an in-progress snapshot. To accomplish this, we have added a new signal, <code>stop-snapshot</code>, which allows stopping an in-progress incremental snapshot. This signal is to be sent just like any other, by inserting a row into the signal table/collection, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">INSERT</span> <span class="class">INTO</span> schema.signal_table (id, type,data)
<span class="keyword">VALUES</span> (<span class="string"><span class="delimiter">'</span><span class="content">unique-id</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">stop-snapshot</span><span class="delimiter">'</span></span>, <span class="string"><span class="delimiter">'</span><span class="content">_&lt;signal payload&gt;_`);</span></span></code></pre> </div> </div> <div class="paragraph"> <p>The <code>stop-snapshot</code> payload looks very similar to its <code>execute-snapshot</code> counterpart. An example:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data-collections</span><span class="delimiter">&quot;</span></span>: [<span class="string"><span class="delimiter">&quot;</span><span class="content">schema1.table1</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">schema2.table2</span><span class="delimiter">&quot;</span></span>],
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">incremental</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>This example removes both <code>schema1.table1</code> and <code>schema2.table2</code> from the incremental snapshot, so long as the table or collection had not already finished its incremental snapshot. If other tables or collections remain outstanding after the removal of those specified by <code>data-collections</code>, the incremental snapshot will continue to process those that are outstanding. If no other table or collection remains, the incremental snapshot will stop.</p> </div> <div class="paragraph"> <p>Another example of a <code>stop-snapshot</code> payload is quite simply:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">incremental</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>This example does not specify the <code>data-collections</code> property, it is optional for the <code>stop-snapshot</code> signal. When this property isn&#8217;t specified, the signal implies the current in-progress incremental snapshot should be stopped entirely. This gives the ability to stop an incremental snapshot without knowledge of the current or outstanding tables or collections yet to be captured.</p> </div> </div> <div class="sect3"> <h4 id="pausing_and_resuming">Pausing and Resuming</h4> <div class="paragraph"> <p>Incremental snapshots have become an integral feature in Debezium. The incremental snapshot feature allows users to re-run a snapshot on one or more collections/tables for a variety of reasons. Incremental snapshots were originally introduced with just a <em>start</em> signal. We eventually added the ability to <em>stop</em> an ongoing incremental snapshot or to be able to remove a subset of collections/tables from an in-progress incremental snapshot.</p> </div> <div class="paragraph"> <p>In this release, we&#8217;ve built on top of the existing signal foundation and we&#8217;ve introduced two new signals, one to <em>pause</em> an in-progress incremental snapshot and then another to <em>resume</em> the incremental snapshot if it has previously been paused. To pause an incremental snapshot, a <code>pause-snapshot</code> signal must be sent, and to resume, a <code>resume-snapshot</code> signal can be used.</p> </div> <div class="paragraph"> <p>These two new signals can be sent using the signal table strategy or the Kafka signal topic strategy for MySQL. Please refer to the <a href="https://debezium.io/documentation/reference/2.0/configuration/signalling.html#_signal_actions">signal support documentation</a> for more details on signals and how they work.</p> </div> </div> <div class="sect3"> <h4 id="using_regular_expressions">Using Regular Expressions</h4> <div class="paragraph"> <p>Incremental snapshot signals have required the use of explicit table/collection names in the <code>data-collections</code> payload attribute. While this worked well, there may be situations where broad capture configurations could take advantage of regular expression usage. We already support regular expressions in connector configuration options, such as include/exclude lists, so it made sense to extend that to incremental snapshots as well.</p> </div> <div class="paragraph"> <p>Starting in Debezium 2.0, all incremental snapshot signals can use regular expressions in the <code>data-collections</code> payload property. Using one of the stop signal examples from above, the payload can be rewritten using regular expressions:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data-collections</span><span class="delimiter">&quot;</span></span>: [<span class="string"><span class="delimiter">&quot;</span><span class="content">schema[1|2].table[1|2]</span><span class="delimiter">&quot;</span></span>],
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">incremental</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Just like the explicit usage, this signal with regular expressions would also stop both <code>schema1.table1</code> and <code>schema2.table2</code>.</p> </div> </div> <div class="sect3"> <h4 id="applying_filters_with_sql_conditions">Applying filters with SQL conditions</h4> <div class="paragraph"> <p>Although uncommon, there may be scenarios such as a connector misconfiguration, where a specific record or subset of records needs to be re-emitted to the topic. Unfortunately, incremental snapshots have traditionally been an all-or-nothing type of process, where we would re-emit all records from a collection or table as a part of the snapshot.</p> </div> <div class="paragraph"> <p>In this release, a new <code>additional-condition</code> property can be specified in the signal payload, allowing the signal to dictate a SQL-based predicate to control what subset of records should be included in the incremental snapshot instead of the default behavior of <em>all rows</em>.</p> </div> <div class="paragraph"> <p>The following example illustrates sending an incremental snapshot signal for the <code>products</code> table, but instead of sending all rows from the table to the topic, the <code>additional-condition</code> property has been specified to restrict the snapshot to only send events that relate to product id equal to <code>12</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">execute-snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">data-collections</span><span class="delimiter">&quot;</span></span>: [<span class="string"><span class="delimiter">&quot;</span><span class="content">inventory.products</span><span class="delimiter">&quot;</span></span>],
    <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">INCREMENTAL</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">additional-condition</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">product_id=12</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>We believe this new incremental snapshot feature will be tremendously helpful for a variety of reasons, without always having to re-snapshot all rows when only a subset of data is required.</p> </div> </div> <div class="sect3"> <h4 id="signal_database_collection_added_to_inclusion_filter_automatically">Signal database collection added to inclusion filter automatically</h4> <div class="paragraph"> <p>In prior releases of Debezium, the signal collection/table used for incremental snapshots had to be manually added to your <code>table.include.list</code> connector property. A big theme in this release was improvements on incremental snapshots, so we&#8217;ve taken this opportunity to streamline this as well. Starting in this release, Debezium will automatically add the signal collection/table to the table inclusion filters, avoiding the need for users to manually add it.</p> </div> <div class="paragraph"> <p>This change does not impose any compatibility issues. Connector configurations that already include the signal collection/table in the <code>table.include.list</code> property will continue to work without requiring any changes. However, if you wish to align your configuration with current behavior, you can also safely remove the signal collection/table from the <code>table.include.list</code>, and Debezium will begin to handle this for you automatically.</p> </div> </div> </div> <div class="sect2"> <h3 id="transaction_metadata_changes">Transaction Metadata changes</h3> <div class="paragraph"> <p>A transaction metadata event describes the <em>beginning</em> and the <em>end</em> (commit) of a database transaction. These events are useful for a variety of reasons, including auditing. By default, transaction metadata events are not generated by a connector and to enable this feature, the <code>provide.transaction.metadata</code> option must be enabled.</p> </div> <div class="paragraph"> <p>In Debezium 2.0, both <code>BEGIN</code> and <code>END</code> events include a new field, <code>ts_ms</code>, which is the database timestamp of when the transaction either began or committed depending on the event type. An example of such an event now looks like:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">status</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">END</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">12345</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">2</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1657033173441</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collections</span><span class="delimiter">&quot;</span></span>: [
    {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">s1.a</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    },
    {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">s2.a</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    }
  ]
}</code></pre> </div> </div> <div class="paragraph"> <p>If you are already using the transaction metadata feature, new events will contain this field after upgrading.</p> </div> <div class="paragraph"> <p>If you are not using the transaction metadata feature but find this useful, simply add the <code>provide.transaction.metadata</code> option set to <em>true</em> to your connector configuration. By default, metadata events are emitted to a topic named after your <code>topic.prefix</code> option. This can be overridden by specifying the <code>transaction.topic</code> option, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">topic.prefix=server1
provide.transaction.metadata=true
transaction.topic=my-transaction-events</code></pre> </div> </div> <div class="paragraph"> <p>In this example, all transaction metadata events will be emitted to <code>my-transaction-events</code>. Please see your connector specific configuration for more details.</p> </div> </div> <div class="sect2"> <h3 id="multi_partition_mode_now_the_default">Multi-partition mode now the default</h3> <div class="paragraph"> <p>Many database platforms support multi-tenancy out of the box, meaning you can have one installation of the database engine and have many unique databases. In cases like SQL Server, this traditionally required a separate connector deployment for each unique database. Over the last year, a large effort has been made to break down that barrier and to introduce a common way that any single connector deployment could connect and stream changes from multiple databases.</p> </div> <div class="paragraph"> <p>The first notable change is with the SQL Server connector&#8217;s configuration option, <code>database.dbname</code>. This option has been replaced with a new option called <code>database.names</code>. As multi-partition mode is now default, this new <code>database.names</code> option can be specified using a comma-separated list of database names, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">database.names=TEST1,TEST2</code></pre> </div> </div> <div class="paragraph"> <p>In this example, the connector is being configured to capture changes from two unique databases on the same host installation. The connector will start two unique tasks in Kafka Connect and each task will be responsible for streaming changes from its respective database concurrently.</p> </div> <div class="paragraph"> <p>The second notable change is with connector metrics naming. A connector exposes JMX metrics via beans that are identified with a unique name. With multi-partition mode the default with multiple tasks, each task requires its own metrics bean and so a change in the naming strategy was necessary.</p> </div> <div class="paragraph"> <p>In older versions of Debezium using SQL Server as an example, metrics were available using the following naming strategy:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sql_server:type=connector-metrics,server=&lt;sqlserver.server.name&gt;,context=&lt;context&gt;</code></pre> </div> </div> <div class="paragraph"> <p>In this release, the naming strategy now includes a new <code>task</code> component in the JMX MBean name:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sql_server:type=connector-metrics,server=&lt;sqlserver.server.name&gt;,task=&lt;task.id&gt;,context=&lt;context&gt;</code></pre> </div> </div> <div class="paragraph"> <p>Please review your metrics configurations as the naming changes could have an impact when collecting Debezium metrics.</p> </div> </div> <div class="sect2"> <h3 id="new_storage_module">New storage module</h3> <div class="paragraph"> <p>In this release, we have introduced a new <code>debezium-storage</code> set of artifacts for file- and kafka- based database history and offset storage. This change is the first of several future implementations set to support platforms such as Amazon S3, Redis, and possibly JDBC.</p> </div> <div class="paragraph"> <p>For users who install connectors via plugin artifacts, this should be a seamless change as all dependencies are bundled in those plugin downloadable archives. For users who may embed Debezium in their applications or who may be building their own connector, be aware you may need to add a new storage dependency depending on which storage implementations used.</p> </div> </div> <div class="sect2"> <h3 id="pluggable_topic_selector">Pluggable topic selector</h3> <div class="paragraph"> <p>Debezium&#8217;s default topic naming strategy emits change events to topics named <code>database.schema.table</code>. If you require that topics be named differently, an SMT would normally be added to the connector configuration to adjust this behavior. But, this presents a challenge in situations where one of the components of this topic name, perhaps the database or table name, contains a dot (<code>.</code>) and perhaps an SMT doesn&#8217;t have adequate context.</p> </div> <div class="paragraph"> <p>In this release, a new <code>TopicNamingStrategy</code> was introduced to allow fully customizing this behavior directly inside Debezium. The default naming strategy implementation should suffice in most cases, but if you find that it doesn&#8217;t you can provide a custom implementation of the <code>TopicNamingStrategy</code> contract to fully control various namings used by the connector. To provide your own custom strategy, you would specify the <code>topic.naming.strategy</code> connector option with the fully-qualified class name of the strategy, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">topic.naming.strategy=org.myorganization.MyCustomTopicNamingStrategy</code></pre> </div> </div> <div class="paragraph"> <p>This custom strategy is not just limited to controlling the names of topics for table mappings, but also for schema changes, transaction metadata, and heartbeats. You can refer to the <code>DefaultTopicNamingStrategy</code> found <a href="https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/schema/DefaultTopicNamingStrategy.java">here</a> as an example. This feature is still incubating, and we&#8217;ll continue to improve and develop it as feedback is received.</p> </div> </div> <div class="sect2"> <h3 id="improved_unique_index_handling">Improved unique index handling</h3> <div class="paragraph"> <p>A table does not have to have a primary key to be captured by a Debezium connector. In cases where a primary key is not defined, Debezium will inspect a table&#8217;s unique indices to see whether a reasonable key substitution can be made. In some situations, the index may refer to columns such as <code>CTID</code> for PostgreSQL or <code>ROWID</code> in Oracle. These columns are not visible nor user-defined, but instead are hidden synthetic columns generated automatically by the database. In addition, the index may also use database functions to transform the column value that is stored, such as <code>UPPER</code> or <code>LOWER</code> for example.</p> </div> <div class="paragraph"> <p>In this release, indices that rely on hidden, auto-generated columns, or columns wrapped in database functions are no longer eligible as primary key alternatives. This guarantees that when relying on an index as a primary key rather than a defined primary key itself, the generated message&#8217;s primary key value tuple directly maps to the same values used by the database to represent uniqueness.</p> </div> </div> <div class="sect2"> <h3 id="new_configuration_namespaces">New configuration namespaces</h3> <div class="paragraph"> <p>One of the largest overhauls going into Debezium 2.0 is the introduction of new connector property namespaces. Starting in Debezium 2.0 Beta2 and onward, many connector properties have been relocated with new names. This is a breaking change and affects most, if not all, connector deployments during the upgrade process.</p> </div> <div class="paragraph"> <p>Debezium previously used the prefix "database." with a plethora of varied connector properties. Some of these properties were meant to be passed directly to the JDBC driver and in other cases to the database history implementations, and so on. Unfortunately, we identified situations where some properties were being passed to underlying implementations that weren&#8217;t intended. While this wasn&#8217;t creating any type of regression or problem, it could potentially introduce a future issue if there were property name collisions, for example, a JDBC driver property that matched with a "database." prefixed Debezium connector property.</p> </div> <div class="paragraph"> <p>The following describes the changes to the connector properties</p> </div> <div class="ulist"> <ul> <li> <p>All configurations previously prefixed as <code>database.history.</code> are now to be prefixed using <code>schema.history.internal.</code> instead.</p> </li> <li> <p>All JDBC pass-thru options previously specified using <code>database.</code> prefix should now be prefixed using <code>driver.</code> instead.</p> </li> <li> <p>The <code>database.server.name</code> connector property renamed to <code>topic.prefix</code>.</p> </li> <li> <p>The MongoDB <code>mongodb.name</code> connector property aligned to use <code>topic.prefix</code> instead.</p> </li> </ul> </div> <div class="paragraph"> <p>Again, please review your connector configurations prior to deployment and adjust accordingly.</p> </div> </div> <div class="sect2"> <h3 id="all_schemas_named_and_versioned">All schemas named and versioned</h3> <div class="paragraph"> <p>Debezium change events are emitted with a schema definition, which contains metadata about the fields such as the type, whether it&#8217;s required, and so on. In previous iterations of Debezium, some schema definitions did not have explicit names nor were they being explicitly versioned. In this release, we&#8217;ve moved to making sure that all schema definitions have an explicit name and version associated with them. The goal of this change is to help with future event structure compatibility, particularly for those who are using schema registries. However, if you are currently using a schema registry, be aware that this change may lead to schema compatibility issues during the upgrade process.</p> </div> </div> <div class="sect2"> <h3 id="truncate_events_are_skipped_by_default">Truncate events are skipped by default</h3> <div class="paragraph"> <p>Debezium supports skipping specific event types by including the <code>skipped.operations</code> connector property in the connector&#8217;s configuration. This feature can be useful if you&#8217;re only interested in a subset of operations, such as only inserts and updates but not deletions.</p> </div> <div class="paragraph"> <p>One specific event type, truncates (<code>t</code>), is only supported by a subset of relational connectors and whether these events were to be skipped wasn&#8217;t consistent. In this release, we have aligned the <code>skipped.operations</code> behavior so that if the connector supports truncate events, these events are skipped by default.</p> </div> <div class="paragraph"> <p>Please review the following rule-set:</p> </div> <div class="ulist"> <ul> <li> <p>Connector supports truncate events and isn&#8217;t the Oracle connector</p> </li> <li> <p>Connector configuration does not specify the <code>skipped.operations</code> in the configuration</p> </li> </ul> </div> <div class="paragraph"> <p>If all the above are true, then the connector&#8217;s behavior will change after the upgrade. If you wish to continue to emit truncate events, the <code>skipped.operations=none</code> configuration will be required.</p> </div> </div> <div class="sect2"> <h3 id="change_in_schema_name_adjustment_behavior">Change in <code>schema.name.adjustment</code> behavior</h3> <div class="paragraph"> <p>The <code>schema.name.adjustment.mode</code> configuration property controls how schema names should be adjusted for compatibility with the message converter used by the connector. This configuration option can be one of two values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>avro</code></dt> <dd> <p>Replicates the characters that cannot be used in the Avro type name with an underscore.</p> </dd> <dt class="hdlist1"><code>none</code></dt> <dd> <p>Does not adjust the names, even when non-Avro compliant characters are detected.</p> </dd> </dl> </div> <div class="paragraph"> <p>In prior releases, Debezium always defaulted to the safe value of <code>avro</code>; however, starting with Debezium 2.0.0.CR1 the default value will now be <code>none</code>. We believe that given that the use of Avro serialization is something opted in by users based on their needs, this option should align with the same opt-in behavior.</p> </div> <div class="paragraph"> <p>The safe upgrade path would be to adjust your configuration and explicitly use <code>schema.name.adjustment.mode</code> as <code>avro</code> and use the default for new connector deployments. But you can also review your topic names and configurations, checking that no underscore substitutions are happening and ergo this change will have no impact.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="cassandra-changes">Changes to Cassandra connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="cassandra_4_incremental_commit_log_support">Cassandra 4 incremental commit log support</h3> <div class="paragraph"> <p><a href="https://cassandra.apache.org/doc/latest/cassandra/operating/cdc.html">Cassandra 4</a> has improved the integration with CDC by adding a feature that when the fsync operation occurs, Cassandra will update a CDC-based index file to contain the latest offset values. This index file allows CDC implementations to read up to the offset that is considered durable in Cassandra.</p> </div> <div class="paragraph"> <p>In this release, Debezium now uses this CDC-based index file to eliminate the inherent delay in processing CDC events from Cassandra that previously existed. This should provide Cassandra users a substantial improvement in CDC with Debezium, and gives an incentive to consider Cassandra 4 over Cassandra 3.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="mongodb-changes">Changes to MongoDB connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="removal_of_the_oplog_implementation">Removal of the oplog implementation</h3> <div class="paragraph"> <p>In Debezium 1.8, we introduced the new MongoDB change stream feature while also deprecating the oplog implementation. The transition to change streams offers a variety of benefits, such as being able to stream changes from non-primary nodes, the ability to emit update events with a full document representation for downstream consumers, and so much more. In short, change streams is just a much more superior way to perform change data capture with MongoDB.</p> </div> <div class="paragraph"> <p>The removal of the oplog implementation also means that MongoDB 3.x is no longer supported. If you are using MongoDB 3.x, you will need to upgrade to at least MongoDB 4.0 or later with Debezium 2.0.</p> </div> </div> <div class="sect2"> <h3 id="before_state_support_mongodb_6_0">Before state support (MongoDB 6.0)</h3> <div class="paragraph"> <p>MongoDB 6 supports capturing the state of the document before the change is applied. This has long since been a feature that has been available only to the relational-based connectors, but this now enables Debezium to also include the <code>before</code> field as part of the event&#8217;s payload for MongoDB.</p> </div> <div class="paragraph"> <p>To enable this new MongoDB 6+ behavior, the <code>capture.mode</code> setting has been adjusted to include two new values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>change_streams_with_pre_image</code></dt> <dd> <p>The change event will also contain the full document from <em>before</em> the change as well as the final state of the document fields that were changed as a part of the change event.</p> </dd> <dt class="hdlist1"><code>change_streams_update_full_with_pre_image</code></dt> <dd> <p>When an update occurs, not only will the full document be present to represent the current state after the update, but the event will also contain the full document from <em>before</em> the change as well.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The MongoDB <code>before</code> field behavior is only available on MongoDB 6 or later. If you are using a version of MongoDB before 6.0, the <code>before</code> field is omitted from the event output, even if configured.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="mysql-changes">Changes to MySQL connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="legacy_mysql_implementation_removed">Legacy MySQL implementation removed</h3> <div class="paragraph"> <p>As some of you may or may not know, we implemented the MySQL connector based on the common-connector framework back in Debezium 1.5 (Feb 2021). As a part of that re-write, we introduced the ability for MySQL users to enable the legacy connector behavior using the configuration option <code>internal.implementation</code> set as <code>legacy</code>. This legacy implementation was deprecated in favor of the new common-connector framework behavior. With Debezium 2.0, this <code>internal.implementation</code> configuration option and the legacy connector implementation have been removed.</p> </div> <div class="paragraph"> <p>If your current connector deployment relies on this legacy implementation, you should be aware that by upgrading to Debezium 2.0, the connector will no longer use that older implementation and will use the common-connector implementation only. Feature-wise, both implementations are on-par with one another with one exception: the legacy implementation had experimental support for changing filter configurations. If you have relied on this legacy behavior, be aware that feature is no longer available.</p> </div> </div> <div class="sect2"> <h3 id="binlog_compression_support">Binlog Compression Support</h3> <div class="paragraph"> <p>In this release, Debezium now supports reading of binlog entries that have been written with compression enabled. In version 8.0.20, MySQL adds the ability to compress binlog events using the ZSTD algorithm. To enable compression, you must toggle the <code>binlog.transaction_compression</code> variable on the MySQL server to <code>ON</code>. When compression is enabled, the binlog behaves as usual, except that the contents of the binlog entries are compressed to save space, and are replicated to in compressed format to replicas, significantly reducing network overhead for larger transactions.</p> </div> <div class="paragraph"> <p>If you&#8217;re interested in reading more about MySQL binlog compression, you can refer to the <a href="https://dev.mysql.com/doc/refman/8.0/en/binary-log-transaction-compression.html">Binary Log Transaction Compression</a> section of the MySQL documentation for more details.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="oracle-changes">Changes to Oracle connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="oracle_source_info_changes">Oracle source info changes</h3> <div class="paragraph"> <p>The <code>source</code> information block is a section in the change event&#8217;s payload that describes the database attributes of what generated the change event. For example, this section includes the system change number, the database timestamp of the change, and the transaction the change was part of.</p> </div> <div class="paragraph"> <p>In this release, we identified a regression where the <code>scn</code> field did not correctly reflect the right <code>source</code> of where the change event occurred. While it isn&#8217;t abnormal for Oracle to generate multiple changes with the same system change number, we did find a regression that caused the wrong system change number to get assigned to each individual event within a scoped transaction, which made it difficult for some to use this information for auditing purposes. The <code>source.scn</code> field should now correctly reflect the system change number from Oracle LogMiner or Oracle Xstream.</p> </div> <div class="paragraph"> <p>Additionally, several new fields were added to the <code>source</code> information block to improve integration with the LogMiner implementation and Oracle RAC. An example of the new source information block:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">version</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2.0.0.Alpha3</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">server1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1520085154000</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">6.28.807</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2122184</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2122185</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">rs_id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">001234.00012345.0124</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">ssn</span><span class="delimiter">&quot;</span></span>: <span class="integer">0</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">redo_thread</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>The newly added fields are:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>rs_id</code></dt> <dd> <p>Specifies the rollback segment identifier associated with the change.</p> </dd> <dt class="hdlist1"><code>ssn</code></dt> <dd> <p>Specifies the SQL sequence number, this combined with the <code>rs_id</code> represent a unique tuple for a change.</p> </dd> <dt class="hdlist1"><code>redo_thread</code></dt> <dd> <p>Specifies the actual database redo thread that managed the change&#8217;s lifecycle.</p> </dd> </dl> </div> <div class="paragraph"> <p>Whether using Oracle Standalone or RAC, these values will always be provided when using Oracle LogMiner. These values have more importance on an Oracle RAC installation because you have multiple database servers manipulating the shared database concurrently. These fields specifically annotate which node and at what position on that node that the change originated.</p> </div> </div> <div class="sect2"> <h3 id="oracle_connector_offset_changes">Oracle connector offset changes</h3> <div class="paragraph"> <p>In an Oracle Real Application Clusters (RAC) environment, multiple nodes access and manipulate the Oracle database concurrently. Each node maintains its own redo log buffers and executes its own redo writer thread. This means that at any given moment, each node has its own unique "position" and these will differ entirely on the activity that takes place on each respective node.</p> </div> <div class="paragraph"> <p>In this release, a small change was necessary in <a href="https://issues.redhat.com/browse/DBZ-5245">DBZ-5245</a> to support Oracle RAC. Previously, the connector offsets maintained a field called <code>scn</code> which represented this "position" of where the connector should stream changes from. But since each node could be at different positions in the redo, a single <code>scn</code> value was inadequate for Oracle RAC.</p> </div> <div class="paragraph"> <p>The old Oracle connector offsets looked like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1234567890</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2345678901</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">lcr_position</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Starting in Debezium 2.0, the new offset structure now has this form:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1234567890:00124.234567890.1234:0:1,1234567891:42100.0987656432.4321:0:2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2345678901</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">lcr_position</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>You will notice that the <code>scn</code> field now consists of a comma-separated list of values, where each entry represents a tuple of values. This new tuple has the format of <code>scn:rollback-segment-id:ssn:redo-thread</code>.</p> </div> <div class="paragraph"> <p>This change is forward compatible, meaning that once you have upgraded to Debezium 2.0, an older version of the connector will be unable to read the offsets. If you do upgrade and decide to rollback, be aware the offsets will require manually adjusting the offset&#8217;s <code>scn</code> field to simply contain a string of the most recent <code>scn</code> value across all redo threads.</p> </div> </div> <div class="sect2"> <h3 id="oracle_commit_user_in_change_events">Oracle commit user in change events</h3> <div class="paragraph"> <p>The source information block of change events carry a variety of context about where the change event originated. In this release, the Oracle connector now includes the user who made the database change in the captured change event. A new field, <code>user_name</code>, can now be found in the source info block with this new information. This field is optional, and is only available when changes are emitted using the LogMiner-based implementation. This field may also contain the value of <code>UNKNOWN</code> if the user associated with a change is dropped prior to the change being captured by the connector.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="postgres-changes">Changes to PostgreSQL connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="support_for_wal2json_removed">Support for wal2json removed</h3> <div class="paragraph"> <p>Throughout Debezium&#8217;s lifecycle, the PostgreSQL connector has supported multiple decoder implementations, including <code>decoderbufs</code>, <code>wal2json</code>, and <code>pgoutput</code>. Both the <code>decoderbufs</code> and <code>wal2json</code> plugins have required special libraries to be installed on the database server to capture changes from PostgreSQL.</p> </div> <div class="paragraph"> <p>With PostgreSQL 9.6 marked as <a href="https://www.postgresql.org/support/versioning/">end of life</a> in November 2021, we felt now was a great opportunity to streamline the number of supported decoders. With PostgreSQL 10 and later supporting the <code>pgoutput</code> decoder natively, we concluded that it made sense to remove support for the <code>wal2json</code> plugin in Debezium 2.0.</p> </div> <div class="paragraph"> <p>If you are still using PostgreSQL 9.6 or the <code>wal2json</code> decoder, you will be required to upgrade to PostgreSQL 10+ or to either to the <code>decoderbufs</code> or the native <code>pgoutput</code> plugin to use Debezium going forward.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="vitess-changes">Changes to Vitess connector</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="multitasking_support_for_vitess">Multitasking support for Vitess</h3> <div class="paragraph"> <p>The Vitess connector previously allowed operation in two different modes that depended entirely on whether the connector configuration specified any shard details. Unfortunately in both cases, each resulted in a single task responsible for performing the VStream processing. For larger Vitess installations with many shards, this architecture could begin to show latency issues as it may not be able to keep up with all the changes across all shards. And even more complex, when specifying the shard details, this required manually resolving the shards across the cluster and starting a single Debezium connector per shard, which is both error-prone and more importantly could result in deploying many Debezium connectors.</p> </div> <div class="paragraph"> <p>The Vitess community recognized this and sought to find a solution that addresses all these problems, both from a maintenance and error perspective. In Debezium 2.0 Beta2, the Vitess connector now automatically resolves the shards via a discovery mechanism, quite similar to that of MongoDB. This discovery mechanism will then split the load across multiple tasks, allowing for a single deployment of Debezium running a task per shard or shard lists, depending on the maximum number of allowed tasks for the connector.</p> </div> <div class="paragraph"> <p>During the upgrade, the Vitess connector will automatically migrate the offset storage to the new format used with the multitasking behavior. But be aware that once you&#8217;ve upgraded, you won&#8217;t be able to downgrade to an earlier version as the offset storage format will have changed.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="container-changes">Changes for Debezium container images</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="support_for_arm64">Support for ARM64</h3> <div class="paragraph"> <p>There has been a shift in recent years with the performance of ARM64, even at AWS where their 64-bit ARM processors have projected performance over the latest x86-64 processors. This has helped put an emphasis across the industry at looking at the cost benefits of supporting both architectures with containers.</p> </div> <div class="paragraph"> <p>Since Debezium has traditionally released <code>linux/amd64</code> -based container images, this required that you either run the images using emulation of inside a Virtual Machine. This leads to unnecessary overhead and potential performance concerns and the goal of Debezium is low-latency and hyper speed! Starting with Debezium 2.0, Debezium is now also released using <code>ARM64</code> -based container images, reducing the overhead needed.</p> </div> <div class="paragraph"> <p>We hope the new ARM64 container images improve the adoption of Debezium, and show that we&#8217;re committed to delivering the best change data capture experience across the industry universally.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="community-spaces">Community spaces</h2> <div class="sectionbody"> <div class="paragraph"> <p>Later this week, there will be several new <em>community-driven</em> discussion spaces available on our Zulip chat platform. We will be publishing a blog post that discusses the purpose of these new channels and their goals, but we wanted to also include a note here about this new feature.</p> </div> <div class="paragraph"> <p>Unlike the <code>#users</code> channel that is meant to provide community-driven support, these spaces are meant to provide a place for the community to discuss experiences with specific database technologies, Debezium services, and topics that are substantially broader than just support. These spaces will be divided by technology, allowing the user community to target specific areas of interest easily, and engage in discussions that pertain to specific databases and services.</p> </div> <div class="paragraph"> <p>These spaces are not meant to be support venues, we will still expect those to continue to foster in the <code>#users</code> channel going forward, so keep an eye out for these new community spaces later this week and the blog to follow.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were many bugfixes, stability changes, and improvements throughout the development of Debezium 2.0. Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(2.0.0.Alpha1%2C%202.0.0.Alpha2%2C%202.0.0.Alpha3%2C%202.0.0.Beta1%2C%202.0.0.Beta2%2C%202.0.0.CR1%2C%202.0.0.Final)%20ORDER%20BY%20component%20ASC">463 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this major release: Wang Min Chao, Rotem[Adhoh], <a href="https://github.com/ahmedjami">Ahmed ELJAMI</a>, <a href="https://github.com/almartino">Alberto Martino</a>, <a href="https://github.com/ahus1">Alexander Schwartz</a>, <a href="https://github.com/aloubyansky">Alexey Loubyansky</a>, <a href="https://github.com/AlexMiroshnikov">Alexey Miroshnikov</a>, Gabor[Andras], <a href="https://github.com/ajunwalker">Andrew Walker</a>, <a href="https://github.com/jchipmunk">Andrey Pustovetov</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/avis408">Avinash Vishwakarma</a>, <a href="https://github.com/xinbinhuang">Bin Huang</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/bmorganpa">Brad Morgan</a>, <a href="https://github.com/calinilie">Calin Laurentiu Ilie</a>, <a href="https://github.com/chadthman">Chad Marmon</a>, <a href="https://github.com/ProofOfPizza">Chai Stofkoper</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/Chrisss93">Chris Lee</a>, <a href="https://github.com/davsclaus">Claus Ibsen</a>, <a href="https://github.com/connorszczepaniak-wk">Connor Szczepaniak</a>, <a href="https://github.com/cmartinez-enve">César Martínez</a>, <a href="https://github.com/debjeetsarkar">Debjeet Sarkar</a>, Mikhail[Dubrovin], <a href="https://github.com/elirag">Eliran Agranovich</a>, <a href="https://github.com/EthanZ328">Ethan Zou</a>, <a href="https://github.com/ezerk">Ezer Karavani</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/giljae">Giljae Joo</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/ruanhang1993">Hang Ruan</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/Himanshu-LT">Himanshu Mishra</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/nicholas-fwang">Inki Hwang</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/domsj">Jan Doms</a>, <a href="https://github.com/DerGut">Jannik Steinmann</a>, <a href="https://github.com/jerrinot">Jaromir Hamala</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/Jiabao-Sun">Jiabao Sun</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/yannickzj">Jun Zhao</a>, <a href="https://github.com/kanha-gupta">Kanha Gupta</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/LarsWerkman">Lars Werkman</a>, <a href="https://github.com/winklerm">Marek Winkler</a>, <a href="https://github.com/markallanson">Mark Allanson</a>, <a href="https://github.com/alwaysbemark">Mark Bereznitsky</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/mimaison">Mickael Maison</a>, <a href="https://github.com/mikekamornikov">Mike Kamornikov</a>, <a href="https://github.com/yzia2000">Mohammad Yousuf Minhaj Zia</a>, <a href="https://github.com/nathan-bradshaw-at">Nathan Bradshaw</a>, <a href="https://github.com/nathan-smit-1">Nathan Smit</a>, <a href="https://github.com/krnaveen14">Naveen Kumar KR</a>, <a href="https://github.com/nilshartmann">Nils Hartmann</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/nitinitt">Nitin Chhabra</a>, <a href="https://github.com/zalmane">Oren Elias</a>, <a href="https://github.com/ypt">Paul Tzen</a>, <a href="https://github.com/pmalon">Paweł Malon</a>, <a href="https://github.com/smallYellowCat">Pengwei Dou</a>, <a href="https://github.com/thangdc94">Phạm Ngọc Thắng</a>, <a href="https://github.com/PlugaruT">Plugaru Tudor</a>, Oskar[Polak], <a href="https://github.com/rahulkhanna2">Rahul Khanna</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/rk3rn3r">René Kerner</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/druud">Ruud H.G. van Tol</a>, <a href="https://github.com/sagarrao12">Sagar Rao</a>, <a href="https://github.com/Sage-Pierce">Sage Pierce</a>, <a href="https://github.com/jaegwonseo">Seo Jae-kwon</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/shichao-an">Shichao An</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/tim-patterson">Tim Patterson</a>, <a href="https://github.com/troeselereos">Timo Roeseler</a>, <a href="https://github.com/ramanenka">Vadzim Ramanenka</a>, <a href="https://github.com/vivekwassan">Vivek Wassan</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/xinbinhuang">Xinbin Huang</a>, <a href="https://github.com/y5w">Yang</a>, <a href="https://github.com/spicy-sauce">Yossi Shirizli</a>, <a href="https://github.com/GOODBOY008">Zhongqiang Gong</a>, <a href="https://github.com/gmouss">moustapha mahfoud</a>, <a href="https://github.com/yangrong688">yangrong688</a>, <a href="https://github.com/BetaCat0">合龙 张</a>, <a href="https://github.com/comil4444">崔世杰</a>, and <a href="https://github.com/pkgonan">민규 김</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>While we are heading into the holiday season, we have started the work on Debezium 2.1, which will be out later this year. Some potential features you can expect include:</p> </div> <div class="ulist"> <ul> <li> <p>Truncate support for MySQL</p> </li> <li> <p>PostgreSQL 15 support</p> </li> <li> <p>JDBC history and offset storage support</p> </li> </ul> </div> <div class="paragraph"> <p>As always, this roadmap is heavily influenced by the community, i.e. you. So if you would like to see any particular items here, please let us know. For now, lets celebrate the hard work in the release of Debezium 2.0 and look forward to what&#8217;s coming later this year and in 2023!</p> </div> <div class="paragraph"> <p>Onwards and Upwards!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="mongodb"/><summary type="html"><![CDATA[Today it&#8217;s my great pleasure to announce the availability of Debezium 2.0.0.Final! Since our 1.0 release in December 2019, the community has worked vigorously to build a comprehensive open-source low-latency platform for change data capture (CDC). Over the past three years, we have extended Debezium&#8217;s portfolio to include a stable connector for Oracle, a community led connector for Vitess, the introduction of incremental snapshots, multi-partition support, and so much more. With the help of our active community of contributors and committers, Debezium is the de facto leader in the CDC space, deployed to production within lots of organizations from across multiple industries, using hundreds of connectors to stream data changes out of thousands of database platforms. The 2.0 release marks a new milestone for Debezium, one that we are proud to share with each of you.]]></summary></entry><entry><title type="html">Debezium 2.0.0.CR1 Released</title><link href="https://debezium.io/blog/2022/10/10/debezium-2.0-cr1-released/" rel="alternate" type="text/html" title="Debezium 2.0.0.CR1 Released"/><published>2022-10-10T00:00:00+00:00</published><updated>2022-10-10T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/10/debezium-2.0-cr1-released</id><content type="html" xml:base="https://debezium.io/blog/2022/10/10/debezium-2.0-cr1-released/"><![CDATA[<div class="paragraph"> <p>I am excited to announce the release of Debezium <strong>2.0.0.CR1</strong>!</p> </div> <div class="paragraph"> <p>This release contains breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.CR1%20ORDER%20BY%20component%20ASC">53 issues</a> that were fixed.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>If you intend to upgrade to 2.0.0.CR1, we strongly recommend that you read the release notes before the upgrade to understand all breaking changes. There was one noteworthy breaking changes with the 2.0.0.CR1 release:</p> </div> <div class="ulist"> <ul> <li> <p>[breaking] <a href="#schema-name-adjustment-mode">Behavior of <code>schema.name.adjustment.mode</code> has changed</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="schema-name-adjustment-mode">Behavior of schema.name.adjustment.mode has changed</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>schema.name.adjustment.mode</code> configuration property controls how schema names should be adjusted for compatibility with the message converter used by the connector. This configuration option can be one of two values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>avro</code></dt> <dd> <p>Repliaces the characters that cannot be used in the Avro type name with an underscore.</p> </dd> <dt class="hdlist1"><code>none</code></dt> <dd> <p>Does not adjust the names, even when non-Avro compliant characters are detected.</p> </dd> </dl> </div> <div class="paragraph"> <p>In prior releases, Debezium always defaulted to the safe value of <code>avro</code>; however, starting with Debezium 2.0.0.CR1 the default value will now be <code>none</code>. We believe that given that the use of Avro serialization is something opted in by users based on their needs, this option should align with the same opt-in behavior.</p> </div> <div class="paragraph"> <p>The safe upgrade path would be to adjust your configuration and explicitly use <code>schema.name.adjustment.mode</code> as <code>avro</code> and use the default for new connector deployments. But you can also review your topic names and configurations, checking that no underscore substitutions are happening and ergo this change will have no impact.</p> </div> </div> </div> <div class="sect1"> <h2 id="mongodb_6_0_before_state_support">MongoDB 6.0 - before state support</h2> <div class="sectionbody"> <div class="paragraph"> <p>MongoDB 6 supports capturing the state of the document before the change is applied. This has long since been a feature that has been available only to the relational-based connectors, but this now enables Debezium to also include the <code>before</code> field as part of the event&#8217;s payload for MongoDB.</p> </div> <div class="paragraph"> <p>To enable this new MongoDB 6+ behavior, the <code>capture.mode</code> setting has been adjusted to include two new values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>change_streams_with_pre_image</code></dt> <dd> <p>The change event will also contain the full document from <em>before</em> the change as well as the final state of the document fields that were changed as a part of the change event.</p> </dd> <dt class="hdlist1"><code>change_streams_update_full_with_pre_image</code></dt> <dd> <p>When an update occurs, not only will the full document be present to represent the current state after the update, but the event will also contain the full document from <em>before</em> the change as well.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The MongoDB <code>before</code> field behavior is only available on MongoDB 6 or later. If you are using a version of MongoDB before 6.0, the <code>before</code> field is omitted from the event output, even if configured.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are many bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Implement retries for Debezium embedded engine <a href="https://issues.redhat.com/browse/DBZ-4629">DBZ-4629</a></p> </li> <li> <p>Traditional snapshot process setting source.ts_ms <a href="https://issues.redhat.com/browse/DBZ-5591">DBZ-5591</a></p> </li> <li> <p>Upgrade Kafka client to 3.3.1 <a href="https://issues.redhat.com/browse/DBZ-5600">DBZ-5600</a></p> </li> <li> <p>Support READ ONLY/ENCRYPTION options for alter database statment <a href="https://issues.redhat.com/browse/DBZ-5622">DBZ-5622</a></p> </li> <li> <p>Clarify semantics of include/exclude options <a href="https://issues.redhat.com/browse/DBZ-5625">DBZ-5625</a></p> </li> <li> <p>Added support for Mongo pre-image in change stream <a href="https://issues.redhat.com/browse/DBZ-5628">DBZ-5628</a></p> </li> <li> <p>Support for using any expression in kill statements <a href="https://issues.redhat.com/browse/DBZ-5636">DBZ-5636</a></p> </li> <li> <p>Debezium Db2 Connector fails to handle default values in schema when is making the snapshot <a href="https://issues.redhat.com/browse/DBZ-4990">DBZ-4990</a></p> </li> <li> <p>Oracle connector parsing SELECT_LOB_LOCATOR event missing constant <code>unavailable.value.placeholder</code> <a href="https://issues.redhat.com/browse/DBZ-5581">DBZ-5581</a></p> </li> <li> <p>Starting Embedded Engine swallows ClassNotFoundException so user cannot see why engine does not work <a href="https://issues.redhat.com/browse/DBZ-5583" class="bare">https://issues.redhat.com/browse/DBZ-5583</a>[DBZ-558</p> </li> <li> <p>Hardcoded driver task properties are not being passed to underlying connections <a href="https://issues.redhat.com/browse/DBZ-5670">DBZ-5670</a></p> </li> <li> <p>MongoDB Connector with DocumentDB errors with "{$natural: -1} is not supported" <a href="https://issues.redhat.com/browse/DBZ-5677">DBZ-5677</a></p> </li> <li> <p>Upgrade apicurio to 2.2.5.Final <a href="https://issues.redhat.com/browse/DBZ-5549">DBZ-5549</a></p> </li> <li> <p>Upgrade binary log client to 0.27.2 <a href="https://issues.redhat.com/browse/DBZ-5620">DBZ-5620</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Beta2%20ORDER%20BY%20component%20ASC">53 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ahus1">Alexander Schwartz</a>, Gabor Andras, <a href="https://github.com/avis408">Avinash Vishwakarma</a>, <a href="https://github.com/xinbinhuang">Bin Huang</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/ezerk">Ezer Karavani</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jerrinot">Jaromir Hamala</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/Sage-Pierce">Sage Pierce</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/xinbinhuang">Xinbin Huang</a>, and <a href="https://github.com/gmouss">moustapha mahfoud</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With the release of Debezium 2.0 CR1, the release of 2.0.0.Final is just around the corner. The community should expect the Final release soon, barring any bug reports. In addition, we are also working on wrapping up the last installation of the 1.9 release stream, 1.9.7.Final which should will be released toward the end of this month.</p> </div> <div class="paragraph"> <p>With the holiday season fast approaching, we will soon begin work on Debezium 2.1. We do intend to have a normal release cycle this quarter despite being behind on Debezium 2.0, so expect that sometime just before the end of the year.</p> </div> <div class="paragraph"> <p>In the meantime, happy capturing!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="mongodb"/><summary type="html"><![CDATA[I am excited to announce the release of Debezium 2.0.0.CR1! This release contains breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of 53 issues that were fixed.]]></summary></entry><entry><title type="html">Debezium for Oracle - Part 2: Running the connector</title><link href="https://debezium.io/blog/2022/10/06/debezium-oracle-series-part-2/" rel="alternate" type="text/html" title="Debezium for Oracle - Part 2: Running the connector"/><published>2022-10-06T12:00:00+00:00</published><updated>2022-10-06T12:00:00+00:00</updated><id>https://debezium.io/blog/2022/10/06/debezium-oracle-series-part-2</id><content type="html" xml:base="https://debezium.io/blog/2022/10/06/debezium-oracle-series-part-2/"><![CDATA[<div class="paragraph"> <p>This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first part of this series is <a href="/blog/2022/09/30/debezium-oracle-series-part-1/">here</a>.</p> </div> <div class="paragraph"> <p>In this second installment, we will build on what we did in part one by deploying the Oracle connector using Zookeeper, Kafka, and Kafka Connect. We are going to discuss a variety of configuration options for the connector and why they&#8217;re essential. And finally, we&#8217;re going to see the connector in action!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="setting_up_kafka_connect_and_prerequisites">Setting up Kafka Connect and prerequisites</h2> <div class="sectionbody"> <div class="paragraph"> <p>In order to use Debezium, three separate services need to be started:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#start-zookeeper">Zookeeper</a></p> </li> <li> <p><a href="#start-kafka">Kafka broker</a></p> </li> <li> <p><a href="#start-kafka-connect">Kafka Connect</a></p> </li> </ul> </div> <div class="paragraph"> <p>We will use <a href="https://www.docker.com">Docker</a> containers to run the above services. Using separate containers simplifies the deployment process so you can see Debezium in action. In addition, we will also download the <a href="#download-oracle-jdbc-driver">Oracle JDBC driver</a> and mount it as part of the Kafka Connect container.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Using multiple instances of these services in production provides performance, reliability, and fault tolerance. The deployment would typically involve a platform like OpenShift or Kubernetes to manage multiple containers, or you would use dedicated hardware and manage this manually.</p> </div> <div class="paragraph"> <p>For this blog, we will use a single instance of each service to keep it simple.</p> </div> </td> </tr> </table> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>The Zookeeper and Kafka containers are ephemeral. Typically, volumes would be mounted on the host machine so that when the container stops, data managed by the container persists. For the sake of simplicity, we are skipping this step so that when the container stops, data is lost.</p> </div> </td> </tr> </table> </div> <div class="sect2"> <h3 id="start-zookeeper">Prerequisites: Starting Zookeeper</h3> <div class="paragraph"> <p>The Zookeeper service is the first service that&#8217;s started. The Kafka broker uses Zookeeper to handle the leadership election of Kafka brokers and manages the service discovery within the cluster so that each broker knows when a sibling has joined or left when a broker terminates, and whom the new leader is for a given topic/partition tuple.</p> </div> <div class="paragraph"> <p>Open a new terminal window and run the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">docker run -it --rm --name zookeeper -p 2181:2181 -p 2888:2888 -p 3888:3888 \
  quay.io/debezium/zookeeper:1.9</code></pre> </div> </div> <div class="paragraph"> <p>The <em>zookeeper</em> container is started in interactive mode and destroyed when stopped. The container is named <code>zookeeper</code>, which will be important when starting future containers.</p> </div> </div> <div class="sect2"> <h3 id="start-kafka">Prerequisites: Starting Kafka</h3> <div class="paragraph"> <p>The Kafka service is the second service that must be started and depends on the Zookeeper service. Debezium produces change events sent to topics managed by the Kafka broker.</p> </div> <div class="paragraph"> <p>Open a new terminal window and run the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">docker run -it --rm --name kafka -p 9092:9092 --link zookeeper:zookeeper \
  quay.io/debezium/kafka:1.9</code></pre> </div> </div> <div class="paragraph"> <p>The <em>kafka</em> container is started in interactive mode and destroyed when stopped. The container is named <code>kafka</code>, which will be important starting future containers. Additionally, the <em>kafka</em> service also links to the <em>zookeeper</em> service, meaning that the canonical name <code>zookeeper</code> will resolve to the container running the <em>zookeeper</em> service.</p> </div> </div> <div class="sect2"> <h3 id="download-oracle-jdbc-driver">Prerequisites: Download Oracle JDBC driver</h3> <div class="paragraph"> <p>The Debezium Kafka Connect image does not ship with the Oracle JDBC driver. To use Debezium for Oracle, the JDBC driver must be manually downloaded and mounted into the Debezium Kafka Connect image.</p> </div> <div class="paragraph"> <p>Navigate to the <a href="https://www.oracle.com/database/technologies/appdev/jdbc-downloads.html">Oracle Database JDBC driver</a> downloads page. At the time of this publication, the latest Oracle database is Oracle 21, so click on the <code>ojdbc8.jar</code> link under the Oracle 21c section. The downloaded jar will be used in the next section, adding the driver to the base image of Debezium&#8217;s Kafka Connect container.</p> </div> </div> <div class="sect2"> <h3 id="start-kafka-connect">Prerequisites: Starting Kafka Connect</h3> <div class="paragraph"> <p>The Kafka Connect service is the third and final service that must be started and depends on the Kafka service. Kafka Connect is responsible for managing all connectors and their related workloads and is the runtime environment accountable for running the Debezium Connector for Oracle when we deploy it shortly.</p> </div> <div class="paragraph"> <p>Open a new terminal window and run the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">docker run -it --rm --name connect -p 8083:8083 \
  -e GROUP_ID=1 \
  -e CONFIG_STORAGE_TOPIC=my_connect_configs \
  -e OFFSET_STORAGE_TOPIC=my_connect_offsets \
  -e STATUS_STORAGE_TOPIC=my_connect_statuses \
  --link kafka:kafka \
  --link dbz_oracle21:dbz_oracle21 \
  -v /path/to/ojdbc8.jar:/kafka/libs/ojdbc8.jar \
  quay.io/debezium/connect:1.9</code></pre> </div> </div> <div class="paragraph"> <p>The <em>connect</em> container is started in interactive mode and destroyed when stopped. The container is named <code>connect</code>, and several environment variables control the naming of several required topics and some required configuration parameters. Additionally, the <em>connect</em> container links to the <em>kafka</em> container, meaning that the canonical name <code>kafka</code> will resolve to the container running the <em>kafka</em> broker service.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Unlike prior containers, we mount a volume using the <code>-v</code> command. The argument takes the format of <code>local-path:container-path</code>.</p> </div> <div class="paragraph"> <p>The <em>local-path</em> represents where the <code>ojdbc8.jar</code> file exists on the host machine. The <em>container-path</em> should remain <code>/kafka/libs/ojdbc8.jar</code>, installing the driver on the Kafka Connect classpath.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="create_some_initial_test_data">Create some initial test data</h2> <div class="sectionbody"> <div class="paragraph"> <p>If the Oracle database created in part one of this series uses the Oracle container registry image, no initial data exists in the database. While this doesn&#8217;t necessarily present a problem, we&#8217;d ideally like to snapshot some data when deploying the Oracle connector; ergo, some initial data must exist before deployment.</p> </div> <div class="paragraph"> <p>In a new terminal, let&#8217;s connect to the database using SQL*Plus and create a new table with some initial data. The following uses the common user, connecting to the pluggable database <code>ORCLPDB1</code>. You can safely skip this step when connecting to an existing environment with tables to be captured.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">docker exec -it -e ORACLE_SID=ORCLPDB1 dbz_oracle21 sqlplus c##dbzuser@ORCLPDB1</code></pre> </div> </div> <div class="paragraph"> <p>Once connected, use the following SQL to create a table and some initial data:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">CREATE</span> <span class="type">TABLE</span> customers (id number(<span class="integer">9</span>,<span class="integer">0</span>) <span class="directive">primary</span> <span class="type">key</span>, name <span class="predefined-type">varchar2</span>(<span class="integer">50</span>));
<span class="class">INSERT</span> <span class="class">INTO</span> customers <span class="keyword">VALUES</span> (<span class="integer">1001</span>, <span class="string"><span class="delimiter">'</span><span class="content">Salles Thomas</span><span class="delimiter">'</span></span>);
<span class="class">INSERT</span> <span class="class">INTO</span> customers <span class="keyword">VALUES</span> (<span class="integer">1002</span>, <span class="string"><span class="delimiter">'</span><span class="content">George Bailey</span><span class="delimiter">'</span></span>);
<span class="class">INSERT</span> <span class="class">INTO</span> customers <span class="keyword">VALUES</span> (<span class="integer">1003</span>, <span class="string"><span class="delimiter">'</span><span class="content">Edward Walker</span><span class="delimiter">'</span></span>);
<span class="class">INSERT</span> <span class="class">INTO</span> customers <span class="keyword">VALUES</span> (<span class="integer">1004</span>, <span class="string"><span class="delimiter">'</span><span class="content">Anne Kretchmar</span><span class="delimiter">'</span></span>);
<span class="class">COMMIT</span>;</code></pre> </div> </div> <div class="paragraph"> <p>By default, the redo logs only capture minimal information about changes in the <code>CUSTOMERS</code> table because supplemental logging is set only at the database level.</p> </div> <div class="paragraph"> <p>If you are familiar with PostgreSQL&#8217;s <code>REPLICA IDENTITY</code> or MySQL&#8217;s <code>binlog_format</code>, Oracle provides a similar mechanism called table-level supplemental logging, which we mentioned in part one of this series. Supplemental logging at the table level controls the columns captured in the redo logs when users modify rows. Setting the table&#8217;s supplemental log level to <code>(ALL) COLUMNS</code> guarantees that Oracle captures changes associated with <code>INSERT</code>, <code>UPDATE</code>, and <code>DELETE</code> operations in the redo logs.</p> </div> <div class="paragraph"> <p>Use the following SQL to set the table&#8217;s supplemental log level:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">ALTER</span> <span class="type">TABLE</span> customers <span class="class">ADD</span> SUPPLEMENTAL LOG DATA (<span class="keyword">ALL</span>) <span class="type">COLUMNS</span>;</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Suppose a captured table&#8217;s supplemental log level is incorrectly set. In that case, the connector will log a warning letting you know there is a problem so that you can adjust the table&#8217;s settings to capture all changes.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>It is worth pointing out that while this example uses the same user account to create this <code>CUSTOMERS</code> table that the connector uses to connect, it&#8217;s not at all uncommon for the user used by the connector to differ from the user who owns the tables in the Oracle database. In this case, the connector user must have permission to read the captured tables, requiring the <code>SELECT</code> permission per table.</p> </div> </div> </div> <div class="sect1"> <h2 id="deploying_the_oracle_connector">Deploying the Oracle connector</h2> <div class="sectionbody"> <div class="paragraph"> <p>We are now ready to deploy the Debezium Oracle connector. Before registering the connector with Kafka Connect, let&#8217;s look at the configuration in-depth.</p> </div> <div class="paragraph"> <p>Below is a sample configuration we will use in this example:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight nowrap"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">customers-connector</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.oracle.OracleConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbz_oracle21</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1521</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">c##dbzuser</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbz</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">ORCLCDB</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.pdb.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">ORCLPDB1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">server1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">table.include.list</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">C##DBZUSER.CUSTOMERS</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.bootstrap.servers</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">kafka:9092</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.topic</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">schema-changes</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>Let&#8217;s take a dive into what each of these configuration options mean.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>name</code></dt> <dd> <p>This is the name assigned to the connector, which must be unique across the Kafka connect cluster.</p> </dd> <dt class="hdlist1"><code>connector.class</code></dt> <dd> <p>This is the class implementation of the deployed connector. Each of the Debezium source connectors have a unique class name to identify which connector is being deployed.</p> </dd> <dt class="hdlist1"><code>tasks.max</code></dt> <dd> <p>This is the maximum number of tasks that will be assigned to the connector deployment in Kafka Connect. Most Debezium connectors read changse from the source database sequentially, therefore, a value of <code>1</code> often makes sense.</p> </dd> <dt class="hdlist1"><code>database.hostname</code></dt> <dd> <p>This is the database hostname or IP address. Since we specified a link to <code>dbz_oracle21</code> container when starting Kafka Connect, we can use that name here to identify the container running the Oracle database. If you have a pre-existing Oracle environment on another host, specify the name of that host in this configuration property.</p> </dd> <dt class="hdlist1"><code>database.port</code></dt> <dd> <p>This is the port the database uses to listen for connections. Oracle&#8217;s default port is <code>1521</code> but a database administrator can configure this to be any available port. If you are connecting to a pre-existing Oracle instance, use the port the database uses.</p> </dd> <dt class="hdlist1"><code>database.user</code></dt> <dd> <p>This is the database user account used for JDBC connections. This should be the common user created in part one of this series, the <code>c##dbzuser</code> user. If you are connecting to an environment that doesn&#8217;t support multi-tenancy, this will be the user you created in the root database without the common-user prefix.</p> </dd> <dt class="hdlist1"><code>database.password</code></dt> <dd> <p>This is the database user account password.</p> </dd> <dt class="hdlist1"><code>database.dbname</code></dt> <dd> <p>This is the database service that the connector communications with. Regardless of whether multi-tenancy is enabled or not, this will always be the singular or root container database.</p> </dd> <dt class="hdlist1"><code>database.pdb.name</code></dt> <dd> <p>This is the optional pluggable database system identifier. This property must be provided when connecting to a database that supports multi-tenancy and refers to the PDB. If this field is omitted, the connector assumes the database does not support multi-tenancy.</p> </dd> <dt class="hdlist1"><code>database.server.name</code></dt> <dd> <p>The prefix used for all topics created by the connector. This value must be unique across all topic deployments within the Kafka Connect cluster.</p> </dd> <dt class="hdlist1"><code>table.include.list</code></dt> <dd> <p>A comma-separated list of regular expression or simple table names using the format of <code>&lt;schema&gt;.&lt;table&gt;</code> identifying what tables will be captured by the connector.</p> </dd> <dt class="hdlist1"><code>database.history.kafka.bootstrap.servers</code></dt> <dd> <p>This is the URL to the Kafka broker where the database history topic will be stored. Since we specified a link to <code>kafka</code> container when starting Kafka Connect, we can use that name here to point to the broker and its port.</p> </dd> <dt class="hdlist1"><code>database.history.kafka.topic</code></dt> <dd> <p>This is the name of the topic that will store the database schema history. This topic will be recovered when the connector restarts, populating the in-memory relational model from this topic.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>All Debezium connectors, except PostgreSQL, use a schema history to store the schemas of all tables. This is often not ideal for Oracle databases, especially when deploying the connector without multi-tenancy.</p> </div> <div class="paragraph"> <p>To restrict the storage to only tables in the include list, modify the connector&#8217;s configuration by setting the <code>database.history.store.only.captured.tables.ddl</code> property to <code>true</code>.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>For more information on other connector properties, you can review the Oracle <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-connector-properties">documentation</a> for more details.</p> </div> <div class="paragraph"> <p>To deploy the connector, save the above configuration to a file called <code>register-oracle.json</code>. Now, open a new terminal window and use the <code>curl</code> command to register the connector with Kafka Connect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -i -X POST -H &quot;Accept:application/json&quot; \
  -H &quot;Content-Type:application/json&quot; \
  localhost:8083/connectors \
  -d @register-oracle.json</code></pre> </div> </div> <div class="paragraph"> <p>If the registration is successful, the terminal where the <em>connect</em> container is running will start performing a snapshot of the data in the <code>CUSTOMERS</code> table. We can also confirm that the data exists in Kafka by using the Kafka console consumer tool and reading the topic&#8217;s contents to the local terminal.</p> </div> <div class="paragraph"> <p>To check the contents of the topic, use the same terminal where the connector was registered and execute the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">docker exec -it kafka /kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server 0.0.0.0:9092 \
  --from-beginning \
  --property print.key=true \
  --topic server1.C__DBZUSER.CUSTOMERS</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The topic converts the schema name from <code>C##DBZUSER</code> to <code>C__DBZUSER</code> because the topic naming strategy automatically guarantees that the topic&#8217;s name is compatible with Avro, which does not allow the hash sign character.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>The output of the above command should look similar to the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>:{
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">payload</span><span class="delimiter">&quot;</span></span>:{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">before</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">after</span><span class="delimiter">&quot;</span></span>:{
      <span class="key"><span class="delimiter">&quot;</span><span class="content">ID</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">1001</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">NAME</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">Salles Thomas</span><span class="delimiter">&quot;</span></span>
    },
    <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>:{
      <span class="key"><span class="delimiter">&quot;</span><span class="content">version</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">1.9.6.Final</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">connector</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">oracle</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">server1</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>:<span class="integer">1665102121000</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">snapshot</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">db</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">ORCLPDB1</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">sequence</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">C##DBZUSER</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">table</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">CUSTOMERS</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">2868546</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">lcr_position</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">rs_id</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">ssn</span><span class="delimiter">&quot;</span></span>:<span class="integer">0</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">redo_thread</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>
    },
    <span class="key"><span class="delimiter">&quot;</span><span class="content">op</span><span class="delimiter">&quot;</span></span>:<span class="string"><span class="delimiter">&quot;</span><span class="content">r</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>:<span class="integer">1665102126961</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">transaction</span><span class="delimiter">&quot;</span></span>:<span class="value">null</span>
  }
}
<span class="error">.</span><span class="error">.</span><span class="error">.</span></code></pre> </div> </div> <div class="paragraph"> <p>You can now use the SQLPlus terminal where you created the initial test data to <code>INSERT</code>, <code>UPDATE</code>, or <code>DELETE</code> records within the <code>CUSTOMERS</code> table. You will see corresponding change events in the terminal that is presently tailing the <code>server1.C__DBZUSER.CUSTOMERS</code> topic.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Be mindful that SQLPlus does not enable <code>auto-commit</code> by default, so be sure that you automatically commit changes when you change data in the <code>CUSTOMERS</code> table so that it will be visible to the connector&#8217;s mining process.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="conclusion">Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>During part one of this series, we discussed what Oracle is, why it&#8217;s so popular in the database world, and how to install and configure the database. During this part of the series, we&#8217;ve discussed how to install all the prerequisite services, including Zookeeper, Apache Kafka, and Apache Kafka Connect. In addition, we have also deployed a sample Oracle connector captured changes for the <code>CUSTOMERS</code> table.</p> </div> <div class="paragraph"> <p>In the next part of this series, I will discuss performance, how to monitor the connector, and the most critical metrics and why they are essential. We may even build a small dashboard with metrics.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="debezium"/><category term="oracle"/><category term="examples"/><summary type="html"><![CDATA[This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first part of this series is here. In this second installment, we will build on what we did in part one by deploying the Oracle connector using Zookeeper, Kafka, and Kafka Connect. We are going to discuss a variety of configuration options for the connector and why they&#8217;re essential. And finally, we&#8217;re going to see the connector in action!]]></summary></entry><entry><title type="html">Debezium for Oracle - Part 1: Installation and Setup</title><link href="https://debezium.io/blog/2022/09/30/debezium-oracle-series-part-1/" rel="alternate" type="text/html" title="Debezium for Oracle - Part 1: Installation and Setup"/><published>2022-09-30T12:00:00+00:00</published><updated>2022-09-30T12:00:00+00:00</updated><id>https://debezium.io/blog/2022/09/30/debezium-oracle-series-part-1</id><content type="html" xml:base="https://debezium.io/blog/2022/09/30/debezium-oracle-series-part-1/"><![CDATA[<div class="paragraph"> <p>This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. Throughout the series, we&#8217;ll examine all the steps to setting up a proof of concept (POC) deployment for Debezium for Oracle. We will discuss setup and configurations as well as the nuances of multi-tenancy. We will also dive into any known pitfalls and concerns you may need to know and how to debug specific problems. And finally, we&#8217;ll talk about performance and monitoring to maintain a healthy connector deployment.</p> </div> <div class="paragraph"> <p>Throughout this exercise, we hope that this will show you just how simple it is to deploy Debezium for Oracle. This installation and setup portion of the series may seem quite complicated, but many of these steps likely already exist in a pre-existing environment. We will dive into each step, explaining it is essential should you use a container image deployment.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>So without any further delay, let&#8217;s dive right in!</p> </div> <div class="sect1"> <h2 id="what_is_oracle">What is Oracle?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As ironic as the question may seem, some people may not know what an Oracle database is. An Oracle database is a relational database management system (RBDMS), a database that stores and provides access to data points that are often related. The database is developed and marketed by Oracle Corporation and is one of the most trusted and widely used relational database engines in the market, providing a scalable relational database architecture.</p> </div> <div class="paragraph"> <p>An Oracle database consists of a collection of schemas that represent a collection of logical structures of data or schema objects. A schema object can be anything from a trigger, view, table, data type, sequence, procedure, function, and others. Furthermore, Oracle 12 introduced a multi-tenant architecture, allowing a single database instance to function as a multi-tenant container database (CDB) that houses zero, one, or many customer-created pluggable databases (PDBs).</p> </div> <div class="paragraph"> <p>The goal, install an Oracle database, connect Debezium to Oracle, and convert ingested changes into change events stored in Apache Kafka.</p> </div> </div> </div> <div class="sect1"> <h2 id="install-oracle">Installing Oracle</h2> <div class="sectionbody"> <div class="paragraph"> <p>To get started, we will need a running Oracle database environment. One of the easiest ways to do this is to use <a href="https://www.docker.com">Docker</a> by deploying a container running the Oracle database. Oracle has published such containers in their <a href="https://container-registry.oracle.com/">container registry</a>, allowing anyone to run the database and test-drive it.the dat You can skip this section if you intend to ingest changes from an existing Oracle database.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The Oracle container images are all pre-built using Oracle with multi-tenancy. This means we will follow the setups according to a multi-tenant architecture. Some minor adjustments may be needed if you&#8217;re using an installation that does not use multi-tenancy.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>For this exercise, we&#8217;re going to use <a href="https://container-registry.oracle.com/ords/f?p=113:4:11477241761337:::4:P4_REPOSITORY,AI_REPOSITORY,AI_REPOSITORY_NAME,P4_REPOSITORY_NAME,P4_EULA_ID,P4_BUSINESS_AREA_ID:9,9,Oracle%20Database%20Enterprise%20Edition,Oracle%20Database%20Enterprise%20Edition,1,0&amp;cs=318vPzAxLqFaC2tslO9ao27bihoUQ6MP-WvtaqYfx_ifILYwLl_2hLJU5hG8HeJv8G5w9JXbcv4i-DZD7zgTDtg">this container image</a>. Using <code>docker pull</code>, the image is pulled after authenticating with the Oracle container registry using the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">docker pull container-registry.oracle.com/database/enterprise:latest</code></pre> </div> </div> <div class="paragraph"> <p>To start the container, use the <code>docker run</code> command. Several environment variables, such as the database SID, pluggable database name, and password, are required since the database container image must first install the Oracle database.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">docker run -d \
  --name dbz_oracle21 \
  -p 1521:1521 \
  -e ORACLE_SID=ORCLCDB \
  -e ORACLE_PDB=ORCLPDB1 \
  -e ORACLE_PWD=oraclepw \
  container-registry.oracle.com/database/enterprise:latest</code></pre> </div> </div> <div class="paragraph"> <p>The <code>ORACLE_SID</code> refers to the system/service ID used to identify the database. Since we are using multi-tenancy, we will use the name <code>ORCLCDB</code> to represent the <em>container database</em>, or CDB. In Oracle&#8217;s multi-tenant architecture, the <code>ORACLE_PDB</code> refers to the system/service ID used to identify the <em>pluggable database</em>, or PDB. And finally, the <code>ORACLE_PWD</code> refers to the password used for the <code>SYS</code> and <code>SYSTEM</code> users, which we&#8217;ll use later.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The container will persist data to database files on the container&#8217;s filesystem. The data will be lost when removing the container. To persist the data outside the container, please refer to the Oracle registry&#8217;s README to understand how to set a volume on the <code>docker run</code> command.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>We explicitly started the docker container as a daemon, a background process. If you wish to see what is happening within the container, you can use the command <code>docker logs -f dbz_oracle21</code> to tail the container&#8217;s database log.</p> </div> <div class="paragraph"> <p>For the next few minutes, the database will be configured and installed inside the container, which happens when a new container starts and no initial configuration and database exists. You will know whether the installation was successful by looking for a banner in the logs that would look similar to the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">#########################
DATABASE IS READY TO USE!
#########################</code></pre> </div> </div> <div class="paragraph"> <p>At this point, the installation has concluded, and it&#8217;s safe to move on to the next section.</p> </div> </div> </div> <div class="sect1"> <h2 id="configuring_oracle">Configuring Oracle</h2> <div class="sectionbody"> <div class="paragraph"> <p>Several database configurations are necessary to ingest changes from an Oracle database. If you are using a pre-existing environment, you may be able to skip some of these steps.</p> </div> <div class="paragraph"> <p>The following configurations are necessary:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p><a href="#configure-oracle-archive-logs">Archive logs</a></p> </li> <li> <p><a href="#configure-oracle-redo-logs">Redo logs</a></p> </li> <li> <p><a href="#configure-oracle-logging">Supplemental logging</a></p> </li> <li> <p><a href="#configure-oracle-users">Users and tablespaces/schemas in CDB and PDB</a></p> </li> </ol> </div> <div class="sect2"> <h3 id="configure-oracle-archive-logs">Configure Oracle: Archive logs</h3> <div class="paragraph"> <p>Oracle saves filled groups of redo logs (the database transaction logs) to one or more offline destinations, collectively known as the archived redo log or the archive logs. Changes main in a primary database are replicated to logical or physical standby environments using archive logs.</p> </div> <div class="paragraph"> <p>A log switch happens when a redo log fills up and is archived. Debezium ingests changes across both redo and archive logs. Debezium requires access to the archive log to finish processing redo entries when the redo log is archived.</p> </div> <div class="paragraph"> <p>The Oracle container registry image used in the <a href="#install_oracle">Install Oracle</a> section does not have archive logging enabled. If you use another image or a pre-existing environment, you must check whether archive logging is enabled. To check the status, use the <code>SYS</code> user and the password defined during the installation for <code>ORACLE_PWD</code> to connect to the <code>ORCLCDB</code> database and execute the following query:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">SELECT LOG_MODE FROM V$DATABASE</code></pre> </div> </div> <div class="paragraph"> <p>If the column contains <code>ARCHIVELOG</code>, then archive logging is enabled. If the column contains the value <code>NOARCHIVELOG</code>, archive logging isn&#8217;t enabled, and further configuration is necessary.</p> </div> <div class="paragraph"> <p>When setting up the Oracle archive log, not only do we need to enable the logging feature, but we&#8217;ll also need to specify a location on the disk to store the logs. If you are using a pre-existing environment, you will need to consult with your database administrator for this. Most database servers store archive log files using special paths, and you will need to know if Oracle Automatic Storage Management (ASM) is used or what volume has adequate space on the database server.</p> </div> <div class="paragraph"> <p>Let&#8217;s open a terminal to the Oracle database container. We want to connect to the database container using SQL*Plus to use a client that allows easy unmounting and restarting of the database. So in a new terminal, execute:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">docker exec -it dbz_oracle21 -e ORACLE_SID=ORCLCDB sqlplus sys as sysdba</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you connect to an existing Oracle environment, you can also <code>ssh</code> to the database server&#8217;s shell to run SQL*Plus.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>The above command will start Oracle&#8217;s SQL*Plus, a command line Oracle SQL client. The client will ask for your password, which will be the same as the <code>ORACLE_PWD</code> environment variable or <code>oraclepw</code> if you&#8217;re using the Oracle registry container. Use your environment&#8217;s <code>SYS</code> user password when connecting to an existing Oracle environment.</p> </div> <div class="paragraph"> <p>We need to set two database parameters:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>db_recovery_file_dest_size</code></dt> <dd> <p>The number of bytes available to store archive logs.</p> </dd> </dl> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>Suppose the size of the existing archive logs and the next log to be archived exceeds this configured value. In that case, the Oracle database archiver process will block. If all redo logs require archiving and the archiver process is blocked, the database prevents changes until the archiver process unblocks. Deleting older archive logs using the <code>RMAN</code> utility unblocks the archiver process, allowing any pending redo logs to be archived. So it&#8217;s generally a good idea to pick a decent size based on your database retention policy.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>db_recovery_file_dest</code></dt> <dd> <p>The location on the disk where the archive logs are stored.</p> </dd> </dl> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>This location must be readable and writable by the Oracle database user, often called the <code>oracle</code> user.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>To set these values, we&#8217;ll execute the following SQL commands inside the SQL*Plus terminal window where we&#8217;ve already connected to the database as the <code>SYS</code> user. Again, if you&#8217;re connecting to a pre-existing environment, please consult your database administrator before you proceed here.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">ALTER</span> SYSTEM <span class="class">SET</span> db_recovery_file_dest_size = <span class="integer">10</span>G;
<span class="class">ALTER</span> SYSTEM <span class="class">SET</span> db_recovery_file_dest = <span class="string"><span class="delimiter">'</span><span class="content">/opt/oracle/oradata/ORCLCDB</span><span class="delimiter">'</span></span> scope=spfile;
SHUTDOWN <span class="directive">IMMEDIATE</span>
STARTUP MOUNT
<span class="class">ALTER</span> <span class="type">DATABASE</span> ARCHIVELOG;
<span class="class">ALTER</span> <span class="type">DATABASE</span> OPEN;
ARCHIVE LOG LIST;</code></pre> </div> </div> <div class="paragraph"> <p>The above <code>ALTER</code> statements adjust the database parameters, specifying that the retention of archive logs is up to a maximum of 10GB and that <code>/opt/oracle/oradata/ORCLCDB</code> is where the logs are stored.</p> </div> <div class="paragraph"> <p>The final output from SQL*Plus should show the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">SQL&gt; Database log mode             Archive Mode
Automatic archival                 Enabled
Archive destination                USE_DB_RECOVERY_FILE_DEST
Oldest online log sequence         1
Next log sequence to archive       3
Current log sequence               3</code></pre> </div> </div> <div class="paragraph"> <p>The configuration of the archive logs is complete, and when a database log switch occurs, the Oracle ARCH Process will convert the redo log to an archive log stored in the location specified on the disk.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Oracle supports the notion of multiple archive log destinations, allowing the storage of a redo log at different file locations. Multiple storage locations are common when using Oracle DataGuard to transfer copies of the archive logs to a secondary server for disaster recovery or business intelligence. We will not cover how to configure this in this blog post, but it is worth noting that functionality exists and can be helpful.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="configure-oracle-redo-logs">Configure Oracle: Redo logs</h3> <div class="paragraph"> <p>Oracle&#8217;s transaction log is known as a redo log. These logs are vital as they&#8217;re used in a database crash or media failure to recover to a checkpoint. Unfortunately, Oracle container images often use a redo log configuration that isn&#8217;t useful for Debezium.</p> </div> <div class="paragraph"> <p>There are two <a href="/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-strategy">log mining strategies</a> for Debezium&#8217;s Oracle connector. The strategy controls how the connector interacts with Oracle LogMiner and how the connector ingests schema and table changes.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>redo_log_catalog</code></dt> <dd> <p>The data dictionary will be written periodically to the redo logs, causing a higher generation of archive logs over time. This setting enables tracking DDL changes, so if a table&#8217;s schema changes, this will be the ideal strategy for that purpose.</p> </dd> <dt class="hdlist1"><code>online_catalog</code></dt> <dd> <p>The data dictionary will not be written periodically to the redo logs, leaving the generation of archive logs consistent with current behavior. Oracle LogMiner will mine changes substantially faster; however, this performance comes at the cost of <strong>not</strong> tracking DDL changes. If a table&#8217;s schema remains constant, this will be the ideal strategy for that purpose.</p> </dd> </dl> </div> <div class="paragraph"> <p>When using the <code>online_catalog</code> mode, you can safely skip this step entirely.</p> </div> <div class="paragraph"> <p>When using the <code>redo_log_catalog</code> mode (the default), the redo log size is critical to reducing the frequency of log switches. The LogMiner session restarts and the data dictionary is rebuilt in the redo logs when a log switch occurs. The dictionary is read back by LogMiner and used to track DDL changes when the session restarts, which can lead to a slight initial session delay while the dictionary tables are populated. Overall you gain better performance when the redo log is large enough to write the data dictionary to a single log file.</p> </div> <div class="paragraph"> <p>The Oracle container registry images come configured with a redo log size of <code>200MB</code>. This default size is too small when using the default mining strategy, so we will adjust this so that the logs use a size of <code>400MB</code>.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>When working with Oracle installed without multi-tenancy, using <code>400MB</code> may still be slightly small since a host of base tables exist in the root database but do not exist in the pluggable databases when multi-tenancy is enabled. Please use <code>500MB</code> instead if you&#8217;re ingesting changes from an Oracle environment without multi-tenancy at a minimum.</p> </div> <div class="paragraph"> <p>Regardless of multi-tenancy, these values should be much more significant in your production environment. Your DBA will be able to use Oracle&#8217;s sizing guide to determine the best value based on the log switch frequency and load on the system.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Before making any changes, it&#8217;s essential to examine the current state of your environment. In the same terminal where you enabled archive logging, execute the following SQL to determine the current log sizes.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">SELECT GROUP#, BYTES/1024/1024 SIZE_MB, STATUS FROM V$LOG ORDER BY 1;</code></pre> </div> </div> <div class="paragraph"> <p>The Oracle container registry image will return the following output:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">    GROUP#    SIZE_MB STATUS
---------- ---------- ----------------
	 1	  200 INACTIVE
	 2	  200 INACTIVE
	 3	  200 CURRENT</code></pre> </div> </div> <div class="paragraph"> <p>This output tells us there are 3 log groups, and each group consumes <code>200MB</code> of space per log. Additionally, the status associated with each group is crucial as it represents the current state of that log.</p> </div> <div class="paragraph"> <p>The following describes the log statues in detail:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>INACTIVE</code></dt> <dd> <p>This means Oracle has initialized the log and isn&#8217;t currently in use.</p> </dd> <dt class="hdlist1"><code>ACTIVE</code></dt> <dd> <p>This means Oracle has initialized the log and is currently in use. The redo log is required and in use in case of a failure so the database can safely recover.</p> </dd> <dt class="hdlist1"><code>CURRENT</code></dt> <dd> <p>This means Oracle is currently writing to this log. When working with Oracle Real Application Clusters (RAC), multiple logs can be marked as current, representing a log per cluster node.</p> </dd> <dt class="hdlist1"><code>UNUSED</code></dt> <dd> <p>This means Oracle has not initialized the log and isn&#8217;t in use.</p> </dd> </dl> </div> <div class="paragraph"> <p>Now, using the same terminal window, execute the following SQL to determine the filenames and locations of the redo logs.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">SELECT GROUP#, MEMBER FROM V$LOGFILE ORDER BY 1, 2;</code></pre> </div> </div> <div class="paragraph"> <p>The Oracle container registry image will return the following output:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">    GROUP# MEMBER
---------- ---------------------------------------------------
	 1 /opt/oracle/oradata/ORCLCDB/redo01.log
	 2 /opt/oracle/oradata/ORCLCDB/redo02.log
	 3 /opt/oracle/oradata/ORCLCDB/redo03.log</code></pre> </div> </div> <div class="paragraph"> <p>We can glean from this that each log group consists of a single redo log. Oracle does support the notion of multiple logs per group, allowing for what is called multiplexing. You will generally only see this in a production environment and occasionally in a test environment, but it&#8217;s rare to see this in a development or container environment.</p> </div> <div class="paragraph"> <p>The goal is to adjust the <code>BYTES</code> column in the <code>V$LOG</code> table to have a value of <code>400MB</code>. Unfortunately, the only way to make this adjustment is to drop and re-create the log group, and this is only possible if the <code>STATUS</code> of the group is either <code>INACTIVE</code> or <code>UNUSED</code>. Since log group 1 was <code>INACTIVE</code> above, we&#8217;ll start with it, but you can safely perform this procedure on the log groups in any order.</p> </div> <div class="paragraph"> <p>In the terminal where SQL*Plus is running, execute the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">ALTER DATABASE CLEAR LOGFILE GROUP 1;
ALTER DATABASE DROP LOGFILE GROUP 1;
ALTER DATABASE ADD LOGFILE GROUP 1 ('/opt/oracle/oradata/ORCLCDB/redo01.log') size 400M REUSE;</code></pre> </div> </div> <div class="paragraph"> <p>This will drop and re-create the log group with the size of <code>400MB</code>. We will use the same log file name in the <code>MEMBER</code> column from the <code>VLOGFILE</code> table. If the database uses multiplexing, with multiple log files per log group, use a comma-delimited list of filenames to register each log file.</p> </div> <div class="paragraph"> <p>Continue the above procedure for all log groups, changing the log group and filenames accordingly until all <code>INACTIVE</code> or <code>UNUSED</code> groups have a size of <code>400MB</code>. Once all that remains to be changed are those that are <code>CURRENT</code>, you can issue a log switch on the database to advance the database to the next redo log using the following SQL:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">ALTER SYSTEM SWITCH LOGFILE;</code></pre> </div> </div> <div class="paragraph"> <p>If you recheck the size of the logs in the <code>V$LOG</code>, you&#8217;ll see the output looks like the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">SQL> SELECT GROUP#, BYTES/1024/1024 SIZE_MB, STATUS FROM V$LOG ORDER BY 1;

    GROUP#    SIZE_MB STATUS
---------- ---------- ----------------
	 1	  400 CURRENT
	 2	  400 UNUSED
	 3	  200 ACTIVE</code></pre> </div> </div> <div class="paragraph"> <p>We now need to wait for the database to eventually switch the status of log group 3 to <code>INACTIVE</code>. The switch could take several minutes, so be patient and recheck the size periodically. Once the status reaches <code>INACTIVE</code>, modify the final log group and filename using the same procedure.</p> </div> <div class="paragraph"> <p>One last check of the <code>V$LOG</code> table after the final log group, we&#8217;ll see everything looks in order:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">SQL> SELECT GROUP#, BYTES/1024/1024 SIZE_MB, STATUS FROM V$LOG ORDER BY 1;

    GROUP#    SIZE_MB STATUS
---------- ---------- ----------------
	 1	  400 CURRENT
	 2	  400 UNUSED
	 3	  400 UNUSED</code></pre> </div> </div> <div class="paragraph"> <p>At this point, we have modified all redo log sizes, reducing the frequency of log switches when Debezium executes the data dictionary build steps.</p> </div> </div> <div class="sect2"> <h3 id="configure-oracle-logging">Configure Oracle: Supplemental Logging</h3> <div class="paragraph"> <p>Oracle redo logs are used primarily for instance and media recovery because the data required for those operations gets recorded automatically. LogMiner cannot be used by default because Oracle does not provide any supplemental log data out of the box. Since Debezium relies on LogMiner, supplemental logging must be enabled at a minimum for Debezium to perform any change data capture for Oracle.</p> </div> <div class="paragraph"> <p>Two different strategies can be used to set supplemental logging:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>Database supplemental logging</p> </li> <li> <p>Table supplemental logging</p> </li> </ol> </div> <div class="paragraph"> <p>For Debezium to interface with LogMiner and work with chained rows and various storage arrangements, database supplemental logging must be enabled at a minimum level. To enable this level, execute the following SQL in the current SQL*Plus terminal:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">ALTER</span> <span class="type">DATABASE</span> <span class="class">ADD</span> SUPPLEMENTAL LOG DATA;</code></pre> </div> </div> <div class="paragraph"> <p>We will cover table-based supplemental logging in a later section when we discuss configuring the connector.</p> </div> </div> <div class="sect2"> <h3 id="configure-oracle-users">Configure Oracle: User setup</h3> <div class="paragraph"> <p>For the Debezium connector to capture change events, it must establish a JDBC connection to the database and execute a series of LogMiner APIs. A user account will require specific permissions to access these LogMiner APIs and gather data from the captured tables.</p> </div> <div class="paragraph"> <p>When using multi-tenant architecture, as is found with the Oracle container registry image, there are effectively two databases that we will have to work with, <code>ORCLCDB</code> (the container or root database) and <code>ORCLPDB1</code> (the pluggable database). All captured tables will be created and maintained from within the PDB, but there will be moments when the connector will need to access the root database to read specific system tables.</p> </div> <div class="paragraph"> <p>Therefore in a multi-tenant architecture, we must first set up the two tablespaces that our user account will use. To create these tablespaces, execute the following SQL from within the SQL*Plus terminal:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">CONNECT sys/oraclepw<span class="variable">@ORCLCDB</span> <span class="keyword">as</span> sysdba;
<span class="class">CREATE</span> TABLESPACE logminer_tbs DATAFILE <span class="string"><span class="delimiter">'</span><span class="content">/opt/oracle/oradata/ORCLCDB/logminer_tbs.dbf</span><span class="delimiter">'</span></span>
  SIZE <span class="integer">25</span>M REUSE AUTOEXTEND <span class="keyword">ON</span> MAXSIZE UNLIMITED;

CONNECT sys/oraclepw<span class="variable">@ORCLPDB1</span> <span class="keyword">as</span> sysdba;
<span class="class">CREATE</span> TABLESPACE logminer_tbs DATAFILE <span class="string"><span class="delimiter">'</span><span class="content">/opt/oracle/oradata/ORCLCDB/ORCLPDB1/logminer_tbs.dbf</span><span class="delimiter">'</span></span>
  SIZE <span class="integer">25</span>M REUSE AUTOEXTEND <span class="keyword">ON</span> MAXSIZE UNLIMITED;</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If the deployment is not on an Oracle database with multi-tenancy enabled, creating the second tablespace within the <code>ORCLPDB1</code> database is unnecessary. Additionally, ensure the path provided for the tablespace, credentials, and database SID are all correct for your installation. You may need to consult with your DBA to have the tablespace created correctly.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Once the tablespaces exist, it is now time to create the user account itself. If you are using a multi-tenant environment, the user name must use the common-user prefix so that Oracle creates it in both the CDB root database and the PDB pluggable database; otherwise, the user name can be anything. Since we are working with a multi-tenant database installation with our container, we will create a user account called <code>c##dbzuser</code>.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql">CONNECT sys/oraclepw<span class="variable">@ORCLCDB</span> <span class="keyword">as</span> sysdba;

<span class="class">CREATE</span> USER c<span class="comment">##dbzuser IDENTIFIED BY dbz DEFAULT TABLESPACE LOGMINER_TBS</span>
  QUOTA UNLIMITED <span class="keyword">ON</span> LOGMINER_TBS
  CONTAINER=<span class="keyword">ALL</span>;</code></pre> </div> </div> <div class="paragraph"> <p>The user account requires several permissions. At the time of this publication, the list of permissions included the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">GRANT</span> <span class="class">CREATE</span> SESSION <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SET</span> CONTAINER <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$DATABASE <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> FLASHBACK <span class="keyword">ANY</span> <span class="type">TABLE</span> <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ANY</span> <span class="type">TABLE</span> <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> SELECT_CATALOG_ROLE <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> EXECUTE_CATALOG_ROLE <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ANY</span> <span class="type">TRANSACTION</span> <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ANY</span> DICTIONARY <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> LOGMINING <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>

<span class="class">GRANT</span> <span class="class">CREATE</span> <span class="type">TABLE</span> <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> LOCK <span class="keyword">ANY</span> <span class="type">TABLE</span> <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">CREATE</span> SEQUENCE <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>

<span class="class">GRANT</span> EXECUTE <span class="keyword">ON</span> DBMS_LOGMNR <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> EXECUTE <span class="keyword">ON</span> DBMS_LOGMNR_D <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>

<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOG <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOG_HISTORY <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOGMNR_LOGS <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOGMNR_CONTENTS <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOGMNR_PARAMETERS <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$LOGFILE <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$ARCHIVED_LOG <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$ARCHIVE_DEST_STATUS <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span>
<span class="class">GRANT</span> <span class="class">SELECT</span> <span class="keyword">ON</span> V_$TRANSACTION <span class="keyword">TO</span> c<span class="comment">##dbzuser CONTAINER=ALL;</span></code></pre> </div> </div> <div class="paragraph"> <p>You can refer to the latest <a href="/documentation/reference/stable/connectors/oracle.html#creating-users-for-the-connector">documentation</a> to review whether the required grants may have changed. We have created the connector user we will use in the configuration and given the user all the necessary database permissions.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusion">Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this part of the series, we have covered what Oracle is and why it is so popular. We&#8217;ve also covered installing an Oracle database using a container and configuring the Oracle instance to allow Debezium to ingest changes. In the <a href="/blog/2022/10/06/debezium-oracle-series-part-2/">next part</a> of the series, we&#8217;ll dive into deploying the Debezium Oracle connector on Apache Kafka Connect.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="debezium"/><category term="oracle"/><category term="examples"/><summary type="html"><![CDATA[This post is part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. Throughout the series, we&#8217;ll examine all the steps to setting up a proof of concept (POC) deployment for Debezium for Oracle. We will discuss setup and configurations as well as the nuances of multi-tenancy. We will also dive into any known pitfalls and concerns you may need to know and how to debug specific problems. And finally, we&#8217;ll talk about performance and monitoring to maintain a healthy connector deployment. Throughout this exercise, we hope that this will show you just how simple it is to deploy Debezium for Oracle. This installation and setup portion of the series may seem quite complicated, but many of these steps likely already exist in a pre-existing environment. We will dive into each step, explaining it is essential should you use a container image deployment.]]></summary></entry><entry><title type="html">Debezium 1.9.6.Final Released</title><link href="https://debezium.io/blog/2022/09/26/debezium-1-9-6-final-released/" rel="alternate" type="text/html" title="Debezium 1.9.6.Final Released"/><published>2022-09-26T00:00:00+00:00</published><updated>2022-09-26T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/09/26/debezium-1-9-6-final-released</id><content type="html" xml:base="https://debezium.io/blog/2022/09/26/debezium-1-9-6-final-released/"><![CDATA[<div class="paragraph"> <p>I&#8217;m excited to announce the release of Debezium <strong>1.9.6.Final</strong>!</p> </div> <div class="paragraph"> <p>This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains <a href="https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.6.Final">78 resolved issues</a> overall.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="changes">Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>A few noteworthy bug fixes and stability improvements include:</p> </div> <div class="ulist"> <ul> <li> <p>Oracle SCAN VIP support <a href="https://issues.redhat.com/browse/DBZ-3987">DBZ-3987</a></p> </li> <li> <p>Memory leak in EventDeserializer caused by tableMapEventByTableId <a href="https://issues.redhat.com/browse/DBZ-5126">DBZ-5126</a></p> </li> <li> <p>Postgres Incremental Snapshot on parent partitioned table not working <a href="https://issues.redhat.com/browse/DBZ-5240">DBZ-5240</a></p> </li> <li> <p>Lob type data is inconsistent between source and sink, after modifying the primary key <a href="https://issues.redhat.com/browse/DBZ-5295">DBZ-5295</a></p> </li> <li> <p>Unsigned tinyint conversion fails for MySQL 8.x <a href="https://issues.redhat.com/browse/DBZ-5343">DBZ-5343</a></p> </li> <li> <p>NullPointerException thrown when unique index based on both system and non-system generated columns <a href="https://issues.redhat.com/browse/DBZ-5356">DBZ-5356</a></p> </li> <li> <p>AWS DocumentDB (with MongoDB Compatibility) Connect Fail <a href="https://issues.redhat.com/browse/DBZ-5371">DBZ-5371</a></p> </li> <li> <p>org.postgresql.util.PSQLException: Bad value for type timestamp/date/time: CURRENT_TIMESTAMP <a href="https://issues.redhat.com/browse/DBZ-5384">DBZ-5384</a></p> </li> <li> <p>Timestamp with time zone column&#8217;s default values not in GMT <a href="https://issues.redhat.com/browse/DBZ-5403">DBZ-5403</a></p> </li> <li> <p>OffsetStore not stopped if it fails to fully start <a href="https://issues.redhat.com/browse/DBZ-5433">DBZ-5433</a></p> </li> <li> <p>Duplicate SCNs on same thread Oracle RAC mode incorrectly processed <a href="https://issues.redhat.com/browse/DBZ-5439">DBZ-5439</a></p> </li> <li> <p>Outbox doesn&#8217;t check array consistecy properly when it detemines its schema <a href="https://issues.redhat.com/browse/DBZ-5475">DBZ-5475</a></p> </li> <li> <p>Debezium connector task didn&#8217;t retry when failover in mongodb 5 <a href="https://issues.redhat.com/browse/DBZ-5479">DBZ-5479</a></p> </li> <li> <p>Use TCCL as the default classloader to load interface implementations <a href="https://issues.redhat.com/browse/DBZ-5561">DBZ-5561</a></p> </li> <li> <p>Vitess: Handle VStream close unepectedly <a href="https://issues.redhat.com/browse/DBZ-5579">DBZ-5579</a></p> </li> <li> <p>Oracle connector parsing SELECT_LOB_LOCATOR event missing constant <code>unavailable.value.placeholder</code> <a href="https://issues.redhat.com/browse/DBZ-5581">DBZ-5581</a></p> </li> <li> <p>Message with LSN foo larger than expected LSN bar <a href="https://issues.redhat.com/browse/DBZ-5597">DBZ-5597</a></p> </li> <li> <p>Continuously WARNs about undo transactions when LOB is enabled <a href="https://issues.redhat.com/browse/DBZ-5635">DBZ-5635</a></p> </li> <li> <p>Large numbers of ROLLBACK transactions can lead to memory leak when LOB is not enabled. <a href="https://issues.redhat.com/browse/DBZ-5645">DBZ-5645</a></p> </li> </ul> </div> <div class="paragraph"> <p>In addition, there were quite a number of SQL parser fixes for both MySQL and Oracle, <a href="https://issues.redhat.com/browse/DBZ-5472">DBZ-5472</a>, <a href="https://issues.redhat.com/browse/DBZ-5488">DBZ-5488</a>, <a href="https://issues.redhat.com/browse/DBZ-5499">DBZ-5499</a>, <a href="https://issues.redhat.com/browse/DBZ-5508">DBZ-5508</a>, <a href="https://issues.redhat.com/browse/DBZ-5521">DBZ-5521</a>, <a href="https://issues.redhat.com/browse/DBZ-5522">DBZ-5522</a>, <a href="https://issues.redhat.com/browse/DBZ-5526">DBZ-5526</a>, <a href="https://issues.redhat.com/browse/DBZ-5550">DBZ-5550</a>, <a href="https://issues.redhat.com/browse/DBZ-5592">DBZ-5592</a>, <a href="https://issues.redhat.com/browse/DBZ-5595">DBZ-5595</a>, <a href="https://issues.redhat.com/browse/DBZ-5605">DBZ-5605</a>, <a href="https://issues.redhat.com/browse/DBZ-5630">DBZ-5630</a>, and <a href="https://issues.redhat.com/browse/DBZ-5643">DBZ-5643</a>.</p> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/1.9/release-notes#release-1.9.6-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 1.9.6.Final: <a href="https://github.com/ahus1">Alexander Schwartz</a>, <a href="https://github.com/aloubyansky">Alexey Loubyansky</a>, Gabor Andras, <a href="https://github.com/ajunwalker">Andrew Walker</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/nicholas-fwang">Inki Hwang</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/domsj">Jan Doms</a>, <a href="https://github.com/Jiabao-Sun">Jiabao Sun</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/thangdc94">Phạm Ngọc Thắng</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/GOODBOY008">Zhongqiang Gong</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook, What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 1.9 will continue to receive bug fix and maintenance changes throughout the early part of the next quarter. I expect there to be at least a 1.9.7.Final in the middle to late October timeframe, potentially wrapping up the 1.9 release stream.</p> </div> <div class="paragraph"> <p>Debezium 2.0 is wrapping up with the latest 2.0.0.Beta2 build released just last week. We are currently focusing on bug fixes, stability, and polishing the Debezium 2.0 release stream. We expect to have 2.0.0.CR1 released in about another week or so with 2.0.0.Final scheduled for mid-October.</p> </div> <div class="paragraph"> <p>Until next time, keep an eye out as we&#8217;ll soon be discussing what&#8217;s to come in Debezium 2.1 later this year!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[I&#8217;m excited to announce the release of Debezium 1.9.6.Final! This release focuses on bug fixes and stability; and is the recommended update for all users from earlier versions. This release contains 78 resolved issues overall.]]></summary></entry><entry><title type="html">Debezium 2.0.0.Beta2 Released</title><link href="https://debezium.io/blog/2022/09/16/debezium-2.0-beta2-released/" rel="alternate" type="text/html" title="Debezium 2.0.0.Beta2 Released"/><published>2022-09-16T00:00:00+00:00</published><updated>2022-09-16T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/09/16/debezium-2.0-beta2-released</id><content type="html" xml:base="https://debezium.io/blog/2022/09/16/debezium-2.0-beta2-released/"><![CDATA[<div class="paragraph"> <p>I am excited to announce the release of Debezium <strong>2.0.0.Beta2</strong>!</p> </div> <div class="paragraph"> <p>This release contains several breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Beta2%20ORDER%20BY%20component%20ASC">107 issues</a> that were fixed.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>If you intend to upgrade to 2.0.0.Beta2, we strongly recommend that you read the release notes before the upgrade to understand all breaking changes. The following noteworthy list of changes are those we&#8217;ll cover in this blog post, some of which are breaking:</p> </div> <div class="ulist"> <ul> <li> <p>[breaking] <a href="#new-connector-property-namespaces">New connector property namespaces</a></p> </li> <li> <p>[potentially breaking] <a href="#all-debezium-schemas-are-named">All event schemas properly named and versioned</a></p> </li> <li> <p>[potentially breaking] <a href="#skipped-operations">Skipped operations now includes truncate events by default</a></p> </li> <li> <p><a href="#mysql-binlog-compression-support">MySQL binlog compression support</a></p> </li> <li> <p><a href="#cassandra4-incremental-commit-log-support">Cassandra 4 incremental commit log support</a></p> </li> <li> <p><a href="#pause-and-resume-incremental-snapshots">Pausing and resuming paused incremental snapshots</a></p> </li> <li> <p><a href="#custom-sql-filtering-incremental-snapshots">Custom SQL filtering for incremental snapshots</a></p> </li> <li> <p><a href="#signal-collection-automatically-registered">Signal collection now added to table include list automatically</a></p> </li> <li> <p><a href="#multitasking-vitess">Multitasking support for the Vitess connector</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="new-connector-property-namespaces">New connector property namespaces</h2> <div class="sectionbody"> <div class="paragraph"> <p>One of the largest overhauls going into Debezium 2.0 is the introduction of new connector property namespaces. Starting in Debezium 2.0 Beta2 and onward, many connector properties have been relocated with new names. This is a breaking change and affects most, if not all, connector deployments during the upgrade process.</p> </div> <div class="paragraph"> <p>Debezium previously used the prefix "database." with a plethora of varied connector properties. Some of these properties were meant to be passed directly to the JDBC driver and in other cases to the database history implementations, and so on. Unfortunately, we identified situations where some properties were being passed to underlying implementations that weren&#8217;t intended. While this wasn&#8217;t creating any type of regression or problem, it could potentially introduce a future issue if there were property name collisions, for example, a JDBC driver property that matched with a "database." prefixed Debezium connector property.</p> </div> <div class="paragraph"> <p>The following describes the changes to the connector properties</p> </div> <div class="ulist"> <ul> <li> <p>All configurations previously prefixed as <code>database.history.</code> are now to be prefixed using <code>schema.history.internal.</code> instead.</p> </li> <li> <p>All JDBC pass-thru options previously specified using <code>database.</code> prefix should now be prefixed using <code>driver.</code> instead.</p> </li> <li> <p>The <code>database.server.name</code> connector property renamed to <code>topic.prefix</code>.</p> </li> <li> <p>The MongoDB <code>mongodb.name</code> connector property aligned to use <code>topic.prefix</code> instead.</p> </li> </ul> </div> <div class="paragraph"> <p>Again, please review your connector configurations prior to deployment and adjust accordingly.</p> </div> </div> </div> <div class="sect1"> <h2 id="all-debezium-schemas-are-named">All Debezium event schemas are named and versioned</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium change events are emitted with a schema definition, which contains metadata about the fields such as the type, whether it&#8217;s required, and so on. In previous iterations of Debezium, some schema definitions did not have explicit names nor were they being explicitly versioned. In this release, we&#8217;ve moved to making sure that all schema definitions have an explicit name and version associated with them. The goal of this change is to help with future event structure compatibility, particularly for those who are using schema registries. However, if you are currently using a schema registry, be aware that this change may lead to schema compatibility issues during the upgrade process.</p> </div> </div> </div> <div class="sect1"> <h2 id="skipped-operations">Skipped operations default to truncate events</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium supports skipping specific event types by including the <code>skipped.operations</code> connector property in the connector&#8217;s configuration. This feature can be useful if you&#8217;re only interested in a subset of operations, such as only inserts and updates but not deletions.</p> </div> <div class="paragraph"> <p>One specific event type, truncates (<code>t</code>), is only supported by a subset of relational connectors and whether these events were to be skipped wasn&#8217;t consistent. In this release, we have aligned the <code>skipped.operations</code> behavior so that if the connector supports truncate events, these events are skipped by default.</p> </div> <div class="paragraph"> <p>Please review the following rule-set:</p> </div> <div class="ulist"> <ul> <li> <p>Connector supports truncate events and isn&#8217;t the Oracle connector</p> </li> <li> <p>Connector configuration does not specify the <code>skipped.operations</code> in the configuration</p> </li> </ul> </div> <div class="paragraph"> <p>If all the above are true, then the connector&#8217;s behavior will change after the upgrade. If you wish to continue to emit truncate events, the <code>skipped.operations=none</code> configuration will be required.</p> </div> </div> </div> <div class="sect1"> <h2 id="mysql-binlog-compression-support">MySQL binlog compression support</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this release, Debezium now supports reading of binlog entries that have been written with compression enabled. In version 8.0.20, MySQL adds the ability to compress binlog events using the ZSTD algorithm. To enable compression, you must toggle the <code>binlog.transaction_compression</code> variable on the MySQL server to <code>ON</code>. When compression is enabled, the binlog behaves as usual, except that the contents of the binlog entries are compressed to save space, and are replicated to in compressed format to replicas, significantly reducing network overhead for larger transactions.</p> </div> <div class="paragraph"> <p>If you&#8217;re interested in reading more about MySQL binlog compression, you can refer to the <a href="https://dev.mysql.com/doc/refman/8.0/en/binary-log-transaction-compression.html">Binary Log Transaction Compression</a> section of the MySQL documentation for more details.</p> </div> </div> </div> <div class="sect1"> <h2 id="cassandra4-incremental-commit-log-support">Cassandra 4 incremental commit log support</h2> <div class="sectionbody"> <div class="paragraph"> <p><a href="https://cassandra.apache.org/doc/latest/cassandra/operating/cdc.html">Cassandra 4</a> has improved the integration with CDC by adding a feature that when the fsync operation occurs, Cassandra will update a CDC-based index file to contain the latest offset values. This index file allows CDC implementations to read up to the offset that is considered durable in Cassandra.</p> </div> <div class="paragraph"> <p>In this release, Debezium now uses this CDC-based index file to eliminate the inherent delay in processing CDC events from Cassandra that previously existed. This should provide Cassandra users a substantial improvement in CDC with Debezium, and gives an incentive to consider Cassandra 4 over Cassandra 3.</p> </div> </div> </div> <div class="sect1"> <h2 id="pause-and-resume-incremental-snapshots">Pause and resume incremental snapshots</h2> <div class="sectionbody"> <div class="paragraph"> <p>Incremental snapshots have become an integral feature in Debezium. The incremental snapshot feature allows users to re-run a snapshot on one or more collections/tables for a variety of reasons. Incremental snapshots were originally introduced with just a <em>start</em> signal. We eventually added the ability to <em>stop</em> an ongoing incremental snapshot or to be able to remove a subset of collections/tables from an in-progress incremental snapshot.</p> </div> <div class="paragraph"> <p>In this release, we&#8217;ve built on top of the existing signal foundation and we&#8217;ve introduced two new signals, one to <em>pause</em> an in-progress incremental snapshot and then another to <em>resume</em> the incremental snapshot if it has previously been paused. To pause an incremental snapshot, a <code>pause-snapshot</code> signal must be sent, and to resume, a <code>resume-snapshot</code> signal can be used.</p> </div> <div class="paragraph"> <p>These two new signals can be sent using the signal table strategy or the Kafka signal topic strategy for MySQL. Please refer to the <a href="https://debezium.io/documentation/reference/2.0/configuration/signalling.html#_signal_actions">signal support documentation</a> for more details on signals and how they work.</p> </div> </div> </div> <div class="sect1"> <h2 id="custom-sql-filtering-incremental-snapshots">Custom SQL filtering for incremental snapshots</h2> <div class="sectionbody"> <div class="paragraph"> <p>Although uncommon, there may be scenarios such as a connector misconfiguration, where a specific record or subset of records needs to be re-emitted to the topic. Unfortunately, incremental snapshots have traditionally been an all-or-nothing type of process, where we would re-emit all records from a collection or table as a part of the snapshot.</p> </div> <div class="paragraph"> <p>In this release, a new <code>additional-condition</code> property can be specified in the signal payload, allowing the signal to dictate a SQL-based predicate to control what subset of records should be included in the incremental snapshot instead of the default behavior of <em>all rows</em>.</p> </div> <div class="paragraph"> <p>The following example illustrates sending an incremental snapshot signal for the <code>products</code> table, but instead of sending all rows from the table to the topic, the <code>additional-condition</code> property has been specified to restrict the snapshot to only send events that relate to product id equal to <code>12</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">execute-snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">data-collections</span><span class="delimiter">&quot;</span></span>: [<span class="string"><span class="delimiter">&quot;</span><span class="content">inventory.products</span><span class="delimiter">&quot;</span></span>],
    <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">INCREMENTAL</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">additional-condition</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">product_id=12</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>We believe this new incremental snapshot feature will be tremendously helpful for a variety of reasons, without always having to re-snapshot all rows when only a subset of data is required.</p> </div> </div> </div> <div class="sect1"> <h2 id="signal-collection-automatically-registered">Signal collection automatically added to include filters</h2> <div class="sectionbody"> <div class="paragraph"> <p>In prior releases of Debezium, the signal collection/table used for incremental snapshots had to be manually added to your <code>table.include.list</code> connector property. A big theme in this release was improvements on incremental snapshots, so we&#8217;ve taken this opportunity to streamline this as well. Starting in this release, Debezium will automatically add the signal collection/table to the table inclusion filters, avoiding the need for users to manually add it.</p> </div> <div class="paragraph"> <p>This change does not impose any compatibility issues. Connector configurations that already include the signal collection/table in the <code>table.include.list</code> property will continue to work without requiring any changes. However, if you wish to align your configuration with current behavior, you can also safely remove the signal collection/table from the <code>table.include.list</code>, and Debezium will begin to handle this for you automatically.</p> </div> </div> </div> <div class="sect1"> <h2 id="multitasking-vitess">Multitasking support for Vitess connector</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Vitess connector previously allowed operation in two different modes that depended entirely on whether the connector configuration specified any shard details. Unfortunately in both cases, each resulted in a single task responsible for performing the VStream processing. For larger Vitess installations with many shards, this architecture could begin to show latency issues as it may not be able to keep up with all the changes across all shards. And even more complex, when specifying the shard details, this required manually resolving the shards across the cluster and starting a single Debezium connector per shard, which is both error-prone and more importantly could result in deploying many Debezium connectors.</p> </div> <div class="paragraph"> <p>The Vitess community recognized this and sought to find a solution that addresses all these problems, both from a maintenance and error perspective. In Debezium 2.0 Beta2, the Vitess connector now automatically resolves the shards via a discovery mechanism, quite similar to that of MongoDB. This discovery mechanism will then split the load across multiple tasks, allowing for a single deployment of Debezium running a task per shard or shard lists, depending on the maximum number of allowed tasks for the connector.</p> </div> <div class="paragraph"> <p>During the upgrade, the Vitess connector will automatically migrate the offset storage to the new format used with the multitasking behavior. But be aware that once you&#8217;ve upgraded, you won&#8217;t be able to downgrade to an earlier version as the offset storage format will have changed.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are many bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Source info of incremental snapshot events exports wrong data <a href="https://issues.redhat.com/browse/DBZ-4329">DBZ-4329</a></p> </li> <li> <p>Deprecate internal key/value converter options <a href="https://issues.redhat.com/browse/DBZ-4617">DBZ-4617</a></p> </li> <li> <p>"No maximum LSN recorded" log message can be spammed on low-activity databases <a href="https://issues.redhat.com/browse/DBZ-4631">DBZ-4631</a></p> </li> <li> <p>Redis Sink config properties are not passed to DB history <a href="https://issues.redhat.com/browse/DBZ-5035">DBZ-5035</a></p> </li> <li> <p>Upgrade SQL Server driver to 10.2.1.jre8 <a href="https://issues.redhat.com/browse/DBZ-5290">DBZ-5290</a></p> </li> <li> <p>HTTP sink not retrying failing requests <a href="https://issues.redhat.com/browse/DBZ-5307">DBZ-5307</a></p> </li> <li> <p>Translation from mongodb document to kafka connect schema fails when nested arrays contain no elements <a href="https://issues.redhat.com/browse/DBZ-5434">DBZ-5434</a></p> </li> <li> <p>Duplicate SCNs on same thread Oracle RAC mode incorrectly processed <a href="https://issues.redhat.com/browse/DBZ-5439">DBZ-5439</a></p> </li> <li> <p>Deprecate legacy topic selector for all connectors <a href="https://issues.redhat.com/browse/DBZ-5457">DBZ-5457</a></p> </li> <li> <p>Remove the dependency of JdbcConnection on DatabaseSchema <a href="https://issues.redhat.com/browse/DBZ-5470">DBZ-5470</a></p> </li> <li> <p>Missing the regex properties validation before start connector of DefaultRegexTopicNamingStrategy <a href="https://issues.redhat.com/browse/DBZ-5471">DBZ-5471</a></p> </li> <li> <p>Create Index DDL fails to parse when using TABLESPACE clause with quoted identifier <a href="https://issues.redhat.com/browse/DBZ-5472">DBZ-5472</a></p> </li> <li> <p>Outbox doesn&#8217;t check array consistency properly when it determines its schema <a href="https://issues.redhat.com/browse/DBZ-5475">DBZ-5475</a></p> </li> <li> <p>Misleading statistics written to the log <a href="https://issues.redhat.com/browse/DBZ-5476">DBZ-5476</a></p> </li> <li> <p>Remove SQL Server SourceTimestampMode <a href="https://issues.redhat.com/browse/DBZ-5477">DBZ-5477</a></p> </li> <li> <p>Debezium connector task didn&#8217;t retry when failover in mongodb 5 <a href="https://issues.redhat.com/browse/DBZ-5479">DBZ-5479</a></p> </li> <li> <p>Better error reporting for signal table failures <a href="https://issues.redhat.com/browse/DBZ-5484">DBZ-5484</a></p> </li> <li> <p>Oracle DATADUMP DDL cannot be parsed <a href="https://issues.redhat.com/browse/DBZ-5488">DBZ-5488</a></p> </li> <li> <p>Upgrade PostgreSQL driver to 42.4.1 <a href="https://issues.redhat.com/browse/DBZ-5493">DBZ-5493</a></p> </li> <li> <p>Mysql connector parser the ddl statement failed when including keyword "buckets" <a href="https://issues.redhat.com/browse/DBZ-5499">DBZ-5499</a></p> </li> <li> <p>duplicate call to config.validateAndRecord() in RedisDatabaseHistory <a href="https://issues.redhat.com/browse/DBZ-5506">DBZ-5506</a></p> </li> <li> <p>DDL statement couldn&#8217;t be parsed : mismatched input 'ENGINE' <a href="https://issues.redhat.com/browse/DBZ-5508">DBZ-5508</a></p> </li> <li> <p>Use “database.dbnames” in SQL Server docs <a href="https://issues.redhat.com/browse/DBZ-5516">DBZ-5516</a></p> </li> <li> <p>LogMiner DML parser incorrectly interprets concatenation operator inside quoted column value <a href="https://issues.redhat.com/browse/DBZ-5521">DBZ-5521</a></p> </li> <li> <p>Mysql Connector DDL Parser does not parse all privileges <a href="https://issues.redhat.com/browse/DBZ-5522">DBZ-5522</a></p> </li> <li> <p>CREATE TABLE with JSON-based CHECK constraint clause causes MultipleParsingExceptions <a href="https://issues.redhat.com/browse/DBZ-5526">DBZ-5526</a></p> </li> <li> <p>Disable preferring DDL before logical schema in history recovery <a href="https://issues.redhat.com/browse/DBZ-5535">DBZ-5535</a></p> </li> <li> <p>EmbeddedEngine should initialize Connector using SourceConnectorContext <a href="https://issues.redhat.com/browse/DBZ-5534">DBZ-5534</a></p> </li> <li> <p>Support EMPTY column identifier <a href="https://issues.redhat.com/browse/DBZ-5550">DBZ-5550</a></p> </li> <li> <p>Use TCCL as the default classloader to load interface implementations <a href="https://issues.redhat.com/browse/DBZ-5561">DBZ-5561</a></p> </li> <li> <p>max.queue.size.in.bytes is invalid <a href="https://issues.redhat.com/browse/DBZ-5569">DBZ-5569</a></p> </li> <li> <p>Language type for listings in automatic topic creation <a href="https://issues.redhat.com/browse/DBZ-5573">DBZ-5573</a></p> </li> <li> <p>Upgrade mysql-binlog-connector-java library version <a href="https://issues.redhat.com/browse/DBZ-5574">DBZ-5574</a></p> </li> <li> <p>Vitess: Handle VStream close unexpectedly <a href="https://issues.redhat.com/browse/DBZ-5579">DBZ-5579</a></p> </li> <li> <p>Error when parsing alter sql <a href="https://issues.redhat.com/browse/DBZ-5587">DBZ-5587</a></p> </li> <li> <p>Field validation errors are misleading for positive, non-zero expectations <a href="https://issues.redhat.com/browse/DBZ-5588">DBZ-5588</a></p> </li> <li> <p>Mysql connector can&#8217;t handle the case-sensitive of rename/change column statement <a href="https://issues.redhat.com/browse/DBZ-5589">DBZ-5589</a></p> </li> <li> <p>LIST_VALUE_CLAUSE not allowing TIMESTAMP LITERAL <a href="https://issues.redhat.com/browse/DBZ-5592">DBZ-5592</a></p> </li> <li> <p>Oracle DDL does not support comments on materialized views <a href="https://issues.redhat.com/browse/DBZ-5595">DBZ-5595</a></p> </li> <li> <p>Oracle DDL does not support DEFAULT ON NULL <a href="https://issues.redhat.com/browse/DBZ-5605">DBZ-5605</a></p> </li> <li> <p>Datatype mdsys.sdo_geometry not supported <a href="https://issues.redhat.com/browse/DBZ-5609">DBZ-5609</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Beta2%20ORDER%20BY%20component%20ASC">107 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ahmedjami">Ahmed ELJAMI</a>, <a href="https://github.com/ahus1">Alexander Schwartz</a>, <a href="https://github.com/aloubyansky">Alexey Loubyansky</a>, Gabor[Andras], <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/davsclaus">Claus Ibsen</a>, <a href="https://github.com/debjeetsarkar">Debjeet Sarkar</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/ruanhang1993">Hang Ruan</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/nicholas-fwang">Inki Hwang</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/DerGut">Jannik Steinmann</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/winklerm">Marek Winkler</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/nitinitt">Nitin Chhabra</a>, <a href="https://github.com/thangdc94">Phạm Ngọc Thắng</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/druud">Ruud H.G. van Tol</a>, <a href="https://github.com/jaegwonseo">Seo Jae-kwon</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/ramanenka">Vadzim Ramanenka</a>, <a href="https://github.com/vivekwassan">Vivek Wassan</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/GOODBOY008">Zhongqiang Gong</a>, <a href="https://github.com/BetaCat0">合龙 张</a>, <a href="https://github.com/comil4444">崔世杰</a>, and <a href="https://github.com/pkgonan">민규 김</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With the release of Debezium 2.0 Beta2, we&#8217;re in the home stretch toward 2.0.0.Final. The community should expect a CR1 by the end of September and 2.0.0.Final released by the middle of October.</p> </div> <div class="paragraph"> <p>In addition, our very own Gunnar Morling and I will be guests on the upcoming <a href="https://quarkus.io/insights">Quarkus Insights</a> podcast, episode #103. We will be discussing Debezium and Quarkus, how Debezium leverages the power of Quarkus, a virtual how-to on embedding Debezium in a Quarkus-based application, and discussing all new features in Debezium 2.0. Be sure to check out the podcast and let us what you think!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[I am excited to announce the release of Debezium 2.0.0.Beta2! This release contains several breaking changes, stability fixes, and bug fixes, all to inch us closer to 2.0.0.Final. Overall, this release contains a total of 107 issues that were fixed.]]></summary></entry><entry><title type="html">Debezium 2.0.0.Beta1 Released</title><link href="https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released/" rel="alternate" type="text/html" title="Debezium 2.0.0.Beta1 Released"/><published>2022-07-27T00:00:00+00:00</published><updated>2022-07-27T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released</id><content type="html" xml:base="https://debezium.io/blog/2022/07/27/debezium-2.0-beta1-released/"><![CDATA[<div class="paragraph"> <p>I am thrilled to share that Debezium <strong>2.0.0.Beta1</strong> has been released!</p> </div> <div class="paragraph"> <p>This release contains several new features including a pluggable topic selector, the inclusion of database user who committed changes for Oracle change events, and improved handling of table unique indices as primary keys. In addition, there are several breaking changes such as the move to multi-partition mode as default and the introduction of the <code>debezium-storage</code> module and its implementations. So lets take a look at all these in closer detail.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="multi_partition_mode_now_default">Multi-partition mode now default</h2> <div class="sectionbody"> <div class="paragraph"> <p>Many database platforms support multi-tenancy out of the box, meaning you can have one installation of the database engine and have many unique databases. In cases like SQL Server, this traditionally required a separate connector deployment for each unique database. Over the last year, a large effort has been made to break down that barrier and to introduce a common way that any single connector deployment could connect and stream changes from multiple databases.</p> </div> <div class="paragraph"> <p>The first notable change is with the SQL Server connector&#8217;s configuration option, <code>database.dbname</code>. This option has been replaced with a new option called <code>database.names</code>. As multi-partition mode is now default, this new <code>database.names</code> option can be specified using a comma-separated list of database names, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">database.names=TEST1,TEST2</code></pre> </div> </div> <div class="paragraph"> <p>In this example, the connector is being configured to capture changes from two unique databases on the same host installation. The connector will start two unique tasks in Kafka Connect and each task will be responsible for streaming changes from its respective database concurrently.</p> </div> <div class="paragraph"> <p>The second notable change is with connector metrics naming. A connector exposes JMX metrics via beans that are identified with a unique name. With multi-partition mode the default with multiple tasks, each task requires its own metrics bean and so a change in the naming strategy was necessary.</p> </div> <div class="paragraph"> <p>In older versions of Debezium using SQL Server as an example, metrics were available using the following naming strategy:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sql_server:type=connector-metrics,server=&lt;sqlserver.server.name&gt;,context=&lt;context&gt;</code></pre> </div> </div> <div class="paragraph"> <p>In this release, the naming strategy now includes a new <code>task</code> component in the JMX MBean name:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sql_server:type=connector-metrics,server=&lt;sqlserver.server.name&gt;,task=&lt;task.id&gt;,context=&lt;context&gt;</code></pre> </div> </div> <div class="paragraph"> <p>Please review your metrics configurations as the naming changes could have an impact when collecting Debezium metrics.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_storage_module">Debezium storage module</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this release, we have introduced a new <code>debezium-storage</code> set of artifacts for file- and kafka- based database history and offset storage. This change is the first of several future implementations set to support platforms such as Amazon S3, Redis, and possibly JDBC.</p> </div> <div class="paragraph"> <p>For users who install connectors via plugin artifacts, this should be a seamless change as all dependencies are bundled in those plugin downloadable archives. For users who may embed Debezium in their applications or who may be building their own connector, be aware you may need to add a new storage dependency depending on which storage implementations used.</p> </div> </div> </div> <div class="sect1"> <h2 id="pluggable_topic_selector">Pluggable topic selector</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium&#8217;s default topic naming strategy emits change events to topics named <code>database.schema.table</code>. If you require that topics be named differently, an SMT would normally be added to the connector configuration to adjust this behavior. But, this presents a challenge in situations where one of the components of this topic name, perhaps the database or table name, contains a dot (<code>.</code>) and perhaps an SMT doesn&#8217;t have adequate context.</p> </div> <div class="paragraph"> <p>In this release, a new <code>TopicNamingStrategy</code> was introduced to allow fully customizing this behavior directly inside Debezium. The default naming strategy implementation should suffice in most cases, but if you find that it doesn&#8217;t you can provide a custom implementation of the <code>TopicNamingStrategy</code> contract to fully control various namings used by the connector. To provide your own custom strategy, you would specify the <code>topic.naming.strategy</code> connector option with the fully-qualified class name of the strategy, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">topic.naming.strategy=org.myorganization.MyCustomTopicNamingStrategy</code></pre> </div> </div> <div class="paragraph"> <p>This custom strategy is not just limited to controlling the names of topics for table mappings, but also for schema changes, transaction metadata, and heartbeats. You can refer to the <code>DefaultTopicNamingStrategy</code> found <a href="https://github.com/debezium/debezium/blob/main/debezium-core/src/main/java/io/debezium/schema/DefaultTopicNamingStrategy.java">here</a> as an example. This feature is still incubating and we&#8217;ll continue to improve and develop it as feedback is received.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_commit_user_in_change_events">Oracle commit user in change events</h2> <div class="sectionbody"> <div class="paragraph"> <p>The source information block of change events carry a variety of context about where the change event originated. In this release, the Oracle connector now includes the user who made the database change in the captured change event. A new field, <code>user_name</code>, can now be found in the source info block with this new information. This field is optional, and is only available when changes are emitted using the LogMiner-based implementation. This field may also contain the value of <code>UNKNOWN</code> if the user associated with a change is dropped prior to the change being captured by the connector.</p> </div> </div> </div> <div class="sect1"> <h2 id="improved_table_unique_index_handling">Improved table unique index handling</h2> <div class="sectionbody"> <div class="paragraph"> <p>A table does not have to have a primary key to be captured by a Debezium connector. In cases where a primary key is not defined, Debezium will inspect a table&#8217;s unique indices to see whether a reasonable key substitution can be made. In some situations, the index may refer to columns such as <code>CTID</code> for PostgreSQL or <code>ROWID</code> in Oracle. These columns are not visible nor user-defined, but instead are hidden synthetic columns generated automatically by the database. In addition, the index may also use database functions to transform the column value that is stored, such as <code>UPPER</code> or <code>LOWER</code> for example.</p> </div> <div class="paragraph"> <p>In this release, indices that rely on hidden, auto-generated columns, or columns wrapped in database functions are no longer eligible as primary key alternatives. This guarantees that when relying on an index as a primary key rather than a defined primary key itself, the generated message&#8217;s primary key value tuple directly maps to the same values used by the database to represent uniqueness.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are several bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>MongoConnector&#8217;s field exclusion configuration does not work with fields with the same name but from different collections <a href="https://issues.redhat.com/browse/DBZ-4846">DBZ-4846</a></p> </li> <li> <p>Remove redundant setting of last events <a href="https://issues.redhat.com/browse/DBZ-5047">DBZ-5047</a></p> </li> <li> <p>Rename <code>docker-images</code> repository and JIRA component to <code>container-images</code> <a href="https://issues.redhat.com/browse/DBZ-5048">DBZ-5048</a></p> </li> <li> <p>Read Debezium Metrics From Debezium Server Consumer <a href="https://issues.redhat.com/browse/DBZ-5235">DBZ-5235</a></p> </li> <li> <p>User input are not consistent on Filter step for the DBZ connectors <a href="https://issues.redhat.com/browse/DBZ-5246">DBZ-5246</a></p> </li> <li> <p>KafkaDatabaseHistory without check database history topic create result caused UnknowTopicOrPartitionException <a href="https://issues.redhat.com/browse/DBZ-5249">DBZ-5249</a></p> </li> <li> <p>Treat SQLServerException with "Broken pipe (Write failed)" exception message as a retriable exception <a href="https://issues.redhat.com/browse/DBZ-5292">DBZ-5292</a></p> </li> <li> <p>Lob type data is inconsistent between source and sink, after modifying the primary key <a href="https://issues.redhat.com/browse/DBZ-5295">DBZ-5295</a></p> </li> <li> <p>Caused by: java.io.EOFException: Failed to read next byte from position 2005308603 <a href="https://issues.redhat.com/browse/DBZ-5333">DBZ-5333</a></p> </li> <li> <p>Incremental Snapshot: Oracle table name parsing does not support periods in DB name <a href="https://issues.redhat.com/browse/DBZ-5336">DBZ-5336</a></p> </li> <li> <p>Support PostgreSQL default value function calls with schema prefixes <a href="https://issues.redhat.com/browse/DBZ-5340">DBZ-5340</a></p> </li> <li> <p>Unsigned tinyint conversion fails for MySQL 8.x <a href="https://issues.redhat.com/browse/DBZ-5343">DBZ-5343</a></p> </li> <li> <p>Log a warning when an unsupported LogMiner operation is detected for a captured table <a href="https://issues.redhat.com/browse/DBZ-5351">DBZ-5351</a></p> </li> <li> <p>NullPointerException thrown when unique index based on both system and non-system generated columns <a href="https://issues.redhat.com/browse/DBZ-5356">DBZ-5356</a></p> </li> <li> <p>MySQL Connector column hash v2 does not work <a href="https://issues.redhat.com/browse/DBZ-5366">DBZ-5366</a></p> </li> <li> <p>Outbox JSON expansion fails when nested arrays contain no elements <a href="https://issues.redhat.com/browse/DBZ-5367">DBZ-5367</a></p> </li> <li> <p>docker-maven-plugin needs to be upgraded for Mac Apple M1 <a href="https://issues.redhat.com/browse/DBZ-5369">DBZ-5369</a></p> </li> <li> <p>AWS DocumentDB (with MongoDB Compatibility) Connect Fail <a href="https://issues.redhat.com/browse/DBZ-5371">DBZ-5371</a></p> </li> <li> <p>Oracle Xstream does not propagate commit timestamp to transaction metadata <a href="https://issues.redhat.com/browse/DBZ-5373">DBZ-5373</a></p> </li> <li> <p>UI View connector config in non-first cluster return 404 <a href="https://issues.redhat.com/browse/DBZ-5378">DBZ-5378</a></p> </li> <li> <p>CommitScn not logged in expected format <a href="https://issues.redhat.com/browse/DBZ-5381">DBZ-5381</a></p> </li> <li> <p>org.postgresql.util.PSQLException: Bad value for type timestamp/date/time: CURRENT_TIMESTAMP <a href="https://issues.redhat.com/browse/DBZ-5384">DBZ-5384</a></p> </li> <li> <p>Missing "previousId" property with parsing the rename statement in kafka history topic <a href="https://issues.redhat.com/browse/DBZ-5386">DBZ-5386</a></p> </li> <li> <p>Check constraint introduces a column based on constraint in the schema change event. <a href="https://issues.redhat.com/browse/DBZ-5390">DBZ-5390</a></p> </li> <li> <p>Support storing extended attributes in relational model and JSON schema history topic <a href="https://issues.redhat.com/browse/DBZ-5396">DBZ-5396</a></p> </li> <li> <p>The column is referenced as PRIMARY KEY, but a matching column is not defined in table <a href="https://issues.redhat.com/browse/DBZ-5398">DBZ-5398</a></p> </li> <li> <p>Clarify which database name to use for signal.data.collection when using Oracle with pluggable database support <a href="https://issues.redhat.com/browse/DBZ-5399">DBZ-5399</a></p> </li> <li> <p>Timestamp with time zone column&#8217;s default values not in GMT <a href="https://issues.redhat.com/browse/DBZ-5403">DBZ-5403</a></p> </li> <li> <p>Upgrade to Kafka 3.1 broke build compatibility with Kafka 2.x and Kafka 3.0 <a href="https://issues.redhat.com/browse/DBZ-5404">DBZ-5404</a></p> </li> <li> <p>Remove the duplicated SimpleDdlParserListener from mysql connector <a href="https://issues.redhat.com/browse/DBZ-5425">DBZ-5425</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Beta1%20ORDER%20BY%20component%20ASC">59 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ajunwalker">Andrew Walker</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/giljae">Giljae Joo</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/domsj">Jan Doms</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/nathan-smit-1">Nathan Smit</a>, <a href="https://github.com/pmalon">Paweł Malon</a>, <a href="https://github.com/smallYellowCat">Pengwei Dou</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/GOODBOY008">Zhongqiang Gong</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>In these last few months, the team has made some incredible progress on Debezium 2.0, and we can begin to see the finish line in the distance. A large of this is in part to the grew work the community has done to contribute changes, provide feedback, and to test and help make new features stable. But we&#8217;re not done, so you can continue to expect another 2.0.0.Beta2 release in approximately 3 weeks, sticking with our usual cadence.</p> </div> <div class="paragraph"> <p>In addition, we do continue to backport changes to the 1.9 branch and will likely look at a 1.9.6.Final release sometime in August to round out that release stream just before we wrap up Debezium 2.0.0.Final.</p> </div> <div class="paragraph"> <p>So stay cool and safe and happy capturing!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[I am thrilled to share that Debezium 2.0.0.Beta1 has been released! This release contains several new features including a pluggable topic selector, the inclusion of database user who committed changes for Oracle change events, and improved handling of table unique indices as primary keys. In addition, there are several breaking changes such as the move to multi-partition mode as default and the introduction of the debezium-storage module and its implementations. So lets take a look at all these in closer detail.]]></summary></entry><entry><title type="html">Debezium 1.9.5.Final Released</title><link href="https://debezium.io/blog/2022/07/11/debezium-1-9-5-final-released/" rel="alternate" type="text/html" title="Debezium 1.9.5.Final Released"/><published>2022-07-11T00:00:00+00:00</published><updated>2022-07-11T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/07/11/debezium-1-9-5-final-released</id><content type="html" xml:base="https://debezium.io/blog/2022/07/11/debezium-1-9-5-final-released/"><![CDATA[<div class="paragraph"> <p>With the summer in full swing, the team is pleased to announce the release of Debezium <strong>1.9.5.Final</strong>!</p> </div> <div class="paragraph"> <p>This release primarily focuses on bugfixes and stability; and is the recommended update for all users from earlier versions. This release contains <a href="https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+1.9.5.Final">24 resolved issues</a> overall.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="changes">Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>This release focused entirely on stability and bugfixes. A few noteworthy changes include:</p> </div> <div class="ulist"> <ul> <li> <p>Data duplication problem using postgresql source on debezium server <a href="https://issues.redhat.com/browse/DBZ-5070">DBZ-5070</a></p> </li> <li> <p>Duplicate SCNs on Oracle RAC installations incorrectly processed <a href="https://issues.redhat.com/browse/DBZ-5245">DBZ-5245</a></p> </li> <li> <p>NPE when using Debezium Embedded in Quarkus <a href="https://issues.redhat.com/browse/DBZ-5251">DBZ-5251</a></p> </li> <li> <p>No changes to commit_scn when oracle-connector got new lob data <a href="https://issues.redhat.com/browse/DBZ-5266">DBZ-5266</a></p> </li> <li> <p>database.history.store.only.captured.tables.ddl not suppressing logs <a href="https://issues.redhat.com/browse/DBZ-5270">DBZ-5270</a></p> </li> <li> <p>Debezium server fail when connect to Azure Event Hubs <a href="https://issues.redhat.com/browse/DBZ-5279">DBZ-5279</a></p> </li> <li> <p>Enabling database.history.store.only.captured.tables.ddl does not restrict history topic records <a href="https://issues.redhat.com/browse/DBZ-5285">DBZ-5285</a></p> </li> <li> <p>Snapshot fails when table&#8217;s relational model is created using an abstract data type as unique index <a href="https://issues.redhat.com/browse/DBZ-5300">DBZ-5300</a></p> </li> <li> <p>Incremental Snapshot: Oracle table name parsing does not support periods in DB name <a href="https://issues.redhat.com/browse/DBZ-5336">DBZ-5336</a></p> </li> <li> <p>Support PostgreSQL default value function calls with schema prefixes <a href="https://issues.redhat.com/browse/DBZ-5340">DBZ-5340</a></p> </li> <li> <p>Log a warning when an unsupported LogMiner operation is detected for a captured table <a href="https://issues.redhat.com/browse/DBZ-5351">DBZ-5351</a></p> </li> <li> <p>MySQL Connector column hash v2 does not work <a href="https://issues.redhat.com/browse/DBZ-5366">DBZ-5366</a></p> </li> <li> <p>Outbox JSON expansion fails when nested arrays contain no elements <a href="https://issues.redhat.com/browse/DBZ-5367">DBZ-5367</a></p> </li> <li> <p>docker-maven-plugin needs to be upgraded for Mac Apple M1 <a href="https://issues.redhat.com/browse/DBZ-5369">DBZ-5369</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/1.9/release-notes#release-1.9.5-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community which contributed to Debezium 1.9.5.Final: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/ProofOfPizza">Chai Stofkoper</a>, <a href="https://github.com/Naros">Chris Cranford</a>, Mikhail Dubrovin, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/pmalon">Paweł Malon</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/yangrong688">yangrong688</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook">Outlook</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium 1.9 release stream will remain the current long-running version for the next three months. During this time, we will continue to evaluate user reports and do micro-releases to address bugs and regressions depending on severity.</p> </div> <div class="paragraph"> <p>The development on Debezium 2.0 is moving along quite nicely. We have entered the second half of the development cycle, and we&#8217;ll begin beta releases with the next release toward the end of July.</p> </div> <div class="paragraph"> <p>Stay tuned for more in the coming weeks, stay cool out there, and happy capturing!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[With the summer in full swing, the team is pleased to announce the release of Debezium 1.9.5.Final! This release primarily focuses on bugfixes and stability; and is the recommended update for all users from earlier versions. This release contains 24 resolved issues overall.]]></summary></entry><entry><title type="html">Debezium 2.0.0.Alpha3 Released</title><link href="https://debezium.io/blog/2022/07/05/debezium-2.0-alpha3-released/" rel="alternate" type="text/html" title="Debezium 2.0.0.Alpha3 Released"/><published>2022-07-05T00:00:00+00:00</published><updated>2022-07-05T00:00:00+00:00</updated><id>https://debezium.io/blog/2022/07/05/debezium-2.0-alpha3-released</id><content type="html" xml:base="https://debezium.io/blog/2022/07/05/debezium-2.0-alpha3-released/"><![CDATA[<div class="paragraph"> <p>I am thrilled to share that Debezium <strong>2.0.0.Alpha3</strong> has been released!</p> </div> <div class="paragraph"> <p>While this release contains a plethora of bugfixes, there are a few noteworthy improvements, which include providing a timestamp in transaction metadata events, the addition of several new fields in Oracle&#8217;s change event source block, and a non-backward compatible change to the Oracle connector&#8217;s offsets.</p> </div> <div class="paragraph"> <p>Lets take a look at these in closer detail.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="transaction_metadata_changes">Transaction metadata changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>A transaction metadata event describes the <em>beginning</em> and the <em>end</em> (commit) of a database transaction. These events are useful for a variety of reasons, including auditing. By default, transaction metadata events are not generated by a connector and to enable this feature, the <code>provide.transaction.metadata</code> option must be enabled.</p> </div> <div class="paragraph"> <p>In this release, both <code>BEGIN</code> and <code>END</code> events include a new field, <code>ts_ms</code>, which is the database timestamp of when the transaction either began or committed depending on the event type. An example of such an event now looks like:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">status</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">END</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">12345</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">2</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1657033173441</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collections</span><span class="delimiter">&quot;</span></span>: [
    {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">s1.a</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    },
    {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">data_collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">s2.a</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">event_count</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    }
  ]
}</code></pre> </div> </div> <div class="paragraph"> <p>If you are already using the transaction metadata feature, new events will contain this field after upgrading.</p> </div> <div class="paragraph"> <p>If you are not using the transaction metadata feature but find this useful, simply add the <code>provide.transaction.metadata</code> option set to <em>true</em> to your connector configuration. By default, metadata events are emitted to a topic named after your <code>database.server.name</code> option. This can be overridden by specifying the <code>transaction.topic</code> option, as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">database.server.name=server1
provide.transaction.metadata=true
transaction.topic=my-transaction-events</code></pre> </div> </div> <div class="paragraph"> <p>In this example, all transaction metadata events will be emitted to <code>my-transaction-events</code>. Please see your connector specific configuration for more details.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_source_info_changes">Oracle source info changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>source</code> information block is a section in the change event&#8217;s payload that describes the database attributes of what generated the change event. For example, this section includes the system change number, the database timestamp of the change, and the transaction the change was part of.</p> </div> <div class="paragraph"> <p>In this release, we identified a regression where the <code>scn</code> field did not correctly reflect the right <code>source</code> of where the change event occurred. While it isn&#8217;t abnormal for Oracle to generate multiple changes with the same system change number, we did find a regression that caused the wrong system change number to get assigned to each individual event within a scoped transaction, which made it difficult for some to use this information for auditing purposes. The <code>source.scn</code> field should now correctly reflect the system change number from Oracle LogMiner or Oracle Xstream.</p> </div> <div class="paragraph"> <p>Additionally, several new fields were added to the <code>source</code> information block to improve integration with the LogMiner implementation and Oracle RAC. An example of the new source information block:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">version</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2.0.0.Alpha3</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">server1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1520085154000</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">6.28.807</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2122184</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2122185</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">rs_id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">001234.00012345.0124</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">ssn</span><span class="delimiter">&quot;</span></span>: <span class="integer">0</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">redo_thread</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>The newly added fields are:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>rs_id</code></dt> <dd> <p>Specifies the rollback segment identifier associated with the change.</p> </dd> <dt class="hdlist1"><code>ssn</code></dt> <dd> <p>Specifies the SQL sequence number, this combined with the <code>rs_id</code> represent a unique tuple for a change.</p> </dd> <dt class="hdlist1"><code>redo_thread</code></dt> <dd> <p>Specifies the actual database redo thread that managed the change&#8217;s lifecycle.</p> </dd> </dl> </div> <div class="paragraph"> <p>Whether using Oracle Standalone or RAC, these values will always be provided when using Oracle LogMiner. These values have more importance on an Oracle RAC installation because you have multiple database servers manipulating the shared database concurrently. These fields specifically annotate which node and at what position on that node that the change originated.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_connector_offset_changes">Oracle connector offset changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In an Oracle Real Application Clusters (RAC) environment, multiple nodes access and manipulate the Oracle database concurrently. Each node maintains its own redo log buffers and executes its own redo writer thread. This means that at any given moment, each node has its own unique "position" and these will differ entirely on the activity that takes place on each respective node.</p> </div> <div class="paragraph"> <p>In this release, a small change was necessary in <a href="https://issues.redhat.com/browse/DBZ-5245">DBZ-5245</a> to support Oracle RAC. Previously, the connector offsets maintained a field called <code>scn</code> which represented this "position" of where the connector should stream changes from. But since each node could be at different positions in the redo, a single <code>scn</code> value was inadequate for Oracle RAC.</p> </div> <div class="paragraph"> <p>The old Oracle connector offsets looked like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1234567890</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2345678901</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">lcr_position</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Starting in 2.0.0.Alpha3, the new offset structure now has this form:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1234567890:00124.234567890.1234:0:1,1234567891:42100.0987656432.4321:0:2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">commit_scn</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">2345678901</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">lcr_position</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>You will notice that the <code>scn</code> field now consists of a comma-separated list of values, where each entry represents a tuple of values. This new tuple has the format of <code>scn:rollback-segment-id:ssn:redo-thread</code>.</p> </div> <div class="paragraph"> <p>While this change is forward compatible, meaning you can safely upgrade to 2.0.0.Alpha3 and the old format can be read, once the new format is written to the offsets, the older versions of the connector will be unable to read the offsets. If you upgrade and decide you need to roll back, be aware you&#8217;ll need to manually adjust the connector offset&#8217;s <code>scn</code> field to simply contain a string of the most recent <code>scn</code> value across all redo threads.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are several bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Incorrect loading of LSN from offsets <a href="https://issues.redhat.com/browse/DBZ-3942">DBZ-3942</a></p> </li> <li> <p>Database history recovery will retain old tables after they&#8217;ve been renamed <a href="https://issues.redhat.com/browse/DBZ-4451">DBZ-4451</a></p> </li> <li> <p>Adding new table with incremental snapshots not working <a href="https://issues.redhat.com/browse/DBZ-4834">DBZ-4834</a></p> </li> <li> <p>BigDecimal has mismatching scale value for given Decimal schema <a href="https://issues.redhat.com/browse/DBZ-4890">DBZ-4890</a></p> </li> <li> <p>Debezium has never found starting LSN <a href="https://issues.redhat.com/browse/DBZ-5031">DBZ-5031</a></p> </li> <li> <p>Data duplication problem using postgresql source on debezium server <a href="https://issues.redhat.com/browse/DBZ-5070">DBZ-5070</a></p> </li> <li> <p>Cursor fetch is used for all results during connection <a href="https://issues.redhat.com/browse/DBZ-5084">DBZ-5084</a></p> </li> <li> <p>Debezuim connector fails at parsing select statement overrides when table name has space <a href="https://issues.redhat.com/browse/DBZ-5198">DBZ-5198</a></p> </li> <li> <p>DDL statement couldn&#8217;t be parsed 2 - Oracle connector 1.9.3.Final <a href="https://issues.redhat.com/browse/DBZ-5230">DBZ-5230</a></p> </li> <li> <p>Debezium server duplicates scripting jar files <a href="https://issues.redhat.com/browse/DBZ-5232">DBZ-5232</a></p> </li> <li> <p>Cannot convert field type tinyint(1) unsigned to boolean <a href="https://issues.redhat.com/browse/DBZ-5236">DBZ-5236</a></p> </li> <li> <p>Oracle unparsable ddl create table <a href="https://issues.redhat.com/browse/DBZ-5237">DBZ-5237</a></p> </li> <li> <p>Postgres Incremental Snapshot on parent partitioned table not working <a href="https://issues.redhat.com/browse/DBZ-5240">DBZ-5240</a></p> </li> <li> <p>Character set influencers are not properly parsed on default values <a href="https://issues.redhat.com/browse/DBZ-5241">DBZ-5241</a></p> </li> <li> <p>NPE when using Debezium Embedded in Quarkus <a href="https://issues.redhat.com/browse/DBZ-5251">DBZ-5251</a></p> </li> <li> <p>Oracle LogMiner may fail with an in-progress transaction in an archive log that has been deleted <a href="https://issues.redhat.com/browse/DBZ-5256">DBZ-5256</a></p> </li> <li> <p>Order of source block table names in a rename schema change event is not deterministic <a href="https://issues.redhat.com/browse/DBZ-5257">DBZ-5257</a></p> </li> <li> <p>Debezium fails to connect to replicaset if a node is down <a href="https://issues.redhat.com/browse/DBZ-5260">DBZ-5260</a></p> </li> <li> <p>No changes to commit_scn when oracle-connector got new lob data <a href="https://issues.redhat.com/browse/DBZ-5266">DBZ-5266</a></p> </li> <li> <p>Invalid date 'SEPTEMBER 31' <a href="https://issues.redhat.com/browse/DBZ-5267">DBZ-5267</a></p> </li> <li> <p>database.history.store.only.captured.tables.ddl not suppressing logs <a href="https://issues.redhat.com/browse/DBZ-5270">DBZ-5270</a></p> </li> <li> <p>io.debezium.text.ParsingException: DDL statement couldn&#8217;t be parsed <a href="https://issues.redhat.com/browse/DBZ-5271">DBZ-5271</a></p> </li> <li> <p>Deadlock during snapshot with Mongo connector <a href="https://issues.redhat.com/browse/DBZ-5272">DBZ-5272</a></p> </li> <li> <p>Mysql parser is not able to handle variables in KILL command <a href="https://issues.redhat.com/browse/DBZ-5273">DBZ-5273</a></p> </li> <li> <p>Debezium server fail when connect to Azure Event Hubs <a href="https://issues.redhat.com/browse/DBZ-5279">DBZ-5279</a></p> </li> <li> <p>ORA-01086 savepoint never established raised when database history topic cannot be created or does not exist <a href="https://issues.redhat.com/browse/DBZ-5281">DBZ-5281</a></p> </li> <li> <p>Enabling database.history.store.only.captured.tables.ddl does not restrict history topic records <a href="https://issues.redhat.com/browse/DBZ-5285">DBZ-5285</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.0.0.Alpha3%20ORDER%20BY%20component%20ASC">66 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/ProofOfPizza">Chai Stofkoper</a>, <a href="https://github.com/Naros">Chris Cranford</a>, Mikhail Dubrovin, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/yannickzj">Jun Zhao</a>, <a href="https://github.com/kanha-gupta">Kanha Gupta</a>, <a href="https://github.com/alwaysbemark">Mark Bereznitsky</a>, <a href="https://github.com/mimaison">Mickael Maison</a>, <a href="https://github.com/mikekamornikov">Mike Kamornikov</a>, <a href="https://github.com/krnaveen14">Naveen Kumar KR</a>, Oskar Polak, <a href="https://github.com/rahulkhanna2">Rahul Khanna</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/tim-patterson">Tim Patterson</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/yangrong688">yangrong688</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>You can expect a 1.9.5.Final release in the next week. This release will include many of the bugfixes that are part of this release, as we continue to improve the stability of 1.9 in micro-releases.</p> </div> <div class="paragraph"> <p>You can also expect 2.0.0.Beta1 in the next 3 weeks, keeping with our usual release cadence. The next major milestones includes unifying snapshot modes across connectors, a new <code>Snapshotter</code> API for all connectors, compactable JSON database history, offset unification, offset storage API and much more.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[I am thrilled to share that Debezium 2.0.0.Alpha3 has been released! While this release contains a plethora of bugfixes, there are a few noteworthy improvements, which include providing a timestamp in transaction metadata events, the addition of several new fields in Oracle&#8217;s change event source block, and a non-backward compatible change to the Oracle connector&#8217;s offsets. Lets take a look at these in closer detail.]]></summary></entry></feed>