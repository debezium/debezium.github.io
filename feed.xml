<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://debezium.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debezium.io/" rel="alternate" type="text/html"/><updated>2023-09-29T14:04:44+00:00</updated><id>https://debezium.io/feed.xml</id><title type="html">Debezium</title><subtitle>Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.</subtitle><entry><title type="html">Online machine learning with the data streams from the database</title><link href="https://debezium.io/blog/2023/09/23/flink-spark-online-learning/" rel="alternate" type="text/html" title="Online machine learning with the data streams from the database"/><published>2023-09-23T00:00:00+00:00</published><updated>2023-09-23T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/09/23/flink-spark-online-learning</id><content type="html" xml:base="https://debezium.io/blog/2023/09/23/flink-spark-online-learning/"><![CDATA[<div class="paragraph"> <p>In <a href="https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification/">the previous blog post</a>, we have shown how to leverage Debezium to train neural-network model with the existing data from the database and use this pre-trained model to classify images newly stored into the database. In this blog post, we will move it one step further - we will use Debezium to create multiple data streams from the database and use one of the streams for continuous learning and to improve our model, and the second one for making predictions on the data. When the model is constantly improved or adjusted to recent data samples, this approach is known as <a href="https://en.wikipedia.org/wiki/Online_machine_learning">online machine learning</a>. Online learning is only suitable for some use cases, and implementing an online variant of a given algorithm may be challenging or even impossible. However, in situations where online learning is possible, it becomes a very powerful tool as it allows one to react to the changes in the data in real-time and avoids the need to re-train and re-deploy new models, thus saving the hardware and operational costs. As the streams of data become more and more common, e.g. with the advent of IoT, we can expect online learning to become more and more popular. It&#8217;s usually a perfect fit for analyzing streaming data in use cases where it&#8217;s possible.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>As mentioned in the previous blog, our goal here is not to build the best possible model for a given use case but to investigate how we can build a complete pipeline from inserting the data into the database through delivering it to the model and using it for model training and predictions. To keep things simple, we will use another well-known data sample often used in ML tutorials. We will explore how to classify various species of the Iris flower using an online variant of <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-mean clustering algorithm</a>. We use <a href="https://flink.apache.org/">Apache Flink</a> and <a href="https://spark.apache.org/">Apache Spark</a> to process the data streams. Both these frameworks are very popular data processing frameworks and include a machine learning library, which, besides others, implements online k-means algorithms. Thus, we can focus on building a complete pipeline for delivering the data from the database into a given model, processing it in real time, and not having to deal with the algorithm&#8217;s implementation details.</p> </div> <div class="paragraph"> <p>All the code mentioned later in this blog post is available as a Debezium example in <a href="https://github.com/debezium/debezium-examples/tree/blog_flink_spark_ml/machine-learning/flink-spark-iris">Debezium example repository</a>, with all other useful stuff, like Docker composes and step-by-step instructions in the <a href="https://github.com/debezium/debezium-examples/blob/blog_flink_spark_ml/machine-learning/flink-spark-iris/README.adoc">README</a> file.</p> </div> <div class="sect1"> <h2 id="data_set_preparation">Data set preparation</h2> <div class="sectionbody"> <div class="paragraph"> <p>We will use <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">Iris flower data set</a>. Our goal is to determine the Iris species based on a couple of measurements of the Iris flower: its sepal length, sepal width, petal length, and petal width.</p> </div> <div class="imageblock centered-image"> <figure> <img src="/assets/images/2023-09-23-flink-spark-online-learning/iris_versicolor.jpg" class="responsive-image" alt="Iris versicolor"> <figcaption>Iris versicolor, source <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set#/media/File:Iris_versicolor_3.jpg">Wikipedia</a></figcaption> </figure> </div> <div class="paragraph"> <p>The data set can be downloaded from various sources. We can take advantage of the fact that it&#8217;s available already pre-processed in e.g. <a href="https://scikit-learn.org">scikit-learn</a> toolkit and use it from there. Each sample row contains a data point (sepal length, sepal width, petal length, and petal width) and a label. Label is number 0, 1, or 2, where 0 stands for Iris setosa, 1 stands for Iris versicolor, and 2 for Iris virginica. The data set is small - containing only 150 data points.</p> </div> <div class="paragraph"> <p>As we load the data into the database, we will first prepare SQL files, which we will later pass to the database. We need to divide the original data sample into three sub-samples - two for training and one for testing. The initial training will use the first training data sample. This data sample is intentionally small to not generate good predictions when we test the model for the first time so that we can see how the model&#8217;s prediction will increase in real-time when we feed it with more data.</p> </div> <div class="paragraph"> <p>You can use the following Python script from the accompanying demo repository for generating all three SQL files.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>$ ./iris2sql.py</code></pre> </div> </div> <div class="paragraph"> <p>The <code>postgres</code> directory contains the files used for this demo. <code>train1.sql</code> will be loaded automatically into the Postgres database upon its start. <code>test.sql</code> and <code>train2.sql</code> will be loaded manually into the database later.</p> </div> </div> </div> <div class="sect1"> <h2 id="classification_with_apache_flink">Classification with Apache Flink</h2> <div class="sectionbody"> <div class="paragraph"> <p>First, let&#8217;s look at how to do online Iris flower classification and learning in Apache Flink. The following figure depicts the high-level schema for the entire pipeline.</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-09-23-flink-spark-online-learning/postgres_to_flink.png" class="responsive-image" alt="Postgres to Flink schema"> </div> <div class="paragraph"> <p>We will use Postgres as our source database. Debezium, deployed as a Kafka Connect source connector, tracks the changes in the database and creates the streams of data sent to Kafka from newly inserted data. Kafka sends these streams to Apache Flink, which employs the streaming k-means algorithm for model fitting and data classification. The predictions of the model for test data streams are produced as another stream and sent back to Kafka.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>You can also ingest database changes directly into the Flink without using Kafka. Ververika&#8217;s implementation of CDC source connectors embeds the Debezium directly into the Flink. See Flink CDC connectors <a href="https://ververica.github.io/flink-cdc-connectors/">documentation</a> for more details.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Our database contains two tables. The first stores our training data, while the second stores the test data. Therefore, there are two data streams, each corresponding to one table - one data stream for learning and one with data points that need to be classified. In real applications, you can use only one table or, on the contrary, many more tables. You can even deploy more Debezium connectors and thus combine data from several databases.</p> </div> <div class="sect2"> <h3 id="using_debezium_and_kafka_as_a_source_data_stream">Using Debezium and Kafka as a source data stream</h3> <div class="paragraph"> <p>Apache Flink has excellent integration with Kafka. We can pass the Debezium records as e.g. JSON records. For creating Flink tables, it even has support for Debezium&#8217;s record format, but for streams, we need to extract part of the Debezium message, which contains the newly stored row of the table. However, this is very easy as Debezium provides SMT, <a href="https://debezium.io/documentation/reference/nightly/transformations/event-flattening.html">extract new record state SMT</a>, which does precisely this. The complete Debezium configuration can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">iris-connector-flink</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5432</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">topic.prefix</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">flink</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">table.include.list</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.iris_.*</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">unwrap</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.unwrap.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.ExtractNewRecordState</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>The configuration captures all tables in the <code>public</code> schema with tables that begin with the <code>iris_</code> prefix. Since we are storing training and test data in two tables, two Kafka topics named <code>flink.public.iris_train</code> and <code>flink.public.iris_test</code> are created, respectively. Flink&#8217;s <code>DataStreamSource</code> represents the incoming stream of data. As we encode the records as a JSON, it will be a stream of JSON <code>ObjectNode</code> objects. Constructing the source stream is very straightforward:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">KafkaSource&lt;ObjectNode&gt; train = KafkaSource.&lt;ObjectNode&gt;builder()
    .setBootstrapServers(<span class="string"><span class="delimiter">&quot;</span><span class="content">kafka:9092</span><span class="delimiter">&quot;</span></span>)
    .setTopics(<span class="string"><span class="delimiter">&quot;</span><span class="content">flink.public.iris_train</span><span class="delimiter">&quot;</span></span>)
    .setClientIdPrefix(<span class="string"><span class="delimiter">&quot;</span><span class="content">train</span><span class="delimiter">&quot;</span></span>)
    .setGroupId(<span class="string"><span class="delimiter">&quot;</span><span class="content">dbz</span><span class="delimiter">&quot;</span></span>)
    .setStartingOffsets(OffsetsInitializer.earliest())
    .setDeserializer(KafkaRecordDeserializationSchema.of(<span class="keyword">new</span> JSONKeyValueDeserializationSchema(<span class="predefined-constant">false</span>)))
    .build();
DataStreamSource&lt;ObjectNode&gt; trainStream = env.fromSource(train, WatermarkStrategy.noWatermarks(), <span class="string"><span class="delimiter">&quot;</span><span class="content">Debezium train</span><span class="delimiter">&quot;</span></span>);</code></pre> </div> </div> <div class="paragraph"> <p>Flink operates primarily on the <code>Table</code> abstraction object. Also, ML models accept only tables as input, and predictions are produced as tables too. Therefore, we must first convert our input stream into a <code>Table</code> object. We will start by transforming our input data stream into a stream of table rows. We need to define a map function that would return a <code>Row</code> object with a vector containing one data point. As the k-means algorithm belongs to <a href="https://en.wikipedia.org/wiki/Unsupervised_learning">unsupervised learning</a> algorithms, i.e. the model doesn&#8217;t need corresponding "right answers" for the data points, we can skip the <code>label</code> field from the vector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="directive">private</span> <span class="directive">static</span> <span class="type">class</span> <span class="class">RecordMapper</span> <span class="directive">implements</span> MapFunction&lt;ObjectNode, Row&gt; {
    <span class="annotation">@Override</span>
    <span class="directive">public</span> Row map(ObjectNode node) {
        JsonNode payload = node.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">value</span><span class="delimiter">&quot;</span></span>).get(<span class="string"><span class="delimiter">&quot;</span><span class="content">payload</span><span class="delimiter">&quot;</span></span>);
        <span class="predefined-type">StringBuffer</span> sb = <span class="keyword">new</span> <span class="predefined-type">StringBuffer</span>();
        <span class="keyword">return</span> Row.of(Vectors.dense(
                        payload.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">sepal_length</span><span class="delimiter">&quot;</span></span>).asDouble(),
                        payload.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">sepal_width</span><span class="delimiter">&quot;</span></span>).asDouble(),
                        payload.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">petal_length</span><span class="delimiter">&quot;</span></span>).asDouble(),
                        payload.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">petal_width</span><span class="delimiter">&quot;</span></span>).asDouble()));
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>Various parts of the internal Flink pipeline can run on different worker nodes, and therefore, we also need to provide type information about the table. With that, we are ready to create the table object:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);
TypeInformation&lt;?&gt;<span class="type">[]</span> types = {DenseVectorTypeInfo.INSTANCE};
<span class="predefined-type">String</span> names<span class="type">[]</span> = {<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>};
RowTypeInfo typeInfo = <span class="keyword">new</span> RowTypeInfo(types, names);

DataStream&lt;Row&gt; inputStream = trainStream.map(<span class="keyword">new</span> RecordMapper()).returns(typeInfo);
Table trainTable = tEnv.fromDataStream(inputStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>);</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="building_flink_stream_k_means">Building Flink stream k-means</h3> <div class="paragraph"> <p>Once we have a <code>Table</code> object, we can pass it to our model. So let&#8217;s create one and pass a train stream to it for continuous model training:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">OnlineKMeans onlineKMeans = <span class="keyword">new</span> OnlineKMeans()
    .setFeaturesCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>)
    .setPredictionCol(<span class="string"><span class="delimiter">&quot;</span><span class="content">prediction</span><span class="delimiter">&quot;</span></span>)
    .setInitialModelData(tEnv.fromDataStream(env.fromElements(<span class="integer">1</span>).map(<span class="keyword">new</span> IrisInitCentroids())))
    .setK(<span class="integer">3</span>);
OnlineKMeansModel model = onlineKMeans.fit(trainTable);</code></pre> </div> </div> <div class="paragraph"> <p>To make things more straightforward, we directly set the number of desired clusters to 3 instead of finding the optimal number of clusters by digging into the data (using e.g. <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">elbow method</a>). We also set some initial values for the centers of the clusters instead of using random numbers (Flink provides a convenient method for it - <code>KMeansModelData.generateRandomModelData()</code> if you want to try with random centers).</p> </div> <div class="paragraph"> <p>To obtain the predictions for our test data, we again need to convert our test stream into a table. The model transforms the table with test data into a table with predictions. Finally, convert the prediction into a stream and persisted, e.g. in a Kafka topic:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">DataStream&lt;Row&gt; testInputStream = testStream.map(<span class="keyword">new</span> RecordMapper()).returns(typeInfo);
Table testTable = tEnv.fromDataStream(testInputStream).as(<span class="string"><span class="delimiter">&quot;</span><span class="content">features</span><span class="delimiter">&quot;</span></span>);
Table outputTable = model.transform(testTable)[<span class="integer">0</span>];

DataStream&lt;Row&gt; resultStream = tEnv.toChangelogStream(outputTable);
resultStream.map(<span class="keyword">new</span> ResultMapper()).sinkTo(kafkaSink);</code></pre> </div> </div> <div class="paragraph"> <p>Now, we are ready to build our application and almost ready to submit it to Flink for execution. Before we do, we need to create the required Kafka topics first. While the topics can be empty, Flink requires that they at least exist. As we include a small set of data in the Postgres training table when the database starts, Debezium will create a corresponding topic when registering the Debezium Postgres connector in Kafka Connect. Since the test data table does not yet exist, we need to create the topic in Kafka manually:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ docker compose -f docker-compose-flink.yaml exec kafka /kafka/bin/kafka-topics.sh --create --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1  --topic flink.public.iris_test</code></pre> </div> </div> <div class="paragraph"> <p>Now, we are ready to submit our application to Flink. For the complete code, please see the corresponding source code in Debezium <a href="https://github.com/debezium/debezium-examples/blob/blog_flink_spark_ml/machine-learning/flink-spark-iris/iris-flink/src/main/java/io/github/vjuranek/FlinkKafkaKmeans.java">example repository</a></p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you don&#8217;t use Docker compose provided as part of the source code for this demo, please include <a href="https://nightlies.apache.org/flink/flink-ml-docs-master/">Flink ML library</a> in the Flink <code>lib</code> folder, as the ML library is not part of default Flink distribution.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Flink provides a friendly UI, which is available on <a href="http://localhost:8081/" class="bare">http://localhost:8081/</a>. There, you can check, besides other things, the status of your jobs and also, e.g. job execution plan in an excellent graphical representation:</p> </div> <div class="imageblock centered-image"> <a href="/assets/images/2023-09-23-flink-spark-online-learning/flink_dag.png" target="_blank"><img src="/assets/images/2023-09-23-flink-spark-online-learning/flink_dag.png" class="responsive-image" alt="Postgres to Flink schema"></a> </div> </div> <div class="sect2"> <h3 id="evaluating_the_model">Evaluating the model</h3> <div class="paragraph"> <p>From the user&#8217;s point of view, all the interactions with our model occur by inserting new records into the database or reading Kafka topics with predictions. As we already created a very small initial training data sample in the database when it started, we can directly check our model predictions by inserting our test data sample into the database:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ psql -h localhost -U postgres -f postgres/iris_test.sql</code></pre> </div> </div> <div class="paragraph"> <p>The insert results in an immediate data stream of test data in Kafka, passing it into the model and sending the prediction back to the <code>iris_predictions</code> Kafka topic. The predictions are not accurate when training the model on a very small data set with just two clusters. The following shows our initial predictions:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>[5.4, 3.7, 1.5, 0.2] is classified as 0
[4.8, 3.4, 1.6, 0.2] is classified as 0
[7.6, 3.0, 6.6, 2.1] is classified as 2
[6.4, 2.8, 5.6, 2.2] is classified as 2
[6.0, 2.7, 5.1, 1.6] is classified as 2
[5.4, 3.0, 4.5, 1.5] is classified as 2
[6.7, 3.1, 4.7, 1.5] is classified as 2
[5.5, 2.4, 3.8, 1.1] is classified as 2
[6.1, 2.8, 4.7, 1.2] is classified as 2
[4.3, 3.0, 1.1, 0.1] is classified as 0
[5.8, 2.7, 3.9, 1.2] is classified as 2</code></pre> </div> </div> <div class="paragraph"> <p>In our case, the correct answer should be:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>[5.4, 3.7, 1.5, 0.2] is 0
[4.8, 3.4, 1.6, 0.2] is 0
[7.6, 3.0, 6.6, 2.1] is 2
[6.4, 2.8, 5.6, 2.2] is 2
[6.0, 2.7, 5.1, 1.6] is 1
[5.4, 3.0, 4.5, 1.5] is 1
[6.7, 3.1, 4.7, 1.5] is 1
[5.5, 2.4, 3.8, 1.1] is 1
[6.1, 2.8, 4.7, 1.2] is 1
[4.3, 3.0, 1.1, 0.1] is 0
[5.8, 2.7, 3.9, 1.2] is 1</code></pre> </div> </div> <div class="paragraph"> <p>When comparing the result, we only have 5 of 11 data points correctly classified due to the initial sample training data size. On the other hand, as we didn&#8217;t start with completely random clusters, our predictions are also not completely wrong.</p> </div> <div class="paragraph"> <p>Let&#8217;s see how things change when we supply more training data into the model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ psql -h localhost -U postgres -f postgres/iris_train2.sql</code></pre> </div> </div> <div class="paragraph"> <p>To see the updated predictions, we insert the same test data sample again into the database:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ psql -h localhost -U postgres -f postgres/iris_test.sql</code></pre> </div> </div> <div class="paragraph"> <p>The following predictions are much better since we have all three categories present. We have also correctly classified 7 out of the 11 data points.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>[5.4, 3.7, 1.5, 0.2] is classified as 0
[4.8, 3.4, 1.6, 0.2] is classified as 0
[7.6, 3.0, 6.6, 2.1] is classified as 2
[6.4, 2.8, 5.6, 2.2] is classified as 2
[6.0, 2.7, 5.1, 1.6] is classified as 2
[5.4, 3.0, 4.5, 1.5] is classified as 2
[6.7, 3.1, 4.7, 1.5] is classified as 2
[5.5, 2.4, 3.8, 1.1] is classified as 1
[6.1, 2.8, 4.7, 1.2] is classified as 2
[4.3, 3.0, 1.1, 0.1] is classified as 0
[5.8, 2.7, 3.9, 1.2] is classified as 1</code></pre> </div> </div> <div class="paragraph"> <p>As the whole data sample is pretty small, for further model training we can re-use our second train data sample:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ psql -h localhost -U postgres -f postgres/iris_train2.sql
$ psql -h localhost -U postgres -f postgres/iris_test.sql</code></pre> </div> </div> <div class="paragraph"> <p>This results in the following prediction.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>[5.4, 3.7, 1.5, 0.2] is classified as 0
[4.8, 3.4, 1.6, 0.2] is classified as 0
[7.6, 3.0, 6.6, 2.1] is classified as 2
[6.4, 2.8, 5.6, 2.2] is classified as 2
[6.0, 2.7, 5.1, 1.6] is classified as 2
[5.4, 3.0, 4.5, 1.5] is classified as 1
[6.7, 3.1, 4.7, 1.5] is classified as 2
[5.5, 2.4, 3.8, 1.1] is classified as 1
[6.1, 2.8, 4.7, 1.2] is classified as 1
[4.3, 3.0, 1.1, 0.1] is classified as 0
[5.8, 2.7, 3.9, 1.2] is classified as 1</code></pre> </div> </div> <div class="paragraph"> <p>We now find we have 9 out of 11 data points correctly classified. While this is still not an excellent result, we expect only partially accurate results as this is simply a prediction. The primary motivation here is to show the whole pipeline and demonstrate that the model improves the predictions without re-training and re-deploying the model when adding new data.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="classification_with_apache_spark">Classification with Apache Spark</h2> <div class="sectionbody"> <div class="paragraph"> <p>From the user&#8217;s point of view, Apache Spark is very similar to Flink, and the implementation would be quite similar. This chapter is briefer to make this blog post more digestible.</p> </div> <div class="paragraph"> <p>Spark has two streaming models: the older <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">DStreams</a>, which is now in legacy state, and the more recent and recommended <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">structured streaming</a>. However, as the streaming k-means algorithm contained in the Spark ML library works only with the DStreams, for simplicity, DStreams are used in this example. A better approach would be to use structured streaming and implement the streaming k-means ourselves. This is, however, outside this blog post&#8217;s scope and main goal.</p> </div> <div class="paragraph"> <p>Spark supports streaming from Kafka using DStreams. However, writing DStreams back to Kafka is not supported, although it is possible but isn&#8217;t straightforward.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Structured streaming supports both directions, reading and writing to Kafka, very easily.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Again, for the sake of simplicity, we skip the final part and will write the predictions only to the console instead of writing them back to Kafka. The big picture of our pipelines thus looks like this:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-09-23-flink-spark-online-learning/postgres_to_spark.png" class="responsive-image" alt="Postgres to Spark schema"> </div> <div class="sect2"> <h3 id="defining_the_data_streams">Defining the data streams</h3> <div class="paragraph"> <p>Similarly to Flink, creating Spark streams from Kafka streams is straightforward, and most of the parameters are self-explanatory:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="predefined-type">Set</span>&lt;<span class="predefined-type">String</span>&gt; trainTopic = <span class="keyword">new</span> <span class="predefined-type">HashSet</span>&lt;&gt;(<span class="predefined-type">Arrays</span>.asList(<span class="string"><span class="delimiter">&quot;</span><span class="content">spark.public.iris_train</span><span class="delimiter">&quot;</span></span>));
<span class="predefined-type">Set</span>&lt;<span class="predefined-type">String</span>&gt; testTopic = <span class="keyword">new</span> <span class="predefined-type">HashSet</span>&lt;&gt;(<span class="predefined-type">Arrays</span>.asList(<span class="string"><span class="delimiter">&quot;</span><span class="content">spark.public.iris_test</span><span class="delimiter">&quot;</span></span>));
<span class="predefined-type">Map</span>&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Object</span>&gt; kafkaParams = <span class="keyword">new</span> <span class="predefined-type">HashMap</span>&lt;&gt;();
kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string"><span class="delimiter">&quot;</span><span class="content">kafka:9092</span><span class="delimiter">&quot;</span></span>);
kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string"><span class="delimiter">&quot;</span><span class="content">dbz</span><span class="delimiter">&quot;</span></span>);
kafkaParams.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string"><span class="delimiter">&quot;</span><span class="content">earliest</span><span class="delimiter">&quot;</span></span>);
kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);

JavaInputDStream&lt;ConsumerRecord&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt;&gt; trainStream = KafkaUtils.createDirectStream(
        jssc,
        LocationStrategies.PreferConsistent(),
        ConsumerStrategies.Subscribe(trainTopic, kafkaParams));
JavaDStream&lt;LabeledPoint&gt; train = trainStream.map(ConsumerRecord::value)
        .map(SparkKafkaStreamingKmeans::toLabeledPointString)
        .map(LabeledPoint::parse);</code></pre> </div> </div> <div class="paragraph"> <p>On the last line, we transform the Kafka stream to a labeled point stream, which the Spark ML library uses for working with its ML models. Labeled points are expected as the strings formatted as data point labels separated by the comma from space-separated data point values. So the map function looks like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="directive">private</span> <span class="directive">static</span> <span class="predefined-type">String</span> toLabeledPointString(<span class="predefined-type">String</span> json) <span class="directive">throws</span> <span class="exception">ParseException</span> {
    JSONParser jsonParser = <span class="keyword">new</span> JSONParser();
    JSONObject o = (JSONObject)jsonParser.parse(json);
    <span class="keyword">return</span> <span class="predefined-type">String</span>.format(<span class="string"><span class="delimiter">&quot;</span><span class="content">%s, %s %s %s %s</span><span class="delimiter">&quot;</span></span>,
            o.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">iris_class</span><span class="delimiter">&quot;</span></span>),
            o.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">sepal_length</span><span class="delimiter">&quot;</span></span>),
            o.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">sepal_width</span><span class="delimiter">&quot;</span></span>),
            o.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">petal_length</span><span class="delimiter">&quot;</span></span>),
            o.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">petal_width</span><span class="delimiter">&quot;</span></span>));
}</code></pre> </div> </div> <div class="paragraph"> <p>It still applies that k-means is an unsupervised algorithm and doesn&#8217;t use the data point labels. However, it&#8217;s convenient to pass them to <code>LabeledPoint</code> class as later on, we can show them together with model predictions.</p> </div> <div class="paragraph"> <p>We chain one more map function to parse the string and create a labeled data point from it. In this case, it&#8217;s a built-in function of Spark <code>LabeledPoint</code>.</p> </div> <div class="paragraph"> <p>Contrary to Flink, Spark doesn&#8217;t require Kafka topics to exist in advance, so when deploying the model, we don&#8217;t have to create the topics. We can let Debezium create them once the table with the test data is created and populated with the data.</p> </div> </div> <div class="sect2"> <h3 id="defining_and_evaluating_the_model">Defining and evaluating the model</h3> <div class="paragraph"> <p>Defining the streaming k-means model is very similar to Flink:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">StreamingKMeans model = <span class="keyword">new</span> StreamingKMeans()
        .setK(<span class="integer">3</span>)
        .setInitialCenters(initCenters, weights);
model.trainOn(train.map(lp -&gt; lp.getFeatures()));</code></pre> </div> </div> <div class="paragraph"> <p>Also, in this case, we directly set the number of clusters to 3 and provide the same initial central points to the clusters. We also only pass the data points for training, not the labels.</p> </div> <div class="paragraph"> <p>As mentioned above, we can use the labels to show them together with the predictions:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">JavaPairDStream&lt;<span class="predefined-type">Double</span>, <span class="predefined-type">Vector</span>&gt; predict = test.mapToPair(lp -&gt; <span class="keyword">new</span> Tuple2&lt;&gt;(lp.label(), lp.features()));
model.predictOnValues(predict).print(<span class="integer">11</span>);</code></pre> </div> </div> <div class="paragraph"> <p>We print 11 stream elements to the console on the resulting stream with the predictions, as this is the size of our test sample. Like Flink, the results after initial training on a very small data sample could be better. The first number in the tuple is the data point label, while the second one is the corresponding prediction done by our model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>spark_1      | (0.0,0)
spark_1      | (0.0,0)
spark_1      | (2.0,2)
spark_1      | (2.0,2)
spark_1      | (1.0,0)
spark_1      | (1.0,0)
spark_1      | (1.0,2)
spark_1      | (1.0,0)
spark_1      | (1.0,0)
spark_1      | (0.0,0)
spark_1      | (1.0,0)</code></pre> </div> </div> <div class="paragraph"> <p>However, when we provide more training data, predictions are much better:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>spark_1      | (0.0,0)
spark_1      | (0.0,0)
spark_1      | (2.0,2)
spark_1      | (2.0,2)
spark_1      | (1.0,1)
spark_1      | (1.0,1)
spark_1      | (1.0,2)
spark_1      | (1.0,0)
spark_1      | (1.0,1)
spark_1      | (0.0,0)
spark_1      | (1.0,0)</code></pre> </div> </div> <div class="paragraph"> <p>If we pass the second training data sample once again for the training, our model makes correct predictions for the whole test sample:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>---
spark_1      | (0.0,0)
spark_1      | (0.0,0)
spark_1      | (2.0,2)
spark_1      | (2.0,2)
spark_1      | (1.0,1)
spark_1      | (1.0,1)
spark_1      | (1.0,1)
spark_1      | (1.0,1)
spark_1      | (1.0,1)
spark_1      | (0.0,0)
spark_1      | (1.0,1)
----</code></pre> </div> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>The prediction is a number of the cluster which k-means algorithm created and has no relation to labels in our data sample. That means that e.g. <code>(0.0,1)</code> doesn&#8217;t have to be a wrong prediction. It can happen that a data point with label 0 was assigned to the correct cluster, however, Spark internally marked it as a cluster number 1. This needs to be kept in mind when evaluating the model.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>So, similar to Flink, we get better results as we pass more training data without the need to re-train and re-deploy the model. In this case, we get even better results than Flink&#8217;s model.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusions">Conclusions</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post, we continued exploring how Debezium can help make data ingestion into various ML frameworks seamless. We have shown how to pass the data from the database to Apache Flink and Apache Spark in real time as a stream of the data. The integration is easy to set up in both cases and works well. We demonstrated it in an example that allows us to use an online learning algorithm, namely the online k-means algorithm, to highlight the power of data streaming. Online machine learning allows us to make real-time predictions on the data stream and improve or adjust the model immediately as the new training data arrives. Model adjustment doesn&#8217;t require any model re-training on a separate compute cluster and re-deploying a new model, making ML-ops more straightforward and cost-effective.</p> </div> <div class="paragraph"> <p>As usual, we would appreciate any feedback on this blog post. Do you have any ideas on how Debezium or change data capture can be helpful in this area? What would be helpful to investigate, whether integration with another ML framework, integration with a specific ML feature store, etc.? In case you have any input any this regard, don&#8217;t hesitate to reach out to us on the <a href="http://debezium.zulipchat.com/">Zulip chat</a>, <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or you can transform your ideas directly into <a href="http://issues.redhat.com/projects/DBZ/issues">Jira feature requests</a>.</p> </div> </div> </div>]]></content><author><name>Vojtěch Juránek</name></author><category term="machine-learning"/><category term="flink"/><category term="spark"/><category term="online-learning"/><category term="examples"/><category term="apache-kafka"/><summary type="html"><![CDATA[In the previous blog post, we have shown how to leverage Debezium to train neural-network model with the existing data from the database and use this pre-trained model to classify images newly stored into the database. In this blog post, we will move it one step further - we will use Debezium to create multiple data streams from the database and use one of the streams for continuous learning and to improve our model, and the second one for making predictions on the data. When the model is constantly improved or adjusted to recent data samples, this approach is known as online machine learning. Online learning is only suitable for some use cases, and implementing an online variant of a given algorithm may be challenging or even impossible. However, in situations where online learning is possible, it becomes a very powerful tool as it allows one to react to the changes in the data in real-time and avoids the need to re-train and re-deploy new models, thus saving the hardware and operational costs. As the streams of data become more and more common, e.g. with the advent of IoT, we can expect online learning to become more and more popular. It&#8217;s usually a perfect fit for analyzing streaming data in use cases where it&#8217;s possible.]]></summary></entry><entry><title type="html">Debezium 2.4.0.CR1 Released</title><link href="https://debezium.io/blog/2023/09/22/debezium-2-4-cr1-released/" rel="alternate" type="text/html" title="Debezium 2.4.0.CR1 Released"/><published>2023-09-22T00:00:00+00:00</published><updated>2023-09-22T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/09/22/debezium-2-4-cr1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/09/22/debezium-2-4-cr1-released/"><![CDATA[<div class="paragraph"> <p>As the summer concludes for us in the north and we await the autumn colors, the team has been busy preparing for the next major release of Debezium 2.4. It&#8217;s my pleasure to announce today that we are nearly there with the release of Debezium <strong>2.4.0.CR1</strong>.</p> </div> <div class="paragraph"> <p>The focus for this release is primarily on stability; however, we do have a few new last minute addititons that we should highlight, so let&#8217;s dive right in, shall we?!</p> </div> <div class="paragraph"> <p><!- more -></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>The community led Vitess connector was retrying only a subset of errors by default. This behavior has been changed and now only explicitly defined errors are not retried. For more details, please see <a href="https://issues.redhat.com">DBZ-6944</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="new_features">New Features</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="mongodb_parallel_incremental_snapshots">MongoDB parallel incremental snapshots</h3> <div class="paragraph"> <p>Since the introduction of incremental snapshots back in Debezium 1.x, the process to incremental snapshot existing data while concurrently capturing changes from a database transaction has been a single-threaded activity. It&#8217;s not uncommon when adding new features to focus on the basics and build upon that foundation, which is precisely what has happened with MongoDB.</p> </div> <div class="paragraph"> <p>In Debezium 2.4, we are taking the first steps to add parallel support to incremental snapshots with the MongoDB connector by reading multiple chunks in parallel. This should allow faster throughput at the cost of memory while the chunks are being collected, sorted, and deduplication occurs against the transaction log capture data set. Thanks to <a href="https://github.com/xidui">Yue Wang</a> for starting this effort in <a href="https://issues.redhat.com/browse/DBZ-6518">DBZ-6518</a>, it&#8217;s most definitely something we are looking to explore for the relational connectors in an upcoming Debezium release.</p> </div> </div> <div class="sect2"> <h3 id="postgresql_16_support">PostgreSQL 16 support</h3> <div class="paragraph"> <p>PostgreSQL announced the immediate release for PostgreSQL 16 just over a week ago, and we&#8217;re pleased to announce that Debezium 2.4 will support that release.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>PostgreSQL 16 introduces logical replication from standby servers; however, this feature has not yet been tested by Debezium and will be a feature introduced in a later build of Debezium. For now, logical replication remains only supported via the primary.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="google_spanner_gke_workload_identity_support">Google Spanner GKE workload identity support</h3> <div class="paragraph"> <p>Google Kubernetes Engine (GKE) supports identity workloads, allowing you to use a more secure authentication mechanism than the traditional JSON-based keys. In Debezium 2.4, when no JSON key is explicitly set, the Spanner connector will now automatically default to GKE workload identity authentication. Thanks to <a href="https://github.com/laughingman7743">laughingman7743</a> for this effort as a part of <a href="https://issues.redhat.com/browse/DBZ-6885">DBZ-6885</a>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other Fixes</h2> <div class="sectionbody"> <div class="ulist"> <ul> <li> <p>Ad-hoc blocking snaps trigger emits schema changes of all tables <a href="https://issues.redhat.com/browse/DBZ-6828">DBZ-6828</a></p> </li> <li> <p>When the start_scn corresponding to the existence of a transaction in V$TRANSACTION is 0, log mining starts from the oldest scn when the oracle connector is started for the first time <a href="https://issues.redhat.com/browse/DBZ-6869">DBZ-6869</a></p> </li> <li> <p>Ensure that the connector can handle rebalance events robustly <a href="https://issues.redhat.com/browse/DBZ-6870">DBZ-6870</a></p> </li> <li> <p>OpenLogReplicator confirmation can resend or omit events on restarts <a href="https://issues.redhat.com/browse/DBZ-6895">DBZ-6895</a></p> </li> <li> <p>ExtractNewRecordState&#8217;s schema cache is not updated with arrival of the ddl change event <a href="https://issues.redhat.com/browse/DBZ-6901">DBZ-6901</a></p> </li> <li> <p>Misleading Debezium error message when RDI port is not specified in application.properties <a href="https://issues.redhat.com/browse/DBZ-6902">DBZ-6902</a></p> </li> <li> <p>Generting protobuf files to target/generated-sources breaks build <a href="https://issues.redhat.com/browse/DBZ-6903">DBZ-6903</a></p> </li> <li> <p>Clean log printout in Redis Debezium Sink <a href="https://issues.redhat.com/browse/DBZ-6908">DBZ-6908</a></p> </li> <li> <p>Values being omitted from list of JSON object <a href="https://issues.redhat.com/browse/DBZ-6910">DBZ-6910</a></p> </li> <li> <p>fix logger named <a href="https://issues.redhat.com/browse/DBZ-6935">DBZ-6935</a></p> </li> <li> <p>MySql connector get NPE when snapshot.mode is set to never and signal data collection configured <a href="https://issues.redhat.com/browse/DBZ-6937">DBZ-6937</a></p> </li> <li> <p>Sanity check / retry for redo logs does not work per Oracle RAC thread <a href="https://issues.redhat.com/browse/DBZ-6938">DBZ-6938</a></p> </li> <li> <p>Drop events has wrong table changes information <a href="https://issues.redhat.com/browse/DBZ-6945">DBZ-6945</a></p> </li> <li> <p>Remove spaces from Signal and Notification MBean&#8217;s ObjectName <a href="https://issues.redhat.com/browse/DBZ-6957">DBZ-6957</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.CR1%20ORDER%20BY%20component%20ASC">20 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/BigGillyStyle">Andy Pickler</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/rk3rn3r">René Kerner</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/wuzhenhua01">Wu Zhenhua</a>, Zheng Wang, <a href="https://github.com/laughingman7743">laughingman7743</a>, and <a href="https://github.com/tisonkun">tison</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_and_whats_next">Outlook and What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We&#8217;re now at the junction where we begin to set our vision on Debezium 2.5 and what lies ahead. We recently held our first Community Meeting and discussed a number of our 2.5 roadmap ideas, some of which include:</p> </div> <div class="ulist"> <ul> <li> <p>Parallel incremental snapshots for relational connectors.</p> </li> <li> <p>Improved MongoDB support for BSONDocument exceeding 16MB.</p> </li> <li> <p>Db2 support on z/OS and iSeries platforms.</p> </li> <li> <p>Batch support in the JDBC sink connector.</p> </li> <li> <p>Parallelization of tasks and other Debezium Engine internals.</p> </li> <li> <p>Preview of MariaDB and Oracle 23 support</p> </li> </ul> </div> <div class="paragraph"> <p>For more details, please check out our <a href="https://debezium.io/roadmap">road map</a> for all upcoming details around Debezium 2.5 and beyond.</p> </div> <div class="paragraph"> <p>Additionally, Debezium will be at <a href="https://www.confluent.io/events/current/">Current 2023</a> next week. If you are attending, be sure to stop by the Ask-The-Experts session on Wednesday at 2:30PM to catch a sesssion on Debezium and Kafka. Additionally, be sure to check out the sponsored session on Wednesday at 4:30PM to find out just how easy it is to deploy data pipelines from the edge to the cloud using open-source projects such as Debezium, Strimzi, Apicurio, and Kubernetes.</p> </div> <div class="paragraph"> <p>As always, if you have any questions, suggestions, or feedback, please reach out to us on our <a href="https://groups.google.com/g/debezium">mailing list</a> or <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>. We always enjoy hearing what you have to share. Until next time, be safe.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="caassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[As the summer concludes for us in the north and we await the autumn colors, the team has been busy preparing for the next major release of Debezium 2.4. It&#8217;s my pleasure to announce today that we are nearly there with the release of Debezium 2.4.0.CR1. The focus for this release is primarily on stability; however, we do have a few new last minute addititons that we should highlight, so let&#8217;s dive right in, shall we?! Breaking changes The community led Vitess connector was retrying only a subset of errors by default. This behavior has been changed and now only explicitly defined errors are not retried. For more details, please see DBZ-6944. New Features MongoDB parallel incremental snapshots Since the introduction of incremental snapshots back in Debezium 1.x, the process to incremental snapshot existing data while concurrently capturing changes from a database transaction has been a single-threaded activity. It&#8217;s not uncommon when adding new features to focus on the basics and build upon that foundation, which is precisely what has happened with MongoDB. In Debezium 2.4, we are taking the first steps to add parallel support to incremental snapshots with the MongoDB connector by reading multiple chunks in parallel. This should allow faster throughput at the cost of memory while the chunks are being collected, sorted, and deduplication occurs against the transaction log capture data set. Thanks to Yue Wang for starting this effort in DBZ-6518, it&#8217;s most definitely something we are looking to explore for the relational connectors in an upcoming Debezium release. PostgreSQL 16 support PostgreSQL announced the immediate release for PostgreSQL 16 just over a week ago, and we&#8217;re pleased to announce that Debezium 2.4 will support that release. PostgreSQL 16 introduces logical replication from standby servers; however, this feature has not yet been tested by Debezium and will be a feature introduced in a later build of Debezium. For now, logical replication remains only supported via the primary. Google Spanner GKE workload identity support Google Kubernetes Engine (GKE) supports identity workloads, allowing you to use a more secure authentication mechanism than the traditional JSON-based keys. In Debezium 2.4, when no JSON key is explicitly set, the Spanner connector will now automatically default to GKE workload identity authentication. Thanks to laughingman7743 for this effort as a part of DBZ-6885. Other Fixes Ad-hoc blocking snaps trigger emits schema changes of all tables DBZ-6828 When the start_scn corresponding to the existence of a transaction in V$TRANSACTION is 0, log mining starts from the oldest scn when the oracle connector is started for the first time DBZ-6869 Ensure that the connector can handle rebalance events robustly DBZ-6870 OpenLogReplicator confirmation can resend or omit events on restarts DBZ-6895 ExtractNewRecordState&#8217;s schema cache is not updated with arrival of the ddl change event DBZ-6901 Misleading Debezium error message when RDI port is not specified in application.properties DBZ-6902 Generting protobuf files to target/generated-sources breaks build DBZ-6903 Clean log printout in Redis Debezium Sink DBZ-6908 Values being omitted from list of JSON object DBZ-6910 fix logger named DBZ-6935 MySql connector get NPE when snapshot.mode is set to never and signal data collection configured DBZ-6937 Sanity check / retry for redo logs does not work per Oracle RAC thread DBZ-6938 Drop events has wrong table changes information DBZ-6945 Remove spaces from Signal and Notification MBean&#8217;s ObjectName DBZ-6957 Altogether, 20 issues were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Andy Pickler, Anisha Mohanty, Breno Moreira, Chris Cranford, Harvey Yue, Indra Shukla, Jakub Cechacek, Jiri Pechanec, Mario Fiore Vitale, Nancy Xu, Nir Levy, Ondrej Babec, René Kerner, Sergey Eizner, Thomas Thornton, Wu Zhenhua, Zheng Wang, laughingman7743, and tison! Outlook and What&#8217;s next? We&#8217;re now at the junction where we begin to set our vision on Debezium 2.5 and what lies ahead. We recently held our first Community Meeting and discussed a number of our 2.5 roadmap ideas, some of which include: Parallel incremental snapshots for relational connectors. Improved MongoDB support for BSONDocument exceeding 16MB. Db2 support on z/OS and iSeries platforms. Batch support in the JDBC sink connector. Parallelization of tasks and other Debezium Engine internals. Preview of MariaDB and Oracle 23 support For more details, please check out our road map for all upcoming details around Debezium 2.5 and beyond. Additionally, Debezium will be at Current 2023 next week. If you are attending, be sure to stop by the Ask-The-Experts session on Wednesday at 2:30PM to catch a sesssion on Debezium and Kafka. Additionally, be sure to check out the sponsored session on Wednesday at 4:30PM to find out just how easy it is to deploy data pipelines from the edge to the cloud using open-source projects such as Debezium, Strimzi, Apicurio, and Kubernetes. As always, if you have any questions, suggestions, or feedback, please reach out to us on our mailing list or chat. We always enjoy hearing what you have to share. Until next time, be safe.]]></summary></entry><entry><title type="html">Debezium 2.4.0.Beta2 Released</title><link href="https://debezium.io/blog/2023/09/13/debezium-2-4-beta2-released/" rel="alternate" type="text/html" title="Debezium 2.4.0.Beta2 Released"/><published>2023-09-13T00:00:00+00:00</published><updated>2023-09-13T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/09/13/debezium-2-4-beta2-released</id><content type="html" xml:base="https://debezium.io/blog/2023/09/13/debezium-2-4-beta2-released/"><![CDATA[<div class="paragraph"> <p>It has been nearly two weeks since our last preview release of the Debezium 2.4 series, and I am thrilled to announcement the next installation of that series, Debezium <strong>2.4.0.Beta2</strong>.</p> </div> <div class="paragraph"> <p>While typically beta releases focus on stability and bugs, this release includes quite a number of noteworthy improves and new features including a new ingestion method for Oracle using OpenLogReplicator, a new single message transform to handle timezone conversions, custom authentication support for MongoDB, configurable order for the MongoDB aggregation pipeline, and lastly support for MongoDB 7.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and dive into all these new features, improvements, and changes in more detail.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="oracle_ingestion_using_openlogreplicator">Oracle ingestion using OpenLogReplicator</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium for Oracle connector has traditionally shipped with two adapters, one for Oracle XStream and another to integrate directly with Oracle LogMiner. While each adapter has its own benefits and is quite mature with features and support for a wide array of data types and use cases, we wanted to explore a completely different way of capturing changes.</p> </div> <div class="paragraph"> <p>Debezium 2.4.0.Beta2 introduces a new, experimental Oracle ingestion adapter based on <a href="https://github.com/bersler/OpenLogReplicator">OpenLogReplicator</a>. The adapter integrates directly with the OpenLogReplicator process in order to create change events in a similar way that the XStream implementation acts as a client to Oracle GoldenGate.</p> </div> <div class="paragraph"> <p>OpenLogReplicator is a standalone process that must either run on the Oracle database server or can run independently of the database server but requires direct communication with the database via TCP/IP and have direct read access to the Oracle redo and archive log files. OpenLogReplicator also does not ship with any pre-built binaries, so the code must either be built directly from source or deployed in a <a href="https://github.com/bersler/OpenLogReplicator-docker">container image</a> that can access the database and its files remotely via file shares.</p> </div> <div class="paragraph"> <p>Once OpenLogReplicator is installed, set up requires the following steps:</p> </div> <div class="ulist"> <ul> <li> <p>Configure the OpenLogReplicator&#8217;s configuration, <code>OpenLogReplicator.json</code>.</p> </li> <li> <p>Configure the Oracle connector to use the OpenLogReplicator adapter.</p> </li> </ul> </div> <div class="paragraph"> <p>At this time, the Debezium for Oracle connector expects the OpenLogReplicator configuration to use very specific settings so that the data is transferred to the connector using the right serialization. The <a href="https://debezium.io/documentation/reference/2.4/connectors/oracle.html#oracle-openlogreplicator-configuration">example configuration</a> shows the critical configuration parameters that must be set for Debezium to ingest the data properly.</p> </div> <div class="paragraph"> <p>When OpenLogReplicator is configured, you should see OpenLogReplicator start with the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>OpenLogReplicator v1.2.1 (C) 2018-2023 by Adam Leszczynski (aleszczynski@bersler.com), see LICENSE file for licensing information, arch: x86_64, system: Linux, release: 6.4.11-200.fc38.x86_64, build: Debug, modules: OCI Probobuf
adding source: ORACLE <i class="conum" data-value="1"></i><b>(1)</b>
adding target: DBZ-NETWORK <i class="conum" data-value="2"></i><b>(2)</b>
writer is starting with Network:0.0.0.0:9000 <i class="conum" data-value="3"></i><b>(3)</b></code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The source alias configured in <code>OpenLogReplicator.json</code></td> </tr> <tr> <td><i class="conum" data-value="2"></i><b>2</b></td> <td>The target alias configured in <code>OpenLogReplicator.json</code></td> </tr> <tr> <td><i class="conum" data-value="3"></i><b>3</b></td> <td>The host and port the OpenLogReplicator is listening on.</td> </tr> </table> </div> <div class="paragraph"> <p>Lastly to configure the connector, set the following connector configuration options:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">database.connection.adapter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">olr</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">openlogreplicator.source</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;source-alias&gt;</span><span class="delimiter">&quot;</span></span>, <i class="conum" data-value="1"></i><b>(1)</b>
  <span class="key"><span class="delimiter">&quot;</span><span class="content">openlogreplicator.host</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;host&gt;</span><span class="delimiter">&quot;</span></span>, <i class="conum" data-value="2"></i><b>(2)</b>
  <span class="key"><span class="delimiter">&quot;</span><span class="content">openlogreplicator.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;port&gt;</span><span class="delimiter">&quot;</span></span> <i class="conum" data-value="3"></i><b>(3)</b></code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The source alias defined in the <code>OpenLogReplicator.json</code> configuration that is to be used.</td> </tr> <tr> <td><i class="conum" data-value="2"></i><b>2</b></td> <td>The host that is running the OpenLogReplicator.</td> </tr> <tr> <td><i class="conum" data-value="3"></i><b>3</b></td> <td>The port the OpenLogReplicator is listening on.</td> </tr> </table> </div> <div class="paragraph"> <p>When the connector starts and begins to stream, it will connect to the OpenLogReplicator process' network endpoint, negotiate the connection with the serialization process, and then will begin to receive redo log entries.</p> </div> <div class="paragraph"> <p>We will have another blog post that goes over OpenLogReplicator in more detail in the coming weeks leading up to the final release, but in the meantime feel free to experiment with the new ingestion method as we would love to hear your feedback.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>As this ingestion method is experimental, there are a few known limitations, please review the connector <a href="https://debezium.io/documentation/reference/2.4/connectors/oracle.html#oracle-openlogreplicator-known-issues">documentation</a> for details.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="new_timezone_transformation">New Timezone Transformation</h2> <div class="sectionbody"> <div class="paragraph"> <p>A common request we have often heard from the community has been to emit temporal columns using other time zones besides UTC. Debezium has supported this by using a <code>CustomConverter</code> to change the way temporal columns are emitted by default to writing your own single message transformation; however, these approaches may not be for everyone.</p> </div> <div class="paragraph"> <p>Debezium 2.4 now ships with a brand-new time zone transformation that enables you to control, to a granular level, which temporal columns in an emitted event will be converted from UTC into whatever desired time zone your pipeline requires. To get started with this new transformation, add the following basic configuration to your connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">tz</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.TimezoneConverter</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.converted.timezone</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">America/New_York</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>By specifying the above configuration, all temporal columns that are emitted in UTC will be converted from UTC to the America/New_York time zone. But you are not limited to just changing the timezone for all temporal fields, you can also target specific fields using the <code>include.fields</code> property as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">tz</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.TimezoneConverter</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.converted.timezone</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">America/New_York</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.include.fields</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">source:customers:created_at,customers:updated_at</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>In the above example, the first entry will convert the <code>created_at</code> field where the <em>source table name</em> is <code>customers</code> whereas the latter will convert the <code>updated_at</code> field where the <em>topic name</em> is <code>customers</code>. Additionally, you can also exclude fields from the conversion using <code>exclude.fields</code> to apply the conversion to all but a subset:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">tz</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.TimezoneConverter</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.converted.timezone</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">America/New_York</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.tz.exclude.fields</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">source:customers:updated_at</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>In the above example, all temporal fields will be converted to the America/New_York time zone except where the <em>source table name</em> is <code>customers</code> and the field is <code>updated_at</code>.</p> </div> <div class="paragraph"> <p>You can find more information about this new transformation in the <a href="https://debezium.io/documentation/reference/2.4/transformations/timezone-converter.html">documentation</a> and we would love to hear your feedback.</p> </div> </div> </div> <div class="sect1"> <h2 id="mongodb_changes">MongoDB changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.4.0.Beta2 also ships with several MongoDB connector changes, lets take a look at those separately.</p> </div> <div class="sect2"> <h3 id="breaking_changes">Breaking changes</h3> <div class="paragraph"> <p>The <code>mongodb.hosts</code> and <code>mongodb.members.autodiscover</code> configuration properties were removed and no have any influence on the MongoDB connector behavior. If you previously relied on these configuration properties, you must now use the MongoDB <a href="https://debezium.io/documentation/reference/2.4/connectors/mongodb.html#mongodb-property-mongodb-connection-string">connection string</a> configuration property moving forward (<a href="https://issues.redhat.com/browse/DBZ-6892">DBZ-6892</a>).</p> </div> </div> <div class="sect2"> <h3 id="custom_authentication">Custom Authentication</h3> <div class="paragraph"> <p>In specific environments such as AWS, you need to use AWS IAM role-based authentication to connect to the MongoDB cluster; however, this requires setting the property u sing <code>AWS_CREDENTIAL_PROVIDER</code>. This provider is responsible for creating a session and providing the credentials.</p> </div> <div class="paragraph"> <p>To integrate more seamlessly in such environments, a new configuration property, <code>mongodb.authentication.class</code> has been added that allows you to define the credential provider class directly in the connector configuration. If you need to use such a provider configuration, you can now add the following to the connector configuration:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">mongodb.authentication.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;fully-qualified-class-name-to-use&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">mongodb.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">username</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">mongodb.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">password</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>In addition, if the authentication needs to use another database besides <code>admin</code>, the connector configuration can also include the <code>mongodb.authsource</code> property to control what authentication database should be used.</p> </div> <div class="paragraph"> <p>For more information, please see the <a href="https://debezium.io/documentation/reference/2.4/connectors/mongodb.html#mongodb-property-mongodb-authentication-class">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="configurable_order_of_aggregation_pipeline">Configurable order of aggregation pipeline</h3> <div class="paragraph"> <p>Debezium 2.4 now provides a way to control the aggregation order of the change streams pipeline. This can be critical when specific documents are being aggregated that could lead to pipeline problems such as large documents.</p> </div> <div class="paragraph"> <p>By default, the connector applies the MongoDB internal pipeline filters and then any user-constructed filters; however this could lead to situations where large documents make it into the pipeline and MongoDB could throw an error if the document exceeds the internal 16Mb limit. In such use cases, the connector can now be configured to apply the user stages to the pipeline first defined by <code>cursor.pipeline</code> to filter out such use cases to avoid the pipeline from failing due to the 16Mb limit.</p> </div> <div class="paragraph"> <p>To accomplish this, simply apply the following configuration to the connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">cursor.pipeline.order</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">user_first</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">cursor.pipeline</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;custom-pipeline-filters&gt;</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>For more details, please see the <a href="https://debezium.io/documentation/reference/2.4/connectors/mongodb.html#mongodb-property-cursor-pipeline">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="mongodb_7_support">MongoDB 7 support</h3> <div class="paragraph"> <p>MongoDB 7.0 was released just last month and Debezium 2.4 ships with MongoDB 7 support.</p> </div> <div class="paragraph"> <p>If you are looking to upgrade to MongoDB 7 for your environment, you can easily do so as Debezium 2.4+ is fully compatible with the newer version. If you encounter any problems, please let us know.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are several bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Documentation content section in the debezium.io scroll over to the top header. <a href="https://issues.redhat.com/browse/DBZ-5942">DBZ-5942</a></p> </li> <li> <p>Only publish deltas instead of full snapshots to reduce size of sync event messages <a href="https://issues.redhat.com/browse/DBZ-6458">DBZ-6458</a></p> </li> <li> <p>Postgres - Incremental snapshot fails on tables with an enum type in the primary key <a href="https://issues.redhat.com/browse/DBZ-6481">DBZ-6481</a></p> </li> <li> <p>schema.history.internal.store.only.captured.databases.ddl flag not considered while snapshot schema to history topic <a href="https://issues.redhat.com/browse/DBZ-6712">DBZ-6712</a></p> </li> <li> <p>ExtractNewDocumentState for MongoDB ignore previous document state when handling delete event&#8217;s with REWRITE <a href="https://issues.redhat.com/browse/DBZ-6725">DBZ-6725</a></p> </li> <li> <p>MongoDB New Document State Extraction: original name overriding does not work <a href="https://issues.redhat.com/browse/DBZ-6773">DBZ-6773</a></p> </li> <li> <p>Error with propagation source column name <a href="https://issues.redhat.com/browse/DBZ-6831">DBZ-6831</a></p> </li> <li> <p>Support truncating large columns <a href="https://issues.redhat.com/browse/DBZ-6844">DBZ-6844</a></p> </li> <li> <p>Always reset VStream grpc channel when max size is exceeded <a href="https://issues.redhat.com/browse/DBZ-6852">DBZ-6852</a></p> </li> <li> <p>Kafka offset store fails with NPE <a href="https://issues.redhat.com/browse/DBZ-6853">DBZ-6853</a></p> </li> <li> <p>JDBC Offset storage - configuration of table name does not work <a href="https://issues.redhat.com/browse/DBZ-6855">DBZ-6855</a></p> </li> <li> <p>JDBC sink insert fails with Oracle target database due to semicolon <a href="https://issues.redhat.com/browse/DBZ-6857">DBZ-6857</a></p> </li> <li> <p>Oracle test shouldContinueToUpdateOffsetsEvenWhenTableIsNotChanged fails with NPE <a href="https://issues.redhat.com/browse/DBZ-6860">DBZ-6860</a></p> </li> <li> <p>Tombstone events causes NPE on JDBC connector <a href="https://issues.redhat.com/browse/DBZ-6862">DBZ-6862</a></p> </li> <li> <p>Debezium-MySQL not filtering AWS RDS internal events <a href="https://issues.redhat.com/browse/DBZ-6864">DBZ-6864</a></p> </li> <li> <p>Avoid getting NPE when executing the arrived method in ExecuteSnapshot <a href="https://issues.redhat.com/browse/DBZ-6865">DBZ-6865</a></p> </li> <li> <p>errors.max.retries = 0 Causes retrievable error to be ignored <a href="https://issues.redhat.com/browse/DBZ-6866">DBZ-6866</a></p> </li> <li> <p>Streaming aggregation pipeline broken for combination of database filter and signal collection <a href="https://issues.redhat.com/browse/DBZ-6867">DBZ-6867</a></p> </li> <li> <p>ChangeStream aggregation pipeline fails on large documents which should be excluded <a href="https://issues.redhat.com/browse/DBZ-6871">DBZ-6871</a></p> </li> <li> <p>Oracle alter table drop constraint fails when cascading index <a href="https://issues.redhat.com/browse/DBZ-6876">DBZ-6876</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.Beta2%20ORDER%20BY%20component%20ASC">36 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/BigGillyStyle">Andy Pickler</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, and <a href="https://github.com/tisonkun">tison</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.4 is shaping up quite nicely with our second Beta2 preview release which now includes OpenLogReplicator support. We intend to spend the remaining several weeks as we move toward a 2.4 final working on stability and any regressions that are identified. We encourage you to give Debezium 2.4.0.Beta2 a try. I would anticipate a Beta3 likely next week to address any shortcomings with OpenLogReplicator with the hope of a final by end of the month.</p> </div> <div class="paragraph"> <p>Don&#8217;t forget about the Debezium Community Event, which I shared with you on the <a href="https://groups.google.com/g/debezium">mailing list</a>. The event will be held on Thursday, September 21st at 8:00am EDT (12:00pm UTC) where we&#8217;ll discuss Debezium 2.4 and the future. Details are available on the <a href="https://debezium.zulipchat.com/#narrow/stream/302529-community-general/topic/Community.20Event/near/390297046">Zulip chat thread</a>, so be sure to join if you are able, we&#8217;d love to see you there.</p> </div> <div class="paragraph"> <p>Additionally, if you intend to participate at Current 2023 (formerly Kafka Summit) in San Jose, California, I will be there doing on a presentation on Debezium and data pipelines Wednesday afternoon with my good friend Carles Arnal. There will also be another presentation by my colleague Hans-Peter Grahsl on event-driven design you shouldn&#8217;t miss. If you&#8217;d like to meet up and have a quick chat about Debezium, your experiences, or even just to say "Hi", I&#8217;d love to chat. Please feel free to ping me on Zulip (@Chris Cranford) or send me a notification on Twitter (@crancran77).</p> </div> <div class="paragraph"> <p>As always, if you have any ideas or suggestions, you can also get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It has been nearly two weeks since our last preview release of the Debezium 2.4 series, and I am thrilled to announcement the next installation of that series, Debezium 2.4.0.Beta2. While typically beta releases focus on stability and bugs, this release includes quite a number of noteworthy improves and new features including a new ingestion method for Oracle using OpenLogReplicator, a new single message transform to handle timezone conversions, custom authentication support for MongoDB, configurable order for the MongoDB aggregation pipeline, and lastly support for MongoDB 7. Let&#8217;s take a few moments and dive into all these new features, improvements, and changes in more detail.]]></summary></entry><entry><title type="html">Enable pass-through of additional properties in Debezium UI</title><link href="https://debezium.io/blog/2023/09/10/Debezium-UI-connectors-custom-property/" rel="alternate" type="text/html" title="Enable pass-through of additional properties in Debezium UI"/><published>2023-09-10T00:00:00+00:00</published><updated>2023-09-10T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/09/10/Debezium-UI-connectors-custom-property</id><content type="html" xml:base="https://debezium.io/blog/2023/09/10/Debezium-UI-connectors-custom-property/"><![CDATA[<div class="paragraph"> <p>The Debezium UI team continues to add support for more features, allowing users to configure connectors more easily. In this article, we&#8217;ll describe and demonstrate how to provide the additional properties for configuration that the UI does not expose by default. Read further for more information!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="need_for_custom_properties_step">Need for custom properties step</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium is very flexible - each connector can be configured and fine-tuned in various ways. Debezium UI already exposes an extensive list of properties that are segregated under different steps for configuration. However, despite the UI exposing a wide range of properties, there are situations where additional custom properties may be necessary beyond those exposed out of the box. For example, <code>producer.<strong></code>, <code>consumer.</strong></code>, and even <code>driver.*</code> properties are pass-through configuration options for configuring parts of the Kafka Connect pipeline or the JDBC driver, respectively.</p> </div> <div class="paragraph"> <p><strong>Example</strong></p> </div> <div class="paragraph"> <p>Debezium must configure the Kafka producer settings to write schema change events to the schema history topic for connectors that utilize a schema history topic, like MySQL. The connector configuration includes a subset of options that act as pass-through properties that begin with the <code>schema.history.internal.producer.*</code> prefix.</p> </div> <div class="paragraph"> <p>Debezium strips the prefix from the property names before passing the properties to the Kafka client.</p> </div> </div> </div> <div class="sect1"> <h2 id="custom_properties_step">Custom properties step</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium UI allows you to configure additional properties for connectors not exposed in the UI. For this, we have created a new extra step called <strong>Custom Properties</strong> in the configuration wizard.</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-09-10-enable-passthru-ui/customStep2.png" class="responsive-image"> </div> </div> <div class="paragraph"> <p>The <strong>custom properties</strong> step allows you to provide the additional properties by entering a key-value pair. Once you enter the additional configuration properties <code>key</code> and <code>value</code>, click the <strong>Apply</strong> button.</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-09-10-enable-passthru-ui/configureEmptylist.png" class="responsive-image"> </div> </div> <div class="paragraph"> <p>When clicking the <strong>Apply</strong> button, the UI validates the user entries and provides feedback if there are any problems. If there are no problems, the UI will show a green check icon on the right side of all the successfully added additional properties. You can also remove any added property by clicking on the <strong>Remove</strong> button on the right side of the property.</p> </div> <div class="paragraph"> <p>On the left-hand side, you can see the list of already configured properties from the previous wizard step, and the newly added custom properties in this step are highlighted in <strong>blue</strong> color.</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-09-10-enable-passthru-ui/CustomProp.png" class="responsive-image"> </div> </div> <div class="paragraph"> <p>In the screenshot above, the user added <code>schema.history.internal.producer.*</code> related additional properties.</p> </div> </div> </div> <div class="sect1"> <h2 id="self_contained_example">Self-contained example</h2> <div class="sectionbody"> <div class="paragraph"> <p>You can try out configuring the connector with some custom properties (and more) with our self-contained example <a href="https://github.com/debezium/debezium-examples/tree/main/ui-demo">UI demo</a> - which is included under <a href="https://github.com/debezium/debezium-examples">debezium-examples</a> on GitHub. The UI demo includes a Docker Compose file, which brings up several sources with data and the UI. Please refer to the <a href="https://github.com/debezium/debezium-examples/tree/main/ui-demo">README file</a> for more details on running the Debezium UI demo.</p> </div> <div class="paragraph"> <p>To learn more about the Debezium UI, please refer to the <a href="/documentation/reference/operations/debezium-ui.html">reference documentation</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="more_coming_soon">More coming soon!</h2> <div class="sectionbody"> <div class="paragraph"> <p>Stay tuned for further improvements and new feature announcements in the UI in the coming releases.</p> </div> <div class="paragraph"> <p><em>A big thank you to the team who have contributed in many ways: Anisha Mohanty, René Kerner and Chrish Cranford!</em></p> </div> </div> </div>]]></content><author><name>Indra Raj Shukla</name></author><category term="community"/><category term="debezium-ui"/><summary type="html"><![CDATA[The Debezium UI team continues to add support for more features, allowing users to configure connectors more easily. In this article, we&#8217;ll describe and demonstrate how to provide the additional properties for configuration that the UI does not expose by default. Read further for more information!]]></summary></entry><entry><title type="html">Debezium 2.3.3.Final Released</title><link href="https://debezium.io/blog/2023/09/05/debezium-2-3-3-final-released/" rel="alternate" type="text/html" title="Debezium 2.3.3.Final Released"/><published>2023-09-05T00:00:00+00:00</published><updated>2023-09-05T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/09/05/debezium-2-3-3-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/09/05/debezium-2-3-3-final-released/"><![CDATA[<div class="paragraph"> <p>It is my pleasure to announce the immediate release of Debezium <strong>2.3.3.Final</strong>.</p> </div> <div class="paragraph"> <p>This release includes several bug fixes to address regressions, stability, documentation updates. If you are currently looking to upgrade to the Debezium 2.3.x release stream, we highly recommend you consider using this release. Let&#8217;s take a quick look into the regressions and bug fixes.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There is currently one breaking change in this release, where the behavior of Debezium 2.3.3.Final changes from the previous release of Debezium 2.3.2.Final. If you are upgrading from Debezium 2.3.2.Final or any prior version, please review the following breaking changes for details.</p> </div> <div class="sect2"> <h3 id="oracle_connector_scn_based_metrics">Oracle connector SCN-based metrics</h3> <div class="paragraph"> <p>Oracle tracks a variety of system change number, often called SCN, values in its JMX metrics including <code>OffsetScn</code>, <code>CurrentScn</code>, <code>OldestScn</code>, and <code>CommittedScn</code>. These SCN values are numeric and can often exceed the upper bounds of a <code>Long</code> data type, and so Debezium has traditionally exposed these values as <code>String</code> s.</p> </div> <div class="paragraph"> <p>Unfortunately, tooling such as Grafana and Prometheus do not work with <code>String</code> -based values, and it has been raised on several occasions that the community would like to be able to view these values from metrics gathering frameworks. With Debezium 2.3.3.Final or later, there is a small behavior change with the these JMX metrics as they are no longer exposed as <code>String</code> values but instead are now exposed as <code>BigInteger</code> values.</p> </div> <div class="paragraph"> <p>This change in behavior allows tooling such as Grafana and Prometheus to now scrape these values from the JMX beans automatically for reporting and observability stacks.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you were previously gathering these values for other purposes, be aware they&#8217;re no longer string-based and should be interpreted as <code>BigInteger</code> numerical values moving forward.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are several bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Support PostgreSQL coercion for UUID, JSON, and JSONB data types <a href="https://issues.redhat.com/browse/DBZ-6589">DBZ-6589</a></p> </li> <li> <p>Debezium 2.3.0.Final Missing Kafka Channel Documentation <a href="https://issues.redhat.com/browse/DBZ-6688">DBZ-6688</a></p> </li> <li> <p>Debezium crashes on parsing MySQL DDL statement (specific JOIN) <a href="https://issues.redhat.com/browse/DBZ-6724">DBZ-6724</a></p> </li> <li> <p>ExtractNewDocumentState for MongoDB ignore previous document state when handling delete event&#8217;s with REWRITE <a href="https://issues.redhat.com/browse/DBZ-6725">DBZ-6725</a></p> </li> <li> <p>Missing or misspelled IDs result in downstream build errors <a href="https://issues.redhat.com/browse/DBZ-6754">DBZ-6754</a></p> </li> <li> <p>When using pgoutput in postgres connector, (+/-)Infinity is not supported in decimal values <a href="https://issues.redhat.com/browse/DBZ-6758">DBZ-6758</a></p> </li> <li> <p>Outbox transformation can cause connector to crash <a href="https://issues.redhat.com/browse/DBZ-6760">DBZ-6760</a></p> </li> <li> <p>Postgres tests for toasted byte array and toasted date array fail with decoderbufs plugin <a href="https://issues.redhat.com/browse/DBZ-6767">DBZ-6767</a></p> </li> <li> <p>MongoDB New Document State Extraction: nonexistent field for add.headers <a href="https://issues.redhat.com/browse/DBZ-6774">DBZ-6774</a></p> </li> <li> <p>Notifications and signals leaks between MBean instances when using JMX channels <a href="https://issues.redhat.com/browse/DBZ-6777">DBZ-6777</a></p> </li> <li> <p>Dbz crashes on parsing MySQL DDL statement (SELECT 1.;) <a href="https://issues.redhat.com/browse/DBZ-6780">DBZ-6780</a></p> </li> <li> <p>Dbz crashed on parsing MySQL DDL statement (SELECT 1 + @sum:=1 AS ss;) <a href="https://issues.redhat.com/browse/DBZ-6794">DBZ-6794</a></p> </li> <li> <p>MySQL DDL parser - REPEAT function not accepted <a href="https://issues.redhat.com/browse/DBZ-6803">DBZ-6803</a></p> </li> <li> <p>Dbz crashes on DDL statement (non Latin chars in variables) <a href="https://issues.redhat.com/browse/DBZ-6821">DBZ-6821</a></p> </li> <li> <p>Not trim the default value for the BIGINT and SMALLINT types when parsing MySQL DDL <a href="https://issues.redhat.com/browse/DBZ-6824">DBZ-6824</a></p> </li> <li> <p>Make partial and multi-response transactions debug level logs <a href="https://issues.redhat.com/browse/DBZ-6830">DBZ-6830</a></p> </li> <li> <p>Oracle test shouldContinueToUpdateOffsetsEvenWhenTableIsNotChanged fails with NPE <a href="https://issues.redhat.com/browse/DBZ-6860">DBZ-6860</a></p> </li> <li> <p>Streaming aggregation pipeline broken for combination of database filter and signal collection <a href="https://issues.redhat.com/browse/DBZ-6867">DBZ-6867</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.3.Final%20ORDER%20BY%20component%20ASC">19 issues</a> were fixed for this release. <a href="https://github.com/andreas-ibm">Andreas Martens</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/chrisbeard">Chris Beard</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/vidourem">David Remy</a>, <a href="https://github.com/Fraznist">Emre Akgün</a>, <a href="https://github.com/ericpangiawan">Eric Pangiawan</a>, <a href="https://github.com/ruanhang1993">Hang Ruan</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/vidourem">david remy</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As the team continues active development of Debezium 2.4, I would expect another maintenance release of Debezium 2.3 later this mont, barring any reported regressions or bugs.</p> </div> <div class="paragraph"> <p>In addition, the Debezium Community Event&#8217;s agenda and date will be published this week, so keep an eye out for that news. And finally, we&#8217;ll be presenting at Kafka Summit 2023 (aka Current 2023) later this upcoming month. If you&#8217;re planning to attend and would like to ask the experts, be sure to get in touch with me or anyone on the team and we can plan to meet up and discuss anything related to Debezium and CDC.</p> </div> <div class="paragraph"> <p>As always, if you have any ideas or suggestions, you can also get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It is my pleasure to announce the immediate release of Debezium 2.3.3.Final. This release includes several bug fixes to address regressions, stability, documentation updates. If you are currently looking to upgrade to the Debezium 2.3.x release stream, we highly recommend you consider using this release. Let&#8217;s take a quick look into the regressions and bug fixes.]]></summary></entry><entry><title type="html">Debezium 2.4.0.Beta1 Released</title><link href="https://debezium.io/blog/2023/08/29/debezium-2-4-beta1-released/" rel="alternate" type="text/html" title="Debezium 2.4.0.Beta1 Released"/><published>2023-08-29T00:00:00+00:00</published><updated>2023-08-29T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/08/29/debezium-2-4-beta1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/08/29/debezium-2-4-beta1-released/"><![CDATA[<div class="paragraph"> <p>While development remains steadfast as we continue forward on Debezium 2.4, I am thrilled to announce the immediate availability of Debezium <strong>2.4.0.Beta1</strong>.</p> </div> <div class="paragraph"> <p>While this release focuses on stability and bug fixes, there are several new noteworthy features including TimescaleDB support, JMX notifications using JSON payloads, multiple improvements to the Oracle connector&#8217;s metrics and embedded Infinispan buffer implementation, SQL Server heartbeats, Vitess shardless strategy, JDBC sink with SQL Server identity-based inserts, and much more. Let&#8217;s dive into each of thees new features and others in more detail.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="timescaledb_support">TimescaleDB support</h2> <div class="sectionbody"> <div class="paragraph"> <p>TimescaleDB is an open-source time series-based database that is based on PostgreSQL. This means that a great deal of functionality to support TimescaleDB directly comes from the existing PostgreSQL connector; however there are certain aspects of TimescaleDB such as chunks, hypertables, and agregates that are not.</p> </div> <div class="paragraph"> <p>Therefore, if you want to get started with Debezium 2.4 and TimescaleDB, the integration requires a combination of both the PostgreSQL connector combined with a new <code>TimescaleDb</code> single message transformation (SMT). The combination of these two provide the ability to stream changes from a TimescaleDB environment with appropriate table names based on chunks, hypertables, and aggregates.</p> </div> <div class="paragraph"> <p>The <code>TimescaleDb</code> transformation is available as <code>io.debezium.connector.postgresql.transforms.timescaledb</code> and is responsible for adjusting the final topic names when working with chunks, hypertables, and aggregates. Additionally, this transformation adds metadata headers to the change event so you know the original chunk name, chunk table, the hypertable schema and table names accordingly.</p> </div> </div> </div> <div class="sect1"> <h2 id="jmx_notifications_with_json_user_data">JMX notifications with JSON user data</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.4 changes how JMX notifications provide user data. In previous versions, the notification used a <code>toString()</code> style implementation, which while it worked, it doesn&#8217;t provide any good forward or backward compatibility semantics unlike other more structured formats such as JSON.</p> </div> <div class="paragraph"> <p>Moving forward, JMX notification&#8217;s user data will be provided as JSON, making it easier and more reliable to parse and to support extensibility in the future with less concerns about backward compatibility. We hope this makes this feature easier to use moving forward and welcome any additional feedback.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_connector_scn_based_metrics">Oracle connector SCN-based metrics</h2> <div class="sectionbody"> <div class="paragraph"> <p>Oracle tracks a variety of system change number, often called SCN, values in its JMX metrics including <code>OffsetScn</code>, <code>CurrentScn</code>, <code>OldestScn</code>, and <code>CommittedScn</code>. These SCN values are numeric and can often exceed the upper bounds of a <code>Long</code> data type, and so Debezium has traditionally exposed these values as <code>String</code> s.</p> </div> <div class="paragraph"> <p>Unfortunately, tooling such as Grafana and Prometheus do not work with <code>String</code> -based values, and it has been raised on several occasions that the community would like to be able to view these values from metrics gathering frameworks. With Debezium 2.4, there is a small behavior change with the these JMX metrics as they are no longer exposed as <code>String</code> values but instead are now exposed as <code>BigInteger</code> values.</p> </div> <div class="paragraph"> <p>This change in behavior allows tooling such as Grafana and Prometheus to now scrape these values from the JMX beans automatically for reporting and observability stacks.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you were previously gathering these values for other purposes, be aware they&#8217;re no longer string-based and should be interpreted as <code>BigInteger</code> numerical values moving forward.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="oracle_connector_max_transaction_age_metric">Oracle connector max transaction age metric</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Oracle connector provides a myriad of metrics for LogMiner, including the <code>OldestScn</code> metric representing the oldest system change number in the connector&#8217;s transaction buffer. This SCN can be useful to know how far back a transaction may still be buffered relative to the current system change number, <code>CurrentScn</code>. However, system change numbers are simply that, numerical values that require the use of a database function call to know when the change occurred.</p> </div> <div class="paragraph"> <p>Starting with Debezium 2.4, the connector will now also track the age of the oldest system change number by providing a new metric called <code>OldestScnAgeInMilliseconds</code>. This metric is calculated by taking the timestamp of the <code>OffsetScn</code> and calculating the difference between that time and the query time of the metric, giving a rough age in milliseconds of the oldest transaction in the buffer that has yet to be committed or rolled back.</p> </div> <div class="paragraph"> <p>If there are other metrics you may be interested in to help, please reach out and let us know.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_embedded_infinispan_configuration_changes">Oracle embedded Infinispan configuration changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Oracle connector supports three different buffering techniques, one is based on JVM heap while the other two are based on off-heap storage using Infinispan. When working with Infinispan, you can choose to use a remote cluster, where the caches are stored and managed across a remote connection, or using an embedded cluster, where the cluster is managed locally by the connector itself.</p> </div> <div class="paragraph"> <p>When working with a remote Infinispan cluster, there is some cluster configuration that is made as a part of the Infinispan installation itself, this is often referred to as the global or cluster configuration. However when working with an embedded Infinispan cluster, Debezium simply used the default configuration for an embedded cluster, which may not always provide all the necessary behaviors for each environment.</p> </div> <div class="paragraph"> <p>Debezium 2.4 introduces a new configuration property, <code>log.mining.buffer.infinispan.cache.global</code>. This property allows specifying the XML configuration for the Infinispan "global" or "cluster" configuration.</p> </div> <div class="listingblock"> <div class="title">An example configuration</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;infinispan&gt;</span>
  <span class="tag">&lt;threads&gt;</span>
    <span class="tag">&lt;blocking-bounded-queue-thread-pool</span>
        <span class="attribute-name">max-threads</span>=<span class="string"><span class="delimiter">&quot;</span><span class="content">10</span><span class="delimiter">&quot;</span></span>
        <span class="attribute-name">name</span>=<span class="string"><span class="delimiter">&quot;</span><span class="content">myexec</span><span class="delimiter">&quot;</span></span>
        <span class="attribute-name">keepalive-time</span>=<span class="string"><span class="delimiter">&quot;</span><span class="content">10000</span><span class="delimiter">&quot;</span></span>
        <span class="attribute-name">queue-length</span>=<span class="string"><span class="delimiter">&quot;</span><span class="content">5000</span><span class="delimiter">&quot;</span></span> <span class="tag">/&gt;</span>
  <span class="tag">&lt;/threads&gt;</span>
<span class="tag">&lt;/infinispan&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>With Debezium 2.4, if you are using the Infinispan-embedded buffer, you can now safely configure the overall embedded global configuration for Infinispan, which can allow you to tune and improve the overall performance when using the embedded Infinispan engine.</p> </div> </div> </div> <div class="sect1"> <h2 id="sql_sever_heartbeat_improvements">SQL Sever heartbeat improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>It&#8217;s not an uncommon situation for a database to go for a period of time without there being any relevant changes, whether that is due to inactivity or changes that do occur being of no interest to the connector based on configuration. In these cases, it&#8217;s critical that offset metadata managed by the connector remains synchronized with the offset backing store during these periods so that a restart of the connector works as expected.</p> </div> <div class="paragraph"> <p>With Debezium 2.4, if a SQL Server change capture loop does not find any changes or the changes that did occur are not of any relevance to the connector, the connector will continue to emit heartbeat events when enabled. This should improve the reliability of the offsets stored in the offset backing store across a variety of use cases.</p> </div> </div> </div> <div class="sect1"> <h2 id="vitess_shardless_naming_strategy">Vitess shardless naming strategy</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.4.0.Alpha2 introduced a mechanism to handle schema changes per shard by using the shard name as the catalog when identifying the relational identifier for a table. When using the <code>DefaultTopicNamingStrategy</code>, this had the side effect that the shard would be included within the topic name, which may not be desirable.</p> </div> <div class="paragraph"> <p>Debezium 2.4.0.Beta1 introduces a new strategy that enables the old behavior called <code>TableTopicNamingStrategy</code>.</p> </div> <div class="paragraph"> <p>The following table shows the output differences for topic names based on the different strategies:</p> </div> <table class="tableblock frame-all grid-all stretch"> <colgroup> <col style="width: 50%;"> <col style="width: 50%;"> </colgroup> <thead> <tr> <th class="tableblock halign-left valign-top">Strategy</th> <th class="tableblock halign-left valign-top">Topic Output</th> </tr> </thead> <tbody> <tr> <td class="tableblock halign-left valign-top"><p class="tableblock"><code>DefaultTopicNamingStrategy</code></p></td> <td class="tableblock halign-left valign-top"><p class="tableblock"><code>&lt;topic.prefix&gt;.&lt;shard&gt;.&lt;table-name&gt;</code></p></td> </tr> <tr> <td class="tableblock halign-left valign-top"><p class="tableblock"><code>TableTopicNamingStrategy</code></p></td> <td class="tableblock halign-left valign-top"><p class="tableblock"><code>&lt;topic.prefix&gt;.&lt;table-name&gt;</code></p></td> </tr> </tbody> </table> <div class="paragraph"> <p>In order to configure the table topic naming strategy, include the following configuration for the connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">topic.naming.strategy=io.debezium.connector.vitess.TableTopicNamingStrategy</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="jdbc_sink_sql_server_identity_inserts">JDBC sink SQL Server identity inserts</h2> <div class="sectionbody"> <div class="paragraph"> <p>Each database handles the insertion of values into an identity-based column differently. With SQL Server, this requires the explicit enablement of <code>IDENTITY_INSERT</code> prior to the insert and the disabling of this feature afterward. With Debezium 2.4, the Debezium JDBC sink connector provides support for this in the target database.</p> </div> <div class="paragraph"> <p>In order to take advantage of identity-based inserts, the JDBC sink connector must be configured with a new dialect-based property called <code>dialect.sqlserver.identity.inserts</code>, which can be set to <code>true</code> or <code>false</code>. By default, this feature is set to <code>false</code> and must be enabled if you wish to insert into identity-based columns.</p> </div> <div class="paragraph"> <p>When enabled, all <em>insert</em> and <em>upsert</em> operations will be wrapped as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SET</span> IDENTITY_INSERT &lt;<span class="type">table</span>-name&gt; <span class="keyword">ON</span>;
&lt;the <span class="class">insert</span> <span class="keyword">or</span> upsert statement&gt;
<span class="class">SET</span> IDENTITY_INSERT &lt;<span class="type">table</span>-name&gt; OFF;</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes_improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There are several bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium heartbeat.action.query does not start before writing to WAL <a href="https://issues.redhat.com/browse/DBZ-6635">DBZ-6635</a></p> </li> <li> <p>Schema name changed with Custom topic naming strategy <a href="https://issues.redhat.com/browse/DBZ-6641">DBZ-6641</a></p> </li> <li> <p>Wrong behavior of quote.identifiers in JdbcSinkConnector <a href="https://issues.redhat.com/browse/DBZ-6682">DBZ-6682</a></p> </li> <li> <p>Toasted UUID array is not properly processed <a href="https://issues.redhat.com/browse/DBZ-6720">DBZ-6720</a></p> </li> <li> <p>Debezium crashes on parsing MySQL DDL statement (specific JOIN) <a href="https://issues.redhat.com/browse/DBZ-6724">DBZ-6724</a></p> </li> <li> <p>Blocking snapshot must take snapshot configurations from signal <a href="https://issues.redhat.com/browse/DBZ-6731">DBZ-6731</a></p> </li> <li> <p>When using pgoutput in postgres connector, (+/-)Infinity is not supported in decimal values <a href="https://issues.redhat.com/browse/DBZ-6758">DBZ-6758</a></p> </li> <li> <p>Outbox transformation can cause connector to crash <a href="https://issues.redhat.com/browse/DBZ-6760">DBZ-6760</a></p> </li> <li> <p>MongoDB New Document State Extraction: nonexistent field for add.headers <a href="https://issues.redhat.com/browse/DBZ-6774">DBZ-6774</a></p> </li> <li> <p>Mongodb connector tests are massively failing when executed on 7.0-rc version <a href="https://issues.redhat.com/browse/DBZ-6779">DBZ-6779</a></p> </li> <li> <p>Dbz crashes on parsing MySQL DDL statement (SELECT 1.;) <a href="https://issues.redhat.com/browse/DBZ-6780">DBZ-6780</a></p> </li> <li> <p>Mysql connector tests are failing when executed without any profile <a href="https://issues.redhat.com/browse/DBZ-6791">DBZ-6791</a></p> </li> <li> <p>Dbz crashed on parsing MySQL DDL statement (SELECT 1 + @sum:=1 AS ss;) <a href="https://issues.redhat.com/browse/DBZ-6794">DBZ-6794</a></p> </li> <li> <p>MySQL DDL parser - REPEAT function not accepted <a href="https://issues.redhat.com/browse/DBZ-6803">DBZ-6803</a></p> </li> <li> <p>Fix bug with getSnapshottingTask <a href="https://issues.redhat.com/browse/DBZ-6820">DBZ-6820</a></p> </li> <li> <p>Dbz crashes on DDL statement (non-Latin chars in variables) <a href="https://issues.redhat.com/browse/DBZ-6821">DBZ-6821</a></p> </li> <li> <p>Not trim the default value for the BIGINT and SMALLINT types when parsing MySQL DDL <a href="https://issues.redhat.com/browse/DBZ-6824">DBZ-6824</a></p> </li> <li> <p>PostgresConnectorIT#shouldAddNewFieldToSourceInfo fails randomly <a href="https://issues.redhat.com/browse/DBZ-6839">DBZ-6839</a></p> </li> <li> <p>Wrong filtered comments <a href="https://issues.redhat.com/browse/DBZ-6840">DBZ-6840</a></p> </li> <li> <p>Intermittent test failure: BaseSourceTaskTest.verifyTaskRestartsSuccessfully <a href="https://issues.redhat.com/browse/DBZ-6841">DBZ-6841</a></p> </li> <li> <p>When using <code>skip.messages.without.change=true</code> a WARN log message is reported for each record <a href="https://issues.redhat.com/browse/DBZ-6843">DBZ-6843</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.Beta1%20ORDER%20BY%20component%20ASC">39 issues</a> were fixed for this release. <a href="https://github.com/andreas-ibm">Andreas Martens</a>, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/chrisbeard">Chris Beard</a>, <a href="https://github.com/Naros">Chris Cranford</a>, Matan Cohen, <a href="https://github.com/Fraznist">Emre Akgün</a>, <a href="https://github.com/ericpangiawan">Eric Pangiawan</a>, <a href="https://github.com/ruanhang1993">Hang Ruan</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/novotnyJiri">Jiri Novotny</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/Naros">M. Gökhan Akgül</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/ShuranZhang">Shuran Zhang</a>, <a href="https://github.com/rolevinks">Stein Rolevink</a>, <a href="https://github.com/sunxiaojian">Sun Xiao Jian</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/wuzhenhua01">Wu Zhenhua</a>, <a href="https://github.com/sunxiaojian">Xiaojian Sun</a></p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As we enter the beta-phase of Debezium 2.4, the next several weeks will primarily focus on bugfixes and stability as we continue to march forward to a final release at the end of September. We are also close on the last minute changes for the OpenLogReplicator ingestion method for Oracle and once complete, expect a Beta2 shortly afterward. Furthermore, there will be a Debezium 2.3.3.Final maintenance release early next week and likely at least one more 2.3 release as we make the transition to Debezium 2.4 as the new stable release later this coming month.</p> </div> <div class="paragraph"> <p>In addition, the Debezium Community Event&#8217;s agenda and date will be published later this week, so keep an eye out for that news. And finally, we&#8217;ll be presenting at Kafka Summit 2023 (aka Current 2023) later this upcoming month. If you&#8217;re planning to attend and would like to ask the experts, be sure to get in touch with me or anyone on the team and we can plan to meet up and discuss anything related to Debezium and CDC.</p> </div> <div class="paragraph"> <p>As always, if you have any ideas or suggestions, you can also get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[While development remains steadfast as we continue forward on Debezium 2.4, I am thrilled to announce the immediate availability of Debezium 2.4.0.Beta1. While this release focuses on stability and bug fixes, there are several new noteworthy features including TimescaleDB support, JMX notifications using JSON payloads, multiple improvements to the Oracle connector&#8217;s metrics and embedded Infinispan buffer implementation, SQL Server heartbeats, Vitess shardless strategy, JDBC sink with SQL Server identity-based inserts, and much more. Let&#8217;s dive into each of thees new features and others in more detail.]]></summary></entry><entry><title type="html">Debezium 2.4.0.Alpha2 Released</title><link href="https://debezium.io/blog/2023/08/09/debezium-2-4-alpha2-released/" rel="alternate" type="text/html" title="Debezium 2.4.0.Alpha2 Released"/><published>2023-08-09T00:00:00+00:00</published><updated>2023-08-09T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/08/09/debezium-2-4-alpha2-released</id><content type="html" xml:base="https://debezium.io/blog/2023/08/09/debezium-2-4-alpha2-released/"><![CDATA[<div class="paragraph"> <p>Despite summer being well underway, Debezium contributors remain hard at work, and it&#8217;s my pleasure to announce the next preview release of Debezium 2.4 series, <strong>2.4.0.Alpha2</strong>.</p> </div> <div class="paragraph"> <p>This preview release includes a mix of improvements, bug fixes, and new features that are available for the Debezium community to test and offer feedback. Some highlights from this release include ad-hoc blocking snapshots, source-to-sink column name propagation, support for alternative MySQL drivers, and all Cassandra connectors with Debezium Server. Let&#8217;s take a few moments and dive into these and others.</p> </div> <div class="paragraph"> <p><!- more --></p> </div> <div class="ulist"> <ul> <li> <p><a href="#breaking-changes">Breaking changes</a></p> </li> <li> <p><a href="#new-features">New features</a></p> </li> <li> <p><a href="#other-changes">Other changes</a></p> </li> <li> <p><a href="#whats-next">What&#8217;s next</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="breaking-changes">Breaking changes</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="debezium_server_and_cassandra_connectors">Debezium Server and Cassandra connectors</h3> <div class="paragraph"> <p>For Cassandra connector users who may have been using Debezium Server or who may have wanted to use Debezium Server, we previously only shipped Cassandra 4 with the Debezium Server distribution. With Debezium 2.4, we now include all three Cassandra connector variants with the distribution, meaning that Cassandra 3 and DSE can now be used directly.</p> </div> <div class="paragraph"> <p>However, for this to work, a new environment variable <code>EXTRA_CONNECTOR</code> was introduced to specify specifically which Cassandra connector variant should be used by Debezium Server. This means that if you were using Cassandra 4 with Debezium Server, you must include this environment variable when upgrading to have the same configuration continue to work as it did in prior versions.</p> </div> <div class="paragraph"> <p>This new environment variable should be set to <code>dse</code>, <code>cassandra-3</code>, or <code>cassandra-4</code> depending on the Cassandra version you intend to use for your Debezium Server source connector.</p> </div> </div> <div class="sect2"> <h3 id="mysql_bigint_precision_changed">MySQL BIGINT precision changed</h3> <div class="paragraph"> <p>The Debezium for MySQL connector was not properly setting the precision for <code>BIGINT</code> data types when configuring the connector with <code>bigint.unsigned.handling.mode</code> as <code>precise</code>. Unfortunately, this led to a situation where the schema for such fields did not include the correct precision value.</p> </div> <div class="paragraph"> <p>Debezium 2.4 includes <a href="https://issues.redhat.com/browse/DBZ-6714">DBZ-6714</a>, which provides a fix to address the incorrect precision for such fields. This can lead to schema incompatibilities when using schema registry, so you may need to adjust your compatibility settings or take other actions if you need to use strict compatibility rules.</p> </div> </div> <div class="sect2"> <h3 id="oracle_snapshot_and_query_fetch_sizes_increased">Oracle snapshot and query fetch sizes increased</h3> <div class="paragraph"> <p>Debezium 2.4 introduces a change in the default values for the <code>snapshot.fetch.size</code> and the <code>query.fetch.size</code> Oracle connector configuration properties. Previously, these properties used a default of <code>2000</code>; however, thanks to a community contributor, it was identified that these values may likely be too low for production use.</p> </div> <div class="paragraph"> <p>With this release, the Oracle connector will now use a default of <code>10000</code> for both properties, which should have a positive improvement on throughput for most users who were not explicitly setting these values. If you were previously using custom values for these settings in your connector configurations, then you will not see a change in your existing behavior. Only users who previously were not explicitly setting these values will notice that the new defaults will be used.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>These configuration properties are meant to act as tuning knobs, as a specific configuration for one JDBC environment may not work as well in a different environment. While we believe this change will have no negative impact, if you do notice a drop in performance, you can add these properties to your connector configuration setting them to their previous defaults of <code>2000</code>.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="vitess_incorrectly_mapped_bin_columns">Vitess incorrectly mapped <code>_bin</code> columns</h3> <div class="paragraph"> <p>For collations that end with the <code>_bin</code> designator, Vitess maps these to a data type of <code>VARBINARY</code>. As a result, the Vitess connector was inferring that these columns should be emitted as binary data; however, for character-based columns that used such collations, this was incorrect.</p> </div> <div class="paragraph"> <p>Debezium 2.4 will now properly emit character-based columns that are collated with a <code>_bin</code> designator as string-based data rather than binary data. This means that if you are using schema registry, you may observe somee schema incompatibilities and you may need to adjust your compatibility settings or take other actions to mitigate this change.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="new-features">New Features</h2> <div class="sectionbody"> <div class="sect2"> <h3 id="ad_hoc_blocking_snapshots">Ad-hoc blocking snapshots</h3> <div class="paragraph"> <p>Incremental snapshots were first introduced nearly two years ago in Debezium 1.6 and has remained quite popular in the community to deal with a variety of re-snapshot use cases. However, there are some use cases where the intertwined nature of read events with create, updates, and deletes may be less than ideal or even not supported by some consumer application. For those use cases, Debezium 2.4 introduces ad-hoc blocking snapshots.</p> </div> <div class="paragraph"> <p>An ad-hoc blocking snapshot works in a similar way that ad-hoc incremental snapshots work; however, with one major difference. The snapshot is still triggered by sending a signal to Debezium; however when the signal is processed by the connector, the key difference is that streaming is put on hold while the snapshot process runs. This means you won&#8217;t be receiving a series of read events interwoven with create, update, or delete events. This also means that we&#8217;ll be processing the snapshot in a similar way to traditional snapshots, so the throughput should generally be higher than incremental snapshots.</p> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>Be aware that ad-hoc blocking snapshots puts the reading of the transaction logs on hold while the snapshot is performed. This means the same requirements that a traditional snapshot has on transaction log availability also applies when using this type of ad-hoc snapshot mode. When streaming resumes, if a transaction log that is needed has since been removed, the connector will raise an error and stop.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>The signal to initiate an ad-hoc blocking snapshot is very similar to its ad-hoc incremental snapshot counterpart. The following signal below shows the payload to snapshot a specific table with a condition, but this uses the new blocking snapshot rather than the incremental snapshot:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">execute-snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">data-collections</span><span class="delimiter">&quot;</span></span>: [<span class="string"><span class="delimiter">&quot;</span><span class="content">public.my_table</span><span class="delimiter">&quot;</span></span>],
    <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">BLOCKING</span><span class="delimiter">&quot;</span></span>, <i class="conum" data-value="1"></i><b>(1)</b>
    <span class="key"><span class="delimiter">&quot;</span><span class="content">additional-condition</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">last_update_date &gt;= '2023-01-01'</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The use of <code>BLOCKING</code> rather than <code>INCREMENTAL</code> differentiates the two ad-hoc snapshot modes.</td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="source_to_sink_column_name_propagation">Source-to-sink column name propagation</h3> <div class="paragraph"> <p>Normally column names map directly to field names and vice versa when consumed by sink connectors such as a JDBC connector. However, there are situations where the serialization technology, such as Avro, has very specific rules about field naming conventions. When a column&#8217;s name in a database table conflicts with the serialization rule&#8217;s naming conventions, Debezium will rename the field in the event so that it adheres to the serialization&#8217;s rules. This often means that a field will be prepended with underscores or invalid characters replaced with underscores.</p> </div> <div class="paragraph"> <p>This can create problems for certain types of sinks, such as a JDBC sink connector, because the sink cannot easily deduce the original column name for the destination table nor can it adequately map between the event&#8217;s field name and a column name if they differ. This typically means users must use transformation chains on the sink side in order to reconstruct the event&#8217;s fields with namings that represent the source.</p> </div> <div class="paragraph"> <p>Debezium 2.4 introduces a way to minimize and potentially avoid that entirely by propagating the original column name as a field schema parameter, much in the same way that we do for data types, precision, scale, and length. The schema parameter <code>__debezium.source.column.name</code> now includes the original column name when column or data type propagation is enabled.</p> </div> <div class="admonitionblock tip"> <table> <tr> <td class="icon"> <i class="fa icon-tip" title="Tip"></i> </td> <td class="content"> <div class="paragraph"> <p>The Debezium JDBC sink connector already works with column and data type propagation, allowing for the sink connector to more accurately deduce column types, length, precision, and scale.</p> </div> <div class="paragraph"> <p>With this new feature, the JDBC sink connector will automatically use the column name from this argument when it is provided to guarantee that the destination table will be created with the same column names as the source, even when using Avro or similar. This means no transformations are needed when using the Debezium JDBC sink connector.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="alternative_mysql_jdbc_drivers">Alternative MySQL JDBC drivers</h3> <div class="paragraph"> <p>In order to use IAM authentication on AWS, a special MySQL driver is required to provide that type of functionality. With Debezium 2.4, you can now provide a reference to this specific driver and the connector will use that driver instead of the default driver shipped with the connector.</p> </div> <div class="paragraph"> <p>As an example, to connect using IAM authentication on AWS, the following configuration is needed:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="text">database.jdbc.driver=software.aws.rds.jdbc.mysql.Driver
database.jdbc.protocol=jdbc:mysql:aws</code></pre> </div> </div> <div class="paragraph"> <p>The <code>database.jdbc.driver</code> specifies the driver that should be loaded by the connector and used to communicate with the MySQL database. The <code>database.jdbc.protocol</code> is a supplemental configuration property that may not be required in all contexts. It defaults to <code>jdbc:mysql</code> but since AWS requires <code>jdbc:mysql:aws</code>, this allows you to specify this derivative within the configuration.</p> </div> <div class="paragraph"> <p>We&#8217;ve love to hear feedback and whether something like this might be useful for other scenarios.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-changes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In addition, there were quite a number of stability and bug fixes that made it into this release. These include the following:</p> </div> <div class="ulist"> <ul> <li> <p>Switch tracing to OpenTelemetry <a href="https://issues.redhat.com/browse/DBZ-2862">DBZ-2862</a></p> </li> <li> <p>Connector drop down causes a scroll bar <a href="https://issues.redhat.com/browse/DBZ-5421">DBZ-5421</a></p> </li> <li> <p>Provide outline for drawer component showing connector details <a href="https://issues.redhat.com/browse/DBZ-5831">DBZ-5831</a></p> </li> <li> <p>Modify scroll for the running connector component <a href="https://issues.redhat.com/browse/DBZ-5832">DBZ-5832</a></p> </li> <li> <p>Connector restart regression <a href="https://issues.redhat.com/browse/DBZ-6213">DBZ-6213</a></p> </li> <li> <p>Highlight information about how to configure the schema history topic to store data only for intended tables <a href="https://issues.redhat.com/browse/DBZ-6219">DBZ-6219</a></p> </li> <li> <p>Document Optimal MongoDB Oplog Config for Resiliency <a href="https://issues.redhat.com/browse/DBZ-6455">DBZ-6455</a></p> </li> <li> <p>JDBC Schema History: When the table name is passed as dbName.tableName, the connector does not start <a href="https://issues.redhat.com/browse/DBZ-6484">DBZ-6484</a></p> </li> <li> <p>Update the Edit connector UI to incorporate the feedback received from team in demo <a href="https://issues.redhat.com/browse/DBZ-6514">DBZ-6514</a></p> </li> <li> <p>Support blocking ad-hoc snapshots <a href="https://issues.redhat.com/browse/DBZ-6566">DBZ-6566</a></p> </li> <li> <p>Add new parameters to RabbitMQ consumer <a href="https://issues.redhat.com/browse/DBZ-6581">DBZ-6581</a></p> </li> <li> <p>Document read preference changes in 2.4 <a href="https://issues.redhat.com/browse/DBZ-6591">DBZ-6591</a></p> </li> <li> <p>Oracle DDL parser does not properly detect end of statement when comments obfuscate the semicolon <a href="https://issues.redhat.com/browse/DBZ-6599">DBZ-6599</a></p> </li> <li> <p>Received an unexpected message type that does not have an 'after' Debezium block <a href="https://issues.redhat.com/browse/DBZ-6637">DBZ-6637</a></p> </li> <li> <p>When Debezium Mongodb connector encounter authentication or under privilege errors, the connection between debezium and mongodb keeps going up. <a href="https://issues.redhat.com/browse/DBZ-6643">DBZ-6643</a></p> </li> <li> <p>Log appropriate error when JDBC connector receive SchemaChange record <a href="https://issues.redhat.com/browse/DBZ-6655">DBZ-6655</a></p> </li> <li> <p>Send tombstone events when partition queries are finished <a href="https://issues.redhat.com/browse/DBZ-6658">DBZ-6658</a></p> </li> <li> <p>Snapshot will not capture data when signal.data.collection is present without table.include.list <a href="https://issues.redhat.com/browse/DBZ-6669">DBZ-6669</a></p> </li> <li> <p>Retriable operations are retried infinitely since error handlers are not reused <a href="https://issues.redhat.com/browse/DBZ-6670">DBZ-6670</a></p> </li> <li> <p>Oracle DDL parser does not support column visibility on ALTER TABLE <a href="https://issues.redhat.com/browse/DBZ-6677">DBZ-6677</a></p> </li> <li> <p>Propagate source column name and allow sink to use it <a href="https://issues.redhat.com/browse/DBZ-6684">DBZ-6684</a></p> </li> <li> <p>Partition duplication after rebalances with single leader task <a href="https://issues.redhat.com/browse/DBZ-6685">DBZ-6685</a></p> </li> <li> <p>JDBC Sink Connector Fails on Loading Flat Data Containing Struct Type Fields from Kafka <a href="https://issues.redhat.com/browse/DBZ-6686">DBZ-6686</a></p> </li> <li> <p>SQLSyntaxErrorException using Debezium JDBC Sink connector <a href="https://issues.redhat.com/browse/DBZ-6687">DBZ-6687</a></p> </li> <li> <p>Should use topic.prefix rather than connector.server.name in MBean namings <a href="https://issues.redhat.com/browse/DBZ-6690">DBZ-6690</a></p> </li> <li> <p>CDC - Debezium x RabbitMQ - Debezium Server crashes when an UPDATE/DELETE on source database (PostgreSQL) <a href="https://issues.redhat.com/browse/DBZ-6691">DBZ-6691</a></p> </li> <li> <p>Missing operationTime field on ping command when executed against Atlas <a href="https://issues.redhat.com/browse/DBZ-6700">DBZ-6700</a></p> </li> <li> <p>MongoDB SRV protocol not working in Debezium Server <a href="https://issues.redhat.com/browse/DBZ-6701">DBZ-6701</a></p> </li> <li> <p>Disable jdk-outreach-workflow.yml in forked personal repo <a href="https://issues.redhat.com/browse/DBZ-6702">DBZ-6702</a></p> </li> <li> <p>Custom properties step not working correctly in validation of the properties added by user <a href="https://issues.redhat.com/browse/DBZ-6711">DBZ-6711</a></p> </li> <li> <p>Add tzdata-java to UI installation Dockerfile <a href="https://issues.redhat.com/browse/DBZ-6713">DBZ-6713</a></p> </li> <li> <p>Refactor EmbeddedEngine::run method <a href="https://issues.redhat.com/browse/DBZ-6715">DBZ-6715</a></p> </li> <li> <p>Oracle fails to process a DROP USER <a href="https://issues.redhat.com/browse/DBZ-6716">DBZ-6716</a></p> </li> <li> <p>Support alternative JDBC drivers in MySQL connector <a href="https://issues.redhat.com/browse/DBZ-6727">DBZ-6727</a></p> </li> <li> <p>Oracle LogMiner mining distance calculation should be skipped when upper bounds is not within distance <a href="https://issues.redhat.com/browse/DBZ-6733">DBZ-6733</a></p> </li> <li> <p>Add STOPPED and RESTARTING connector states to testing library <a href="https://issues.redhat.com/browse/DBZ-6734">DBZ-6734</a></p> </li> <li> <p>MariaDB: Unparseable DDL statement (ALTER TABLE IF EXISTS) <a href="https://issues.redhat.com/browse/DBZ-6736">DBZ-6736</a></p> </li> <li> <p>Update Quarkus to 3.2.3.Final <a href="https://issues.redhat.com/browse/DBZ-6740">DBZ-6740</a></p> </li> <li> <p>Decouple Debezium Server and Extension Quarkus versions <a href="https://issues.redhat.com/browse/DBZ-6744">DBZ-6744</a></p> </li> <li> <p>SingleProcessor remove redundant filter logic <a href="https://issues.redhat.com/browse/DBZ-6745">DBZ-6745</a></p> </li> <li> <p>MySQL dialect does not properly recognize non-default value longblob types due to typo <a href="https://issues.redhat.com/browse/DBZ-6753">DBZ-6753</a></p> </li> <li> <p>Add a new parameter for selecting the db index when using Redis Storage <a href="https://issues.redhat.com/browse/DBZ-6759">DBZ-6759</a></p> </li> <li> <p>Postgres tests for toasted byte array and toasted date array fail with decoderbufs plugin <a href="https://issues.redhat.com/browse/DBZ-6767">DBZ-6767</a></p> </li> <li> <p>Table schemas should be updated for each shard individually <a href="https://issues.redhat.com/browse/DBZ-6775">DBZ-6775</a></p> </li> <li> <p>Notifications and signals leaks between MBean instances when using JMX channels <a href="https://issues.redhat.com/browse/DBZ-6777">DBZ-6777</a></p> </li> <li> <p>Oracle XML column types are not properly resolved when adding XMLTYPE column during streaming <a href="https://issues.redhat.com/browse/DBZ-6782">DBZ-6782</a></p> </li> <li> <p>Bump the MySQL binlog client version to 0.28.1 which includes significant GTID event performance improvements <a href="https://issues.redhat.com/browse/DBZ-6783">DBZ-6783</a></p> </li> <li> <p>Add new Redis Sink connector parameter description to the documentation <a href="https://issues.redhat.com/browse/DBZ-6784">DBZ-6784</a></p> </li> <li> <p>Upgrade Kafka to 3.5.1 <a href="https://issues.redhat.com/browse/DBZ-6785">DBZ-6785</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.Alpha2%20ORDER%20BY%20component%20ASC">62 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/TyrantLucifer">Chao Tian</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/C0urante">Chris Egerton</a>, <a href="https://github.com/vidourem">David Remy</a>, <a href="https://github.com/faihofu">Fai Ho Fu</a>, <a href="https://github.com/gurpiarbassi">Gurps Bassi</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/mfortunat">Massimo Fortunat</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/benesch">Nikhil Benesch</a>, <a href="https://github.com/paul-cheung">Paul Cheung</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/ryanvanhuuksloot">Ryan van Huuksloot</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/j2gg0s">Yanjie Wang</a>, <a href="https://github.com/Ychopada">Yashashree Chopada</a>, <a href="https://github.com/vidourem">david remy</a>, and <a href="https://github.com/ibnubay">ibnubay</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats-next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium 2.4 series is already packed with lots of new features, and we&#8217;re only scratching the surface. We have more in-store, including the new Oracle OpenLogReplicator adapter coming with Debezium 2.4 Alpha3 next week. After that, we&#8217;ll begin to wind down the development and shift our focus in the beta and release candidate cycle, targeting the end of September for a Debezium 2.4 final release.</p> </div> <div class="paragraph"> <p>Don&#8217;t forget about the Debezium Community Event, which I shared with you on the <a href="https://groups.google.com/g/debezium">mailing list</a>. If you have any ideas or suggestions, I&#8217;d love your feedback. We will be making an announcement in the next two weeks about the date/time and agenda.</p> </div> <div class="paragraph"> <p>Additionally, if you&#8217;re going to Current 2023 this year in San Jose, I&#8217;d love to meet up and discuss your experiences with Debezium. I&#8217;ll be there doing a talk on event-driven design with Debezium and Apicurio with my good friends Hans-Peter Grahsl and Carles Arnal. If you&#8217;re interested in more details, feel free to drop me a line in chat, on the mailing list or directly via email.</p> </div> <div class="paragraph"> <p>As always, if you have any ideas or suggestions, you can also get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>. Until next time, don&#8217;t be a stranger and stay cool out there!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[Despite summer being well underway, Debezium contributors remain hard at work, and it&#8217;s my pleasure to announce the next preview release of Debezium 2.4 series, 2.4.0.Alpha2. This preview release includes a mix of improvements, bug fixes, and new features that are available for the Debezium community to test and offer feedback. Some highlights from this release include ad-hoc blocking snapshots, source-to-sink column name propagation, support for alternative MySQL drivers, and all Cassandra connectors with Debezium Server. Let&#8217;s take a few moments and dive into these and others. Breaking changes New features Other changes What&#8217;s next Breaking changes Debezium Server and Cassandra connectors For Cassandra connector users who may have been using Debezium Server or who may have wanted to use Debezium Server, we previously only shipped Cassandra 4 with the Debezium Server distribution. With Debezium 2.4, we now include all three Cassandra connector variants with the distribution, meaning that Cassandra 3 and DSE can now be used directly. However, for this to work, a new environment variable EXTRA_CONNECTOR was introduced to specify specifically which Cassandra connector variant should be used by Debezium Server. This means that if you were using Cassandra 4 with Debezium Server, you must include this environment variable when upgrading to have the same configuration continue to work as it did in prior versions. This new environment variable should be set to dse, cassandra-3, or cassandra-4 depending on the Cassandra version you intend to use for your Debezium Server source connector. MySQL BIGINT precision changed The Debezium for MySQL connector was not properly setting the precision for BIGINT data types when configuring the connector with bigint.unsigned.handling.mode as precise. Unfortunately, this led to a situation where the schema for such fields did not include the correct precision value. Debezium 2.4 includes DBZ-6714, which provides a fix to address the incorrect precision for such fields. This can lead to schema incompatibilities when using schema registry, so you may need to adjust your compatibility settings or take other actions if you need to use strict compatibility rules. Oracle snapshot and query fetch sizes increased Debezium 2.4 introduces a change in the default values for the snapshot.fetch.size and the query.fetch.size Oracle connector configuration properties. Previously, these properties used a default of 2000; however, thanks to a community contributor, it was identified that these values may likely be too low for production use. With this release, the Oracle connector will now use a default of 10000 for both properties, which should have a positive improvement on throughput for most users who were not explicitly setting these values. If you were previously using custom values for these settings in your connector configurations, then you will not see a change in your existing behavior. Only users who previously were not explicitly setting these values will notice that the new defaults will be used. These configuration properties are meant to act as tuning knobs, as a specific configuration for one JDBC environment may not work as well in a different environment. While we believe this change will have no negative impact, if you do notice a drop in performance, you can add these properties to your connector configuration setting them to their previous defaults of 2000. Vitess incorrectly mapped _bin columns For collations that end with the _bin designator, Vitess maps these to a data type of VARBINARY. As a result, the Vitess connector was inferring that these columns should be emitted as binary data; however, for character-based columns that used such collations, this was incorrect. Debezium 2.4 will now properly emit character-based columns that are collated with a _bin designator as string-based data rather than binary data. This means that if you are using schema registry, you may observe somee schema incompatibilities and you may need to adjust your compatibility settings or take other actions to mitigate this change. New Features Ad-hoc blocking snapshots Incremental snapshots were first introduced nearly two years ago in Debezium 1.6 and has remained quite popular in the community to deal with a variety of re-snapshot use cases. However, there are some use cases where the intertwined nature of read events with create, updates, and deletes may be less than ideal or even not supported by some consumer application. For those use cases, Debezium 2.4 introduces ad-hoc blocking snapshots. An ad-hoc blocking snapshot works in a similar way that ad-hoc incremental snapshots work; however, with one major difference. The snapshot is still triggered by sending a signal to Debezium; however when the signal is processed by the connector, the key difference is that streaming is put on hold while the snapshot process runs. This means you won&#8217;t be receiving a series of read events interwoven with create, update, or delete events. This also means that we&#8217;ll be processing the snapshot in a similar way to traditional snapshots, so the throughput should generally be higher than incremental snapshots. Be aware that ad-hoc blocking snapshots puts the reading of the transaction logs on hold while the snapshot is performed. This means the same requirements that a traditional snapshot has on transaction log availability also applies when using this type of ad-hoc snapshot mode. When streaming resumes, if a transaction log that is needed has since been removed, the connector will raise an error and stop. The signal to initiate an ad-hoc blocking snapshot is very similar to its ad-hoc incremental snapshot counterpart. The following signal below shows the payload to snapshot a specific table with a condition, but this uses the new blocking snapshot rather than the incremental snapshot: { &quot;type&quot;: &quot;execute-snapshot&quot;, &quot;data&quot;: { &quot;data-collections&quot;: [&quot;public.my_table&quot;], &quot;type&quot;: &quot;BLOCKING&quot;, (1) &quot;additional-condition&quot;: &quot;last_update_date &gt;= '2023-01-01'&quot; } } 1 The use of BLOCKING rather than INCREMENTAL differentiates the two ad-hoc snapshot modes. Source-to-sink column name propagation Normally column names map directly to field names and vice versa when consumed by sink connectors such as a JDBC connector. However, there are situations where the serialization technology, such as Avro, has very specific rules about field naming conventions. When a column&#8217;s name in a database table conflicts with the serialization rule&#8217;s naming conventions, Debezium will rename the field in the event so that it adheres to the serialization&#8217;s rules. This often means that a field will be prepended with underscores or invalid characters replaced with underscores. This can create problems for certain types of sinks, such as a JDBC sink connector, because the sink cannot easily deduce the original column name for the destination table nor can it adequately map between the event&#8217;s field name and a column name if they differ. This typically means users must use transformation chains on the sink side in order to reconstruct the event&#8217;s fields with namings that represent the source. Debezium 2.4 introduces a way to minimize and potentially avoid that entirely by propagating the original column name as a field schema parameter, much in the same way that we do for data types, precision, scale, and length. The schema parameter __debezium.source.column.name now includes the original column name when column or data type propagation is enabled. The Debezium JDBC sink connector already works with column and data type propagation, allowing for the sink connector to more accurately deduce column types, length, precision, and scale. With this new feature, the JDBC sink connector will automatically use the column name from this argument when it is provided to guarantee that the destination table will be created with the same column names as the source, even when using Avro or similar. This means no transformations are needed when using the Debezium JDBC sink connector. Alternative MySQL JDBC drivers In order to use IAM authentication on AWS, a special MySQL driver is required to provide that type of functionality. With Debezium 2.4, you can now provide a reference to this specific driver and the connector will use that driver instead of the default driver shipped with the connector. As an example, to connect using IAM authentication on AWS, the following configuration is needed: database.jdbc.driver=software.aws.rds.jdbc.mysql.Driver database.jdbc.protocol=jdbc:mysql:aws The database.jdbc.driver specifies the driver that should be loaded by the connector and used to communicate with the MySQL database. The database.jdbc.protocol is a supplemental configuration property that may not be required in all contexts. It defaults to jdbc:mysql but since AWS requires jdbc:mysql:aws, this allows you to specify this derivative within the configuration. We&#8217;ve love to hear feedback and whether something like this might be useful for other scenarios. Other fixes In addition, there were quite a number of stability and bug fixes that made it into this release. These include the following: Switch tracing to OpenTelemetry DBZ-2862 Connector drop down causes a scroll bar DBZ-5421 Provide outline for drawer component showing connector details DBZ-5831 Modify scroll for the running connector component DBZ-5832 Connector restart regression DBZ-6213 Highlight information about how to configure the schema history topic to store data only for intended tables DBZ-6219 Document Optimal MongoDB Oplog Config for Resiliency DBZ-6455 JDBC Schema History: When the table name is passed as dbName.tableName, the connector does not start DBZ-6484 Update the Edit connector UI to incorporate the feedback received from team in demo DBZ-6514 Support blocking ad-hoc snapshots DBZ-6566 Add new parameters to RabbitMQ consumer DBZ-6581 Document read preference changes in 2.4 DBZ-6591 Oracle DDL parser does not properly detect end of statement when comments obfuscate the semicolon DBZ-6599 Received an unexpected message type that does not have an 'after' Debezium block DBZ-6637 When Debezium Mongodb connector encounter authentication or under privilege errors, the connection between debezium and mongodb keeps going up. DBZ-6643 Log appropriate error when JDBC connector receive SchemaChange record DBZ-6655 Send tombstone events when partition queries are finished DBZ-6658 Snapshot will not capture data when signal.data.collection is present without table.include.list DBZ-6669 Retriable operations are retried infinitely since error handlers are not reused DBZ-6670 Oracle DDL parser does not support column visibility on ALTER TABLE DBZ-6677 Propagate source column name and allow sink to use it DBZ-6684 Partition duplication after rebalances with single leader task DBZ-6685 JDBC Sink Connector Fails on Loading Flat Data Containing Struct Type Fields from Kafka DBZ-6686 SQLSyntaxErrorException using Debezium JDBC Sink connector DBZ-6687 Should use topic.prefix rather than connector.server.name in MBean namings DBZ-6690 CDC - Debezium x RabbitMQ - Debezium Server crashes when an UPDATE/DELETE on source database (PostgreSQL) DBZ-6691 Missing operationTime field on ping command when executed against Atlas DBZ-6700 MongoDB SRV protocol not working in Debezium Server DBZ-6701 Disable jdk-outreach-workflow.yml in forked personal repo DBZ-6702 Custom properties step not working correctly in validation of the properties added by user DBZ-6711 Add tzdata-java to UI installation Dockerfile DBZ-6713 Refactor EmbeddedEngine::run method DBZ-6715 Oracle fails to process a DROP USER DBZ-6716 Support alternative JDBC drivers in MySQL connector DBZ-6727 Oracle LogMiner mining distance calculation should be skipped when upper bounds is not within distance DBZ-6733 Add STOPPED and RESTARTING connector states to testing library DBZ-6734 MariaDB: Unparseable DDL statement (ALTER TABLE IF EXISTS) DBZ-6736 Update Quarkus to 3.2.3.Final DBZ-6740 Decouple Debezium Server and Extension Quarkus versions DBZ-6744 SingleProcessor remove redundant filter logic DBZ-6745 MySQL dialect does not properly recognize non-default value longblob types due to typo DBZ-6753 Add a new parameter for selecting the db index when using Redis Storage DBZ-6759 Postgres tests for toasted byte array and toasted date array fail with decoderbufs plugin DBZ-6767 Table schemas should be updated for each shard individually DBZ-6775 Notifications and signals leaks between MBean instances when using JMX channels DBZ-6777 Oracle XML column types are not properly resolved when adding XMLTYPE column during streaming DBZ-6782 Bump the MySQL binlog client version to 0.28.1 which includes significant GTID event performance improvements DBZ-6783 Add new Redis Sink connector parameter description to the documentation DBZ-6784 Upgrade Kafka to 3.5.1 DBZ-6785 Altogether, 62 issues were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Bob Roldan, Chao Tian, Chris Cranford, Chris Egerton, David Remy, Fai Ho Fu, Gurps Bassi, Harvey Yue, Indra Shukla, Jakub Cechacek, Jiri Pechanec, Jochen Schalanda, Mario Fiore Vitale, Massimo Fortunat, Nancy Xu, Nikhil Benesch, Paul Cheung, Robert Roldan, Ronak Jain, Ryan van Huuksloot, Sergey Eizner, Thomas Thornton, Vojtech Juranek, Yanjie Wang, Yashashree Chopada, david remy, and ibnubay! What&#8217;s next? The Debezium 2.4 series is already packed with lots of new features, and we&#8217;re only scratching the surface. We have more in-store, including the new Oracle OpenLogReplicator adapter coming with Debezium 2.4 Alpha3 next week. After that, we&#8217;ll begin to wind down the development and shift our focus in the beta and release candidate cycle, targeting the end of September for a Debezium 2.4 final release. Don&#8217;t forget about the Debezium Community Event, which I shared with you on the mailing list. If you have any ideas or suggestions, I&#8217;d love your feedback. We will be making an announcement in the next two weeks about the date/time and agenda. Additionally, if you&#8217;re going to Current 2023 this year in San Jose, I&#8217;d love to meet up and discuss your experiences with Debezium. I&#8217;ll be there doing a talk on event-driven design with Debezium and Apicurio with my good friends Hans-Peter Grahsl and Carles Arnal. If you&#8217;re interested in more details, feel free to drop me a line in chat, on the mailing list or directly via email. As always, if you have any ideas or suggestions, you can also get in touch with us on the mailing list or our chat. Until next time, don&#8217;t be a stranger and stay cool out there!]]></summary></entry><entry><title type="html">Debezium 2.3.2.Final Released</title><link href="https://debezium.io/blog/2023/08/04/debezium-2-3-2-final-released/" rel="alternate" type="text/html" title="Debezium 2.3.2.Final Released"/><published>2023-08-04T00:00:00+00:00</published><updated>2023-08-04T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/08/04/debezium-2-3-2-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/08/04/debezium-2-3-2-final-released/"><![CDATA[<div class="paragraph"> <p>It is my pleasure to announce the immediate release of Debezium <strong>2.3.2.Final</strong>.</p> </div> <div class="paragraph"> <p>This release includes several bug fixes to address regressions, stability, documentation updates. If you are currently looking to upgrade to the Debezium 2.3.x release stream, we highly recommend you consider using this release. Let&#8217;s take a quick look into the regressions and bug fixes.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="sql_server_refuses_to_start">SQL Server refuses to start</h2> <div class="sectionbody"> <div class="paragraph"> <p>If you have recently tried to upgrade to Debezium 2.3.1.Final, you may have found when using the SQL Server connector that you received an unusual error when starting the connector which said, "Configuration <code>query.fetch.size</code> is defined twice."</p> </div> <div class="paragraph"> <p>Unfortunately, this error was not intended and there is no workaround to remedy the issue. Thankfully, Debezium 2.3.2.Final is here to the recue; this release addresses this regression, allowing SQL Server connectors to start once again. If you are looking to upgrade, and you rely on the SQL Server connector, we strongly recommend that you avoid the 2.3.1.Final build and instead move directly to 2.3.2.Final.</p> </div> </div> </div> <div class="sect1"> <h2 id="oracle_default_fetch_size_changed">Oracle default fetch size changed</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium uses JDBC in order to communicate with the Oracle database. The Debezium for Oracle connector relies on two configuration properties, <code>snapshot.fetch.size</code> and <code>query.fetch.size</code> to control how much data is returned for a query on each database "fetch" call.</p> </div> <div class="paragraph"> <p>When these properties are configured too low, this can cause Debezium to perform more network round trips to the database to read data and that network latency can add up, particularly when working with large result sets. When these properties are configured too high, this can cause Debezium to consume more memory, but reduces the network latency incurred for the fetch round trips to the database. Ultimately, it&#8217;s important to strike a good balance based both on the what your ideal data size may be but also based on the memory and hardware constraints of your environment.</p> </div> <div class="paragraph"> <p>While discussing performance with one community member, we concluded that adjusting these values from their default of <code>2000</code> to <code>10000</code> increased the connector&#8217;s throughput quite substantially for their environment. So in this release, we felt it made logical sense to consider increasing the default to <code>10000</code> to provide a better out-of-the-box experience for Oracle connector users.</p> </div> <div class="paragraph"> <p>Now, these configuration properties are performance tuning knobs, and unfortunately there isn&#8217;t a guarantee that what works well for some environments is going to necessarily be <em>universally</em> good. Please take note of this change and if you experience any issues, you can always set the <code>snapshot.fetch.size</code> and <code>query.fetch.size</code> properties in your connector configuration, even setting them back to their previous default of <code>2000</code> if necessary.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_changes">Other changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium <strong>2.3.2.Final</strong> also includes quite a number of bug fixes and stability improvements, see below:</p> </div> <div class="ulist"> <ul> <li> <p>Highlight information about how to configure the schema history topic to store data only for intended tables <a href="https://issues.redhat.com/browse/DBZ-6219">DBZ-6219</a></p> </li> <li> <p>Should use topic.prefix rather than connector.server.name in MBean namings <a href="https://issues.redhat.com/browse/DBZ-6690">DBZ-6690</a></p> </li> <li> <p>Upstream documentation missing types for configurations <a href="https://issues.redhat.com/browse/DBZ-6707">DBZ-6707</a></p> </li> <li> <p>Custom properties step not working correctly in validation of the properties added by user <a href="https://issues.redhat.com/browse/DBZ-6711">DBZ-6711</a></p> </li> <li> <p>Oracle fails to process a DROP USER <a href="https://issues.redhat.com/browse/DBZ-6716">DBZ-6716</a></p> </li> <li> <p>Oracle LogMiner mining distance calculation should be skipped when upper bounds is not within distance <a href="https://issues.redhat.com/browse/DBZ-6733">DBZ-6733</a></p> </li> <li> <p>MariaDB: Unparseable DDL statement (ALTER TABLE IF EXISTS) <a href="https://issues.redhat.com/browse/DBZ-6736">DBZ-6736</a></p> </li> <li> <p>Decouple Debezium Server and Extension Quarkus versions <a href="https://issues.redhat.com/browse/DBZ-6744">DBZ-6744</a></p> </li> <li> <p>MySQL dialect does not properly recognize non-default value longblob types due to typo <a href="https://issues.redhat.com/browse/DBZ-6753">DBZ-6753</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/2.3/release-notes#release-2.3.2-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 2.3.2.Final: <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/Ychopada">Yashashree Chopada</a>, <a href="https://github.com/faihofu">faihofu</a>, <a href="https://github.com/j2gg0s">j2gg0s</a>, and <a href="https://github.com/paul-cheung">paul cheung</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_and_whats_next">Outlook and what&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>A great deal of work has already gone into the new preview release of Debezium 2.4. We plan to do the next Alpha2 build in the middle of next week, which will include a plethora of new features and improvements. There is still time to share your feedback and suggestions if there are things you&#8217;d like to see in 2.4, so take a look at our <a href="https://debezium.io/roadmap">road map</a> and reach out on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> <div class="paragraph"> <p>Finally, Debezium 2.3 will continue to receive maintenance updates. We&#8217;ll likely release 2.3.3.Final later in the month barring the community feedback on regressions and bug fixes.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It is my pleasure to announce the immediate release of Debezium 2.3.2.Final. This release includes several bug fixes to address regressions, stability, documentation updates. If you are currently looking to upgrade to the Debezium 2.3.x release stream, we highly recommend you consider using this release. Let&#8217;s take a quick look into the regressions and bug fixes.]]></summary></entry><entry><title type="html">Debezium 2.3.1.Final Released</title><link href="https://debezium.io/blog/2023/07/27/debezium-2-3-1-final-released/" rel="alternate" type="text/html" title="Debezium 2.3.1.Final Released"/><published>2023-07-27T00:00:00+00:00</published><updated>2023-07-27T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/07/27/debezium-2-3-1-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/07/27/debezium-2-3-1-final-released/"><![CDATA[<div class="paragraph"> <p>It has been several weeks since we released the first installment of Debezium 2.3, and I&#8217;m excited to announce the next iteration of Debezium 2.3 with <strong>2.3.1.Final</strong>. As with any micro-release, the focus is on stability and bug fixes, as well as adjustments to our documentation; however there are some changes that are noteworthy that I would like to take a few moments to highlight.</p> </div> <div class="paragraph"> <p></p> </div> <div class="ulist"> <ul> <li> <p><a href="#new-features">New features</a></p> </li> <li> <p><a href="#other-changes">Other changes</a></p> </li> <li> <p><a href="#whats-next">Outlook and what&#8217;s neext?</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="new-features">New Features</h2> <div class="sectionbody"> <div class="paragraph"> <p>While micro-releases generally do not include new features, there are situations where it makes sense to backport something that is new, particularly when the change is localized, adequately tested, and we&#8217;re confident that it will provide a strong benefit for the community utilizing stable releases. There are four major new features:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#error-handling-improvements">Error Handling improvements</a></p> </li> <li> <p><a href="#jdbc-schema-change-warning">JDBC sink should not ingest schema change topics</a></p> </li> <li> <p><a href="#mongodb-srv-debezium-server">MongoDB SRV support</a></p> </li> <li> <p><a href="#cassandra-deletes">Cassandra no longer skips batches with DELETE</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="error-handling-improvements">Error Handling improvements</h3> <div class="paragraph"> <p>In earlier releases, a connector configuration property <code>errors.max.retries</code> existed only for the SQL Server connector to control error handling behavior on specific connection-released failures. In short, this property allowed a connection-based failure to be retried up to the configured number of times before Debezium would hard fault. With 2.3.1.Final, we have unified this configuration option and its now available for all connectors.</p> </div> <div class="paragraph"> <p>This setting is useful when using Debezium particularly in non-Kafka Connect environments or where you may have configured Kafka Connect to not allow retry/back-off but you want to have that type of behavior in your Debezium source connector deployments. (<a href="https://issues.redhat.com/browse/DBZ-6573">DBZ-6573</a>)</p> </div> </div> <div class="sect2"> <h3 id="jdbc-schema-change-warning">JDBC sink should not ingest schema change topics</h3> <div class="paragraph"> <p>The JDBC sink connector was never designed to ingest events from the schema change topic; however, the connector should be resilient and provide you with an adequate message rather than an exception indicating that there may be a misconfiguration.</p> </div> <div class="paragraph"> <p>Starting with this micro-release, the JDBC sink connector will analyze the event in more detail and if we determine that the event is a schema change, the connector will provide a descriptive warning stating the problem and request that the configuration be adjusted to avoid ingesting the schema changes topic. We hope this clarity will solve situations where an overlooked configuration error leads to a confusing exception and instead provides a direct remedy. (<a href="https://issues.redhat.com/browse/DBZ-6655">DBZ-6655</a>)</p> </div> </div> <div class="sect2"> <h3 id="mongodb-srv-debezium-server">MongoDB SRV support</h3> <div class="paragraph"> <p>MongoDB SRV support is a recent addition to the MongoDB connector&#8217;s feature set; however, as a part of <a href="https://issues.redhat.com/browse/DBZ-6701">DBZ-6701</a>, we identified a situation where Debezium Server was unable to handle a MongoDB SRV connection string. We identified that the problem is related to the fact that Quarkus disables JNDI by default, and this creates an issue with the MongoDB driver with how it handles SRV connection strings.</p> </div> <div class="paragraph"> <p>With this micro-release, this problem is now resolved in Debezium 2.3. We specifically enable JNDI when building Debezium Server and this now allows the MongoDB SRV connection strings to work properly. A huge thanks to the community for helping diagnose this bug with the team!</p> </div> </div> <div class="sect2"> <h3 id="cassandra-deletes">Cassandra no longer skips batches with DELETE</h3> <div class="paragraph"> <p>The Cassandra connector experienced a rather critical regression with batch operations. Lets assume the following was observed from the Cassandra commit logs:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>BEGIN BATCH
DELETE FROM ...
INSERT INTO ...
INSERT INTO ...
APPLY BATCH</code></pre> </div> </div> <div class="paragraph"> <p>The issue (<a href="https://issues.redhat.com/brwose/DBZ-6576">DBZ-6576</a>) was this particular batch was being interpreted incorrectly and as a a result, the two following INSERT operations were being omitted from the connector&#8217;s event stream. Thanks to the work by the community, a patch was applied and included in Debezium 2.3.1.Final where this is no longer the case and the Debezium change event stream will contain the <code>DELETE</code> as well as all other events that follow.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-changes">Other changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium <strong>2.3.1.Final</strong> also includes quite a number of bug fixes and stability improvements, see below:</p> </div> <div class="ulist"> <ul> <li> <p>Oracle unsupported DDL statement - drop multiple partitions <a href="https://issues.redhat.com/browse/DBZ-6585">DBZ-6585</a></p> </li> <li> <p>Only Struct objects supported for [Header field insertion], found: null <a href="https://issues.redhat.com/browse/DBZ-6588">DBZ-6588</a></p> </li> <li> <p>MySQL parser cannot parse CAST AS dec <a href="https://issues.redhat.com/browse/DBZ-6590">DBZ-6590</a></p> </li> <li> <p>Refactor retry handling in Redis schema history <a href="https://issues.redhat.com/browse/DBZ-6594">DBZ-6594</a></p> </li> <li> <p>Excessive Log Message 'Marking Processed Record for Topic' <a href="https://issues.redhat.com/browse/DBZ-6597">DBZ-6597</a></p> </li> <li> <p>Oracle DDL parser does not properly detect end of statement when comments obfuscate the semicolon <a href="https://issues.redhat.com/browse/DBZ-6599">DBZ-6599</a></p> </li> <li> <p>Fixed DataCollections for table scan completion notificaiton <a href="https://issues.redhat.com/browse/DBZ-6605">DBZ-6605</a></p> </li> <li> <p>Oracle connector is not recoverable if ORA-01327 is wrapped by another JDBC or Oracle exception <a href="https://issues.redhat.com/browse/DBZ-6610">DBZ-6610</a></p> </li> <li> <p>Fatal error when parsing Mysql (Percona 5.7.39-42) procedure <a href="https://issues.redhat.com/browse/DBZ-6613">DBZ-6613</a></p> </li> <li> <p>MySQL ALTER USER with RETAIN CURRENT PASSWORD fails with parsing exception <a href="https://issues.redhat.com/browse/DBZ-6622">DBZ-6622</a></p> </li> <li> <p>Inaccurate documentation regarding additional-condition <a href="https://issues.redhat.com/browse/DBZ-6628">DBZ-6628</a></p> </li> <li> <p>Oracle connection SQLRecoverableExceptions are not retried by default <a href="https://issues.redhat.com/browse/DBZ-6633">DBZ-6633</a></p> </li> <li> <p>When Debezium Mongodb connector encounter authentication or under privilege errors, the connection between debezium and mongodb keeps going up. <a href="https://issues.redhat.com/browse/DBZ-6643">DBZ-6643</a></p> </li> <li> <p>Cannot delete non-null interval value <a href="https://issues.redhat.com/browse/DBZ-6648">DBZ-6648</a></p> </li> <li> <p>ConcurrentModificationException thrown in Debezium 2.3 <a href="https://issues.redhat.com/browse/DBZ-6650">DBZ-6650</a></p> </li> <li> <p>Dbz crashes on parsing Mysql Procedure Code (Statement Labels) <a href="https://issues.redhat.com/browse/DBZ-6651">DBZ-6651</a></p> </li> <li> <p>Vitess: Connector fails if table name is a mysql reserved word <a href="https://issues.redhat.com/browse/DBZ-6656">DBZ-6656</a></p> </li> <li> <p>Retriable operations are retried infinitely since error handlers are not reused <a href="https://issues.redhat.com/browse/DBZ-6670">DBZ-6670</a></p> </li> <li> <p>NotificationIT with Oracle xstream fails randomly <a href="https://issues.redhat.com/browse/DBZ-6672">DBZ-6672</a></p> </li> <li> <p>Flaky Oracle test: shouldCaptureChangesForTransactionsAcrossSnapshotBoundaryWithoutReemittingDDLChanges <a href="https://issues.redhat.com/browse/DBZ-6673">DBZ-6673</a></p> </li> <li> <p>Oracle DDL parser does not support column visibility on ALTER TABLE <a href="https://issues.redhat.com/browse/DBZ-6677">DBZ-6677</a></p> </li> <li> <p>MongoDB SRV protocol not working in Debezium Server <a href="https://issues.redhat.com/browse/DBZ-6701">DBZ-6701</a></p> </li> <li> <p>Add tzdata-java to UI installation Dockerfile <a href="https://issues.redhat.com/browse/DBZ-6713">DBZ-6713</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/2.3/release-notes#release-2.3.1-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 2.3.1.Final: <a href="https://github.com/isacandrei">Andrei Isac</a>, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/maxant">Ant Kutschera</a>, <a href="https://github.com/Artgyk">Artur Gukasian</a>, <a href="https://github.com/bdbene">Balint Bene</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/TechIsCool">David Beck</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/rk3rn3r">René Kerner</a>, <a href="https://github.com/jaegwonseo">Seo Jae-kwon</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats-next">Outlook and what&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 will continue to receive maintenance updates throughout this quarter with the next being early/middle August and likely additional follow-ups late August and September, all barring on community feedback on regressions.</p> </div> <div class="paragraph"> <p>Presently the team is actively working on Debezium 2.4 in parallel, and I would expect to see Alpha2 most likely at the early part of August. We have lots of new features planned for Debezium 2.4, so I urge you to take a look at our <a href="https://debezium.io/roadmap">road map</a> for more details.</p> </div> <div class="paragraph"> <p>I will be making a formal announcement next week about the new, upcoming Debezium community event. This will be hosted by the Debezium team to provide a space where the team, contributors, and community can openly collaborate. I&#8217;ll have more details next week, so be on the look out for this!</p> </div> <div class="paragraph"> <p>And finally, Current 2023 (aka Kafka Summit) is nearing. The event this year is being hosted in San Jose, California on September 26th and 27th. If you&#8217;re planning to attend, please drop me an email. I would enjoy an opportunity to talk with the community and gather your feedback about Debezium and how we can improve moving forward!</p> </div> <div class="paragraph"> <p>Until then&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It has been several weeks since we released the first installment of Debezium 2.3, and I&#8217;m excited to announce the next iteration of Debezium 2.3 with 2.3.1.Final. As with any micro-release, the focus is on stability and bug fixes, as well as adjustments to our documentation; however there are some changes that are noteworthy that I would like to take a few moments to highlight.]]></summary></entry><entry><title type="html">Debezium 2.4.0.Alpha1 Released</title><link href="https://debezium.io/blog/2023/07/17/debezium-2-4-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.4.0.Alpha1 Released"/><published>2023-07-17T00:00:00+00:00</published><updated>2023-07-17T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/07/17/debezium-2-4-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/07/17/debezium-2-4-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s been a busy month in Debezium-land, and it&#8217;s my pleasure to announce the first release of Debezium 2.4 series, <strong>2.4.0.Alpha1</strong>.</p> </div> <div class="paragraph"> <p>This release includes a plethora of changes, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.Alpha1">59 changes</a> to be exact, that cover a wide range of resolved issues, improvement to stability, new features, and several breaking changes. Let&#8217;s dive into each of these and discuss them in more depth.</p> </div> <div class="paragraph"> <p><!- more --></p> </div> <div class="ulist"> <ul> <li> <p><a href="#breaking-changes">Breaking changes</a></p> </li> <li> <p><a href="#new-features">New features</a></p> </li> <li> <p><a href="#other-changes">Other changes</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="breaking-changes">Breaking changes</h2> <div class="sectionbody"> <div class="sect3"> <h4 id="mongodb">MongoDB</h4> <div class="paragraph"> <p>The MongoDB connector explicitly preferred to use the secondary under specific scenarios previously. This explicit usage created problems for users who wanted to connect to the primary node. Thanks to recent changes (<a href="https://issues.redhat.com/browse/DBZ-6521">DBZ-6521</a>), this is no longer the case and the connection string setting is used instead.</p> </div> </div> <div class="sect3"> <h4 id="vitess">Vitess</h4> <div class="paragraph"> <p>The Vitess connector&#8217;s change event structure has been slightly adjusted thanks to changes (<a href="https://issues.redhat.com/browse/DBZ-6617">DBZ-6617</a>). The change event&#8217;s <code>source</code> information block now includes a new field that identifies the shard the event originated from.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="new-features">New Features</h2> <div class="sectionbody"> <div class="sect3"> <h4 id="offset_editor_example">Offset editor example</h4> <div class="paragraph"> <p>Users often express the need to manipulate connector offsets for various reasons. This can often be very difficult for those who may not be familiar with Kafka&#8217;s CLI tools or Java if you use Debezium Server. Thanks to a contribution (<a href="https://issues.redhat.com/browse/DBZ-6338">DBZ-6338</a>) by Nathan Smit, you can now use an editor to manipulate the offsets from the command line or a web-based interface.</p> </div> <div class="paragraph"> <p>Head to our <a href="https://github.com/debezium/debezium-examples/tree/main/offset-editor">examples repository</a> and follow the <code>README.md</code> to get started.</p> </div> </div> <div class="sect3"> <h4 id="error_handling">Error handling</h4> <div class="paragraph"> <p>Some Debezium connectors previously used a connector property, <code>errors.max.retries</code>. This property controlled how often a Debezium connector failure exception would be explicitly wrapped in a <code>RetriableException</code> but the connector threw the raw exception up to the runtime. While this may sound similar to Kafka Connect&#8217;s <code>errors.retry.timeout</code>, this effectively gave users a common way to deal with retries across multiple Debezium runtimes, including Kafka Connect, Debezium Server, and Debezium Embedded.</p> </div> <div class="paragraph"> <p>With this release, <a href="https://issues.redhat.com/browse/DBZ-6573">DBZ-6573</a> unifies this behavior making it available to all connectors.</p> </div> </div> <div class="sect3"> <h4 id="notify_initial_snapshot_progress">Notify initial snapshot progress</h4> <div class="paragraph"> <p>Debezium&#8217;s new notification subsystem provides an easy way to integrate third-party tools and applications with Debezium to gain insight into the ongoing change data capture process, above and beyond the traditional JMX approach. In 2.4, the notification subsystem now includes the ability to notify you about the status of the ongoing initial snapshot (<a href="https://issues.redhat.com/browse?DBZ-6416">DBZ-6416</a>).</p> </div> <div class="paragraph"> <p>Initial snapshot notifications are emitted with an <code>aggregatetType</code> of <code>Initial Snapshot</code> and contain a <code>type</code> field that exposes the current status of the snapshot. The possible values include: <code>STARTED</code>, <code>ABORTED</code>, <code>PAUSED</code>, <code>RESUMED</code>, <code>IN_PROGRESS</code>, <code>TABLE_SCAN_COMPLETED</code>, and <code>COMPLETED</code>.</p> </div> </div> <div class="sect3"> <h4 id="mysql_improvements">MySQL improvements</h4> <div class="paragraph"> <p>Thanks to a contribution provided by Harvey Yue (<a href="https://issues.redhat.com/browse/DBZ-6472">DBZ-6472</a>), the MySQL connector will use parallelization to generate schema events during its snapshot phase. This should improve the overall performance when capturing the schema for many tables in your database. We plan to investigate how this can be extended to other relational connectors.</p> </div> </div> <div class="sect3"> <h4 id="mongodb_improvements">MongoDB improvements</h4> <div class="paragraph"> <p>The MongoDB connector continues to see lots of active development. This release introduces several new features specifically for MongoDB, which include:</p> </div> <div class="ulist"> <ul> <li> <p>Cluster-wide privileges are no longer necessary when watching a single database or collection (<a href="https://issues.redhat.com/browse/DBZ-6182">DBZ-6182</a>).</p> </li> <li> <p>Read preference taken from connection string (<a href="https://issues.redhat.com/browse/DBZ-6468">DBZ-6468</a>, <a href="https://issues.redhat.com/browse/DBZ-6578">DBZ-6578</a>).</p> </li> <li> <p>Support authentication with TC MongoDB deployments (<a href="https://issues.redhat.com/browse/DBZ-6596">DBZ-6596</a>).</p> </li> </ul> </div> <div class="paragraph"> <p>As we continue to make further improvements to the MongoDB connector, please let us know if there are still rough edges or enhancements that will help streamline its usage.</p> </div> </div> <div class="sect3"> <h4 id="oracle_improvements">Oracle improvements</h4> <div class="paragraph"> <p>Debezium 2.4 supports several new Oracle data types, which include <code>XML_TYPE</code> and <code>RAW</code> (<a href="https://issues.redhat.com/browse/DBZ-3605">DBZ-3605</a>). Two new Oracle dependencies were necessary to support XML: <code>xdb</code> and <code>xmlparserv2</code>. These dependencies are not redistributable, so they&#8217;re not included in the connector plugin archive by default, much like the connector&#8217;s driver. You must obtain these directly from Maven Central or oracle, just like the driver dependency.</p> </div> <div class="paragraph"> <p>In addition, XML works similarly to <code>CLOB</code> and <code>BLOB</code> data types; therefore, the connector must be configured with <code>lob.enabled</code> set to <code>true</code> to ingest XML changes. We&#8217;d love to hear your feedback on this new feature as it&#8217;s been requested for quite some time.</p> </div> </div> <div class="sect3"> <h4 id="jdbc_sink_improvements">JDBC sink improvements</h4> <div class="paragraph"> <p>Thanks to a contribution from Nicholas Fwang (<a href="https://issues.redhat.com/browse/DBZ-6595">DBZ-6595</a>), the JDBC sink connector can now reference values from the change event&#8217;s <code>source</code> information block as a part of the connector&#8217;s configuration property <code>table.name.format</code>. If you want to reference such fields, simply use <code>${source.&lt;field-name&gt;}</code> in the configuration, and the field&#8217;s value will be used.</p> </div> <div class="paragraph"> <p>In addition, Roman Kudryashov also contributed the ability to resolve a row&#8217;s primary key from a header defined on the change event (<a href="https://issues.redhat.com/browse/DBZ-6602">DBZ-6602</a>). To use this new feature, specify the connector configuration property <code>primary.key.mode</code> as <code>record_header</code>. If the header value is a primitive type, you will need to define a <code>primary.key.fields</code> configuration similar to how you would if the event&#8217;s record key was a primitive. If the header value is a <code>struct</code> type, all fields of the structure will be used by default, but specifying the <code>primary.key.fields</code> property allows you to choose a subset of fields from the header as the key.</p> </div> </div> <div class="sect3"> <h4 id="spanner_improvements">Spanner improvements</h4> <div class="paragraph"> <p>It was possible due to certain conditions that a Spanner connector may not advance from the <code>START_INITIAL_SYNC</code> state during initialization. After investigation by Nancy Xu, a new configuration option was introduced to supply a configurable timeout. This can be done by adding the following to the connector&#8217;s configuration:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">connector.spanner.task.await.initialization.timeout=&lt;timeout in milliseconds&gt;</code></pre> </div> </div> </div> <div class="sect3"> <h4 id="debezium_ui_metrics">Debezium UI metrics</h4> <div class="paragraph"> <p>The Debezium UI project allows you to easily deploy any Debezium connector onto Kafka Connect using a web-based interface. This release has improved the interface by including several connector metrics (<a href="https://issues.redhat.com/browse/DBZ-5321">DBZ-5321</a>) on the main connector listing view. We&#8217;d love your feedback on this change and welcome any suggestions on other metrics you may find useful.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-changes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In addition, there were quite a number of stability and bug fixes that made it into this release. These include the following:</p> </div> <div class="ulist"> <ul> <li> <p>Mysql connector fails to parse statement FLUSH FIREWALL_RULES <a href="https://issues.redhat.com/browse/DBZ-3925">DBZ-3925</a></p> </li> <li> <p>Add the API endpoint to expose running connector metrics <a href="https://issues.redhat.com/browse/DBZ-5359">DBZ-5359</a></p> </li> <li> <p>Display critical connector metrics <a href="https://issues.redhat.com/browse/DBZ-5360">DBZ-5360</a></p> </li> <li> <p>Snapshot result not saved if LAST record is filtered out <a href="https://issues.redhat.com/browse/DBZ-5464">DBZ-5464</a></p> </li> <li> <p>Define and document schema history topic messages schema <a href="https://issues.redhat.com/browse/DBZ-5518">DBZ-5518</a></p> </li> <li> <p>Align query.fetch.size across connectors <a href="https://issues.redhat.com/browse/DBZ-5676">DBZ-5676</a></p> </li> <li> <p>Upgrade to Apache Kafka 3.5.0 <a href="https://issues.redhat.com/browse/DBZ-6047">DBZ-6047</a></p> </li> <li> <p>Remove downstream related code from UI Frontend code <a href="https://issues.redhat.com/browse/DBZ-6394">DBZ-6394</a></p> </li> <li> <p>Make Signal actions extensible <a href="https://issues.redhat.com/browse/DBZ-6417">DBZ-6417</a></p> </li> <li> <p>CloudEventsConverter throws static error on Kafka Connect 3.5+ <a href="https://issues.redhat.com/browse/DBZ-6517">DBZ-6517</a></p> </li> <li> <p>Dependency io.debezium:debezium-testing-testcontainers affects logback in tests <a href="https://issues.redhat.com/browse/DBZ-6525">DBZ-6525</a></p> </li> <li> <p>Cleanup duplicate jobs from jenkins <a href="https://issues.redhat.com/browse/DBZ-6535">DBZ-6535</a></p> </li> <li> <p>Implement sharded MongoDB ocp deployment and integration tests <a href="https://issues.redhat.com/browse/DBZ-6538">DBZ-6538</a></p> </li> <li> <p>Batches with DELETE statement first will skip everything else <a href="https://issues.redhat.com/browse/DBZ-6576">DBZ-6576</a></p> </li> <li> <p>Oracle unsupported DDL statement - drop multiple partitions <a href="https://issues.redhat.com/browse/DBZ-6585">DBZ-6585</a></p> </li> <li> <p>Only Struct objects supported for [Header field insertion], found: null <a href="https://issues.redhat.com/browse/DBZ-6588">DBZ-6588</a></p> </li> <li> <p>Support PostgreSQL coercion for UUID, JSON, and JSONB data types <a href="https://issues.redhat.com/browse/DBZ-6589">DBZ-6589</a></p> </li> <li> <p>MySQL parser cannot parse CAST AS dec <a href="https://issues.redhat.com/browse/DBZ-6590">DBZ-6590</a></p> </li> <li> <p>Refactor retry handling in Redis schema history <a href="https://issues.redhat.com/browse/DBZ-6594">DBZ-6594</a></p> </li> <li> <p>Excessive Log Message 'Marking Processed Record for Topic' <a href="https://issues.redhat.com/browse/DBZ-6597">DBZ-6597</a></p> </li> <li> <p>Support for custom tags in the connector metrics <a href="https://issues.redhat.com/browse/DBZ-6603">DBZ-6603</a></p> </li> <li> <p>Fixed DataCollections for table scan completion notification <a href="https://issues.redhat.com/browse/DBZ-6605">DBZ-6605</a></p> </li> <li> <p>Oracle connector is not recoverable if ORA-01327 is wrapped by another JDBC or Oracle exception <a href="https://issues.redhat.com/browse/DBZ-6610">DBZ-6610</a></p> </li> <li> <p>Fatal error when parsing Mysql (Percona 5.7.39-42) procedure <a href="https://issues.redhat.com/browse/DBZ-6613">DBZ-6613</a></p> </li> <li> <p>Build of Potgres connector fails when building against Kafka 2.X <a href="https://issues.redhat.com/browse/DBZ-6614">DBZ-6614</a></p> </li> <li> <p>Upgrade postgresql driver to v42.6.0 <a href="https://issues.redhat.com/browse/DBZ-6619">DBZ-6619</a></p> </li> <li> <p>MySQL ALTER USER with RETAIN CURRENT PASSWORD fails with parsing exception <a href="https://issues.redhat.com/browse/DBZ-6622">DBZ-6622</a></p> </li> <li> <p>Upgrade Quarkus to 3.2.0.Final <a href="https://issues.redhat.com/browse/DBZ-6626">DBZ-6626</a></p> </li> <li> <p>Inaccurate documentation regarding additional-condition <a href="https://issues.redhat.com/browse/DBZ-6628">DBZ-6628</a></p> </li> <li> <p>Oracle connection SQLRecoverableExceptions are not retried by default <a href="https://issues.redhat.com/browse/DBZ-6633">DBZ-6633</a></p> </li> <li> <p>Upgrade kcctl to 1.0.0.Beta3 <a href="https://issues.redhat.com/browse/DBZ-6642">DBZ-6642</a></p> </li> <li> <p>Cannot delete non-null interval value <a href="https://issues.redhat.com/browse/DBZ-6648">DBZ-6648</a></p> </li> <li> <p>Upgrade gRPC to 1.56.1 <a href="https://issues.redhat.com/browse/DBZ-6649">DBZ-6649</a></p> </li> <li> <p>ConcurrentModificationException thrown in Debezium 2.3 <a href="https://issues.redhat.com/browse/DBZ-6650">DBZ-6650</a></p> </li> <li> <p>Dbz crashes on parsing Mysql Procedure Code (Statement Labels) <a href="https://issues.redhat.com/browse/DBZ-6651">DBZ-6651</a></p> </li> <li> <p>CloudEvents converter is broken for JSON message deserialization <a href="https://issues.redhat.com/browse/DBZ-6654">DBZ-6654</a></p> </li> <li> <p>Vitess: Connector fails if table name is a mysql reserved word <a href="https://issues.redhat.com/browse/DBZ-6656">DBZ-6656</a></p> </li> <li> <p>Junit conflicts cause by test-containers module using transitive Junit5 from quarkus <a href="https://issues.redhat.com/browse/DBZ-6659">DBZ-6659</a></p> </li> <li> <p>Disable Kafka 2.x CRON trigger <a href="https://issues.redhat.com/browse/DBZ-6667">DBZ-6667</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.4.0.Alpha1%20ORDER%20BY%20component%20ASC">59 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/adamstrickland">Adam Strickland</a>, <a href="https://github.com/aozmen121">Alisa Houskova</a>, <a href="https://github.com/AnatolyPopov">Anatolii Popov</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/maxant">Ant Kutschera</a>, <a href="https://github.com/Artgyk">Artur Gukasian</a>, <a href="https://github.com/bdbene">Balint Bene</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/nicholas-fwang">Inki Hwang</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/rk3rn3r">René Kerner</a>, <a href="https://github.com/jaegwonseo">Seo Jae-kwon</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>This initial release of Debezium 2.4 is already packed with lots of new features and the team is only getting started. Looking at our <a href="https://debezium.io/roadmap">road map</a>, we&#8217;ve already tackled nearly half of our plans for 2.4, but much still remains including:</p> </div> <div class="ulist"> <ul> <li> <p>Single message transforms for TimescaleDB and Timestamps</p> </li> <li> <p>OpenLogReplicator ingestion for Oracle</p> </li> <li> <p>Ad-hoc blocking snapshots</p> </li> <li> <p>Parallelization of Debezium Embedded</p> </li> <li> <p>Parallel incremental snapshots for MongoDB</p> </li> <li> <p>Further improvements to Debezium UI</p> </li> </ul> </div> <div class="paragraph"> <p>We intend to stick to our approximate two week cadence, so expect Alpha2 at the start of August. Until then, please be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It&#8217;s been a busy month in Debezium-land, and it&#8217;s my pleasure to announce the first release of Debezium 2.4 series, 2.4.0.Alpha1. This release includes a plethora of changes, 59 changes to be exact, that cover a wide range of resolved issues, improvement to stability, new features, and several breaking changes. Let&#8217;s dive into each of these and discuss them in more depth. Breaking changes New features Other changes Breaking changes MongoDB The MongoDB connector explicitly preferred to use the secondary under specific scenarios previously. This explicit usage created problems for users who wanted to connect to the primary node. Thanks to recent changes (DBZ-6521), this is no longer the case and the connection string setting is used instead. Vitess The Vitess connector&#8217;s change event structure has been slightly adjusted thanks to changes (DBZ-6617). The change event&#8217;s source information block now includes a new field that identifies the shard the event originated from. New Features Offset editor example Users often express the need to manipulate connector offsets for various reasons. This can often be very difficult for those who may not be familiar with Kafka&#8217;s CLI tools or Java if you use Debezium Server. Thanks to a contribution (DBZ-6338) by Nathan Smit, you can now use an editor to manipulate the offsets from the command line or a web-based interface. Head to our examples repository and follow the README.md to get started. Error handling Some Debezium connectors previously used a connector property, errors.max.retries. This property controlled how often a Debezium connector failure exception would be explicitly wrapped in a RetriableException but the connector threw the raw exception up to the runtime. While this may sound similar to Kafka Connect&#8217;s errors.retry.timeout, this effectively gave users a common way to deal with retries across multiple Debezium runtimes, including Kafka Connect, Debezium Server, and Debezium Embedded. With this release, DBZ-6573 unifies this behavior making it available to all connectors. Notify initial snapshot progress Debezium&#8217;s new notification subsystem provides an easy way to integrate third-party tools and applications with Debezium to gain insight into the ongoing change data capture process, above and beyond the traditional JMX approach. In 2.4, the notification subsystem now includes the ability to notify you about the status of the ongoing initial snapshot (DBZ-6416). Initial snapshot notifications are emitted with an aggregatetType of Initial Snapshot and contain a type field that exposes the current status of the snapshot. The possible values include: STARTED, ABORTED, PAUSED, RESUMED, IN_PROGRESS, TABLE_SCAN_COMPLETED, and COMPLETED. MySQL improvements Thanks to a contribution provided by Harvey Yue (DBZ-6472), the MySQL connector will use parallelization to generate schema events during its snapshot phase. This should improve the overall performance when capturing the schema for many tables in your database. We plan to investigate how this can be extended to other relational connectors. MongoDB improvements The MongoDB connector continues to see lots of active development. This release introduces several new features specifically for MongoDB, which include: Cluster-wide privileges are no longer necessary when watching a single database or collection (DBZ-6182). Read preference taken from connection string (DBZ-6468, DBZ-6578). Support authentication with TC MongoDB deployments (DBZ-6596). As we continue to make further improvements to the MongoDB connector, please let us know if there are still rough edges or enhancements that will help streamline its usage. Oracle improvements Debezium 2.4 supports several new Oracle data types, which include XML_TYPE and RAW (DBZ-3605). Two new Oracle dependencies were necessary to support XML: xdb and xmlparserv2. These dependencies are not redistributable, so they&#8217;re not included in the connector plugin archive by default, much like the connector&#8217;s driver. You must obtain these directly from Maven Central or oracle, just like the driver dependency. In addition, XML works similarly to CLOB and BLOB data types; therefore, the connector must be configured with lob.enabled set to true to ingest XML changes. We&#8217;d love to hear your feedback on this new feature as it&#8217;s been requested for quite some time. JDBC sink improvements Thanks to a contribution from Nicholas Fwang (DBZ-6595), the JDBC sink connector can now reference values from the change event&#8217;s source information block as a part of the connector&#8217;s configuration property table.name.format. If you want to reference such fields, simply use ${source.&lt;field-name&gt;} in the configuration, and the field&#8217;s value will be used. In addition, Roman Kudryashov also contributed the ability to resolve a row&#8217;s primary key from a header defined on the change event (DBZ-6602). To use this new feature, specify the connector configuration property primary.key.mode as record_header. If the header value is a primitive type, you will need to define a primary.key.fields configuration similar to how you would if the event&#8217;s record key was a primitive. If the header value is a struct type, all fields of the structure will be used by default, but specifying the primary.key.fields property allows you to choose a subset of fields from the header as the key. Spanner improvements It was possible due to certain conditions that a Spanner connector may not advance from the START_INITIAL_SYNC state during initialization. After investigation by Nancy Xu, a new configuration option was introduced to supply a configurable timeout. This can be done by adding the following to the connector&#8217;s configuration: connector.spanner.task.await.initialization.timeout=&lt;timeout in milliseconds&gt; Debezium UI metrics The Debezium UI project allows you to easily deploy any Debezium connector onto Kafka Connect using a web-based interface. This release has improved the interface by including several connector metrics (DBZ-5321) on the main connector listing view. We&#8217;d love your feedback on this change and welcome any suggestions on other metrics you may find useful. Other fixes In addition, there were quite a number of stability and bug fixes that made it into this release. These include the following: Mysql connector fails to parse statement FLUSH FIREWALL_RULES DBZ-3925 Add the API endpoint to expose running connector metrics DBZ-5359 Display critical connector metrics DBZ-5360 Snapshot result not saved if LAST record is filtered out DBZ-5464 Define and document schema history topic messages schema DBZ-5518 Align query.fetch.size across connectors DBZ-5676 Upgrade to Apache Kafka 3.5.0 DBZ-6047 Remove downstream related code from UI Frontend code DBZ-6394 Make Signal actions extensible DBZ-6417 CloudEventsConverter throws static error on Kafka Connect 3.5+ DBZ-6517 Dependency io.debezium:debezium-testing-testcontainers affects logback in tests DBZ-6525 Cleanup duplicate jobs from jenkins DBZ-6535 Implement sharded MongoDB ocp deployment and integration tests DBZ-6538 Batches with DELETE statement first will skip everything else DBZ-6576 Oracle unsupported DDL statement - drop multiple partitions DBZ-6585 Only Struct objects supported for [Header field insertion], found: null DBZ-6588 Support PostgreSQL coercion for UUID, JSON, and JSONB data types DBZ-6589 MySQL parser cannot parse CAST AS dec DBZ-6590 Refactor retry handling in Redis schema history DBZ-6594 Excessive Log Message 'Marking Processed Record for Topic' DBZ-6597 Support for custom tags in the connector metrics DBZ-6603 Fixed DataCollections for table scan completion notification DBZ-6605 Oracle connector is not recoverable if ORA-01327 is wrapped by another JDBC or Oracle exception DBZ-6610 Fatal error when parsing Mysql (Percona 5.7.39-42) procedure DBZ-6613 Build of Potgres connector fails when building against Kafka 2.X DBZ-6614 Upgrade postgresql driver to v42.6.0 DBZ-6619 MySQL ALTER USER with RETAIN CURRENT PASSWORD fails with parsing exception DBZ-6622 Upgrade Quarkus to 3.2.0.Final DBZ-6626 Inaccurate documentation regarding additional-condition DBZ-6628 Oracle connection SQLRecoverableExceptions are not retried by default DBZ-6633 Upgrade kcctl to 1.0.0.Beta3 DBZ-6642 Cannot delete non-null interval value DBZ-6648 Upgrade gRPC to 1.56.1 DBZ-6649 ConcurrentModificationException thrown in Debezium 2.3 DBZ-6650 Dbz crashes on parsing Mysql Procedure Code (Statement Labels) DBZ-6651 CloudEvents converter is broken for JSON message deserialization DBZ-6654 Vitess: Connector fails if table name is a mysql reserved word DBZ-6656 Junit conflicts cause by test-containers module using transitive Junit5 from quarkus DBZ-6659 Disable Kafka 2.x CRON trigger DBZ-6667 Altogether, 59 issues were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Adam Strickland, Alisa Houskova, Anatolii Popov, Anisha Mohanty, Ant Kutschera, Artur Gukasian, Balint Bene, Bob Roldan, Chris Cranford, Harvey Yue, Hossein Torabi, Indra Shukla, Inki Hwang, Jakub Cechacek, Jiri Pechanec, Mario Fiore Vitale, Martin Medek, Nancy Xu, Ondrej Babec, René Kerner, Seo Jae-kwon, Stefan Miklosovic, Thomas Thornton, and Vojtech Juranek! What&#8217;s next? This initial release of Debezium 2.4 is already packed with lots of new features and the team is only getting started. Looking at our road map, we&#8217;ve already tackled nearly half of our plans for 2.4, but much still remains including: Single message transforms for TimescaleDB and Timestamps OpenLogReplicator ingestion for Oracle Ad-hoc blocking snapshots Parallelization of Debezium Embedded Parallel incremental snapshots for MongoDB Further improvements to Debezium UI We intend to stick to our approximate two week cadence, so expect Alpha2 at the start of August. Until then, please be sure to get in touch with us on the mailing list or our chat if you have any ideas or suggestions.]]></summary></entry><entry><title type="html">Debezium signaling and notifications - Part 2: Customisation</title><link href="https://debezium.io/blog/2023/07/10/custom-http-signaling-notification/" rel="alternate" type="text/html" title="Debezium signaling and notifications - Part 2: Customisation"/><published>2023-07-10T00:00:00+00:00</published><updated>2023-07-10T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/07/10/custom-http-signaling-notification</id><content type="html" xml:base="https://debezium.io/blog/2023/07/10/custom-http-signaling-notification/"><![CDATA[<div class="paragraph"> <p>Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the second installment in the series, where we will discuss how to customize the signal and notification channels in Debezium.</p> </div> <div class="paragraph"> <p>Debezium 2.3 introduced new improvements in signaling and notification capabilities. You can set up new signals and notification channels in addition to the pre-defined signals and notification channels offered by Debezium. This feature enables users to customize the system to suit their unique needs and combine it with their existing infrastructure or third-party solutions. It enables effective monitoring and a proactive response to data changes by precisely capturing and communicating signal events and triggering notifications through preferred channels.</p> </div> <div class="paragraph"> <p>The first article in this series, <a href="https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications/">Signaling and Notifications in Debezium</a>, provides an overview of the signaling and notification features in Debezium. It also discusses the available channels &amp; their use cases for various scenarios.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="custom_signal_notification_channel">Custom Signal &amp; Notification Channel</h2> <div class="sectionbody"> <div class="paragraph"> <p>In Debezium, it is possible to customize the signal and notification channels to fit specific requirements. For example, we can achieve a customization by creating an <code>HTTP</code> channel for both signals and notifications. This <code>HTTP</code> channel receives signals from an http endpoint, and notifications can be sent back to the endpoint upon the delivery of the signal.</p> </div> <div class="paragraph"> <p>Let&#8217;s explore an example that demonstrates how to create and utilize the <code>HTTP</code> signal and notification channels using the Debezium Postgres connector, a <a href="https://www.mock-server.com/">Mock Server</a> to send signals, and <a href="https://www.toptal.com/developers/postbin/">Postbin</a> to receive notifications via http endpoint.</p> </div> <div class="sect2"> <h3 id="setting_up_the_http_signal_channel">Setting up the <code>HTTP</code> Signal Channel:</h3> <div class="ulist"> <ul> <li> <p>Configure the Debezium Postgres connector to receive signals whenever a relevant database change occurs.</p> </li> <li> <p>Set up a service to send signals to Debezium using the <code>HTTP</code> channel. The service can be a database, a third-party application, or any other system that can send http requests. In this example, we will use the Mock Server to send signals to Debezium. The Mock Server is a service that can be used to mock http requests and responses.</p> </li> <li> <p>Configure the Mock Server to send the signals via a http endpoint using the appropriate HTTP method (e.g., POST).</p> </li> <li> <p>Customize the <code>HTTP</code> channel settings to define the http endpoint URL, authentication, headers, and any additional parameters as needed.</p> </li> </ul> </div> </div> <div class="sect2"> <h3 id="setting_up_the_http_notification_channel">Setting up the <code>HTTP</code> Notification Channel:</h3> <div class="ulist"> <ul> <li> <p>Once the signal is received and processed by Debezium, it can trigger the posting of a notification to a http endpoint. In this example, we will send the notification to the Postbin bin using the <code>HTTP</code> channel. Postbin is a service that can be used to receive http requests and view the request details.</p> </li> <li> <p>Customize the <code>HTTP</code> channel settings for notifications, create a bin in Postbin, and define the http endpoint URL, authentication, headers, and any additional parameters as needed.</p> </li> <li> <p>Forward the notification event to the http endpoint i.e Postbin bin using the appropriate HTTP method (e.g., POST). The notification payload can be customized as needed.</p> </li> </ul> </div> <div class="paragraph"> <p>The complete source code for this example in the blog post is provided in the Debezium examples repository under <a href="https://github.com/debezium/debezium-examples/tree/main/http-signaling-notification">http-signal-notification</a> directory.</p> </div> <div class="paragraph"> <p>Create a java project to build the <code>HTTP</code> signal and notification channels. Run the following command to create a new java project using Maven:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">mvn archetype:generate
    -DgroupId=io.debezium.examples
    -DartifactId=http-signaling-notification</code></pre> </div> </div> <div class="paragraph"> <p>Add the following dependency to the <code>pom.xml</code> file with the Debezium version (2.3 and later versions):</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>debezium-core<span class="tag">&lt;/artifactId&gt;</span>
    <span class="tag">&lt;version&gt;</span>2.3.0.Final<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>To receive signals using a Mock Server, create a Docker Compose file that defines the Mock Server service. The configuration for the Mock Server service will be as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="yaml"><span class="key">services</span>:
  <span class="key">mockServer</span>:
    <span class="key">image</span>: <span class="string"><span class="content">mockserver/mockserver:latest</span></span>
    <span class="key">ports</span>:
      - <span class="string"><span class="content">1080:1080</span></span>
    <span class="key">environment</span>:
      - <span class="string"><span class="content">MOCKSERVER_WATCH_INITIALIZATION_JSON=true</span></span>
      - <span class="string"><span class="content">MOCKSERVER_INITIALIZATION_JSON_PATH=/config/initializerJson.json</span></span>
    <span class="key">volumes</span>:
        - <span class="string"><span class="content">./initializerJson.json:/config/initializerJson.json</span></span></code></pre> </div> </div> <div class="paragraph"> <p>The environment variables <code>MOCKSERVER_WATCH_INITIALIZATION_JSON</code> and <code>MOCKSERVER_INITIALIZATION_JSON_PATH</code> are set to enable the Mock Server to watch for changes in the initialization JSON file and specify its path. The <code>initializerJson.json</code> file, which contains the http request and response information for the signals, is mounted into the Mock Server container.</p> </div> <div class="paragraph"> <p>The <code>initializerJson.json</code> file defines a mock http request to the path <code>/api/signal</code> with the query string parameter <code>code=10969</code>. When this request is received by the Mock Server, it will respond with a JSON body containing an <code>id</code>, <code>type</code>, and <code>data</code>. The response has a status code of 200, indicating a successful response. The definition of the <code>initializerJson.json</code> file is as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">[
  {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">httpRequest</span><span class="delimiter">&quot;</span></span> : {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">method</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">GET</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">path</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">/api/signal</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">queryStringParameters</span><span class="delimiter">&quot;</span></span> : {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">code</span><span class="delimiter">&quot;</span></span> : [<span class="string"><span class="delimiter">&quot;</span><span class="content">10969</span><span class="delimiter">&quot;</span></span>]
      }
    },
    <span class="key"><span class="delimiter">&quot;</span><span class="content">httpResponse</span><span class="delimiter">&quot;</span></span> : {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">body</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">{</span><span class="char">\&quot;</span><span class="content">id</span><span class="char">\&quot;</span><span class="content">:</span><span class="char">\&quot;</span><span class="content">924e3ff8-2245-43ca-ba77-2af9af02fa07</span><span class="char">\&quot;</span><span class="content">,</span><span class="char">\&quot;</span><span class="content">type</span><span class="char">\&quot;</span><span class="content">:</span><span class="char">\&quot;</span><span class="content">log</span><span class="char">\&quot;</span><span class="content">,</span><span class="char">\&quot;</span><span class="content">data</span><span class="char">\&quot;</span><span class="content">:{</span><span class="char">\&quot;</span><span class="content">message</span><span class="char">\&quot;</span><span class="content">: </span><span class="char">\&quot;</span><span class="content">Signal message received from http endpoint.</span><span class="char">\&quot;</span><span class="content">}}</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">statusCode</span><span class="delimiter">&quot;</span></span>: <span class="integer">200</span>
    }
  }
]</code></pre> </div> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p><code>id</code> : an arbitrary unique string that identifies a signal instance.</p> </li> <li> <p><code>type</code> : the type of signal to send. In this example, the type is <code>log</code>, and it requests the connector to add an entry to the connector&#8217;s log file. After the signal is processed, the connector prints the specified message in the log.</p> </li> <li> <p><code>data</code> : the JSON-formatted parameters to pass to a signal event. In this example, the <code>message</code> parameter is passed to the signal event.</p> </li> </ol> </div> <div class="paragraph"> <p>Create the <code>HTTP</code> signal channel by implementing the <code>SignalChannelReader</code> interface as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="directive">public</span> <span class="type">class</span> <span class="class">HttpSignalChannel</span> <span class="directive">implements</span> SignalChannelReader {
    <span class="directive">private</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">Logger</span> LOGGER = LoggerFactory.getLogger(HttpSignalChannel.class);

    <span class="directive">public</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">String</span> CHANNEL_NAME = <span class="string"><span class="delimiter">&quot;</span><span class="content">http</span><span class="delimiter">&quot;</span></span>;
    <span class="directive">private</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">List</span>&lt;SignalRecord&gt; SIGNALS = <span class="keyword">new</span> <span class="predefined-type">ArrayList</span>&lt;&gt;();
    <span class="directive">public</span> CommonConnectorConfig connectorConfig;

        <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">String</span> name() { <i class="conum" data-value="1"></i><b>(1)</b>
        <span class="keyword">return</span> CHANNEL_NAME;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> init(CommonConnectorConfig connectorConfig) { <i class="conum" data-value="2"></i><b>(2)</b>
        <span class="local-variable">this</span>.connectorConfig = connectorConfig;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">List</span>&lt;SignalRecord&gt; read() { <i class="conum" data-value="3"></i><b>(3)</b>
        <span class="keyword">try</span> {
            <span class="predefined-type">String</span> requestUrl = <span class="string"><span class="delimiter">&quot;</span><span class="content">http://mockServer:1080/api/signal?code=10969</span><span class="delimiter">&quot;</span></span>;

            <span class="comment">// send http request to the mock server</span>
            HttpClient httpClient = HttpClient.newHttpClient();
            HttpRequest request = HttpRequest.newBuilder()
                    .uri(<span class="predefined-type">URI</span>.create(requestUrl))
                    .GET()
                    .header(<span class="string"><span class="delimiter">&quot;</span><span class="content">Content-Type</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">application/json</span><span class="delimiter">&quot;</span></span>)
                    .build();

            <span class="comment">// read the response</span>
            HttpResponse&lt;<span class="predefined-type">String</span>&gt; response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
           <span class="keyword">if</span> (response.statusCode() == <span class="integer">200</span>) {
               ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();
               <span class="predefined-type">String</span> responseBody = response.body();

               <span class="comment">// parse the response body</span>
               JsonNode signalJson = mapper.readTree(responseBody);
               <span class="predefined-type">Map</span>&lt;<span class="predefined-type">String</span>, <span class="predefined-type">Object</span>&gt; additionalData = signalJson.has(<span class="string"><span class="delimiter">&quot;</span><span class="content">additionalData</span><span class="delimiter">&quot;</span></span>) ? mapper.convertValue(signalJson.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">additionalData</span><span class="delimiter">&quot;</span></span>), <span class="keyword">new</span> TypeReference&lt;&gt;() {}) : <span class="keyword">new</span> <span class="predefined-type">HashMap</span>&lt;&gt;();
               <span class="predefined-type">String</span> id = signalJson.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>).asText();
               <span class="predefined-type">String</span> type = signalJson.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>).asText();
               <span class="predefined-type">String</span> data = signalJson.get(<span class="string"><span class="delimiter">&quot;</span><span class="content">data</span><span class="delimiter">&quot;</span></span>).toString();
               SignalRecord signal = <span class="keyword">new</span> SignalRecord(id, type, data, additionalData);

               LOGGER.info(<span class="string"><span class="delimiter">&quot;</span><span class="content">Recorded signal event '{}' </span><span class="delimiter">&quot;</span></span>, signal);

               <span class="comment">// process the signal</span>
               SIGNALS.add(signal);
                } <span class="keyword">else</span> {
                    LOGGER.warn(<span class="string"><span class="delimiter">&quot;</span><span class="content">Error while reading signaling events from endpoint: {}</span><span class="delimiter">&quot;</span></span>, response.statusCode());
                }
            } <span class="keyword">catch</span> (<span class="exception">IOException</span> | <span class="exception">InterruptedException</span> e) {
                LOGGER.warn(<span class="string"><span class="delimiter">&quot;</span><span class="content">Exception while preparing to process the signal '{}' from the endpoint</span><span class="delimiter">&quot;</span></span>, e.getMessage());
                e.printStackTrace();
            }
        <span class="keyword">return</span> SIGNALS;
        }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> close() { <i class="conum" data-value="4"></i><b>(4)</b>
       SIGNALS.clear();
    }
}</code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The <code>name()</code> method returns the name of the signal channel. To enable Debezium to use the channel, specify the name <code>http</code> in the connector’s <code>signal.enabled.channels</code> property.</td> </tr> <tr> <td><i class="conum" data-value="2"></i><b>2</b></td> <td>The <code>init()</code> method can be used to initialize specific configuration, variables, or connections that the http channel requires.</td> </tr> <tr> <td><i class="conum" data-value="3"></i><b>3</b></td> <td>The <code>read()</code> method reads signal from the http endpoint and returns a list of <code>SignalRecord</code> objects which will be processed by the Debezium connectors.</td> </tr> <tr> <td><i class="conum" data-value="4"></i><b>4</b></td> <td>The <code>close()</code> method closes all allocated resources.</td> </tr> </table> </div> <div class="paragraph"> <p>Create the notification channel by implementing the <code>NotificationChannel</code> interface as shown below:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="directive">public</span> <span class="type">class</span> <span class="class">HttpNotificationChannel</span> <span class="directive">implements</span> NotificationChannel {
    <span class="directive">private</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">Logger</span> LOGGER = LoggerFactory.getLogger(HttpNotificationChannel.class);

    <span class="directive">public</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">String</span> CHANNEL_NAME = <span class="string"><span class="delimiter">&quot;</span><span class="content">http</span><span class="delimiter">&quot;</span></span>;
    <span class="directive">private</span> <span class="directive">static</span> <span class="directive">final</span> <span class="predefined-type">String</span> NOTIFICATION_PREFIX = <span class="string"><span class="delimiter">&quot;</span><span class="content">[HTTP NOTIFICATION SERVICE]</span><span class="delimiter">&quot;</span></span>;

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="predefined-type">String</span> name() { <i class="conum" data-value="1"></i><b>(1)</b>
        <span class="keyword">return</span> CHANNEL_NAME;
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> init(CommonConnectorConfig config) { <i class="conum" data-value="2"></i><b>(2)</b>
        <span class="comment">// custom configuration</span>
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> send(<span class="predefined-type">Notification</span> notification) { <i class="conum" data-value="3"></i><b>(3)</b>
        LOGGER.info(<span class="predefined-type">String</span>.format(<span class="string"><span class="delimiter">&quot;</span><span class="content">%s Sending notification to http channel</span><span class="delimiter">&quot;</span></span>, NOTIFICATION_PREFIX));
        <span class="predefined-type">String</span> binId = createBin();
        sendNotification(binId, notification);
    }

    <span class="directive">private</span> <span class="directive">static</span> <span class="predefined-type">String</span> createBin()  {
        <span class="comment">// Create a bin on the server</span>
        <span class="keyword">try</span> {
            HttpRequest request = HttpRequest.newBuilder()
                    .uri(<span class="keyword">new</span> <span class="predefined-type">URI</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">https://www.toptal.com/developers/postbin/api/bin</span><span class="delimiter">&quot;</span></span>))
                    .POST(HttpRequest.BodyPublishers.ofString(<span class="string"><span class="delimiter">&quot;</span><span class="content"> </span><span class="delimiter">&quot;</span></span>))
                    .build();

            HttpClient httpClient = HttpClient.newHttpClient();
            HttpResponse&lt;<span class="predefined-type">String</span>&gt; response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());

            <span class="keyword">if</span> (response.statusCode() == HTTP_CREATED) {
                <span class="predefined-type">String</span> binId = response.body().replaceAll(<span class="string"><span class="delimiter">&quot;</span><span class="content">.*</span><span class="char">\&quot;</span><span class="content">binId</span><span class="char">\&quot;</span><span class="content">:</span><span class="char">\&quot;</span><span class="content">([^</span><span class="char">\&quot;</span><span class="content">]+)</span><span class="char">\&quot;</span><span class="content">.*</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">$1</span><span class="delimiter">&quot;</span></span>);
                LOGGER.info(<span class="string"><span class="delimiter">&quot;</span><span class="content">Bin created: </span><span class="delimiter">&quot;</span></span> + response.body());
                <span class="keyword">return</span> binId;
            }
        } <span class="keyword">catch</span> (<span class="exception">URISyntaxException</span> | <span class="exception">InterruptedException</span> | <span class="exception">IOException</span> e) {
            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="exception">RuntimeException</span>(e);
        }
        <span class="keyword">return</span> <span class="predefined-constant">null</span>;
    }

    <span class="directive">private</span> <span class="directive">static</span> <span class="type">void</span> sendNotification (<span class="predefined-type">String</span> binId, <span class="predefined-type">Notification</span> notification) {
        <span class="comment">// Get notification from the bin</span>
        <span class="keyword">try</span> {
            ObjectMapper mapper = <span class="keyword">new</span> ObjectMapper();
            <span class="predefined-type">String</span> notificationString = mapper.writeValueAsString(notification);
            HttpRequest request = HttpRequest.newBuilder()
                    .uri(<span class="keyword">new</span> <span class="predefined-type">URI</span>(<span class="string"><span class="delimiter">&quot;</span><span class="content">https://www.toptal.com/developers/postbin/</span><span class="delimiter">&quot;</span></span> + binId))
                    .header(<span class="string"><span class="delimiter">&quot;</span><span class="content">Content-Type</span><span class="delimiter">&quot;</span></span>, <span class="string"><span class="delimiter">&quot;</span><span class="content">application/json</span><span class="delimiter">&quot;</span></span>)
                    .POST(HttpRequest.BodyPublishers.ofString(notificationString))
                    .build();

            HttpClient httpClient = HttpClient.newHttpClient();
            HttpResponse&lt;<span class="predefined-type">String</span>&gt; response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());

            <span class="keyword">if</span> (response.statusCode() == HTTP_OK) {
                LOGGER.info(<span class="string"><span class="delimiter">&quot;</span><span class="content">Notification received : </span><span class="delimiter">&quot;</span></span> + response.body());
            }
        } <span class="keyword">catch</span> (<span class="exception">URISyntaxException</span> | <span class="exception">InterruptedException</span> | <span class="exception">IOException</span> e) {
            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="exception">RuntimeException</span>(e);
        }
    }

    <span class="annotation">@Override</span>
    <span class="directive">public</span> <span class="type">void</span> close() { <i class="conum" data-value="4"></i><b>(4)</b>
    }
}</code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The <code>name()</code> method returns the name of the notification channel. To enable Debezium to use the channel, specify <code>http</code> in the connector’s <code>notification.enabled.channels</code> property.</td> </tr> <tr> <td><i class="conum" data-value="2"></i><b>2</b></td> <td>The <code>init()</code> method can be used to initialize specific configuration, variables, or connections that the channel requires.</td> </tr> <tr> <td><i class="conum" data-value="3"></i><b>3</b></td> <td>The <code>send()</code> method sends the notification to the channel. The notification contains the <code>SignalRecord</code> object which is processed by the Debezium connectors.</td> </tr> <tr> <td><i class="conum" data-value="4"></i><b>4</b></td> <td>The <code>close()</code> method closes all allocated resources.</td> </tr> </table> </div> <div class="paragraph"> <p>Declare the <code>HTTP</code> signal and notification channels in the <code>META-INF/services</code> directory under <code>io.debezium.pipeline.signal.SignalChannelReader</code> and <code>io.debezium.pipeline.notification.channels.NotificationChannel</code> files respectively.</p> </div> <div class="paragraph"> <p>Compile and export the Java project as a JAR file. This can be done using Maven or your preferred build tool. Copy the JAR file to the directory that contains the JAR file for the Debezium connector you want to use. For example, if you want to use the custom signal and notification channels with the Debezium Postgres connector, copy the JAR file to the <code>/kafka/connect/debezium-connector-postgres</code> directory.</p> </div> <div class="paragraph"> <p>This example provides a Docker Compose file with necessary services defined including the Mock Server, Zookeeper, Kafka Connect, and Postgres database.</p> </div> <div class="paragraph"> <p>To start the services, run the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">export DEBEZIUM_VERSION=2.3
docker-compose up -d</code></pre> </div> </div> <div class="paragraph"> <p>After ensuring that the services are up and running, and the Postgres database is ready to accept connections, the next step is to register the connector. This involves creating a connector configuration file. Let&#8217;s create a file named <code>register-postgres.json</code> with the following properties:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">inventory-connector</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="integer">5432</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">topic.prefix</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbserver1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.include.list</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">inventory</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">signal.enabled.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">http</span><span class="delimiter">&quot;</span></span>, <i class="conum" data-value="1"></i><b>(1)</b>
    <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enabled.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">http</span><span class="delimiter">&quot;</span></span> <i class="conum" data-value="2"></i><b>(2)</b>
  }
}</code></pre> </div> </div> <div class="colist arabic"> <table> <tr> <td><i class="conum" data-value="1"></i><b>1</b></td> <td>The <code>signal.enabled.channels</code> property specifies the signal channel to be used by the connector. In this case, the connector uses the <code>http</code> signal channel.</td> </tr> <tr> <td><i class="conum" data-value="2"></i><b>2</b></td> <td>The <code>notification.enabled.channels</code> property specifies the notification channel to be used by the connector. In this case, the connector uses the <code>http</code> notification channel.</td> </tr> </table> </div> <div class="paragraph"> <p>Now that we have the connector configuration file prepared, we can proceed to register the connector with Kafka Connect by executing the following command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -i -X POST -H &quot;Accept:application/json&quot; \
    -H  &quot;Content-Type:application/json&quot; http://localhost:8083/connectors/ \
    -d @register-postgres.json</code></pre> </div> </div> <div class="paragraph"> <p>Once the connector is successfully registered, you can review the connector logs to observe the signal events. The logs provide insights into the processing and progress of the connector, including any signal-related information. You will encounter log messages similar to the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">Recorded signal event 'SignalRecord{id='924e3ff8-2245-43ca-ba77-2af9af02fa07', type='log', data='{&quot;message&quot;:&quot;Signal message received from http endpoint.&quot;}', additionalData={}}'    [io.debezium.examples.signal.HttpSignalChannel]</code></pre> </div> </div> <div class="paragraph"> <p>Additionally, you might notice log messages related to notification events being sent to the Postbin. For example:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">[HTTP NOTIFICATION SERVICE] Sending notification to http channel   [io.debezium.examples.notification.HttpNotificationChannel]
Bin created: {&quot;binId&quot;:&quot;1688742588469-1816775151528&quot;,&quot;now&quot;:1688742588470,&quot;expires&quot;:1688744388470}   [io.debezium.examples.notification.HttpNotificationChannel]</code></pre> </div> </div> <div class="paragraph"> <p>It provides information about the notification event, such as the creation of a bin with a unique identifier (binId) and other relevant details. To retrieve the notification event from Postbin, fetch the <code>binId</code> from the log message and use it to request the corresponding notification event from Postbin. To view the notification event, you can access Postbin using the following URL: <code><a href="https://www.toptal.com/developers/postbin/b/:binId" class="bare">https://www.toptal.com/developers/postbin/b/:binId</a></code>. Replace <code>:binId</code> in the URL with the actual binId obtained from the connector logs.</p> </div> <div class="paragraph"> <p>The notification event sent to Postbin looks like the following:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/http_notification_postbin.png" class="responsive-image" alt="Postbin Preview"> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusion">Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this tutorial, we explored how to create custom signal and notification channels for Debezium connectors. We created a custom signal channel that receive a signal event from an HTTP endpoint. We also created a custom notification channel that sends a notification event to an HTTP endpoint.</p> </div> <div class="paragraph"> <p>Debezium&#8217;s comprehensive signaling and notification system offers seamless integration with third-party solutions, allowing users to stay informed about the state and progress of Debezium connectors. The system&#8217;s extensibility empowers users to customize both the signals and notification channels to fit their customized needs.</p> </div> <div class="paragraph"> <p>Stay tuned for Part 3 of this series where we will explore about JMX signaling and notifications. In the meantime, you can check out the Debezium documentation for more information about signal and notification channels.</p> </div> <div class="paragraph"> <p>If you have any questions or feedback, please feel free to reach out to us on the Debezium <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or the <a href="https://debezium.zulipchat.com/#narrow/stream/302529-community-general">#community-general</a> channel on the Zulip chat. We would love to hear from you!</p> </div> </div> </div>]]></content><author><name>Anisha Mohanty</name></author><category term="debezium"/><category term="features"/><category term="notifications"/><category term="signaling"/><category term="custom"/><category term="channels"/><summary type="html"><![CDATA[Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the second installment in the series, where we will discuss how to customize the signal and notification channels in Debezium. Debezium 2.3 introduced new improvements in signaling and notification capabilities. You can set up new signals and notification channels in addition to the pre-defined signals and notification channels offered by Debezium. This feature enables users to customize the system to suit their unique needs and combine it with their existing infrastructure or third-party solutions. It enables effective monitoring and a proactive response to data changes by precisely capturing and communicating signal events and triggering notifications through preferred channels. The first article in this series, Signaling and Notifications in Debezium, provides an overview of the signaling and notification features in Debezium. It also discusses the available channels &amp; their use cases for various scenarios.]]></summary></entry><entry><title type="html">Debezium for Oracle - Part 3: Performance and Debugging</title><link href="https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3/" rel="alternate" type="text/html" title="Debezium for Oracle - Part 3: Performance and Debugging"/><published>2023-06-29T12:00:00+00:00</published><updated>2023-06-29T12:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3</id><content type="html" xml:base="https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3/"><![CDATA[<div class="paragraph"> <p>This post is the final part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first installment of this series is found <a href="/blog/2022/09/30/debezium-oracle-series-part-1/">here</a> and the second installment is found <a href="/blog/2022/10/06/debezium-oracle-series-part-2">here</a>.</p> </div> <div class="paragraph"> <p>In this third and final installment, we are going to build on what we have done in the previous two posts, focusing on the following areas:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#performance">Performance optimizations</a></p> </li> <li> <p><a href="#debugging">How to debug and assess problems and how to resolve them</a></p> </li> </ul> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="what_to_know_before_we_start">What to know before we start</h2> <div class="sectionbody"> <div class="paragraph"> <p>This installment is going to focus on a number of highly technical Oracle database features, and these often are within the realm of an Oracle database administrator&#8217;s area of expertise. Depending on your permissions and roles within your environment, you may not necessarily have access to all the tools and commands that will be discussed herein, so you may need to coordinate with your database administrator if you are working in a non-local environment.</p> </div> <div class="paragraph"> <p>Additionally, this post picks where we last left off in <a href="/blog/2022/10/06/debezium-oracle-series-part-2/">Part 2</a>. We had previously deployed a full Kafka ecosystem with Zookeeper, a Kafka broker, and a Kafka Connect environment. These are not necessarily a requirement in this series, but if you would like to follow along manually, I encourage you to follow the steps in the earlier parts of the series to quickly get started.</p> </div> </div> </div> <div class="sect1"> <h2 id="what_is_oracle_and_why_is_it_complex">What is Oracle and why is it complex?</h2> <div class="sectionbody"> <div class="paragraph"> <p>I touched on the first part of this briefly in <a href="/blog/2022/09/30/debezium-oracle-series-part-1">Part 1</a>, but I believe it bears a much deeper answer to support some of the technical concepts we&#8217;re going to discuss today.</p> </div> <div class="paragraph"> <p>A common take-away from newcomers or those unfamiliar with Oracle often ask, "Why is everything with Oracle always complex?". Most often, these users either have little Oracle database experience or have exposure to other open source database solutions that in broad, general terms are easier and simpler to use, particularly directly out of the box. So why is that?</p> </div> <div class="paragraph"> <p>Oracle was first released to the public over four decades ago in 1979, making it the oldest and the most popular database in the world. In the top five most popular, Microsoft SQL Server is the next oldest at three decades released in 1989 and the others all being half the age of Oracle or less.</p> </div> <div class="paragraph"> <p>What has helped drive Oracle&#8217;s market share has been its ability to innovate fast, retain compatibility with existing database platform users, and being flexible enough to provide you features now that you may not need until the future. This has lent itself to allow Oracle to thrive unlike its competition, but we all know that flexibility often comes at the cost of other things; and traditionally that has been at the hand of ease of use. Oracle has a broad and extensive arsenal of tools at your disposal, but these are often tightly coupled leading to complex installations and configurations, but the advantage beyond its flexibility is that it&#8217;s the best at what it does, so often the trade-off is worth it.</p> </div> <div class="paragraph"> <p>As we go through the following parts of this installment, I hope this context provides a newfound perspective. While it may seem commonplace to think that many of Oracle&#8217;s oddities when compared to its competition are pain points, in reality they&#8217;re strengths that have perpetually defined Oracle at the forefront of a critical space in the age of big data.</p> </div> </div> </div> <div class="sect1"> <h2 id="performance">Performance</h2> <div class="sectionbody"> <div class="paragraph"> <p>When choosing to use change data capture (CDC) pipelines, low-latency is very often a driving factor. Whether you are using CDC to provide event-driven communication between microservices or complex data replication, it&#8217;s important that events arrive as soon as possible, so throughput and performance are often at the forefront of evaluating a solution&#8217;s merit, often immediately after reliability.</p> </div> <div class="paragraph"> <p>In this section, we&#8217;re going to cover several configuration properties of the Debezium Oracle connector that can allow you to optimize the connector&#8217;s performance based on your environment and needs.</p> </div> <div class="ulist"> <ul> <li> <p><a href="#performance-disk-io">Disk IO</a></p> </li> <li> <p><a href="#performance-redo-log-size">Redo Log Size</a></p> </li> <li> <p><a href="#performance-log-mining-strategy">Log Mining Strategy</a></p> </li> <li> <p><a href="#performance-batch-size">Batch size</a></p> </li> <li> <p><a href="#performance-query-filter-mode">Query filter mode</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="performance-disk-io">Disk IO</h3> <div class="paragraph"> <p>A fundamental aspect of the Debezium Oracle connector&#8217;s implementation is that it uses Oracle APIs to work with the LogMiner process to read the redo and archive logs. These logs are managed by two Oracle processes, the Oracle LogWriter (LGWR) and the Oracle Archiver (ARCH). The details around these two processes are less important to the discussion other than the fact they&#8217;re responsible for management of both the current redo logs used by the Oracle database and the creation of the archive logs that contain the past historical changes made to Oracle.</p> </div> <div class="paragraph"> <p>The Debezium Oracle connector uses the LogMiner API to read these redo and archive logs off disk and generate change events. Unfortunately, redo and archive logs cannot be retained on the database server indefinitely. Often, logs can be several gigabytes in size, and when combined with supplemental log configurations, an Oracle database can generate many logs in short periods of time, thus disk space gets consumed quickly. When these redo or archive logs exist on a networked drive or a high-latency device, this has a direct impact on LogMiner&#8217;s ability to read and provide the change event data to the connector as quickly as possible.</p> </div> <div class="paragraph"> <p>One possible way to improve performance is to coordinate with your Oracle database administrator to see if you can retain more archive logs longer even if its on a separate mounted disk but local to the database machine. Oracle provides the ability to define what is called <a href="https://docs.oracle.com/en/database/oracle/oracle-database/23/refrn/LOG_ARCHIVE_DEST.html#GUID-AACE967D-EF85-43F4-B895-5E510ABADCC3">log archive destinations</a>, and it supports up to a maximum of 31 different paths where archive logs can be written by the Archiver process.</p> </div> <div class="paragraph"> <p>It&#8217;s not uncommon that your database administrator may have configured several log archive destinations already for other processes such as GoldenGate, DataGuard, etc. If such paths have been defined, and they have a log retention policy that aligns with the ingestion rate, you can safely use Debezium with those destinations if one already exists. If no path has been defined, you can create a new one following the aforementioned Oracle documentation link.</p> </div> <div class="paragraph"> <p>To have Debezium use a specific log archive destination, the <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-archive-destination-name">log.mining.archive.destination.name</a> connector property must be provided.</p> </div> <div class="listingblock"> <div class="title">An example using LOG_ARCHIVE_DEST_5</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">log.mining.archive.destination.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">LOG_ARCHIVE_DEST_5</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The physical path to the logs will be obtained from Oracle directly, you only need to specify the destination name the database administrator configured.</p> </div> </td> </tr> </table> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>This configuration is only applicable for use with the Oracle LogMiner adapter and has no impact when ingesting changes using Oracle XStream.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="performance-redo-log-size">Redo Log Size</h3> <div class="paragraph"> <p>Oracle&#8217;s redo logs are used primarily for recovery and instance failures. When an instance is first created, the administrator provides a starting size for redo logs. If the size of the redo logs is too small or even too large, this can have a direct impact on the performance of your instance.</p> </div> <div class="paragraph"> <p>For example, the size of redo logs has a direct impact on how frequent the Oracle Archiver (ARCH) process transitions redo logs to archive logs, which is referred to as a <em>log switch</em>. Generally, Oracle recommends that administrators minimize the number of log switches in small windows, but this can vary depending on a number of factors like volume or logging configurations.</p> </div> <div class="paragraph"> <p>A log switch is a fairly expensive operation because it&#8217;s the moment in time when a redo log is copied by the Archiver process to an archive log and a new redo log is allocated. If there is ever a period when the Archiver process falls behind and all redo logs have filled, Oracle&#8217;s database can degrade or even halt if a checkpoint cannot occur because all current redo logs are full and awaiting archival.</p> </div> <div class="paragraph"> <p>If you are using an image of Oracle based on <a href="https://github.com/oracle/docker-images">Oracle Docker Images</a>, you will have noticed that by default the redo logs created are extremely small, several megabytes each. For development purposes, this is fine out of the box, but when using such an instance for any type of serious integration such as Debezium; this simply doesn&#8217;t work well, especially with the default mining strategy which we&#8217;ll discuss in more detail in the next section.</p> </div> <div class="paragraph"> <p>However, small redo log sizes aren&#8217;t the only problem. If the redo log files are sized too large, this can have an adverse impact on the read time from disk, making the gap while the connector waits for changes even longer as there is the need to perform more Disk IO due to larger files.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Resizing Oracle&#8217;s redo logs requires existing knowledge of the database server paths and where it is safe to store those files; therefore, since that information is environment dependent, we aren&#8217;t going to cover directly how to do this here. Oracle provides excellent <a href="https://docs.oracle.com">documentation</a> on how to perform this task.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Unfortunately, there isn&#8217;t a simple answer for what size you should use. This requires a bit of finesse, science, and heuristics of your environment to gauge what is the best choice, but this is something in your arsenal that could be adjusted if necessary.</p> </div> </div> <div class="sect2"> <h3 id="performance-log-mining-strategy">Log Mining Strategy</h3> <div class="paragraph"> <p>In <a href="/blog/post/2022/10/06/debezium-oracle-series-part-2#configure-oracle-redo-logs">Part 2</a>, we covered two <a href="/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-strategy">log mining strategies</a> for the Debezium Oracle connector. These strategies control how the connector interacts with Oracle LogMiner and how specific entries from the redo logs are ingested for both schema and table changes.</p> </div> <div class="paragraph"> <p>Redo logs store redo entries and not all redo entries store explicitly every pierce of data needed to re-construct the change that occurred. For example, DML operations (inserts, updates, deletes) do not refer to table or column names but rather object identifiers. These object identifier and version details change in the data dictionary as column or table modifications (DDL changes) occur. This means that the identifier and/or its version will differ from a redo entry for the same table before and after a schema change.</p> </div> <div class="paragraph"> <p>The log mining strategy controls precisely how redo entries are interpreted by Oracle LogMiner, primarily by either writing the data dictionary to the end of the redo logs or omitting this step. There are benefits to using either strategy and we&#8217;re going to dive into what those are and why you may use one strategy over another.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Default Mining Strategy</dt> <dd> <p>The default mining strategy is the safest choice, but it is also the most expensive. This strategy will append a copy of the data dictionary to the redo logs when a log switch is observed.<br> <br> This strategy&#8217;s main benefit is schema and data changes are ingested seamlessly by Oracle LogMiner. In other words, if an <code>INSERT</code> is followed by an <code>ALTER TABLE</code> and that is followed by an <code>UPDATE</code>, Oracle LogMiner will safely deduce the right table and column names from the old and the new object ids and versions. This means that Debezium will be able to safely ingest that change event as one might expect.<br> <br> The unfortunate pain point of this strategy is that it&#8217;s an expensive step at each log switch.<br> <br> First, it requires that the connector append a copy of the data dictionary periodically to the redo logs and Oracle performs a full log switch (all log groups perform a switch) after writing the dictionary. This means that more archive logs will be generated than when using the online catalog strategy we&#8217;ll discuss momentarily.<br> <br> Secondly, it also requires that when a LogMiner process begins to mine the redo logs, it must first read and prepare a section of the SGA with all the dictionary metadata so that resolution of table and column names can happen properly. Depending on the size of the redo logs, and more appropriately the dictionary segment of the logs, this can take upwards of several minutes to prepare. So you can probably guess that when you combine this strategy with a poorly sized redo logs, this can easily create a performance bottleneck.</p> </dd> </dl> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>It is <strong>not recommended</strong> to ever deploy multiple Oracle connectors using this strategy, but instead use a single Oracle connector.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1">Online Catalog Strategy</dt> <dd> <p>The online catalog mining strategy is used when specifying the <code>log.mining.strategy</code> connector configuration property with the value <code>online_catalog</code>. Unlike the default mining strategy, this strategy does not write any additional data to the redo logs, but instead, relies on the current data dictionary to resolve table and column names.<br> <br> The benefit to this strategy is that since we are not writing any dictionary details to the redo logs, redo logs will only transition to archive logs based on existing database activity. In short, Debezium will not influence this frequency beyond the additional supplemental logging configuration required, making it easier to manage the volume of archive logs created. Secondly, because no dictionary details are written to the logs and the number of log switches remains constant to existing behavior, a mining session starts nearly instantaneously and there is no need for LogMiner to prepare any dictionary metadata as the existing data dictionary satisfies that requirement as-is.<br> <br> Unfortunately, this strategy does have a single restriction and that is schema changes are not observed seamlessly. In other words, if a redo entry refers to an object id/version that does not match the object id/version in the online data dictionary, Oracle LogMiner is incapable of reconstructing the SQL for that operation.<br> <br> However, schema changes can be handled with this strategy, but it requires doing schema changes in a lock-step fashion. In other words, you would you halt changes on the table, wait until the last change for the table has been captured by Debezium, apply the schema change, wait for the schema change to be emitted by Debezium, and finally resume allowing changes to the data in the table.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This strategy provides the optimal performance gain both for Oracle and the connector.</p> </div> <div class="paragraph"> <p>The only requirement is that if a table&#8217;s schema isn&#8217;t static, and you may have changes to it periodically, if you can perform the schema changes in lock-step as described above, you can safely perform schema changes using this strategy; otherwise schema changes should be avoided on the table(s) being captured.</p> </div> <div class="paragraph"> <p>Finally, this strategy should be used if deploying multiple Oracle connectors on the same Oracle database.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>In conclusion, the mining strategy chosen can have significant impacts on the performance of the database as well as the ingestion rate of the Debezium Oracle connector. It&#8217;s important to weigh the benefits and consequences of this choice based on what is possible given your environment.</p> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>There is an effort underway to bridge these two strategies and deliver a solution that provides all the performance benefits of the online catalog strategy and the seamless schema management provided by the default mining strategy. The progress for this effort can be found in <a href="https://issues.redhat.com/browse/DBZ-3401">DBZ-3401</a>.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="performance-batch-size">Batch size</h3> <div class="paragraph"> <p>The Debezium Oracle connector uses an adaptive batch size algorithm to determine the number of rows that will be fetched per database call. The algorithm is controlled by the following configuration properties:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>log.mining.batch.size.default</code></dt> <dd> <p>This specifies the default number of rows that will be fetched per database call.</p> </dd> <dt class="hdlist1"><code>log.mining.batch.size.min</code></dt> <dd> <p>This specifies the minimum number of database rows that will be fetched per database call.</p> </dd> <dt class="hdlist1"><code>log.mining.batch.size.max</code></dt> <dd> <p>This specifies the maximum number of database rows that will be fetched per database call.</p> </dd> </dl> </div> <div class="paragraph"> <p>These settings give the connector the ability to read more data and reduce network latency when the connector has fallen behind or observed a large transaction in the logs at the expense of consuming more SGA and JVM memory temporarily and using less SGA and JVM memory when the connector has caught up to near real-time changes.</p> </div> <div class="paragraph"> <p>The connector defaults for these are great starting points, but depending on your change event volume, it may be wise to increase or even shrink these settings based on your environment to improve performance.</p> </div> </div> <div class="sect2"> <h3 id="performance-query-filter-mode">Query filter mode</h3> <div class="paragraph"> <p>Any developer who has ever worked on a SQL-based application will tell you that just because a query performs well in one environment or at one point in time doesn&#8217;t mean that the same query will be as efficient in another environment or even the future as the data set changes. That&#8217;s why with Debezium 2.3, we added a new feature called <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-query-filter-mode">log.mining.query.filter.mode</a>. Through various discussions with Oracle community users with various installations, volume sizes, and integrations, we concluded that the LogMiner query used by the Oracle connector simply cannot be a one-size fits all solution.</p> </div> <div class="paragraph"> <p>In order to be the most efficient, we needed to provide the user community with a way they can tune the LogMiner query that best satisfies their configuration and their environment. There are currently three options for how the LogMiner query is constructed and each influence how the query&#8217;s where-clause is generated.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>none</code></dt> <dd> <p>Specifies that no additional predicates are added to the LogMiner query.<br> <br> Instead, all filtering is delegated primarily to the Oracle connector&#8217;s Java runtime and not the database query. This has the highest network bandwidth usage of all the options and can have the highest throughput depending on the volume and the data-set of redo entries. For lower volume installations, this can easily perform the fastest but does not scale well as the volume of redo entries increases or if the data-set of interest is smaller than the total of the data-set.</p> </dd> <dt class="hdlist1"><code>in</code></dt> <dd> <p>Specifies that the schema and table include/exclude filters are applied using a SQL in-clause.<br> <br> By default, the include/exclude configuration options support comma-separated lists of regular expressions; however, if you elect to avoid the use of regular expressions, you can apply database-level filters to the LogMiner query more efficiently by using this query filter mode. An in-clause is much more efficient over using disjunctions or Oracle&#8217;s <code>REGEXP_LIKE</code> operator which we&#8217;ll discuss with the next option. This also performs extremely well if you have a lot of schema or table include/exclude list options defined in your configuration. And finally, because this choice performs database-level filtering, this reduces the network latency and only returns the necessary rows to the connector.</p> </dd> <dt class="hdlist1"><code>regex</code></dt> <dd> <p>Specifies that the schema and table include/exclude filters are applied using the SQL operator <code>REGEXP_LIKE</code>.<br> <br> Since the include/exclude configuration options support comma-separated lists of regular expressions, must be used instead of <code>in</code> when using regular expressions. While this option performs database-level filtering much like the in-clause choice, the use of regular expressions degrades in performance as more include/exclude options are specified in the connector configuration. Therefore, in order to maximize performance, its generally best when using regular expressions to write as few expressions that match the most tables or schemas as possible to reduce the number of predicates appended to the query.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>As of Debezium 2.3, the default is <code>none</code>, so you can gain additional performance by specifically configuring the <code>log.mining.query.filter.mode</code> to use <code>in</code> ir <code>regex</code> depending on the values provided in your include/exclude list configuration properties.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="debugging">Debugging</h2> <div class="sectionbody"> <div class="paragraph"> <p>As much as I would like to think Software Engineering is butterflies and flowers; it&#8217;s far from the truth and managing an environment that runs software is no different. When problems occur, it&#8217;s important to have the knowledge to self-diagnose and get back to a running state as quickly as possible. So we&#8217;re going down the rabbit-hole and discuss a number of common errors that we&#8217;ve seen, how do you debug those errors, and what might be the potential fixes.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">None of the log files contains offset SCN, re-snapshot is required</dt> <dd> <p>I&#8217;m fairly certain that at some point most Oracle connector users have seen this error in the logs, whether during PoC design or testing, but hopefully not production. The error message itself is relatively clear, but what is often not clear is "why did this happen".<br> <br> For other databases, their transaction logs only contain committed changes that are then consumed by Debezium. Unfortunately, Oracle does not do this and instead writes every single change to the transaction logs, even if the change is later rolled back due to a constraint violation or an explicit user or system rollback. This means that reading the changes from the Oracle redo and archive logs isn&#8217;t as simple as reading from position X until the end of the file and then repeat with the next log in sequence. Instead, the connector must maintain what we call a low and high watermark SCN, or if you&#8217;re familiar with the connector&#8217;s offsets these are represented as <code>scn</code> and <code>commit_scn</code>.<br> <br> The low watermark or <code>scn</code> represents the safe resume point in the redo logs. Generally this points to the position in the logs where the oldest in-progress transaction started. The high watermark or <code>commit_scn</code> represents the position in the logs where we last emitted a transaction batch for a given redo thread. This means that the changes in between these two values are a mixture of uncommitted changes, committed changes, or rolled back changes.<br> <br> When the connector starts, the low watermark or <code>scn</code> read from the offsets is compared to the oldest available archive log in Oracle. If the archive log begins with a system change number that comes after this <code>scn</code> value, this error will occur.<br> <br> Long-running transactions directly impact the low watermark or <code>scn</code> position. If a transaction remains active for longer than your archive log retention policy and the connector is restarted due to a re-balance or failure, this error can occur. If you suspect long-running transactions, you can configure the <code>log.mining.transaction.retention.ms</code> property in order to discard a transaction that lives longer than the specified value. While this does cause data loss as that transaction&#8217;s changes are discarded, it does allow the low watermark to safely advance forward at a reasonable pace even when long-running transactions occur. You should set the transaction retention period to a value less than your archive log retention period.<br> <br> Another use case that can raise this error is if you are capturing changes from an Oracle database with a low volume of changes. In particular, if you are using an older version of Debezium where the LogMiner query applied database-level filters or you&#8217;ve configured the new query filter mode to apply database-level filters, it&#8217;s possible that the connector may go extended periods of time without observing a single change event. Since offset data only synchronizes when the connector sends an event to Kafka, low volumes of changes in a large window of time can mean those Kafka offsets become stale and if a restart occurs, this error could happen. In this use case, configuring both the <code>heartbeat.internval.ms</code> and <code>heartbeat.action.query</code> connector properties is a great way to guarantee that there is some activity flowing to Kafka to keep those offsets from becoming stale.</p> </dd> <dt class="hdlist1">ORA-01555: snapshot too old</dt> <dd> <p>This specific error is most commonly observed during the connector&#8217;s initial snapshot. The Oracle connector relies on what is called flashback queries during the initial snapshot phase.<br> <br> A flashback query is a standard SELECT statement that uses a system change number in order to generate a result-set based on the state of the data at that given point in the database&#8217;s lifetime. This can be useful for a variety of reasons, including being able to restore objects without the need for media recovery because Oracle is capable of retaining that previous state for a certain period of time. The data returned by these queries use the Automatic Undo Management (AUM) subsystem and rely on the undo data area where transactions are recorded and retained for a given period of time, configurable based on the database parameter <code>UNDO_RETENTION</code>.<br> <br> If the SCN used for the flashback query becomes too old and the undo retention no longer maintains historical data for that system change number, Oracle will report an ORA-01555 error that the snapshot is now too old. When this happens during your initial snapshot, the snapshot will need to be retaken from the beginning and unless you reconfigure Oracle&#8217;s undo retention period to allow for a longer retention time, rerunning the snapshot on the same data set will result in the same outcome.<br> <br> So either a) have your DBA increase the <code>UNDO_RETENTION</code> database parameter temporarily or b) use a schema-only snapshot and then rely on incremental snapshots to generate the initial data set from your existing table data.</p> </dd> <dt class="hdlist1">Streaming changes takes several minutes to appear</dt> <dd> <p>Occasionally users will notice there is a latency when the connector first starts or at specific periods during the connector&#8217;s lifetime. One of the best ways to identify what is going on is to coordinate with your DBA and to take a close look at your database&#8217;s alert log, which records all the LogMiner and XStream interactions that Debezium performs with your database. But most often this latency is quite common for users who use the default log mining strategy.<br> <br> The default mining strategy that we covered earlier performs a what is called a data dictionary build step and depending on your database, this can take some time to be written to your redo logs and then parsed by the LogMiner process. It&#8217;s not uncommon for this to take 30 seconds up to several minutes to complete, and when using the default mining strategy, this process occurs on each log switch.<br> <br> So we normally suggest that if you experience this latency frequently that you check the frequency of your log switches. If your database is performing excessive log switches within a small window that does not adhere to Oracle&#8217;s guidelines, your DBA may need to tune the database accordingly. Reducing the frequency of log switches increases the time that Debzium can reuse the same log file for mining and therefore reduces the need to build and parse the data dictionary.<br> <br> If your table schema does not change often or won&#8217;t change at all, you can reconfigure the connector to use the <code>online_catalog</code> mining strategy as an alternative. This avoids the writing the data dictionary to the redo logs and the parse phase performed by LogMiner, greatly increasing the speed at which a mining session begins at both connector start-up and at each log switch interval.</p> </dd> <dt class="hdlist1">How do you know if an archive log can be deleted?</dt> <dd> <p>Database administrators typically keep archive logs on the database server for a short period of time before they&#8217;re removed. This interval varies and depends on a number of factors including the frequency the logs are created, their respective sizes, and the physical space available on the server. What is most important is that if Debezium requires a specific archive log that it remains available until it&#8217;s no longer needed for CDC.<br> <br> The easiest way to determine what logs are needed is via JMX metrics, looking at the field <code>OffsetScn</code>. This field references the system change number where the connector will resume from in case of a restart, and so any archive or redo log that contains this system change number or comes after this change number must remain available.<br> <br> Debezium 2.4 will add another JMX metric that will provide the cut-off timestamp for archive logs based on this <code>OffsetScn</code>. This means that you will be able to use this timestamp directly in shell scripts to compare the filesystem&#8217;s timestamp with the one from JMX metrics, safely knowing which logs can must be retained and which can safely be removed via RMAN.</p> </dd> <dt class="hdlist1">Memory footprint, how do you manage it efficiently?</dt> <dd> <p>Due to the nature of how transaction data is written to the Oracle archive and redo logs, a buffer of the transaction state must be managed by the connector. Under ideal circumstances, this buffer maintains short-lived data, the transaction starts, we buffer its relevant changes, and we observe the rollback or commit, and the data managed in the buffer is handled and the buffer cleared.<br> <br> Because the connector buffers transactions, it&#8217;s extremely important that you have some prior knowledge of your environment&#8217;s transaction patterns. If this information can vary and cannot be predicted, you may want to consider using an alternative buffer type than the default Heap (memory) based buffer as this can easily lead to <code>OutOfMemory</code> exceptions under these circumstances when memory is configured too low.<br> <br> Please refer to the documentation about <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-event-buffering">Event Buffering</a>. The Oracle connector offers two Infinispan based solutions that allow the connector store the buffer off-heap, reducing the connector&#8217;s memory footprint and being capable of dealing with very large transactions seamlessly.</p> </dd> </dl> </div> <div class="paragraph"> <p>Lastly, we have composed a collection of <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-frequently-asked-questions">frequently asked questions</a> in the documentation. We generally do our best to keep the most commonly asked questions there as a reference. Please take a moment and read through those and if you suspect anything may be missing, please open a <a href="https://issues.redhat.com/projects/DBZ">Jira issue</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="wrap-up">Wrapping up</h2> <div class="sectionbody"> <div class="paragraph"> <p>I really hope this series on the Oracle connector has been helpful and informative. We&#8217;ve covered topics ranging from installation, configuration, deploying the connector, performance optimizations, and how to debug or evaluate specific common use cases we hear from the community.</p> </div> <div class="paragraph"> <p>As I mentioned earlier, Oracle is unlike most other database platforms and requires a bit more care and precision to maximize it&#8217;s potential. When evaluating the Oracle connector, it is important that you coordinate with an Oracle administrator professional to make sure that you&#8217;re maximizing the connector&#8217;s potential, particularly if performance is a critical metric in your evaluation.</p> </div> <div class="paragraph"> <p>As always, if you have questions about anything related to the content in this post or about something you observe in your environment, the team will do their best to provide you with the answers you need either by using our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or reaching out to us on our <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">chat</a>.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="debezium"/><category term="oracle"/><category term="examples"/><summary type="html"><![CDATA[This post is the final part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first installment of this series is found here and the second installment is found here. In this third and final installment, we are going to build on what we have done in the previous two posts, focusing on the following areas: Performance optimizations How to debug and assess problems and how to resolve them]]></summary></entry><entry><title type="html">Debezium signaling and notifications - Part 1</title><link href="https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications/" rel="alternate" type="text/html" title="Debezium signaling and notifications - Part 1"/><published>2023-06-27T10:11:11+00:00</published><updated>2023-06-27T10:11:11+00:00</updated><id>https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications</id><content type="html" xml:base="https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications/"><![CDATA[<div class="paragraph"> <p>Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the first installment in the series, where we will introduce the signaling and notification features offered by Debezium and discuss the available channels for interacting with the platform.</p> </div> <div class="paragraph"> <p>In the subsequent parts of this series, we will delve deeper into customizing signaling channels and explore additional topics such as JMX signaling and notifications.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>In today&#8217;s interconnected software applications and systems, seamlessly integrating with other products is crucial for building robust and efficient solutions. One of the key challenges in building integrated software ecosystems is establishing effective communication channels between different components and services.</p> </div> <div class="paragraph"> <p>Debezium introduces a comprehensive signaling and notification system providing seamless integration with any third-party solution. Combining the signal and notification systems, you can now orchestrate complex yet efficient pipelines to stay informed about the status and progress of the state managed by Debezium.</p> </div> <div class="paragraph"> <p>The new signaling system has been re-designed to be extensible, providing a simplified approach to adding new, custom signal channels on top of the various implementations available. Let&#8217;s explore the different signals and notification channels, covering their functionality and describing their benefits.</p> </div> <div class="sect1"> <h2 id="signaling_in_debezium">Signaling in Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Signaling in Debezium refers to the mechanism through which users can trigger specific actions within the product. It allows users to interact with Debezium and control some behaviors. One notable use case of signaling is the execution of incremental snapshots, which permits to start an ad-hoc snapshot of database.</p> </div> <div class="paragraph"> <p>Debezium provides different signaling channels through which users can send signals to the platform. Let&#8217;s explore the available signaling channels:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Signaling through Database Tables</dt> <dd> <p>In earlier versions of Debezium, signaling required a dedicated database table where you would insert specific records to trigger Debezium to execute a task. For example, an inserted row could initiate an incremental snapshot, prompting the connector to run a snapshot operation based on the given signal criteria.</p> </dd> <dt class="hdlist1">Signaling through Kafka Topics</dt> <dd> <p>In earlier versions of Debezium, you could send signals via a configured Kafka topic; however, this was only available for MySQL using read-only access and global transaction identifiers (GTID) behavior. With Debezium 2.3, the Kafka signal channel is available to all connectors.<br> <br> This enhancement provides a simplified integration approach and a unified and consistent approach for signals across all supported Debezium databases. You can send signals to a specific Kafka topic, and Debezium will consume and process that signal as though it originated from the signal table itself.<br> <br> Using a Kafka topic for signals provides several advantages. First, it aligns with event-driven design, making it a natural fit with change data capture and Debezium. Additionally, it provides a secure way to send signals to Debezium without necessarily providing the user with direct access to the underlying database.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Even when using the Kafka signal approach, the incremental snapshot feature still requires the presence and use of the signaling table to manage some bookkeeping needed for the incremental snapshot process. You can only omit the signal table when using MySQL in a read-only way with global transaction identifiers (GTIDs) enabled.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1">Signaling through File</dt> <dd> <p>You can trigger signals from the file system thanks to a contribution by the Debezium user community. This approach is an excellent alternative when not relying on messaging infrastructures like Kafka combined with Debezium Server or the embedded engine or when you cannot use the database for signals.</p> </dd> <dt class="hdlist1">Signaling through Java Management Extensions (JMX)</dt> <dd> <p>With this channel, you can send signals calling the signal operation exposed through a dedicated MBean. You can do it by connecting to the MBeanServer with your preferred client. A dedicated post about this feature will follow.</p> </dd> <dt class="hdlist1">Signaling through custom channel</dt> <dd> <p>We also redesigned the signaling mechanism to be extensible. You can provide your implementations and use it all via configuration. A dedicated post about this feature will follow.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="notification_in_debezium">Notification in Debezium:</h2> <div class="sectionbody"> <div class="paragraph"> <p>Notifications focus on retrieving status and progress information about Debezium and its internal processes. It provides insights into the execution and completion of tasks such as initial snapshots or incremental snapshots. Debezium generates notifications to keep users informed about the progress of these tasks, facilitating monitoring and decision-making based on real-time information. By leveraging the notification functionality, users can track the state of Debezium and take appropriate actions based on the received notifications.</p> </div> <div class="paragraph"> <p>Debezium provides a variety of ways to obtain notifications. Let&#8217;s explore the out-of-the-box channels:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Notification through Sink channel</dt> <dd> <p>This implementation sends notifications through the Connect API to a configured topic. Users can enable this channel and specify the topic name. By publishing notifications to the specified topic, Debezium allows users to consume and process notifications in a way that suits their needs. Since <code>Sink</code> is agnostic (i.e. not necessarily Kafka), this integrates seamlessly with Debezium Server to deliver notifications to any of its sink adapters.</p> </dd> <dt class="hdlist1">Notification through logs</dt> <dd> <p>This channel appends notifications directly to the end of the connector&#8217;s log. This approach provides convenient access for monitoring, debugging, and analyzing notification details.</p> </dd> <dt class="hdlist1">Notifications through Java Management Extensions (JMX)</dt> <dd> <p>This notification channel exposes a dedicated Debezium MBean with several attributes that contain a list of generated notifications. This channel combines common industry standard monitoring techniques like JMX to react and consume Debezium notifications. A dedicated post will follow to discuss this in more detail.</p> </dd> <dt class="hdlist1">Notification through custom channel</dt> <dd> <p>Debezium&#8217;s notification mechanism is extensible, allowing users to implement custom channels to deliver notifications using means that best fit their needs. We will cover how to provide a custom notification channel in a dedicated post.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="conclusion">Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>Signals and Notifications are now foundational features in Debezium. Signaling empowers users to interact with Debezium and trigger actions, while notification provides valuable information about Debezium&#8217;s state and progress.</p> </div> <div class="paragraph"> <p>In previous versions of Debezium, initiating an incremental snapshot was only possible using a signaling table. Users had to configure a specific table as a signaling mechanism to trigger the incremental snapshot. However, for MySQL with GTIDs enabled, it was possible to utilize Kafka for signaling purposes.</p> </div> <div class="paragraph"> <p>With the release of Debezium version 2.3, we have introduced significant improvements to both the signal and notification subsystems. We&#8217;ve unified several behaviors across connectors and made the entire system extensible, aiming to simplify both custom and future contributed implementations. We hope this enhances the overall experience of working with Debezium and provides a way to integrate Debezium with other third-party applications and tools seamlessly. These new and improved features allow you to maximize the capabilities of the Debezium change data capture platform in limitless ways.</p> </div> <div class="paragraph"> <p>Stay tuned for Part 2, where we will discuss how to customize signaling and notification channels in Debezium.</p> </div> </div> </div>]]></content><author><name>Fiore Mario Vitale</name></author><category term="debezium"/><category term="features"/><category term="notifications"/><category term="signaling"/><category term="integration"/><summary type="html"><![CDATA[Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the first installment in the series, where we will introduce the signaling and notification features offered by Debezium and discuss the available channels for interacting with the platform. In the subsequent parts of this series, we will delve deeper into customizing signaling channels and explore additional topics such as JMX signaling and notifications.]]></summary></entry><entry><title type="html">Towards Debezium exactly-once delivery</title><link href="https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery/" rel="alternate" type="text/html" title="Towards Debezium exactly-once delivery"/><published>2023-06-22T01:20:00+00:00</published><updated>2023-06-22T01:20:00+00:00</updated><id>https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery</id><content type="html" xml:base="https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery/"><![CDATA[<div class="paragraph"> <p>Every now and then there is a questions in the Debezium <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">chat</a> or on the <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> how to ensure exactly-once delivery of the records produced by Debezium. So far Debezium aimed only for at-least-once delivery. This means Debezium guarantees every single change will be delivered and there is no missing or skipped change event. However, in case of failures, restarts or DB connection drops, the same event can be delivered more than once. Typical scenario is that the event is delivered twice - once before failure/restart and second time after that. Exactly-once delivery (or semantic) provides stronger guarantee - every single message will be delivered and at the same time there won&#8217;t be any duplicates, every single message will be delivered exactly once. So far our answer was that the users have to implement their own deduplication system if they need exactly-once delivery. However, with Kafka Connect support for exactly-once delivery, it seems we can provide exactly-once delivery for Debezium connectors out-of-the-box, only with a little configuration change.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="kafka_connect_exactly_once_semantics">Kafka Connect exactly-once semantics</h2> <div class="sectionbody"> <div class="paragraph"> <p>Kafka itself provides transaction support, a building block for exactly-once delivery, as well as exactly-once delivery itself for quite some time (since Kafka 0.11). What was missing was support for exactly-once delivery in Kafka Connect. This has change in Kafka 3.3.0, which has added support for exactly-once delivery for the source connectors, see <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors">KIP-618</a> and related <a href="https://issues.apache.org/jira/browse/KAFKA-10000">Jira issue</a>. From the high-level perspective, events produced by the source connector and written during uncommitted transactions are not visible for the consumer and become visible only once a transaction is committed, which also includes a commit of the offset corresponding to these events. The source connector itself doesn&#8217;t create the transaction, it&#8217;s handled by Kafka Connect. Connector only defines transactional boundaries if needed and in many cases even this is not needed.</p> </div> <div class="sect2"> <h3 id="kafka_connect_configuration">Kafka Connect configuration</h3> <div class="paragraph"> <p>Exactly-once semantics (EOS) is currently supported only with Kafka Connect in distributed mode. The only thing the user has to do is to enable exactly once delivery on all Kafka Connect workers. The worker can enable exactly-once support by setting <code>exactly.once.source.support=enabled</code>. If you want to do a rolling update on the cluster without shut down, you can first update all the workers to <code>exactly.once.source.support=preparing</code> and then gradually switch the workers to <code>exactly.once.source.support=enabled</code>.</p> </div> <div class="paragraph"> <p>Source connector which requires exactly-once delivery needs to be configured in a similar way by setting <code>exactly.once.support</code>. You can set it either to <code>requested</code> or <code>required</code>:</p> </div> <div class="ulist"> <ul> <li> <p><code>required</code> - Kafka Connect checks that the source connector explicitly supports exactly-once semantics by implementing the <code>SourceConnector::exactlyOnceSupport</code> method. If the connector doesn&#8217;t implement this method, the start of the connector would fail.</p> </li> <li> <p><code>requested</code> - skips the check if the source connector explicitly supports exactly-once delivery or not. In such cases the documentation or the source code of the connector needs to be carefully checked if it can provide EOS or not.</p> </li> </ul> </div> <div class="paragraph"> <p>To be sure EOS works properly, it&#8217;s always better to use <code>exactly.once.support=required</code> when exactly-once delivery is required by the application and <code>exactly.once.support=requested</code> use only for testing purposes.</p> </div> <div class="paragraph"> <p>Another related source connector configuration option is <code>transaction.boundary</code>. It determines the span on the transaction. Possibles values are:</p> </div> <div class="ulist"> <ul> <li> <p><code>poll</code> - all event return in single <code>SourceTask::poll</code> method call will be wrapped into a single transaction,</p> </li> <li> <p><code>interval</code> - events will be chunked into the transactions based on the specified time interval, which can be configured via <code>transaction.boundary.interval.ms</code> option,</p> </li> <li> <p><code>connector</code> - definition of transaction boundaries is delegated to the given source connector.</p> </li> </ul> </div> <div class="paragraph"> <p>The default value is <code>poll</code> option.</p> </div> <div class="paragraph"> <p>To sum up, if you want to enable EOS and your source connector supports it, you need to configure <code>exactly.once.source.support=enabled</code> on all workers and update connector configurations with <code>exactly.once.support=required</code>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="exactly_once_delivery_and_debezium_connectors">Exactly once delivery and Debezium connectors</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium has two phases, initial snapshotting phase and streaming phase. We have a hard requirement that the initial snapshot has to finish successfully, otherwise it has to be repeated. If there is any failure during the snapshot phase, after the next start or restart of the connector the snapshot will be taken again and therefore there will be duplicates. Repeating the whole snapshot after the next start makes sense as the data may change until next start or during the restart. As the snapshot should reflect the exact data in time of taking the snapshot, thus in case of failure we have to start from scratch. There would probably be ways to avoid duplicate events in the initial snapshot phase, but for now let&#8217;s focus only on the streaming phase.</p> </div> <div class="paragraph"> <p>In the case of streaming the data, the situation is on the other hand quite easy. We store the events obtained from the database in Debezium internal queue and on each Kafka Connect <code>poll()</code> method call we dump the queue and update Kafka offset. Therefore the default transaction boundary, which wraps the <code>poll()</code> method, is a perfect fit for Debezium and Debezium doesn&#8217;t have to define any custom transaction boundaries.</p> </div> <div class="paragraph"> <p>It seems Debezium connectors can work with Kafka Connect exactly-once delivery out-of-the-box without any further modifications. However, we have to develop more robust tests for testing EOS and test all the connectors first. For connectors which we would test at least to some extent, we will add implementation of <code>SourceConnector::exactlyOnceSupport</code> method. In the meantime, if you want to test the EOS on your tests or stage environment on your own, you can use <code>exactly.once.support=requested</code>.</p> </div> <div class="sect2"> <h3 id="simple_test_of_exactly_once_delivery_when_db_connection_breaks">Simple test of exactly once delivery when DB connection breaks</h3> <div class="paragraph"> <p>Let&#8217;s show the simple test of EOS. We will try to break the Debezium connection to the database while the connector is running and data is continuously stored into the database. In such a case Debezium will throw retriable exception, Kafka connect would restart the connector and we would be interested if the connector really started from the point where it stopped the last time and doesn&#8217;t send any event twice.</p> </div> <div class="paragraph"> <p>The scenario can look like this:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>start Kafka Connect</p> </li> <li> <p>deploy the Debezium connector</p> </li> <li> <p>start a custom loader which will continuously insert data into the captured table in the database</p> </li> <li> <p>kill Debezium connection to the database</p> </li> <li> <p>let Kafka Connect restart Debezium connector</p> </li> <li> <p>stop the loader</p> </li> <li> <p>check, if there are any duplicates in the related Kafka topic.</p> </li> </ol> </div> <div class="paragraph"> <p>Here is a concrete example for Postgres database and Debezium Postgres connector.</p> </div> <div class="paragraph"> <p>Let&#8217;s create a simple test table which would contain ID, some integer value (can be the same as ID) and let&#8217;s say timestamp if we want to check when the record was created:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">DROP</span> <span class="type">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> public.eos_test;  <span class="comment">-- remove the table first if exists</span>
<span class="class">CREATE</span> <span class="type">TABLE</span> public.eos_test(id SERIAL <span class="keyword">NOT</span> <span class="predefined-constant">NULL</span> <span class="directive">PRIMARY</span> <span class="type">KEY</span>, val SERIAL, t <span class="predefined-type">TIMESTAMP</span> <span class="directive">DEFAULT</span> <span class="predefined">now</span>());</code></pre> </div> </div> <div class="paragraph"> <p>Don&#8217;t forget to update Kafka Connect with <code>exactly.once.source.support=enabled</code>. Also, Kafka Connect has to run in distributed mode so you need to modify <code>config/connect-distributed.properties</code> and run the worker with these properties. Once you have Kafka Connect running, you can start Debezium Postgres connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">eos</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">localhost</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5432</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">topic.prefix</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">eos</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">table.include.list</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.eos_test</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">unwrap</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.unwrap.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.ExtractNewRecordState</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">exactly.once.support</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">requested</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>To simplify reading of the topic, here we turned off adding schema into the records and also added Debezium SMT for extracting new record state so that only the new value is stored into the Kafka topic.</p> </div> <div class="paragraph"> <p>Now, you can start loading the data into the Postgres test table using e.g. this Python script:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="comment">#!/usr/bin/env python3</span>

<span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Loads data into Postgres EOS test table.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

<span class="keyword">import</span> <span class="include">psycopg2</span>

DROP_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">DROP TABLE IF EXISTS public.eos_test;</span><span class="delimiter">&quot;</span></span>
CREATE_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">CREATE TABLE public.eos_test(id SERIAL NOT NULL PRIMARY KEY,</span><span class="delimiter">&quot;</span></span> \
               <span class="string"><span class="delimiter">&quot;</span><span class="content">val SERIAL, t TIMESTAMP DEFAULT now());</span><span class="delimiter">&quot;</span></span>
INSERT_INTO_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">INSERT INTO public.eos_test(val) VALUES (%s)</span><span class="delimiter">&quot;</span></span>

<span class="keyword">try</span>:
    connection = psycopg2.connect(
        user=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        password=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        host=<span class="string"><span class="delimiter">&quot;</span><span class="content">127.0.0.1</span><span class="delimiter">&quot;</span></span>,
        port=<span class="string"><span class="delimiter">&quot;</span><span class="content">5432</span><span class="delimiter">&quot;</span></span>,
        database=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>)
    cursor = connection.cursor()

    <span class="comment"># cursor.execute(DROP_TABLE)</span>
    <span class="comment"># cursor.execute(CREATE_TABLE)</span>
    <span class="comment"># connection.commit()</span>

    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(<span class="integer">1</span>, <span class="integer">50000</span>):
        cursor.execute(INSERT_INTO_TABLE, (i,))
        connection.commit()
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">wrote {i}</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">finally</span>:
    <span class="keyword">if</span> connection:
        cursor.close()
        connection.close()</code></pre> </div> </div> <div class="paragraph"> <p>It will load 50,000 records into the test table. This should last long enough to check that the data is captured by the Debezium connector and switch to another window and kill the Debezium connection to the database. Depending on the speed of your test environment, you can possibly manage to kill the database connection several times during one run. You can eventually increase/decrease the number of loaded events.</p> </div> <div class="paragraph"> <p>Once out loader script loads the data into the database, kill the Debezium connection to the database, e.g. by connecting to Postgres database and running this command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span> pg_terminate_backend(pid) <span class="keyword">FROM</span> pg_stat_activity <span class="keyword">WHERE</span> pid &lt;&gt; pg_backend_pid() <span class="keyword">AND</span> datname = <span class="string"><span class="delimiter">'</span><span class="content">postgres</span><span class="delimiter">'</span></span> <span class="keyword">AND</span> query <span class="keyword">like</span> <span class="string"><span class="delimiter">'</span><span class="content">START_REPLICATION SLOT %</span><span class="delimiter">'</span></span>;</code></pre> </div> </div> <div class="paragraph"> <p>This will terminate Debezium connection to the database, which will subsequently cause throwing Postgres driver <code>PSQLException</code> and Kafka Connect <code>RetriableException</code>. As the exception is retriable, Kafka Connect will restart the connector automatically and it will continue with capturing the data from the test table. You can check that all the data has arrived into the Kafka topic using <code>kafka-console-consumer</code> utility:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --property print.key=true --topic eos.public.eos_test</code></pre> </div> </div> <div class="paragraph"> <p>Once you are sure all the records are in the Kafka topic, you can finally run the checker script which would search for duplicate events. The script (again in Python) can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="comment">#!/usr/bin/env python3</span>

<span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Check Kafka topic for records with duplicate key/value.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

<span class="keyword">import</span> <span class="include">json</span>
<span class="keyword">import</span> <span class="include">kafka</span>

UNIQUE_COLUMN = <span class="string"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>


<span class="keyword">def</span> <span class="function">deserialize</span>(rec):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Deserialize JSON payload and extract value number.</span><span class="content">
</span><span class="content">    Ignore transaction boundaries markers.</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">try</span>:
        value = json.loads(rec.decode(<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>))
        <span class="keyword">if</span> UNIQUE_COLUMN <span class="keyword">in</span> value:
            <span class="keyword">return</span> value[UNIQUE_COLUMN]
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: no value in record {rec}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">except</span>:
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: cannot deserialize record {rec}</span><span class="delimiter">&quot;</span></span>)

    <span class="keyword">return</span> <span class="predefined-constant">None</span>


<span class="keyword">def</span> <span class="function">check_topic</span>():
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Check Kafka topic for duplicates and prin statistics, including skipped</span><span class="content">
</span><span class="content">    records.</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>
    consumer = kafka.KafkaConsumer(
        <span class="string"><span class="delimiter">&quot;</span><span class="content">eos.public.eos_test</span><span class="delimiter">&quot;</span></span>,
        bootstrap_servers=[<span class="string"><span class="delimiter">&quot;</span><span class="content">localhost:9092</span><span class="delimiter">&quot;</span></span>],
        auto_offset_reset=<span class="string"><span class="delimiter">&quot;</span><span class="content">earliest</span><span class="delimiter">&quot;</span></span>,
        consumer_timeout_ms=<span class="integer">1000</span>,
        enable_auto_commit=<span class="predefined-constant">True</span>,
        group_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">eos-group</span><span class="delimiter">&quot;</span></span>,
        value_deserializer=deserialize)

    vals = []
    items = <span class="integer">0</span>
    duplicates = <span class="integer">0</span>
    skipped = <span class="integer">0</span>
    <span class="keyword">for</span> msg <span class="keyword">in</span> consumer:
        val = msg.value
        <span class="keyword">if</span> <span class="keyword">not</span> val:
            print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: skipping None value, items: {items}</span><span class="delimiter">&quot;</span></span>)
            skipped = skipped + <span class="integer">1</span>
            <span class="keyword">continue</span>

        items = items + <span class="integer">1</span>
        <span class="keyword">if</span> val <span class="keyword">in</span> vals:
            print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Duplicate value: {val}</span><span class="delimiter">&quot;</span></span>)
            duplicates = duplicates + <span class="integer">1</span>
        <span class="keyword">else</span>:
            vals.append(val)

    print(
        f<span class="string"><span class="delimiter">&quot;</span><span class="content">Found {duplicates} duplicates in {items} items (unique values: </span><span class="delimiter">&quot;</span></span> \
        f<span class="string"><span class="delimiter">&quot;</span><span class="content">{len(vals)}, skipped values: {skipped})</span><span class="delimiter">&quot;</span></span>)


check_topic()</code></pre> </div> </div> <div class="paragraph"> <p>Tail of the output can look as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">....
WARN: skipping None value, items: 46297
WARN: skipping None value, items: 48345
WARN: cannot deserialize record b'\x00\x00\x00\x00\x00\x00'
WARN: skipping None value, items: 49999
Found 0 duplicates in 49999 items (unique values: 49999, skipped values: 54)</code></pre> </div> </div> <div class="paragraph"> <p>In this case there are no duplicate records, so everything looks good. The only question is what are those skipped events. These are transaction boundaries markers. Python Kafka client for some reason cannot cope with them and fails to deserialize them, so we skip them. Java client should recognize these records and handle them without any issue.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="summary_and_the_next_steps">Summary and the next steps</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we have shown how to configure exactly once semantics for Kafka Connect source connectors and how to use it with Debezium Postgres connector. So far, it seems that there are no issues and at least Debezium Postgres connector can work fine with the exactly-once semantics.</p> </div> <div class="paragraph"> <p>However, not finding an issue of course doesn&#8217;t imply that there are no issues. Therefore, as a next step we would like to develop a more rigorous test framework for testing data consistency and exactly-once delivery. We would like to write the tests using the famous <a href="https://github.com/jepsen-io/jepsen">Jepsen</a> framework. If we succeed in writing the test, we will share the results in a follow-up blog post. In the meantime we would like to encourage you to test exactly-once delivery also in your environments and deployments to increase the chance to discover any potential bugs. If you run any such test, we would very appreciate if you share the results with us, negative ones when you find a bug as well as positive ones when everything passes.</p> </div> </div> </div>]]></content><author><name>Vojtěch Juránek</name></author><category term="apache-kafka"/><category term="exactly-once-semantics"/><category term="transactions"/><category term="deduplication"/><summary type="html"><![CDATA[Every now and then there is a questions in the Debezium chat or on the mailing list how to ensure exactly-once delivery of the records produced by Debezium. So far Debezium aimed only for at-least-once delivery. This means Debezium guarantees every single change will be delivered and there is no missing or skipped change event. However, in case of failures, restarts or DB connection drops, the same event can be delivered more than once. Typical scenario is that the event is delivered twice - once before failure/restart and second time after that. Exactly-once delivery (or semantic) provides stronger guarantee - every single message will be delivered and at the same time there won&#8217;t be any duplicates, every single message will be delivered exactly once. So far our answer was that the users have to implement their own deduplication system if they need exactly-once delivery. However, with Kafka Connect support for exactly-once delivery, it seems we can provide exactly-once delivery for Debezium connectors out-of-the-box, only with a little configuration change.]]></summary></entry><entry><title type="html">Debezium 2.3.0.Final Released</title><link href="https://debezium.io/blog/2023/06/21/debezium-2-3-final-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Final Released"/><published>2023-06-21T00:00:00+00:00</published><updated>2023-06-21T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/21/debezium-2-3-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/06/21/debezium-2-3-final-released/"><![CDATA[<div class="paragraph"> <p>The team has been quite busy these last couple months preparing for a condensed release timeline for Debezium 2.3, and I am thrilled to announce that the next installment has arrived, Debezium <strong>2.3.0.Final</strong> is now available!</p> </div> <div class="paragraph"> <p>Despite a condensed release schedule, this release is packed with tons of new features and improvements. Debezium 2.3 includes a brand-new notification subsystem, a rewrite of the signal subsystem to support additional means to send signals to Debezium connectors, the introduction of the JDBC storage module to maintain Debezium state in relational databases, parallelization of Vitess shard processing, PostgreSQL connector replica identity improvements, a brand-new Debezium Server operator for Kubernetes, support for exactly-once semantics for the PostgreSQL connector, and improvements to the Oracle connector.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and talk about any breaking changes and all these improvements and why they&#8217;re so important!</p> </div> <div class="ulist"> <ul> <li> <p><a href="#breaking-changes">Breaking changes</a></p> </li> <li> <p><a href="#new-features">New features and improvements</a></p> </li> <li> <p><a href="#other-fixes">Other changes</a></p> </li> <li> <p><a href="#whats-next">What&#8217;s next</a></p> </li> </ul> </div> <div class="paragraph"> <p>You can also find a full list of changes in Debezium 2.3&#8217;s <a href="https://debezium.io/releases/2.3/release-notes">release notes</a>.</p> </div> <div class="sect1"> <h2 id="breaking-changes">Breaking changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>We generally attempt to avoid as many breaking changes across minor releases as possible, but unfortunately sometimes such changes are inevitable, particularly if a feature may have been introduced as <em>experimental</em>. Debezium 2.3 introduces two breaking changes depending on whether you are upgrading from a preview release or an earlier stable version:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#postgresql-mysql-secure-connection-changes">PostgreSQL / MySQL secure connection changes</a></p> </li> <li> <p><a href="#jdbc-storage-encoding-changes">JDBC storage encoding changes</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="postgresql-mysql-secure-connection-changes">PostgreSQL / MySQL secure connection changes</h3> <div class="paragraph"> <p>Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-property-database-sslmode"><code>database.sslmode</code></a> while for MySQL this can be done with <a href="https://debezium.io/documentation/reference/2.3/connectors/mysql.html#mysql-property-database-ssl-mode"><code>database.ssl.mode</code></a>.</p> </div> <div class="paragraph"> <p>With Debezium 2.3, this configuration option no longer defaults to <code>disable</code> (PostgreSQL) or <code>disabled</code> (MySQL) but instead defaults to <code>prefer</code> (PostgreSQL) and <code>preferred</code> (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise.</p> </div> </div> <div class="sect2"> <h3 id="jdbc-storage-encoding-changes">JDBC storage encoding changes</h3> <div class="paragraph"> <p>Debezium 2.3.0.alpha1 introduced the new <em>experimental</em> JDBC storage module. This storage module defaulted to using UTF-16 as it&#8217;s default encoding; however, most databases prefer UTF-8 as a default. If you are upgrading from Debezium 2.3.0.Alpha1, Debezium 2.3.0.Beta1 and later now use UTF-8 when storing data using the JDBC storage module to align with typical database defaults.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="new-features">New features and improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 includes a plethora of new features improvements, which include the following:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#debezium-server-operator">Debezium Server Operator for Kubernetes</a></p> </li> <li> <p><a href="#notifications">New notification subsystem</a></p> </li> <li> <p><a href="#signals">New extensible signal subsystem with new channels</a></p> </li> <li> <p><a href="#jmx-signals-notifications">JMX signal and notification integration</a></p> </li> <li> <p><a href="#jdbc-storage">New JDBC storage module</a></p> </li> <li> <p><a href="#exactly-once-delivery">Exactly once delivery for PostgreSQL streaming</a></p> </li> <li> <p><a href="#replica-identity">PostgreSQL replica identity changes</a></p> </li> <li> <p><a href="#rac-specify-sid">Oracle RAC improvements</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="debezium-server-operator">Debezium Server Operator for Kubernetes</h3> <div class="paragraph"> <p>This release introduces a <em>preview</em> version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments.</p> </div> <div class="paragraph"> <p>Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this in upcoming release cycles. You can find a <a href="https://github.com/debezium/debezium-operator#quickstart-example">deployment example</a> and basic description of the <a href="https://github.com/debezium/debezium-operator#debeziumserverspec-reference">custom resource specification</a> in the github repository that you can use as a reference for the short-term.</p> </div> <div class="paragraph"> <p>We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs.</p> </div> </div> <div class="sect2"> <h3 id="notifications">New notification subsystem</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle.</p> </div> <div class="paragraph"> <p>Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event.</p> </div> <div class="listingblock"> <div class="title">Example Notification Event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">c485ccc3-16ff-47cc-b4e8-b56a57c3bad2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Started</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">additional_data</span><span class="delimiter">&quot;</span></span>: {
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>Each notification event consists of an <code>id</code> field, a UUID to identify the notification, an <code>aggregate_type</code> field to which the notification is related based on the concept of domain-driven design, a <code>type</code> field that is mean to given more detail about the aggregate type itself, and an optional <code>additional_data</code> field which consists of a map of string-based key/value pairs with additional information about the event.</p> </div> <div class="paragraph"> <p>At this time, there are two notification event types supported by Debezium:</p> </div> <div class="ulist"> <ul> <li> <p>Status of the initial snapshot</p> </li> <li> <p>Monitoring of the incremental snapshot</p> </li> </ul> </div> <div class="sect3"> <h4 id="initial_snapshot_notifications">Initial Snapshot Notifications</h4> <div class="paragraph"> <p>An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of <code>"Initial Snapshot"</code> and the type of event will consist of one of three logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>SKIPPED</code></dt> <dd> <p>Represents the initial snapshot was skipped.</p> </dd> <dt class="hdlist1"><code>ABORTED</code></dt> <dd> <p>Represents the initial snapshot was aborted.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Represents the initial snapshot has concluded successfully.</p> </dd> </dl> </div> <div class="paragraph"> <p>The following is an example of a notification about the completion of the initial snapshot:</p> </div> <div class="listingblock"> <div class="title">Example snapshot completed event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5563ae14-49f8-4579-9641-c1bbc2d76f99</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Initial Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">COMPLETED</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect3"> <h4 id="incremental_snapshot_notifications">Incremental Snapshot Notifications</h4> <div class="paragraph"> <p>An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of <code>"Incremental Snapshot"</code> and the type will consist of one of several logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>STARTED</code></dt> <dd> <p>Indicates an incremental snapshot has started.</p> </dd> <dt class="hdlist1"><code>PAUSED</code></dt> <dd> <p>Indicates an incremental snapshot has been temporarily paused.</p> </dd> <dt class="hdlist1"><code>RESUMED</code></dt> <dd> <p>Indicates an incremental snapshot that had been paused has now resumed.</p> </dd> <dt class="hdlist1"><code>STOPPED</code></dt> <dd> <p>Indicates an incremental snapshot has stopped.</p> </dd> <dt class="hdlist1"><code>IN_PROGRESS</code></dt> <dd> <p>Indicates an incremental snapshot is in-progress.</p> </dd> <dt class="hdlist1"><code>TABLE_SCAN_COMPLETED</code></dt> <dd> <p>Indicates an incremental snapshot has concluded for a given table.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Indicates that an incremental snapshot has concluded for all tables.</p> </dd> </dl> </div> </div> <div class="sect3"> <h4 id="configuring_notifications">Configuring Notifications</h4> <div class="paragraph"> <p>Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels.</p> </div> <div class="listingblock"> <div class="title">Using a Kafka Topic</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">sink</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.sink.topic.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_notifications</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Using the connector logs</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">log</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> <div class="sect2"> <h3 id="signals">New extensible signal subsystem</h3> <div class="paragraph"> <p>Debezium has supported the concept of a signal since the introduction of the Incremental Snapshot feature well back in Debezium 1.x. Signals are important as it allows you to provide metadata to instruct Debezium to perform a given task, whether that task is to write an entry to the connector log or perform an ad-hoc incremental snapshot.</p> </div> <div class="paragraph"> <p>In Debezium 2.3, the signal subsystem was reworked to introduce the concept called <em>channel</em>, which represents a medium for which Debezium watches or listens and reacts to signals. In previous versions, there was one channel supported universally across connectors, which was the database signal table. In this release, these channels have been unified and the following are available out of the box:</p> </div> <div class="ulist"> <ul> <li> <p>Database signal table</p> </li> <li> <p>Kafka signal topic</p> </li> <li> <p>Filesystem</p> </li> <li> <p>JMX</p> </li> </ul> </div> <div class="paragraph"> <p>But that&#8217;s not all, the signal channel contract is extensible, allowing you to write a custom implementation and make that available to your connectors with ease.</p> </div> </div> <div class="sect2"> <h3 id="jmx-signals-notifications">JMX signals and notifications integration</h3> <div class="paragraph"> <p>Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX.</p> </div> <div class="sect3"> <h4 id="sending_signals">Sending signals</h4> <div class="paragraph"> <p>In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the <code>jconsole</code> window, you can now see there are two new subsections for a connector, a notifications and signal section shown below:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-signal-notifications.png" class="responsive-image" alt="JConsole JMX metrics"> </div> </div> <div class="paragraph"> <p>The new <code>signals</code> section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-send-signal.png" class="responsive-image" alt="JConsole Sending Signals"> </div> </div> </div> <div class="sect3"> <h4 id="receiving_notifications">Receiving notifications</h4> <div class="paragraph"> <p>The new <code>notifications</code> section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-receive-notification.png" class="responsive-image" alt="JConsole Receive Notifications"> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the <code>reset</code> method on the notifications bean once you&#8217;ve processed all notifications.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX.</p> </div> </div> </div> <div class="sect2"> <h3 id="jdbc-storage">New JDBC storage subsystem</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new module, the following dependency must be added to your project or application:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>debezium-storage-jdbc<span class="tag">&lt;/artifactId&gt;</span>
    <span class="tag">&lt;version&gt;</span>2.3.0.Final<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>The following examples show how to configure Offset or Schema History storage via the JDBC storage module:</p> </div> <div class="listingblock"> <div class="title">Configuration example for Offset JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.offset_table_name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_offset_storage</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Configuration example for Schema History JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.schema.history.table.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_database_history</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="exactly-once-delivery">Exactly once delivery for PostgreSQL streaming</h3> <div class="paragraph"> <p>Debezium has traditionally been an at-least-once delivery solution, guaranteeing that no change is ever missed. Exactly-Once is a proposal by the Apache Kafka community as a part of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors">KIP-618</a>. This proposal aims to address a common problem with producers (source connectors) when a producer retries, it may re-send a batch of events to the Kafka broker even if that batch had already been committed by the broker. This means there are situations where duplicate events may be sent and not every consumer (sink connector) may be capable of handling such situations easily.</p> </div> <div class="paragraph"> <p>Debezium plans to roll out exactly-once delivery semantics in a phased style. There are specific corner cases around snapshotting and streaming and these can vary by connector. And with that, Debezium 2.3 starts out by specifically only adding support for exactly-once semantics for the PostgreSQL during its <em>streaming phase</em> only!</p> </div> <div class="paragraph"> <p>In order to take advantage of exactly-once delivery, there is no connector configuration changes required; however, your Kafka Connect worker configuration will need to be adjusted to enable this feature. You can find a reference to the new configuration properties in <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Newproperties">KIP-618</a>.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you are mixing connectors on the same connect cluster that do and do not support exactly once delivery, you will not be able to set <code>exactly.once.support</code> in the worker configuration to <code>required</code> as this will cause connectors that do not support this feature to fail validation at start-up.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="replica-identity">PostgreSQL replica identity changes</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity".</p> </div> <div class="paragraph"> <p>Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up.</p> </div> <div class="paragraph"> <p>The new configuration option, <code>replica.identity.autoset.values</code>, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-replica-identity">documentation</a>.</p> </div> <div class="paragraph"> <p>When specifying the <code>replica.identity.autoset.values</code>, the value is a comma-separated list of values where each element uses the format of <code>&lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;</code>. An example is shown below where two tables are configured to have full replica identity:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">replica.identity.autoset.values</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.table1:FULL,public.table2:FULL</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="rac-specify-sid">Oracle RAC improvements</h3> <div class="paragraph"> <p>When connecting to an Oracle RAC installation, you must specify a <code>rac.nodes</code> configuration property with a minimum of the host or IP address of all individual nodes across the cluster. Older versions of the connector also supported a varied format, allowing the inclusion of a port for each node as not every node on the cluster may use the same port.</p> </div> <div class="paragraph"> <p>Debezium 2.3 improves the Oracle RAC support by also acknowledging that each node may not necessarily use the same Oracle Site Identifier (SID), and therefore, the SID can also now be included in the <code>rac.nodes</code> configuration property to support such installations.</p> </div> <div class="paragraph"> <p>The following example illustrates connecting to two Oracle RAC nodes, each using different ports and SID parameters:</p> </div> <div class="listingblock"> <div class="title">An example configuration</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.oracle.OracleConnector</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">rac.nodes</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">host1.domain.com:1521/ORCLSID1,host2.domain.com:1522/ORCLSID2</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium Server stops sending events to Google Cloud Pub/Sub <a href="https://issues.redhat.com/browse/DBZ-5175">DBZ-5175</a></p> </li> <li> <p>Toasted varying character array and date array are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6122">DBZ-6122</a></p> </li> <li> <p>Upgrade to Infinispan 14.0.11.Final to fix CVE-2022-45047 <a href="https://issues.redhat.com/browse/DBZ-6193">DBZ-6193</a></p> </li> <li> <p>Introduce LogMiner query filtering modes <a href="https://issues.redhat.com/browse/DBZ-6254">DBZ-6254</a></p> </li> <li> <p>Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed <a href="https://issues.redhat.com/browse/DBZ-6256">DBZ-6256</a></p> </li> <li> <p>Ensure that the connector can start from a stale timestamp more than one hour into the past <a href="https://issues.redhat.com/browse/DBZ-6307">DBZ-6307</a></p> </li> <li> <p>The rs_id field is null in Oracle change event source information block <a href="https://issues.redhat.com/browse/DBZ-6329">DBZ-6329</a></p> </li> <li> <p>Add JWT authentication to HTTP Client <a href="https://issues.redhat.com/browse/DBZ-6348">DBZ-6348</a></p> </li> <li> <p>Using pg_replication_slot_advance which is not supported by PostgreSQL10. <a href="https://issues.redhat.com/browse/DBZ-6353">DBZ-6353</a></p> </li> <li> <p>log.mining.transaction.retention.hours should reference last offset and not sysdate <a href="https://issues.redhat.com/browse/DBZ-6355">DBZ-6355</a></p> </li> <li> <p>Support multiple tasks when streaming shard list <a href="https://issues.redhat.com/browse/DBZ-6365">DBZ-6365</a></p> </li> <li> <p>Code Improvements for skip.messages.without.change <a href="https://issues.redhat.com/browse/DBZ-6366">DBZ-6366</a></p> </li> <li> <p>Kinesis Sink - AWS Credentials Provider <a href="https://issues.redhat.com/browse/DBZ-6372">DBZ-6372</a></p> </li> <li> <p>Toasted hstore are not correctly processed <a href="https://issues.redhat.com/browse/DBZ-6379">DBZ-6379</a></p> </li> <li> <p>Oracle DDL shrink space for table partition can not be parsed <a href="https://issues.redhat.com/browse/DBZ-6386">DBZ-6386</a></p> </li> <li> <p>__source_ts_ms r (read) operation date is set to future for SQL Server <a href="https://issues.redhat.com/browse/DBZ-6388">DBZ-6388</a></p> </li> <li> <p>PostgreSQL connector task fails to resume streaming because replication slot is active <a href="https://issues.redhat.com/browse/DBZ-6396">DBZ-6396</a></p> </li> <li> <p>Date and Time values without timezones are not persisted correctly based on database.time_zone <a href="https://issues.redhat.com/browse/DBZ-6399">DBZ-6399</a></p> </li> <li> <p>MongoDB connector crashes on invalid resume token <a href="https://issues.redhat.com/browse/DBZ-6402">DBZ-6402</a></p> </li> <li> <p>Snapshot step 5 - Reading structure of captured tables time too long <a href="https://issues.redhat.com/browse/DBZ-6439">DBZ-6439</a></p> </li> <li> <p>NPE on read-only MySQL connector start up <a href="https://issues.redhat.com/browse/DBZ-6440">DBZ-6440</a></p> </li> <li> <p>Oracle parallel snapshots do not properly set PDB context when using multitenancy <a href="https://issues.redhat.com/browse/DBZ-6457">DBZ-6457</a></p> </li> <li> <p>[MariaDB] Add support for userstat plugin keywords <a href="https://issues.redhat.com/browse/DBZ-6459">DBZ-6459</a></p> </li> <li> <p>Debezium Server cannot recover from Google Pub/Sub errors <a href="https://issues.redhat.com/browse/DBZ-6461">DBZ-6461</a></p> </li> <li> <p>"Ignoring invalid task provided offset" <a href="https://issues.redhat.com/browse/DBZ-6463">DBZ-6463</a></p> </li> <li> <p>Oracle snapshot.include.collection.list should be prefixed with databaseName in documentation. <a href="https://issues.redhat.com/browse/DBZ-6474">DBZ-6474</a></p> </li> <li> <p>Db2 connector can fail with NPE on notification sending <a href="https://issues.redhat.com/browse/DBZ-6485">DBZ-6485</a></p> </li> <li> <p>ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception <a href="https://issues.redhat.com/browse/DBZ-6486">DBZ-6486</a></p> </li> <li> <p>BigDecimal fails when queue memory size limit is in place <a href="https://issues.redhat.com/browse/DBZ-6490">DBZ-6490</a></p> </li> <li> <p>Allow schema to be specified in the Debezium Sink Connector configuration <a href="https://issues.redhat.com/browse/DBZ-6491">DBZ-6491</a></p> </li> <li> <p>ORACLE table can not be captured, got runtime.NoViableAltException <a href="https://issues.redhat.com/browse/DBZ-6492">DBZ-6492</a></p> </li> <li> <p>Signal poll interval has incorrect default value <a href="https://issues.redhat.com/browse/DBZ-6496">DBZ-6496</a></p> </li> <li> <p>Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone <a href="https://issues.redhat.com/browse/DBZ-6502">DBZ-6502</a></p> </li> <li> <p>Alpine postgres images should use llvm/clang 15 explicitly <a href="https://issues.redhat.com/browse/DBZ-6506">DBZ-6506</a></p> </li> <li> <p>FileSignalChannel is not loaded <a href="https://issues.redhat.com/browse/DBZ-6509">DBZ-6509</a></p> </li> <li> <p>Utilize event.processing.failure.handling.mode in Vitess replication connection <a href="https://issues.redhat.com/browse/DBZ-6510">DBZ-6510</a></p> </li> <li> <p>MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization <a href="https://issues.redhat.com/browse/DBZ-6511">DBZ-6511</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6512">DBZ-6512</a></p> </li> <li> <p>Error value of negative seconds in convertOracleIntervalDaySecond <a href="https://issues.redhat.com/browse/DBZ-6513">DBZ-6513</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6515">DBZ-6515</a></p> </li> <li> <p>Only use error processing mode on certain errors <a href="https://issues.redhat.com/browse/DBZ-6523">DBZ-6523</a></p> </li> <li> <p>[PostgreSQL] LTree data is not being captured by streaming <a href="https://issues.redhat.com/browse/DBZ-6524">DBZ-6524</a></p> </li> <li> <p>Oracle Connector: Snapshot fails with specific combination <a href="https://issues.redhat.com/browse/DBZ-6528">DBZ-6528</a></p> </li> <li> <p>Use better hashing function for PartitionRouting <a href="https://issues.redhat.com/browse/DBZ-6529">DBZ-6529</a></p> </li> <li> <p>Table order is incorrect on snapshots <a href="https://issues.redhat.com/browse/DBZ-6533">DBZ-6533</a></p> </li> <li> <p>Start publishing nightly images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6541">DBZ-6541</a></p> </li> <li> <p>Start releasing images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6542">DBZ-6542</a></p> </li> <li> <p>Unhandled NullPointerException in PartitionRouting will crash the whole connect plugin <a href="https://issues.redhat.com/browse/DBZ-6543">DBZ-6543</a></p> </li> <li> <p>Bug in field.name.adjustment.mode Property <a href="https://issues.redhat.com/browse/DBZ-6559">DBZ-6559</a></p> </li> <li> <p>Operator sets incorrect value of transformation.predicate when no predicate is specified <a href="https://issues.redhat.com/browse/DBZ-6560">DBZ-6560</a></p> </li> <li> <p>Upgrade MySQL JDBC driver to 8.0.33 <a href="https://issues.redhat.com/browse/DBZ-6563">DBZ-6563</a></p> </li> <li> <p>Upgrade Google Cloud BOM to 26.17.0 <a href="https://issues.redhat.com/browse/DBZ-6570">DBZ-6570</a></p> </li> <li> <p>Kubernetes-Config extension interferes with SSL tests due to k8 devservice starting up <a href="https://issues.redhat.com/browse/DBZ-6574">DBZ-6574</a></p> </li> <li> <p>MySQL read-only connector with Kafka signals enabled fails on start up <a href="https://issues.redhat.com/browse/DBZ-6579">DBZ-6579</a></p> </li> <li> <p>Redis schema history can fail upon startup <a href="https://issues.redhat.com/browse/DBZ-6580">DBZ-6580</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(2.3.0.Alpha1%2C%202.3.0.Beta1%2C%202.3.0.CR1%2C%202.3.0.Final)%20ORDER%20BY%20component%20ASC">138 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/isacandrei">Andrei Isac</a>, <a href="https://github.com/angsdey2">Angshuman Dey</a>, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/bpaquet">Bertrand Paquet</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/cjmencias">Christian Jacob Mencias</a>, <a href="https://github.com/TechIsCool">David Beck</a>, <a href="https://github.com/fredlegaucher">Frederic Laurent</a>, <a href="https://github.com/ddsr-ops">Gong Chang Hua</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/Hidetomi">Hidetomi Umaki</a>, <a href="https://github.com/uidoyen">Hussain Ansari</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jehrenzweig-pi">Jesse Ehrenzweig</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/miguelbirdie">Miguel Angel Sotomayor</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/zalmane">Oren Elias</a>, <a href="https://github.com/rnowling-memphis">RJ Nowling</a>, <a href="https://github.com/rk3rn3r">René Kerner</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/ShuranZhang">Shuran Zhang</a>, <a href="https://github.com/sclarkson-zoomcare">Stephen Clarkson</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/tommyk-gears">Tommy Karlsson</a>, <a href="https://github.com/imtj1">Tony Joseph</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/caicancai">蔡灿材</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 out, our major focus will be coordinating bugfixes to any reports for Debezium 2.3, but primarily on the upcoming new preview release for Debezium 2.4.</p> </div> <div class="paragraph"> <p>Debezium 2.4 is planned with a ton of changes, and we intend to tackle these changes in a bucketed fashion, with each bucket being assigned a priority. As with any schedule, priorities are subject to change but the following is an outline of what to expect:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Priority 1</dt> <dd> <div class="ulist"> <ul> <li> <p>[Core] TimescaleDB single message transformation support</p> </li> <li> <p>[Core] Timezone single message transformation to ease usages with Debezium temporal types</p> </li> <li> <p>[Core] Initial snapshot notifications</p> </li> <li> <p>[MongoDB] Database-wide change stream support</p> </li> <li> <p>[MongoDB] Multi-task deployment metrics support</p> </li> <li> <p>[Oracle] OpenLogReplicator adapter support</p> </li> <li> <p>[Oracle] XML, LONG, RAW, and LONG RAW data type support</p> </li> <li> <p>[Universal] Exactly-Once semantics support for other connectors</p> </li> <li> <p>[Dependencies] Apache Kafka 3.5.x support</p> </li> </ul> </div> </dd> <dt class="hdlist1">Priority 2</dt> <dd> <div class="ulist"> <ul> <li> <p>[Operator] Next steps for Debezium operator</p> </li> <li> <p>[Core] Ad-hoc blocking snapshot</p> </li> <li> <p>[Dependencies] Use OpenTelemetry</p> </li> </ul> </div> </dd> <dt class="hdlist1">Priority 3</dt> <dd> <div class="ulist"> <ul> <li> <p>[Embedded Engine] Parallelization support</p> </li> <li> <p>[MongoDB] Parallel incremental snapshots support</p> </li> <li> <p>[MySQL] Parallel schema snapshots support</p> </li> </ul> </div> </dd> </dl> </div> <div class="paragraph"> <p>This is not an exhaustive list and its quite ambitious, but given the shortened time with Debezium 2.3, we hope the extra few weeks on Debezium 2.4 will make this next minor release possible with all these features and much more. The roadmap will be updated this week to align with the above for Debezium 2.4 and the future, so please be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions.</p> </div> <div class="paragraph"> <p>Otherwise, I expect with summer in full swing for those of us in the north, holidays and much-deserved time-off will be normal for the next several months. To those who travel or intend to take some time for yourselves and family, enjoy and be safe!</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="caassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[The team has been quite busy these last couple months preparing for a condensed release timeline for Debezium 2.3, and I am thrilled to announce that the next installment has arrived, Debezium 2.3.0.Final is now available! Despite a condensed release schedule, this release is packed with tons of new features and improvements. Debezium 2.3 includes a brand-new notification subsystem, a rewrite of the signal subsystem to support additional means to send signals to Debezium connectors, the introduction of the JDBC storage module to maintain Debezium state in relational databases, parallelization of Vitess shard processing, PostgreSQL connector replica identity improvements, a brand-new Debezium Server operator for Kubernetes, support for exactly-once semantics for the PostgreSQL connector, and improvements to the Oracle connector. Let&#8217;s take a few moments and talk about any breaking changes and all these improvements and why they&#8217;re so important! Breaking changes New features and improvements Other changes What&#8217;s next You can also find a full list of changes in Debezium 2.3&#8217;s release notes. Breaking changes We generally attempt to avoid as many breaking changes across minor releases as possible, but unfortunately sometimes such changes are inevitable, particularly if a feature may have been introduced as experimental. Debezium 2.3 introduces two breaking changes depending on whether you are upgrading from a preview release or an earlier stable version: PostgreSQL / MySQL secure connection changes JDBC storage encoding changes PostgreSQL / MySQL secure connection changes Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring database.sslmode while for MySQL this can be done with database.ssl.mode. With Debezium 2.3, this configuration option no longer defaults to disable (PostgreSQL) or disabled (MySQL) but instead defaults to prefer (PostgreSQL) and preferred (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise. JDBC storage encoding changes Debezium 2.3.0.alpha1 introduced the new experimental JDBC storage module. This storage module defaulted to using UTF-16 as it&#8217;s default encoding; however, most databases prefer UTF-8 as a default. If you are upgrading from Debezium 2.3.0.Alpha1, Debezium 2.3.0.Beta1 and later now use UTF-8 when storing data using the JDBC storage module to align with typical database defaults. New features and improvements Debezium 2.3 includes a plethora of new features improvements, which include the following: Debezium Server Operator for Kubernetes New notification subsystem New extensible signal subsystem with new channels JMX signal and notification integration New JDBC storage module Exactly once delivery for PostgreSQL streaming PostgreSQL replica identity changes Oracle RAC improvements Debezium Server Operator for Kubernetes This release introduces a preview version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments. Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this in upcoming release cycles. You can find a deployment example and basic description of the custom resource specification in the github repository that you can use as a reference for the short-term. We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs. New notification subsystem Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle. Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event. Example Notification Event { &quot;id&quot;: &quot;c485ccc3-16ff-47cc-b4e8-b56a57c3bad2&quot;, &quot;aggregate_type&quot;: &quot;Snapshot&quot;, &quot;type&quot;: &quot;Started&quot;, &quot;additional_data&quot;: { ... } } Each notification event consists of an id field, a UUID to identify the notification, an aggregate_type field to which the notification is related based on the concept of domain-driven design, a type field that is mean to given more detail about the aggregate type itself, and an optional additional_data field which consists of a map of string-based key/value pairs with additional information about the event. At this time, there are two notification event types supported by Debezium: Status of the initial snapshot Monitoring of the incremental snapshot Initial Snapshot Notifications An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of "Initial Snapshot" and the type of event will consist of one of three logical values: SKIPPED Represents the initial snapshot was skipped. ABORTED Represents the initial snapshot was aborted. COMPLETED Represents the initial snapshot has concluded successfully. The following is an example of a notification about the completion of the initial snapshot: Example snapshot completed event { &quot;id&quot;: &quot;5563ae14-49f8-4579-9641-c1bbc2d76f99&quot;, &quot;aggregate_type&quot;: &quot;Initial Snapshot&quot;, &quot;type&quot;: &quot;COMPLETED&quot; } Incremental Snapshot Notifications An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of "Incremental Snapshot" and the type will consist of one of several logical values: STARTED Indicates an incremental snapshot has started. PAUSED Indicates an incremental snapshot has been temporarily paused. RESUMED Indicates an incremental snapshot that had been paused has now resumed. STOPPED Indicates an incremental snapshot has stopped. IN_PROGRESS Indicates an incremental snapshot is in-progress. TABLE_SCAN_COMPLETED Indicates an incremental snapshot has concluded for a given table. COMPLETED Indicates that an incremental snapshot has concluded for all tables. Configuring Notifications Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels. Using a Kafka Topic { &quot;notification.enable.channels&quot;: &quot;sink&quot;, &quot;notification.sink.topic.name&quot;: &quot;debezium_notifications&quot;, ... } Using the connector logs { &quot;notification.enable.channels&quot;: &quot;log&quot; } New extensible signal subsystem Debezium has supported the concept of a signal since the introduction of the Incremental Snapshot feature well back in Debezium 1.x. Signals are important as it allows you to provide metadata to instruct Debezium to perform a given task, whether that task is to write an entry to the connector log or perform an ad-hoc incremental snapshot. In Debezium 2.3, the signal subsystem was reworked to introduce the concept called channel, which represents a medium for which Debezium watches or listens and reacts to signals. In previous versions, there was one channel supported universally across connectors, which was the database signal table. In this release, these channels have been unified and the following are available out of the box: Database signal table Kafka signal topic Filesystem JMX But that&#8217;s not all, the signal channel contract is extensible, allowing you to write a custom implementation and make that available to your connectors with ease. JMX signals and notifications integration Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX. Sending signals In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the jconsole window, you can now see there are two new subsections for a connector, a notifications and signal section shown below: The new signals section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from jconsole: Receiving notifications The new notifications section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from jconsole: The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the reset method on the notifications bean once you&#8217;ve processed all notifications. We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX. New JDBC storage subsystem Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform. In order to take advantage of this new module, the following dependency must be added to your project or application: Maven coordinates &lt;dependency&gt; &lt;groupId&gt;io.debezium&lt;/groupId&gt; &lt;artifactId&gt;debezium-storage-jdbc&lt;/artifactId&gt; &lt;version&gt;2.3.0.Final&lt;/version&gt; &lt;/dependency&gt; The following examples show how to configure Offset or Schema History storage via the JDBC storage module: Configuration example for Offset JDBC storage { &quot;offset.storage.jdbc.url&quot;: &quot;&lt;jdbc-connection-url&gt;&quot;, &quot;offset.storage.jdbc.user&quot;: &quot;dbuser&quot;, &quot;offset.storage.jdbc.password&quot;: &quot;secret&quot;, &quot;offset.storage.jdbc.offset_table_name&quot;: &quot;debezium_offset_storage&quot; } Configuration example for Schema History JDBC storage { &quot;schema.history.internal.jdbc.url&quot;: &quot;&lt;jdbc-connection-url&gt;&quot;, &quot;schema.history.internal.jdbc.user&quot;: &quot;dbuser&quot;, &quot;schema.history.internal.jdbc.password&quot;: &quot;secret&quot;, &quot;schema.history.internal.jdbc.schema.history.table.name&quot;: &quot;debezium_database_history&quot; } Exactly once delivery for PostgreSQL streaming Debezium has traditionally been an at-least-once delivery solution, guaranteeing that no change is ever missed. Exactly-Once is a proposal by the Apache Kafka community as a part of KIP-618. This proposal aims to address a common problem with producers (source connectors) when a producer retries, it may re-send a batch of events to the Kafka broker even if that batch had already been committed by the broker. This means there are situations where duplicate events may be sent and not every consumer (sink connector) may be capable of handling such situations easily. Debezium plans to roll out exactly-once delivery semantics in a phased style. There are specific corner cases around snapshotting and streaming and these can vary by connector. And with that, Debezium 2.3 starts out by specifically only adding support for exactly-once semantics for the PostgreSQL during its streaming phase only! In order to take advantage of exactly-once delivery, there is no connector configuration changes required; however, your Kafka Connect worker configuration will need to be adjusted to enable this feature. You can find a reference to the new configuration properties in KIP-618. If you are mixing connectors on the same connect cluster that do and do not support exactly once delivery, you will not be able to set exactly.once.support in the worker configuration to required as this will cause connectors that do not support this feature to fail validation at start-up. PostgreSQL replica identity changes Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity". Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up. The new configuration option, replica.identity.autoset.values, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the documentation. When specifying the replica.identity.autoset.values, the value is a comma-separated list of values where each element uses the format of &lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;. An example is shown below where two tables are configured to have full replica identity: { &quot;replica.identity.autoset.values&quot;: &quot;public.table1:FULL,public.table2:FULL&quot; } Oracle RAC improvements When connecting to an Oracle RAC installation, you must specify a rac.nodes configuration property with a minimum of the host or IP address of all individual nodes across the cluster. Older versions of the connector also supported a varied format, allowing the inclusion of a port for each node as not every node on the cluster may use the same port. Debezium 2.3 improves the Oracle RAC support by also acknowledging that each node may not necessarily use the same Oracle Site Identifier (SID), and therefore, the SID can also now be included in the rac.nodes configuration property to support such installations. The following example illustrates connecting to two Oracle RAC nodes, each using different ports and SID parameters: An example configuration { &quot;connector.class&quot;: &quot;io.debezium.connector.oracle.OracleConnector&quot;, &quot;rac.nodes&quot;: &quot;host1.domain.com:1521/ORCLSID1,host2.domain.com:1522/ORCLSID2&quot;, ... } Other fixes There were quite a number of bugfixes and stability changes in this release, some noteworthy are: Debezium Server stops sending events to Google Cloud Pub/Sub DBZ-5175 Toasted varying character array and date array are not correcly processed DBZ-6122 Upgrade to Infinispan 14.0.11.Final to fix CVE-2022-45047 DBZ-6193 Introduce LogMiner query filtering modes DBZ-6254 Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed DBZ-6256 Ensure that the connector can start from a stale timestamp more than one hour into the past DBZ-6307 The rs_id field is null in Oracle change event source information block DBZ-6329 Add JWT authentication to HTTP Client DBZ-6348 Using pg_replication_slot_advance which is not supported by PostgreSQL10. DBZ-6353 log.mining.transaction.retention.hours should reference last offset and not sysdate DBZ-6355 Support multiple tasks when streaming shard list DBZ-6365 Code Improvements for skip.messages.without.change DBZ-6366 Kinesis Sink - AWS Credentials Provider DBZ-6372 Toasted hstore are not correctly processed DBZ-6379 Oracle DDL shrink space for table partition can not be parsed DBZ-6386 __source_ts_ms r (read) operation date is set to future for SQL Server DBZ-6388 PostgreSQL connector task fails to resume streaming because replication slot is active DBZ-6396 Date and Time values without timezones are not persisted correctly based on database.time_zone DBZ-6399 MongoDB connector crashes on invalid resume token DBZ-6402 Snapshot step 5 - Reading structure of captured tables time too long DBZ-6439 NPE on read-only MySQL connector start up DBZ-6440 Oracle parallel snapshots do not properly set PDB context when using multitenancy DBZ-6457 [MariaDB] Add support for userstat plugin keywords DBZ-6459 Debezium Server cannot recover from Google Pub/Sub errors DBZ-6461 "Ignoring invalid task provided offset" DBZ-6463 Oracle snapshot.include.collection.list should be prefixed with databaseName in documentation. DBZ-6474 Db2 connector can fail with NPE on notification sending DBZ-6485 ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception DBZ-6486 BigDecimal fails when queue memory size limit is in place DBZ-6490 Allow schema to be specified in the Debezium Sink Connector configuration DBZ-6491 ORACLE table can not be captured, got runtime.NoViableAltException DBZ-6492 Signal poll interval has incorrect default value DBZ-6496 Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone DBZ-6502 Alpine postgres images should use llvm/clang 15 explicitly DBZ-6506 FileSignalChannel is not loaded DBZ-6509 Utilize event.processing.failure.handling.mode in Vitess replication connection DBZ-6510 MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization DBZ-6511 Debezium incremental snapshot chunk size documentation unclear or incorrect DBZ-6512 Error value of negative seconds in convertOracleIntervalDaySecond DBZ-6513 Debezium incremental snapshot chunk size documentation unclear or incorrect DBZ-6515 Only use error processing mode on certain errors DBZ-6523 [PostgreSQL] LTree data is not being captured by streaming DBZ-6524 Oracle Connector: Snapshot fails with specific combination DBZ-6528 Use better hashing function for PartitionRouting DBZ-6529 Table order is incorrect on snapshots DBZ-6533 Start publishing nightly images for Debezium Operator DBZ-6541 Start releasing images for Debezium Operator DBZ-6542 Unhandled NullPointerException in PartitionRouting will crash the whole connect plugin DBZ-6543 Bug in field.name.adjustment.mode Property DBZ-6559 Operator sets incorrect value of transformation.predicate when no predicate is specified DBZ-6560 Upgrade MySQL JDBC driver to 8.0.33 DBZ-6563 Upgrade Google Cloud BOM to 26.17.0 DBZ-6570 Kubernetes-Config extension interferes with SSL tests due to k8 devservice starting up DBZ-6574 MySQL read-only connector with Kafka signals enabled fails on start up DBZ-6579 Redis schema history can fail upon startup DBZ-6580 Altogether, 138 issues were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Andrei Isac, Angshuman Dey, Anil Dasari, Anisha Mohanty, Bertrand Paquet, Bob Roldan, Breno Moreira, Chris Cranford, Christian Jacob Mencias, David Beck, Frederic Laurent, Gong Chang Hua, Harvey Yue, Hidetomi Umaki, Hussain Ansari, Indra Shukla, Ismail Simsek, Jakub Cechacek, Jesse Ehrenzweig, Jiri Pechanec, Jochen Schalanda, Kanthi Subramanian, Katerina Galieva, Mario Fiore Vitale, Martin Medek, Miguel Angel Sotomayor, Nancy Xu, Nir Levy, Ondrej Babec, Oren Elias, RJ Nowling, René Kerner, Robert Roldan, Ronak Jain, Sergey Eizner, Shuran Zhang, Stephen Clarkson, Thomas Thornton, Tommy Karlsson, Tony Joseph, Vojtech Juranek, and 蔡灿材! What&#8217;s next? With Debezium 2.3 out, our major focus will be coordinating bugfixes to any reports for Debezium 2.3, but primarily on the upcoming new preview release for Debezium 2.4. Debezium 2.4 is planned with a ton of changes, and we intend to tackle these changes in a bucketed fashion, with each bucket being assigned a priority. As with any schedule, priorities are subject to change but the following is an outline of what to expect: Priority 1 [Core] TimescaleDB single message transformation support [Core] Timezone single message transformation to ease usages with Debezium temporal types [Core] Initial snapshot notifications [MongoDB] Database-wide change stream support [MongoDB] Multi-task deployment metrics support [Oracle] OpenLogReplicator adapter support [Oracle] XML, LONG, RAW, and LONG RAW data type support [Universal] Exactly-Once semantics support for other connectors [Dependencies] Apache Kafka 3.5.x support Priority 2 [Operator] Next steps for Debezium operator [Core] Ad-hoc blocking snapshot [Dependencies] Use OpenTelemetry Priority 3 [Embedded Engine] Parallelization support [MongoDB] Parallel incremental snapshots support [MySQL] Parallel schema snapshots support This is not an exhaustive list and its quite ambitious, but given the shortened time with Debezium 2.3, we hope the extra few weeks on Debezium 2.4 will make this next minor release possible with all these features and much more. The roadmap will be updated this week to align with the above for Debezium 2.4 and the future, so please be sure to get in touch with us on the mailing list or our chat if you have any ideas or suggestions. Otherwise, I expect with summer in full swing for those of us in the north, holidays and much-deserved time-off will be normal for the next several months. To those who travel or intend to take some time for yourselves and family, enjoy and be safe! Until next time&#8230;&#8203;]]></summary></entry><entry><title type="html">Debezium 2.3.0.CR1 Released</title><link href="https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.CR1 Released"/><published>2023-06-12T00:00:00+00:00</published><updated>2023-06-12T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released/"><![CDATA[<div class="paragraph"> <p>It is my pleasure to announce the next Debezium 2.3 release, <strong>2.3.0.CR1</strong>!</p> </div> <div class="paragraph"> <p>The main focus of this release is to stabilize the Debezium 2.3 release in preparation for a final release in the coming weeks, which typically means we&#8217;re focusing on bugfixes; however, this release includes two new features. Lets take a moment and dive into these new features and any bug fixes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>This release includes no breaking changes, so upgrading from Debezium 2.3.0.Beta1 to 2.3.0.CR1 should be a simple drop-in replacement.</p> </div> <div class="paragraph"> <p>If you are upgrading from a prior version, please review the <a href="https://debezium.io/releases/2.3/release-notes">release notes</a> for any migration steps that may be necessary.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_server_kubernetes_operator">Debezium Server Kubernetes Operator</h2> <div class="sectionbody"> <div class="paragraph"> <p>This release introduces a <em>preview</em> version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments.</p> </div> <div class="paragraph"> <p>Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this throughout the remainder of the 2.3 as well into the Debezium 2.4 release cycles. You can find a <a href="https://github.com/debezium/debezium-operator#quickstart-example">deployment example</a> and basic description of the <a href="https://github.com/debezium/debezium-operator#debeziumserverspec-reference">custom resource specification</a> in the github repository that you can use as a reference for the short-term.</p> </div> <div class="paragraph"> <p>We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs.</p> </div> </div> </div> <div class="sect1"> <h2 id="jmx_signals_and_notifications">JMX signals and notifications</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX.</p> </div> <div class="sect2"> <h3 id="sending_signals">Sending signals</h3> <div class="paragraph"> <p>In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the <code>jconsole</code> window, you can now see there are two new subsections for a connector, a notifications and signal section shown below:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-signal-notifications.png" class="responsive-image" alt="JConsole JMX metrics"> </div> </div> <div class="paragraph"> <p>The new <code>signals</code> section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-send-signal.png" class="responsive-image" alt="JConsole Sending Signals"> </div> </div> </div> <div class="sect2"> <h3 id="receiving_notifications">Receiving notifications</h3> <div class="paragraph"> <p>The new <code>notifications</code> section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-receive-notification.png" class="responsive-image" alt="JConsole Receive Notifications"> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the <code>reset</code> method on the notifications bean once you&#8217;ve processed all notifications.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="ulist"> <ul> <li> <p>Code Improvements for skip.messages.without.change <a href="https://issues.redhat.com/browse/DBZ-6366">DBZ-6366</a></p> </li> <li> <p>FileSignalChannel is not loaded <a href="https://issues.redhat.com/browse/DBZ-6509">DBZ-6509</a></p> </li> <li> <p>Utilize event.processing.failure.handling.mode in Vitess replication connection <a href="https://issues.redhat.com/browse/DBZ-6510">DBZ-6510</a></p> </li> <li> <p>MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization <a href="https://issues.redhat.com/browse/DBZ-6511">DBZ-6511</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6512">DBZ-6512</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6515">DBZ-6515</a></p> </li> <li> <p>Only use error processing mode on certain errors <a href="https://issues.redhat.com/browse/DBZ-6523">DBZ-6523</a></p> </li> <li> <p>[PostgreSQL] LTree data is not being captured by streaming <a href="https://issues.redhat.com/browse/DBZ-6524">DBZ-6524</a></p> </li> <li> <p>Use better hashing function for PartitionRouting <a href="https://issues.redhat.com/browse/DBZ-6529">DBZ-6529</a></p> </li> <li> <p>Start publishing nightly images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6541">DBZ-6541</a></p> </li> <li> <p>Start releasing images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6542">DBZ-6542</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.CR1%20ORDER%20BY%20component%20ASC">24 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jehrenzweig-pi">Jesse Ehrenzweig</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/tommyk-gears">Tommy Karlsson</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 quickly approaching a Final release, Debezium 2.4 is just around the corner.</p> </div> <div class="paragraph"> <p>Debezium 2.4 planning is underway, and we would love to have your feedback. Please reach out to us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions. The roadmap for Debezium 2.4 and beyond will be updated in conjunction with Debezium 2.3 final in the coming week.</p> </div> <div class="paragraph"> <p>And until next time, stay cool as summer approaches!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="caassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It is my pleasure to announce the next Debezium 2.3 release, 2.3.0.CR1! The main focus of this release is to stabilize the Debezium 2.3 release in preparation for a final release in the coming weeks, which typically means we&#8217;re focusing on bugfixes; however, this release includes two new features. Lets take a moment and dive into these new features and any bug fixes that are noteworthy!]]></summary></entry><entry><title type="html">Debezium 2.3.0.Beta1 Released</title><link href="https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Beta1 Released"/><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce the next release of the Debezium 2.3 series, <strong>2.3.0.Beta1</strong>!</p> </div> <div class="paragraph"> <p>While this release focuses primarily on bug fixes and stability improvements, there are some new improvements with the PostgreSQL connector and the new notification and channels subsystem. In addition, there are also some compatibility breaking changes.</p> </div> <div class="paragraph"> <p>This release contains changes for <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Beta1%20ORDER%20BY%20issuetype%20DESC">22 issues</a>, so lets take a moment and dive into the new features and any potential bug fixes or breaking changes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium recently introduced the JDBC storage module that allows you to store offsets and schema history data inside a relational database. The JDBC storage module used UTF-16 as its default encoding; however, most databases use UTF-8. This release of Debezium aligns the JDBC storage module&#8217;s encoding to use UTF-8 moving forward.</p> </div> </div> </div> <div class="sect1"> <h2 id="postgresql_replica_identity_changes">PostgreSQL Replica Identity Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity".</p> </div> <div class="paragraph"> <p>Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up.</p> </div> <div class="paragraph"> <p>The new configuration option, <code>replica.identity.autoset.values</code>, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-replica-identity">documentation</a>.</p> </div> <div class="paragraph"> <p>When specifying the <code>replica.identity.autoset.values</code>, the value is a comma-separated list of values where each element uses the format of <code>&lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;</code>. An example is shown below where two tables are configured to have full replica identity:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">replica.identity.autoset.values</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.table1:FULL,public.table2:FULL</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Be mindful that if the user account used by the connector does not have the appropriate database permissions to set a table&#8217;s replica identity, the use of this feature will result in a failure. In the event of a failure due to permissions, you must make sure the proper replica identity is set manually using a database account with the right permissions.</p> </div> </div> </div> <div class="sect1"> <h2 id="correlate_incremental_snapshot_notification_ids">Correlate Incremental Snapshot notification ids</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new notification and channels subsystem. This subsystem allows you to send a signal via a variety of channels that include the filesystem, Kafka topic, and database table out of the box; however, the feature is extendable. In addition, this subsystem also includes the ability to send notifications about the status of the initial snapshots and incremental snapshots if they&#8217;re used. These notifications can help facilitate an easier line of communication between Debezium and other third-party systems that may need to know when an incremental or traditional snapshot has finished and whether it finished successfully or not.</p> </div> <div class="paragraph"> <p>In this release, the notification and channels subsystem has been improved to correlate the signal to the notification. So when you send a signal and it is consumed by Debezium, any notification that is raised will contain a reference to the signal, allowing any third-party or external process to know precisely which signal the notification references.</p> </div> <div class="paragraph"> <p>This should help close the gap in distribution communications across applications or processes relying on the new notification and channel subsystem.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium Server stops sending events to Google Cloud Pub/Sub <a href="https://issues.redhat.com/browse/DBZ-5175">DBZ-5175</a></p> </li> <li> <p>Snapshot step 5 - Reading structure of captured tables time too long <a href="https://issues.redhat.com/browse/DBZ-6439">DBZ-6439</a></p> </li> <li> <p>Oracle parallel snapshots do not properly set PDB context when using multitenancy <a href="https://issues.redhat.com/browse/DBZ-6457">DBZ-6457</a></p> </li> <li> <p>[MariaDB] Add support for userstat plugin keywords <a href="https://issues.redhat.com/browse/DBZ-6459">DBZ-6459</a></p> </li> <li> <p>Debezium Server cannot recover from Google Pub/Sub errors <a href="https://issues.redhat.com/browse/DBZ-6461">DBZ-6461</a></p> </li> <li> <p>Db2 connector can fail with NPE on notification sending <a href="https://issues.redhat.com/browse/DBZ-6485">DBZ-6485</a></p> </li> <li> <p>BigDecimal fails when queue memory size limit is in place <a href="https://issues.redhat.com/browse/DBZ-6490">DBZ-6490</a></p> </li> <li> <p>ORACLE table can not be captrued, got runtime.NoViableAltException <a href="https://issues.redhat.com/browse/DBZ-6492">DBZ-6492</a></p> </li> <li> <p>Signal poll interval has incorrect default value <a href="https://issues.redhat.com/browse/DBZ-6496">DBZ-6496</a></p> </li> <li> <p>Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone <a href="https://issues.redhat.com/browse/DBZ-6502">DBZ-6502</a></p> </li> <li> <p>Alpine postgres images should use llvm/clang 15 explicitly <a href="https://issues.redhat.com/browse/DBZ-6506">DBZ-6506</a></p> </li> <li> <p>ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception <a href="https://issues.redhat.com/browse/DBZ-6486">DBZ-6486</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Beta1%20ORDER%20BY%20component%20ASC">22 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/angsdey2">Angshuman Dey</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 being released under a condensed schedule, you can expect the next CR1 release within the next 1-2 weeks. The plan is to release Debezium 2.3.0.Final in the middle of June and for the team to begin preparation on Debezium 2.4.</p> </div> <div class="paragraph"> <p>As we begin to prepare to move toward Debezium 2.4, we would love to hear your feedback or suggestions. The roadmap will be updated in the coming week, so please be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce the next release of the Debezium 2.3 series, 2.3.0.Beta1! While this release focuses primarily on bug fixes and stability improvements, there are some new improvements with the PostgreSQL connector and the new notification and channels subsystem. In addition, there are also some compatibility breaking changes. This release contains changes for 22 issues, so lets take a moment and dive into the new features and any potential bug fixes or breaking changes that are noteworthy!]]></summary></entry><entry><title type="html">Debezium 2.3.0.Alpha1 Released</title><link href="https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Alpha1 Released"/><published>2023-05-15T00:00:00+00:00</published><updated>2023-05-15T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce the first release of the Debezium 2.3 series, <strong>2.3.0.Alpha1</strong>!</p> </div> <div class="paragraph"> <p>This release brings many new and exciting features as well as bug fixes, including Debezium status notifications, storage of Debezium state into a JDBC data store, configurable signaling channels, the ability to edit connector configurations via Debezium UI, the parallelization of Vitess shards processing, and much more.</p> </div> <div class="paragraph"> <p>This release contains changes for <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Alpha1%20ORDER%20BY%20issuetype%20DESC">59 issues</a>, so lets take a moment and dive into several of these new features and any potential bug fixes or breaking changes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-property-database-sslmode"><code>database.sslmode</code></a> while for MySQL this can be done with <a href="https://debezium.io/documentation/reference/2.3/connectors/mysql.html#mysql-property-database-ssl-mode"><code>database.ssl.mode</code></a>.</p> </div> <div class="paragraph"> <p>With Debezium 2.3, this configuration option no longer defaults to <code>disable</code> (PostgreSQL) or <code>disabled</code> (MySQL) but instead defaults to <code>prefer</code> (PostgreSQL) and <code>preferred</code> (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise.</p> </div> </div> </div> <div class="sect1"> <h2 id="status_notifications">Status Notifications</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle.</p> </div> <div class="paragraph"> <p>Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event.</p> </div> <div class="listingblock"> <div class="title">Example Notification Event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">c485ccc3-16ff-47cc-b4e8-b56a57c3bad2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Started</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">additional_data</span><span class="delimiter">&quot;</span></span>: {
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>Each notification event consists of an <code>id</code> field, a UUID to identify the notification, an <code>aggregate_type</code> field to which the notification is related based on the concept of domain-driven design, a <code>type</code> field that is mean to given more detail about the aggregate type itself, and an optional <code>additional_data</code> field which consists of a map of string-based key/value pairs with additional information about the event.</p> </div> <div class="paragraph"> <p>At this time, there are two notification event types supported by Debezium:</p> </div> <div class="ulist"> <ul> <li> <p>Status of the initial snapshot</p> </li> <li> <p>Monitoring of the incremental snapshot</p> </li> </ul> </div> <div class="sect2"> <h3 id="initial_snapshot_notifications">Initial Snapshot Notifications</h3> <div class="paragraph"> <p>An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of <code>"Initial Snapshot"</code> and the type of event will consist of one of three logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>SKIPPED</code></dt> <dd> <p>Represents the initial snapshot was skipped.</p> </dd> <dt class="hdlist1"><code>ABORTED</code></dt> <dd> <p>Represents the initial snapshot was aborted.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Represents the initial snapshot has concluded successfully.</p> </dd> </dl> </div> <div class="paragraph"> <p>The following is an example of a notification about the completion of the initial snapshot:</p> </div> <div class="listingblock"> <div class="title">Example snapshot completed event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5563ae14-49f8-4579-9641-c1bbc2d76f99</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Initial Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">COMPLETED</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="incremental_snapshot_notifications">Incremental Snapshot Notifications</h3> <div class="paragraph"> <p>An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of <code>"Incremental Snapshot"</code> and the type will consist of one of several logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>STARTED</code></dt> <dd> <p>Indicates an incremental snapshot has started.</p> </dd> <dt class="hdlist1"><code>PAUSED</code></dt> <dd> <p>Indicates an incremental snapshot has been temporarily paused.</p> </dd> <dt class="hdlist1"><code>RESUMED</code></dt> <dd> <p>Indicates an incremental snapshot that had been paused has now resumed.</p> </dd> <dt class="hdlist1"><code>STOPPED</code></dt> <dd> <p>Indicates an incremental snapshot has stopped.</p> </dd> <dt class="hdlist1"><code>IN_PROGRESS</code></dt> <dd> <p>Indicates an incremental snapshot is in-progress.</p> </dd> <dt class="hdlist1"><code>TABLE_SCAN_COMPLETED</code></dt> <dd> <p>Indicates an incremental snapshot has concluded for a given table.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Indicates that an incremental snapshot has concluded for all tables.</p> </dd> </dl> </div> </div> <div class="sect2"> <h3 id="configuring_notifications">Configuring Notifications</h3> <div class="paragraph"> <p>Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels.</p> </div> <div class="listingblock"> <div class="title">Using a Kafka Topic</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">sink</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.sink.topic.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_notifications</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Using the connector logs</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">log</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="jdbc_storage_module">JDBC Storage Module</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new module, the following dependency must be added to your project or application:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>debezium-storage-jdbc<span class="tag">&lt;/artifactId&gt;</span>
    <span class="tag">&lt;version&gt;</span>2.3.0.Alpha1<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>The following examples show how to configure Offset or Schema History storage via the JDBC storage module:</p> </div> <div class="listingblock"> <div class="title">Configuration example for Offset JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.offset_table_name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_offset_storage</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Configuration example for Schema History JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.schema.history.table.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_database_history</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Toasted varying character array and date array are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6122">DBZ-6122</a></p> </li> <li> <p>Introduce LogMiner query filtering modes <a href="https://issues.redhat.com/browse/DBZ-6254">DBZ-6254</a></p> </li> <li> <p>Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed <a href="https://issues.redhat.com/browse/DBZ-6256">DBZ-6256</a></p> </li> <li> <p>Ensure that the connector can start from a stale timestamp more than one hour into the past <a href="https://issues.redhat.com/browse/DBZ-6307">DBZ-6307</a></p> </li> <li> <p>The rs_id field is null in Oracle change event source information block <a href="https://issues.redhat.com/browse/DBZ-6329">DBZ-6329</a></p> </li> <li> <p>Add JWT authentication to HTTP Client <a href="https://issues.redhat.com/browse/DBZ-6348">DBZ-6348</a></p> </li> <li> <p>Using pg_replication_slot_advance which is not supported by PostgreSQL10. <a href="https://issues.redhat.com/browse/DBZ-6353">DBZ-6353</a></p> </li> <li> <p>log.mining.transaction.retention.hours should reference last offset and not sysdate <a href="https://issues.redhat.com/browse/DBZ-6355">DBZ-6355</a></p> </li> <li> <p>Support multiple tasks when streaming shard list <a href="https://issues.redhat.com/browse/DBZ-6365">DBZ-6365</a></p> </li> <li> <p>Kinesis Sink - AWS Credentials Provider <a href="https://issues.redhat.com/browse/DBZ-6372">DBZ-6372</a></p> </li> <li> <p>Toasted hstore are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6379">DBZ-6379</a></p> </li> <li> <p>Oracle DDL shrink space for table partition can not be parsed <a href="https://issues.redhat.com/browse/DBZ-6386">DBZ-6386</a></p> </li> <li> <p>__source_ts_ms r (read) operation date is set to future for SQL Server <a href="https://issues.redhat.com/browse/DBZ-6388">DBZ-6388</a></p> </li> <li> <p>PostgreSQL connector task fails to resume streaming because replication slot is active <a href="https://issues.redhat.com/browse/DBZ-6396">DBZ-6396</a></p> </li> <li> <p>MongoDB connector crashes on invalid resume token <a href="https://issues.redhat.com/browse/DBZ-6402">DBZ-6402</a></p> </li> <li> <p>NPE on read-only MySQL connector start up <a href="https://issues.redhat.com/browse/DBZ-6440">DBZ-6440</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Alpha1%20ORDER%20BY%20component%20ASC">59 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/bpaquet">Bertrand Paquet</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/fredlegaucher">Frederic Laurent</a>, <a href="https://github.com/ddsr-ops">Gong Chang Hua</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/Hidetomi">Hidetomi Umaki</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/miguelbirdie">Miguel Angel Sotomayor</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/zalmane">Oren Elias</a>, <a href="https://github.com/rnowling-memphis">RJ Nowling</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/sclarkson-zoomcare">Stephen Clarkson</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, and <a href="https://github.com/caicancai">蔡灿材</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 underway, I do expect a rather quick cycle of alpha, beta, and final releases over the next six weeks. We still have a lot to do in this time period that we hope to get into this release, so stay tuned. As we get closer to the end of June, we&#8217;ll begin our planning for Debezium 2.4!</p> </div> <div class="paragraph"> <p>Also, Red Hat Summit 2023 is next week in Boston. There will be a break-out session where Hugo and Chris will be discussing the new Debezium JDBC sink connector. If you&#8217;re able to attend, we&#8217;d love to have an opportunity to chat with you before or after the session.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce the first release of the Debezium 2.3 series, 2.3.0.Alpha1! This release brings many new and exciting features as well as bug fixes, including Debezium status notifications, storage of Debezium state into a JDBC data store, configurable signaling channels, the ability to edit connector configurations via Debezium UI, the parallelization of Vitess shards processing, and much more. This release contains changes for 59 issues, so lets take a moment and dive into several of these new features and any potential bug fixes or breaking changes that are noteworthy!]]></summary></entry><entry><title type="html">Image classification with Debezium and TensorFlow</title><link href="https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification/" rel="alternate" type="text/html" title="Image classification with Debezium and TensorFlow"/><published>2023-05-02T11:20:00+00:00</published><updated>2023-05-02T11:20:00+00:00</updated><id>https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification</id><content type="html" xml:base="https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification/"><![CDATA[<div class="paragraph"> <p>With the recent success of ChatGPT, we can observe another wave of interest in the AI field and machine learning in general. The previous wave of interest in this field was, at least to a certain extent, caused by the fact that excellent ML frameworks like <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a> or general data processing frameworks like <a href="https://spark.apache.org/">Spark</a> became available and made the writing of ML models much more straightforward. Since that time, these frameworks have matured, and writing models are even more accessible, as you will see later in this blog. However, data set preparation and gathering data from various sources can sometimes take time and effort. Creating a complete pipeline that would pull existing or newly created data, adjust it, and ingest it into selected ML libraries can be challenging. Let&#8217;s investigate if Debezium can help with this task and explore how we can leverage Debezium&#8217;s capabilities to make it easier.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="change_data_capture_and_debezium_in_ml_pipelines">Change data capture and Debezium in ML pipelines</h2> <div class="sectionbody"> <div class="paragraph"> <p>Change data capture (CDC) can be a compelling concept in machine learning, especially in online machine learning. However, using pre-trained models, CDC can also be an essential part of the pipeline. We can use CDC to deliver new data immediately into a pre-trained model, which can evaluate it, and other parts of the pipeline can take any action based on the model output in real-time.</p> </div> <div class="paragraph"> <p>Besides these use cases, Debezium is a perfect fit for any pipeline, including loading data from databases. Debezium can capture existing data as well as stream any newly created data. Another vital feature of Debezium is support for single message transforms. We can adjust the data at the very beginning of the whole pipeline. When applying transformations or filters, we can restrict data transmission over the wire to only that is of interest, saving bandwidth and speed within the pipeline. Additionally, Debezium can deliver records to several message brokers, and more brokers are being added (several new ones are available in the recent 2.2.0 release). These continued improvements increase the opportunity to integrate Debezium with other toolchains or data pipelines. The possibilities are endless, and Debezium&#8217;s common connector framework could allow for CDC beyond just databases.</p> </div> <div class="paragraph"> <p>So, this is the theory. Now let&#8217;s explore how it works in reality. This blog post will look at how to stream data into TensorFlow. Based on the interest from the community, this may result in a series of blog posts where we explore possible integrations with other ML libraries and frameworks.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_and_tensorflow_integration">Debezium and TensorFlow integration</h2> <div class="sectionbody"> <div class="paragraph"> <p>TensorFlow is one of the most popular machine learning frameworks. It provides a comprehensive platform for building, training, and deploying machine learning models across various applications.</p> </div> <div class="paragraph"> <p>To keep things simple, we will implement a model for recognizing handwritten digits, which is more or less the Hello World equivalent in the neural networks field. The ultimate goal of this demo is to use Debezium to load MNIST data samples from Postgres that are continuously stored, pass it to our model implemented in Tensorflow for training, and use this trained model for real-time classification of images</p> </div> <div class="paragraph"> <p>The diagram below depicts the complete pipeline:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/pipeline.png" class="responsive-image" alt="Debezium TensorFlow integration pipeline"> </div> <div class="paragraph"> <p>All the code mentioned later in this blog is available as a Debezium example in <a href="https://github.com/debezium/debezium-examples/tree/main/tensorflow-mnist">Debezium example repository</a>.</p> </div> <div class="sect2"> <h3 id="the_data_sample">The data sample</h3> <div class="paragraph"> <p>We will use <a href="http://yann.lecun.com/exdb/mnist/">MNIST data sample</a>. The training sample contains 60,000 images with handwritten digits from 0 to 9 and the same amount of labels with corresponding digits. The test sample contains 1,000 images. The samples are available as gzip binaries. As we assume a use case where the data of interest are in the database, we need to load the data into the database first.</p> </div> <div class="paragraph"> <p>We need to generate two SQL files, one for the train data set, <code>mnist_train.sql</code>, and one for a test data sample, <code>mnist_test.sql</code>. Each file would contain SQL commands for creating a table with two columns: <code>pixels</code> column of type <code>BYTEA</code>, which would contain raw image bytes, and <code>labels</code> column of type <code>SMALLINT</code>, which would contain digit corresponding to the image in given table row. The rest of the file would contain commands for populating the table. Image bytes can be decoded as a HEX string.</p> </div> <div class="paragraph"> <p>As we will show how to leverage Debezium for data streaming later in this post, we will initially load the training data set into the database. The SQL file with training data will be used directly by the Postgres container - when it starts, it will load this data into the training table. We will use the test data SQL file later. However, the preparation of the data is the same for training as well as test samples, and we can prepare both of them in one go.</p> </div> <div class="paragraph"> <p>To prepare these SQL files, you can use <code>mnist2sql.py</code> script from <a href="https://github.com/debezium/debezium-examples/tree/main/tensorflow-mnist">Debezium tensorflow-mnist example</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ ./mnist2sql.py --download</code></pre> </div> </div> <div class="paragraph"> <p>The script assumes MNIST data sets are available in the <code>postgres</code> directory. When using the <code>--download</code> parameter, the script first downloads MNIST data samples into the <code>postgres</code> directory. The <code>postgres</code> directory will contain the resulting SQL files.</p> </div> </div> <div class="sect2"> <h3 id="loading_streamed_data_into_tensorflow">Loading streamed data into Tensorflow</h3> <div class="paragraph"> <p>The most common Debezium usage is the streaming of records to Kafka. TensorFlow provides <a href="https://www.tensorflow.org/io">TensorFlow I/O</a> module for loading data from various sources. Besides other sources, it also allows loading the data from Kafka. There are several ways to do it. <a href="https://www.tensorflow.org/io/api_docs/python/tfio/IODataset#from_kafka">IODataset.from_kafka()</a> method loads only existing data from specified Kafka topics. Two experimental classes support streaming data, <a href="https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaBatchIODataset">KafkaBatchIODataset</a> and <a href="https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaGroupIODataset">KafkaGroupIODataset</a>. Both are very similar and allow them to work with streaming data, i.e., they not only read the existing data from a Kafka topic but also wait for new data and eventually pass new records into the TensorFlow. Streaming concludes when there are no new events within a specified time frame.</p> </div> <div class="paragraph"> <p>In all cases, a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">Dataset</a> represents all loaded records in Tensorflow. This Tensorflow data structure provides convenience for building <a href="https://www.tensorflow.org/guide/data">data pipelines</a>, which may include further data transformations or preprocessing.</p> </div> <div class="paragraph"> <p>This sounds great. However, the most significant caveat is the representation of records within the Dataset. These Kafka loaders completely ignore the schema of the records provided by Kafka, meaning that keys and values are raw bytes of data. Additionally, the ingestion pipeline complicates the process by converting these into strings (i.e., <code>toString()</code> on the object called). So if you pass, e.g., raw image bytes via Kafka, using Kafka <code>BYTES_SCHEMA</code>, it would result in something like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>&lt;tf.Tensor: shape=(64,), dtype=string, numpy=
 array([b'[B@418b353d', b'[B@6aa28a4c', b'[B@b626485', b'[B@6d7491cd',
        b'[B@13fa86c5', b'[B@7c3bc352', b'[B@64e5d61c', b'[B@2dd6d9b4',
        b'[B@6addae65', b'[B@48ded13f', b'[B@2c1bb0e', b'[B@19c1d99b',
        b'[B@1ee8f240', b'[B@20019f8b', b'[B@2f17494e', b'[B@380d4036',
        b'[B@61aecf85', b'[B@4d7fe9fc', b'[B@58b79424', b'[B@ae963f4',
        b'[B@1dac57cb', b'[B@2fae7d8b', b'[B@4b5ccaee', b'[B@aebf6b2',
        b'[B@7506ea2b', b'[B@29989325', b'[B@43e2742', b'[B@51350f11',
        b'[B@13a0f0ae', b'[B@7e4c4844', b'[B@b3d64f8', b'[B@7209bf09',
        b'[B@66380466', b'[B@7aaa7e8d', b'[B@1ad0cf84', b'[B@259eca20',
        b'[B@3a3f1c1', b'[B@36e4ff1f', b'[B@6578fc29', b'[B@79c924be',
        b'[B@765b7f70', b'[B@67567aa3', b'[B@456d4bd4', b'[B@75317b13',
        b'[B@58bc3a3a', b'[B@c6bc0ec', b'[B@2377095e', b'[B@5de017c0',
        b'[B@64b48bac', b'[B@360a5b76', b'[B@2d2c9910', b'[B@70afd562',
        b'[B@3006c930', b'[B@54b3e5ad', b'[B@1d1e0232', b'[B@1394d036',
        b'[B@155dd43d', b'[B@5e88d5b6', b'[B@33ea53c7', b'[B@64a30ec',
        b'[B@7dcdf024', b'[B@6570bf4e', b'[B@4e5bc4c', b'[B@537f216c'],
       dtype=object)&gt;,</code></pre> </div> </div> <div class="paragraph"> <p>Instead of getting a batch of raw image bytes which you can further transform in TensorFlow, you get only string representation of Java byte arrays, which is not very useful.</p> </div> <div class="paragraph"> <p>The most straightforward solution would be to convert the raw image bytes into numbers before sending them to Kafka to mitigate the problem. As TensorFlow provides methods for parsing CSV input, we can convert each image into one CSV line of numbers. Since Tensorflow primarily works with numbers, we would be required to convert the images to numbers regardless. We can pass the number on the image as a message key. Now, a single message transform supported by Debezium comes in handy. The transformation can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">    <span class="annotation">@Override</span>
    <span class="directive">public</span> R apply(R r) {
        <span class="directive">final</span> <span class="predefined-type">Struct</span> value = (<span class="predefined-type">Struct</span>) r.value();
        <span class="predefined-type">String</span> key = value.getInt16(labelFieldName).toString();

        <span class="predefined-type">StringBuilder</span> builder = <span class="keyword">new</span> <span class="predefined-type">StringBuilder</span>();
        <span class="keyword">for</span> (<span class="type">byte</span> pixel : value.getBytes(pixlesFieldName)) {
            builder.append(pixel &amp; <span class="hex">0xFF</span>).append(<span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span>);
        }
        <span class="keyword">if</span> (builder.length() &gt; <span class="integer">0</span>) {
            builder.deleteCharAt(builder.length() - <span class="integer">1</span>);
        }
        <span class="predefined-type">String</span> newValue = builder.toString();

        <span class="keyword">return</span> r.newRecord(r.topic(), r.kafkaPartition(), <span class="predefined-type">Schema</span>.STRING_SCHEMA, key, <span class="predefined-type">Schema</span>.STRING_SCHEMA, newValue, r.timestamp());
    }</code></pre> </div> </div> <div class="paragraph"> <p>On the TensorFlow side, we must convert bytes obtained from Kafka messages into numbers. The following illustrates a map function to handle this easily:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">decode_kafka_record</span>(record):
    img_int = tf.io.decode_csv(record.message, [[<span class="float">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / <span class="float">255.</span>
    label_int = tf.strings.to_number(record.key, out_type=tf.dtypes.int32)
    <span class="keyword">return</span> (img_norm, label_int)</code></pre> </div> </div> <div class="paragraph"> <p>Here we parse CSV lines, potentially provided as the raw bytes, and immediately scale the numbers within the &lt;0, 1&gt; interval, which is convenient for training our model later. Loading the data and creating data batches is very straightforward:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">train_ds = tfio.IODataset.from_kafka(KAFKA_TRAIN_TOPIC, partition=<span class="integer">0</span>, offset=<span class="integer">0</span>, servers=KAFKA_SERVERS)
train_ds = train_ds.map(decode_kafka_record)
train_ds = train_ds.batch(BATCH_SIZE)</code></pre> </div> </div> <div class="paragraph"> <p>Here we use <code>IODataset.from_kafka()</code> for loading existing data from the Kafka topic, use our map function to convert bytes into numbers, and scale the numbers. As a last step, we create batches from the data set for more efficient processing. Parameters of <code>tfio.IODataset.from_kafka()</code> are self-explanatory and probably don&#8217;t need further comments.</p> </div> <div class="paragraph"> <p>As a result, we have a data set formed by two-dimensional tensors. The first dimension is a vector of floats representing the image, while the second dimension is a single number (scalar) describing the number on the picture. Once we have prepared our training data set, we can define our neural network model.</p> </div> </div> <div class="sect2"> <h3 id="defining_the_model">Defining the model</h3> <div class="paragraph"> <p>To keep things simple, as the main goal of this post is not to show the best handwritten digit classifier, but to show how to create the data pipeline, let&#8217;s use a very simple model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(<span class="integer">128</span>, activation=<span class="string"><span class="delimiter">'</span><span class="content">relu</span><span class="delimiter">'</span></span>),
    tf.keras.layers.Dense(<span class="integer">10</span>)
])</code></pre> </div> </div> <div class="paragraph"> <p>This model contains only two layers. Although this model is really simple, it still does a pretty good job in recognition of handwritten digits. Probably more interesting than the model itself is how easy it is to write a mode in TensorFlow (or actually <a href="https://keras.io/">Keras</a>, but it&#8217;s now part of TensorFlow).</p> </div> <div class="paragraph"> <p>Similarly easy is to define model optimizer and the loss function:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.compile(
    optimizer=tf.keras.optimizers.Adam(<span class="float">0.001</span>),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="predefined-constant">True</span>),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)</code></pre> </div> </div> <div class="paragraph"> <p>It&#8217;s outside of this post&#8217;s scope to explain these functions, and you can check almost any machine learning online course or textbook on this topic for a detailed explanation.</p> </div> <div class="paragraph"> <p>Once we have our model ready, we can train it on the trained dataset prepared in the previous section:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.fit(train_ds,epochs=MAX_EPOCHS)</code></pre> </div> </div> <div class="paragraph"> <p>This step may take quite some time to finish. However, once finished, our model is ready to recognize handwritten digits!</p> </div> </div> <div class="sect2"> <h3 id="streaming_the_data_into_the_model">Streaming the data into the model</h3> <div class="paragraph"> <p>Let&#8217;s see how good our model is in digit recognition. But as our primary goal here is to explore the means how to ingest data into TensorFlow, we will start model evaluation on an empty (or, more accurately, even non-existing) Kafka topic and see if we will be able to evaluate the data on the fly as they will pop-up first in the database and then in the corresponding Kafka topic. For this purpose, we can use one of the streaming classes mentioned above:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">test_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)</code></pre> </div> </div> <div class="paragraph"> <p>Again, arguments are mostly self-explanatory. Two things may need further explanation: <code>stream_timeout</code> and <code>configuration</code> parameters. <code>stream_timeout</code> determines the interval of inactivity (in milliseconds) after which the streaming would terminate. <code>configuration</code> is <a href="https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md">librdkafka configuration</a>. It&#8217;s a configuration of the Kafka client; you should configure at least the session timeout (<code>session.timeout.ms</code>), and it&#8217;s poll interval (<code>max.poll.interval.ms</code>). The values of these parameters should be higher than the value of <code>stream_timeout</code>.</p> </div> <div class="paragraph"> <p>The dataset this loader provides is slightly different - instead of providing a single record containing the message and its key, we get the key and message already split. Therefore, we have to define a slightly modified map function with two arguments:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">decode_kafka_stream_record</span>(message, key):
    img_int = tf.io.decode_csv(message, [[<span class="float">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / <span class="float">255.</span>
    label_int = tf.strings.to_number(key, out_type=tf.dtypes.int32)
    <span class="keyword">return</span> (img_norm, label_int)</code></pre> </div> </div> <div class="paragraph"> <p>With this function, we can adjust the dataset and create batches as before:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">test_ds = test_ds.map(decode_kafka_stream_record)
test_ds = test_ds.batch(BATCH_SIZE)</code></pre> </div> </div> <div class="paragraph"> <p>and evaluate the model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.evaluate(test_ds)</code></pre> </div> </div> <div class="paragraph"> <p>You can execute a cell with model evaluation in the Jupyter notebook. The execution will wait because there is no such topic in Kafka and no table with test data in the database. The streaming timeout is 9 seconds, so data must be provided within this time frame after launching the model evaluation. At the start of this demo, we created a SQL file in the <code>postgres</code> directory called <code>mnist_test.sql</code>, which can generate the test data we need:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ export PGPASSWORD=postgres
$ psql -h localhost -U postgres -f postgres/mnist_test.sql</code></pre> </div> </div> <div class="paragraph"> <p>After a short while, you should see in the Jupyter notebook output that some data arrived into the model and, a few moments later final evaluation of the model.</p> </div> <div class="paragraph"> <p>To make the results closer to humans, let&#8217;s define an image manually and serve it to the model. We can also easily show the image in the Jypiter notebook. The function for plotting the images and providing model predictions as a plot title can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">plot_and_predict</span>(pixels):
    test = tf.constant([pixels])
    tf.shape(test)
    test_norm = tf.cast(test, tf.float32) / <span class="float">255.</span>

    prediction = model.predict(test_norm)
    number = tf.nn.softmax(prediction).numpy().argmax()

    pixels_array = np.asarray(pixels)
    raw_img = np.split(pixels_array, <span class="integer">28</span>)
    plt.imshow(raw_img)
    plt.title(number)
    plt.axis(<span class="string"><span class="delimiter">&quot;</span><span class="content">off</span><span class="delimiter">&quot;</span></span>)</code></pre> </div> </div> <div class="paragraph"> <p>Probably the only cryptic line in this function is the one containing the <code>softmax()</code> function. This function converts the resulting vector into a vector of probabilities. Elements of this vector express the probability that the number on a given position is the one on the image. Therefore, the position with the highest probability is the model&#8217;s prediction, where <code>argmax()</code> is derived.</p> </div> <div class="paragraph"> <p>We can try it, e.g., for this image, which contains the handwritten number 3:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>pixels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,108,43,6,6,6,6,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,84,248,254,254,254,254,254,241,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,90,254,254,254,223,173,173,173,253,156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,79,157,228,245,251,188,63,17,0,0,54,252,132,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,254,254,254,244,131,0,0,0,0,13,220,254,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,254,225,160,47,0,0,0,0,59,211,254,206,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,21,14,0,0,0,2,17,146,245,250,194,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,140,140,171,254,254,254,203,55,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,254,254,254,179,211,254,254,202,171,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,167,233,193,69,16,3,9,16,107,231,248,195,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,73,229,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,99,252,254,146,0,0,0,0,0,0,0,0,79,142,0,0,0,0,0,0,0,0,0,26,28,116,147,247,254,239,150,22,0,0,0,0,0,0,0,0,175,230,174,155,66,66,132,174,174,174,174,250,255,254,192,189,99,36,0,0,0,0,0,0,0,0,0,0,106,226,254,254,254,254,254,254,254,254,217,151,80,43,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,7,114,114,114,46,5,5,5,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
plot_and_predict(pixels)</code></pre> </div> </div> <div class="paragraph"> <p>The result would be as follows:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_digit.png" class="responsive-image" alt="TensorFlow digit recognition"> </div> <div class="paragraph"> <p>You can do the same by reading from a Kafka stream, and we can reuse existing topics for this purpose. As we already read all records from the test stream, we need to change the Kafka consumer group if we want to reread it using streaming <code>KafkaGroupIODataset</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">mnistcg2</span><span class="delimiter">&quot;</span></span>,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)</code></pre> </div> </div> <div class="paragraph"> <p>If you want to create a new stream and verify that our model can provide prediction as the new data arrives, you can easily do so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ head -5 mnist_test.sql | sed s/test/manual/ &gt; mnist_manual.sql
$ psql -h localhost -U postgres -f postgres/mnist_manual.sql</code></pre> </div> </div> <div class="paragraph"> <p>In such case you don&#8217;t need to change Kafka consumer group, but you have to change the Kafka topic:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tf.public.mnist_manual</span><span class="delimiter">&quot;</span></span>],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)</code></pre> </div> </div> <div class="paragraph"> <p>In either case, the result should look like this:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_streaming.png" class="responsive-image" alt="TensorFlow digit recognition from streaming"> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusions">Conclusions</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this demo, we have shown how to load existing data from the database, transform it on the fly, ingest it into the TensorFlow model via Kafka, and use it for model training. Later on, we ingested newly created data into this pre-trained model using CDC and data streaming and obtained meaningful results. Debezium can provide valuable service not only for use cases like the one described in this post but can also play a key role in ingesting data to online machine learning pipelines.</p> </div> <div class="paragraph"> <p>While the whole pipeline is relatively easy to implement, some areas can be improved to improve the user experience and/or make the entire pipeline more smooth. As our (Debezium developers) background is not primarily in machine learning and data science, we would appreciate any input from the community on how Debezium can aid machine learning pipelines (or is already used, if there are any such cases) and where are the rooms for improvements. We would also appreciate any new ideas on how Debezium, or in general, change data capture, can be helpful in this area. These ideas further reveal Debezium&#8217;s potential to ingest data into machine learning pipelines and contribute to better user experience in the whole process. In case you have any input any this regard, don&#8217;t hesitate to reach out to us on the <a href="http://debezium.zulipchat.com/">Zulip chat</a>, <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or you can transform your ideas directly into <a href="http://issues.redhat.com/projects/DBZ/issues">Jira feature requests</a>.</p> </div> </div> </div>]]></content><author><name>Vojtěch Juránek</name></author><category term="machine-learning"/><category term="tensorflow"/><category term="examples"/><category term="apache-kafka"/><summary type="html"><![CDATA[With the recent success of ChatGPT, we can observe another wave of interest in the AI field and machine learning in general. The previous wave of interest in this field was, at least to a certain extent, caused by the fact that excellent ML frameworks like TensorFlow, PyTorch or general data processing frameworks like Spark became available and made the writing of ML models much more straightforward. Since that time, these frameworks have matured, and writing models are even more accessible, as you will see later in this blog. However, data set preparation and gathering data from various sources can sometimes take time and effort. Creating a complete pipeline that would pull existing or newly created data, adjust it, and ingest it into selected ML libraries can be challenging. Let&#8217;s investigate if Debezium can help with this task and explore how we can leverage Debezium&#8217;s capabilities to make it easier.]]></summary></entry><entry><title type="html">New Debezium images will be available only Quay.io in the future</title><link href="https://debezium.io/blog/2023/04/25/container-images-quayio/" rel="alternate" type="text/html" title="New Debezium images will be available only Quay.io in the future"/><published>2023-04-25T10:11:11+00:00</published><updated>2023-04-25T10:11:11+00:00</updated><id>https://debezium.io/blog/2023/04/25/container-images-quayio</id><content type="html" xml:base="https://debezium.io/blog/2023/04/25/container-images-quayio/"><![CDATA[<div class="paragraph"> <p>As you may have noticed, the Docker company recently announced a reduction of the free organization accounts offering. The Docker company wanted to provide for free organization accounts only for <a href="https://www.docker.com/community/open-source/application/">Docker-Sponsored Open Source (DSOS) projects</a>. Debezium project doesn&#8217;t meet their definition of open source project as we have a pathway to commercialization. As the accounts ought to be terminated in 30 days, we immediately started to work on moving out the Debezium project from Docker Hub.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Based on the feedback from the community, the Docker company later on <a href="https://www.docker.com/blog/no-longer-sunsetting-the-free-team-plan/">re-evaluated their decision</a> and Free Team plan is still available as before. However, the whole story, and especially the initial intention to give projects which don&#8217;t meet DSOS conditions only 30 days for migration, undermined our trust in the Docker company and raised a question of what will come in the future. As a result we decided <strong>not to publish Debezium images on Docker Hub</strong> in the future.</p> </div> <div class="paragraph"> <p>For quite some time we already publish all Debezium images into two container image registries:</p> </div> <div class="ulist"> <ul> <li> <p>on the <a href="https://hub.docker.com/u/debezium">Docker Hub</a></p> </li> <li> <p>on the <a href="https://quay.io/organization/debezium/">Quay.io</a></p> </li> </ul> </div> <div class="paragraph"> <p>Upcomming 2.2 release and previews of 2.3 (including CR releases), will be still available on the Docker Hub, but starting 2.3.0.Final release, we will stop publishing images there. Images of Debezium 2.3.0.Final and subsequent releases will be available only on the <a href="https://quay.io/organization/debezium/">Quay.io</a>. Older, already published, images will be of course still available through Docker Hub (unless Docker company changes the conditions which would prevent it in the future). Older Debezium images can be found also on <a href="https://quay.io/organization/debezium/">Quay.io</a>.</p> </div> <div class="paragraph"> <p><a href="https://quay.io">Quay.io</a> is a mature container registry service, which provides additional features like e.g. vulnerability scans. As the <a href="https://quay.io">Quay.io</a> is run and sponsored by Red Hat, and we already publish the image there, it was a natural choice for us to move to this container registry.</p> </div> <div class="paragraph"> <p>How to migrate to <a href="https://quay.io/organization/debezium/">Quay.io</a>? It&#8217;s very simple - just add <code>quay.io/</code> prefix to the container image name, e.g. instead of running</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker pull debezium/connect:latest</code></pre> </div> </div> <div class="paragraph"> <p>you run</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker pull quay.io/debezium/connect:latest</code></pre> </div> </div> <div class="paragraph"> <p>and similar for any other images or commands.</p> </div> <div class="paragraph"> <p>If you have any questions or issues with using <a href="https://quay.io">Quay.io</a> images, don&#8217;t hesitate to reach to us and raise your questions or concerns in our <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">user chat room</a>.</p> </div>]]></content><author><name>Vojtěch Juránek</name></author><category term="news"/><category term="docker"/><category term="containers"/><summary type="html"><![CDATA[As you may have noticed, the Docker company recently announced a reduction of the free organization accounts offering. The Docker company wanted to provide for free organization accounts only for Docker-Sponsored Open Source (DSOS) projects. Debezium project doesn&#8217;t meet their definition of open source project as we have a pathway to commercialization. As the accounts ought to be terminated in 30 days, we immediately started to work on moving out the Debezium project from Docker Hub.]]></summary></entry></feed>