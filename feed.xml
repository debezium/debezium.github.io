<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://debezium.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debezium.io/" rel="alternate" type="text/html"/><updated>2023-06-30T17:17:40+00:00</updated><id>https://debezium.io/feed.xml</id><title type="html">Debezium</title><subtitle>Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Debezium is durable and fast, so your apps can respond quickly and never miss an event, even when things go wrong.</subtitle><entry><title type="html">Debezium for Oracle - Part 3: Performance and Debugging</title><link href="https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3/" rel="alternate" type="text/html" title="Debezium for Oracle - Part 3: Performance and Debugging"/><published>2023-06-29T12:00:00+00:00</published><updated>2023-06-29T12:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3</id><content type="html" xml:base="https://debezium.io/blog/2023/06/29/debezium-oracle-series-part-3/"><![CDATA[<div class="paragraph"> <p>This post is the final part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first installment of this series is found <a href="/blog/2022/09/30/debezium-oracle-series-part-1/">here</a> and the second installment is found <a href="/blog/2022/10/06/debezium-oracle-series-part-2">here</a>.</p> </div> <div class="paragraph"> <p>In this third and final installment, we are going to build on what we have done in the previous two posts, focusing on the following areas:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#performance">Performance optimizations</a></p> </li> <li> <p><a href="#debugging">How to debug and assess problems and how to resolve them</a></p> </li> </ul> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="what_to_know_before_we_start">What to know before we start</h2> <div class="sectionbody"> <div class="paragraph"> <p>This installment is going to focus on a number of highly technical Oracle database features, and these often are within the realm of an Oracle database administrator&#8217;s area of expertise. Depending on your permissions and roles within your environment, you may not necessarily have access to all the tools and commands that will be discussed herein, so you may need to coordinate with your database administrator if you are working in a non-local environment.</p> </div> <div class="paragraph"> <p>Additionally, this post picks where we last left off in <a href="/blog/2022/10/06/debezium-oracle-series-part-2/">Part 2</a>. We had previously deployed a full Kafka ecosystem with Zookeeper, a Kafka broker, and a Kafka Connect environment. These are not necessarily a requirement in this series, but if you would like to follow along manually, I encourage you to follow the steps in the earlier parts of the series to quickly get started.</p> </div> </div> </div> <div class="sect1"> <h2 id="what_is_oracle_and_why_is_it_complex">What is Oracle and why is it complex?</h2> <div class="sectionbody"> <div class="paragraph"> <p>I touched on the first part of this briefly in <a href="/blog/2022/09/30/debezium-oracle-series-part-1">Part 1</a>, but I believe it bears a much deeper answer to support some of the technical concepts we&#8217;re going to discuss today.</p> </div> <div class="paragraph"> <p>A common take-away from newcomers or those unfamiliar with Oracle often ask, "Why is everything with Oracle always complex?". Most often, these users either have little Oracle database experience or have exposure to other open source database solutions that in broad, general terms are easier and simpler to use, particularly directly out of the box. So why is that?</p> </div> <div class="paragraph"> <p>Oracle was first released to the public over four decades ago in 1979, making it the oldest and the most popular database in the world. In the top five most popular, Microsoft SQL Server is the next oldest at three decades released in 1989 and the others all being half the age of Oracle or less.</p> </div> <div class="paragraph"> <p>What has helped drive Oracle&#8217;s market share has been its ability to innovate fast, retain compatibility with existing database platform users, and being flexible enough to provide you features now that you may not need until the future. This has lent itself to allow Oracle to thrive unlike its competition, but we all know that flexibility often comes at the cost of other things; and traditionally that has been at the hand of ease of use. Oracle has a broad and extensive arsenal of tools at your disposal, but these are often tightly coupled leading to complex installations and configurations, but the advantage beyond its flexibility is that it&#8217;s the best at what it does, so often the trade-off is worth it.</p> </div> <div class="paragraph"> <p>As we go through the following parts of this installment, I hope this context provides a newfound perspective. While it may seem commonplace to think that many of Oracle&#8217;s oddities when compared to its competition are pain points, in reality they&#8217;re strengths that have perpetually defined Oracle at the forefront of a critical space in the age of big data.</p> </div> </div> </div> <div class="sect1"> <h2 id="performance">Performance</h2> <div class="sectionbody"> <div class="paragraph"> <p>When choosing to use change data capture (CDC) pipelines, low-latency is very often a driving factor. Whether you are using CDC to provide event-driven communication between microservices or complex data replication, it&#8217;s important that events arrive as soon as possible, so throughput and performance are often at the forefront of evaluating a solution&#8217;s merit, often immediately after reliability.</p> </div> <div class="paragraph"> <p>In this section, we&#8217;re going to cover several configuration properties of the Debezium Oracle connector that can allow you to optimize the connector&#8217;s performance based on your environment and needs.</p> </div> <div class="ulist"> <ul> <li> <p><a href="#performance-disk-io">Disk IO</a></p> </li> <li> <p><a href="#performance-redo-log-size">Redo Log Size</a></p> </li> <li> <p><a href="#performance-log-mining-strategy">Log Mining Strategy</a></p> </li> <li> <p><a href="#performance-batch-size">Batch size</a></p> </li> <li> <p><a href="#performance-query-filter-mode">Query filter mode</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="performance-disk-io">Disk IO</h3> <div class="paragraph"> <p>A fundamental aspect of the Debezium Oracle connector&#8217;s implementation is that it uses Oracle APIs to work with the LogMiner process to read the redo and archive logs. These logs are managed by two Oracle processes, the Oracle LogWriter (LGWR) and the Oracle Archiver (ARCH). The details around these two processes are less important to the discussion other than the fact they&#8217;re responsible for management of both the current redo logs used by the Oracle database and the creation of the archive logs that contain the past historical changes made to Oracle.</p> </div> <div class="paragraph"> <p>The Debezium Oracle connector uses the LogMiner API to read these redo and archive logs off disk and generate change events. Unfortunately, redo and archive logs cannot be retained on the database server indefinitely. Often, logs can be several gigabytes in size, and when combined with supplemental log configurations, an Oracle database can generate many logs in short periods of time, thus disk space gets consumed quickly. When these redo or archive logs exist on a networked drive or a high-latency device, this has a direct impact on LogMiner&#8217;s ability to read and provide the change event data to the connector as quickly as possible.</p> </div> <div class="paragraph"> <p>One possible way to improve performance is to coordinate with your Oracle database administrator to see if you can retain more archive logs longer even if its on a separate mounted disk but local to the database machine. Oracle provides the ability to define what is called <a href="https://docs.oracle.com/en/database/oracle/oracle-database/23/refrn/LOG_ARCHIVE_DEST.html#GUID-AACE967D-EF85-43F4-B895-5E510ABADCC3">log archive destinations</a>, and it supports up to a maximum of 31 different paths where archive logs can be written by the Archiver process.</p> </div> <div class="paragraph"> <p>It&#8217;s not uncommon that your database administrator may have configured several log archive destinations already for other processes such as GoldenGate, DataGuard, etc. If such paths have been defined, and they have a log retention policy that aligns with the ingestion rate, you can safely use Debezium with those destinations if one already exists. If no path has been defined, you can create a new one following the aforementioned Oracle documentation link.</p> </div> <div class="paragraph"> <p>To have Debezium use a specific log archive destination, the <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-archive-destination-name">log.mining.archive.destination.name</a> connector property must be provided.</p> </div> <div class="listingblock"> <div class="title">An example using LOG_ARCHIVE_DEST_5</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">log.mining.archive.destination.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">LOG_ARCHIVE_DEST_5</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The physical path to the logs will be obtained from Oracle directly, you only need to specify the destination name the database administrator configured.</p> </div> </td> </tr> </table> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>This configuration is only applicable for use with the Oracle LogMiner adapter and has no impact when ingesting changes using Oracle XStream.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="performance-redo-log-size">Redo Log Size</h3> <div class="paragraph"> <p>Oracle&#8217;s redo logs are used primarily for recovery and instance failures. When an instance is first created, the administrator provides a starting size for redo logs. If the size of the redo logs is too small or even too large, this can have a direct impact on the performance of your instance.</p> </div> <div class="paragraph"> <p>For example, the size of redo logs has a direct impact on how frequent the Oracle Archiver (ARCH) process transitions redo logs to archive logs, which is referred to as a <em>log switch</em>. Generally, Oracle recommends that administrators minimize the number of log switches in small windows, but this can vary depending on a number of factors like volume or logging configurations.</p> </div> <div class="paragraph"> <p>A log switch is a fairly expensive operation because it&#8217;s the moment in time when a redo log is copied by the Archiver process to an archive log and a new redo log is allocated. If there is ever a period when the Archiver process falls behind and all redo logs have filled, Oracle&#8217;s database can degrade or even halt if a checkpoint cannot occur because all current redo logs are full and awaiting archival.</p> </div> <div class="paragraph"> <p>If you are using an image of Oracle based on <a href="https://github.com/oracle/docker-images">Oracle Docker Images</a>, you will have noticed that by default the redo logs created are extremely small, several megabytes each. For development purposes, this is fine out of the box, but when using such an instance for any type of serious integration such as Debezium; this simply doesn&#8217;t work well, especially with the default mining strategy which we&#8217;ll discuss in more detail in the next section.</p> </div> <div class="paragraph"> <p>However, small redo log sizes aren&#8217;t the only problem. If the redo log files are sized too large, this can have an adverse impact on the read time from disk, making the gap while the connector waits for changes even longer as there is the need to perform more Disk IO due to larger files.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Resizing Oracle&#8217;s redo logs requires existing knowledge of the database server paths and where it is safe to store those files; therefore, since that information is environment dependent, we aren&#8217;t going to cover directly how to do this here. Oracle provides excellent <a href="https://docs.oracle.com">documentation</a> on how to perform this task.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>Unfortunately, there isn&#8217;t a simple answer for what size you should use. This requires a bit of finesse, science, and heuristics of your environment to gauge what is the best choice, but this is something in your arsenal that could be adjusted if necessary.</p> </div> </div> <div class="sect2"> <h3 id="performance-log-mining-strategy">Log Mining Strategy</h3> <div class="paragraph"> <p>In <a href="/blog/post/2022/10/06/debezium-oracle-series-part-2#configure-oracle-redo-logs">Part 2</a>, we covered two <a href="/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-strategy">log mining strategies</a> for the Debezium Oracle connector. These strategies control how the connector interacts with Oracle LogMiner and how specific entries from the redo logs are ingested for both schema and table changes.</p> </div> <div class="paragraph"> <p>Redo logs store redo entries and not all redo entries store explicitly every pierce of data needed to re-construct the change that occurred. For example, DML operations (inserts, updates, deletes) do not refer to table or column names but rather object identifiers. These object identifier and version details change in the data dictionary as column or table modifications (DDL changes) occur. This means that the identifier and/or its version will differ from a redo entry for the same table before and after a schema change.</p> </div> <div class="paragraph"> <p>The log mining strategy controls precisely how redo entries are interpreted by Oracle LogMiner, primarily by either writing the data dictionary to the end of the redo logs or omitting this step. There are benefits to using either strategy and we&#8217;re going to dive into what those are and why you may use one strategy over another.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Default Mining Strategy</dt> <dd> <p>The default mining strategy is the safest choice, but it is also the most expensive. This strategy will append a copy of the data dictionary to the redo logs when a log switch is observed.<br> <br> This strategy&#8217;s main benefit is schema and data changes are ingested seamlessly by Oracle LogMiner. In other words, if an <code>INSERT</code> is followed by an <code>ALTER TABLE</code> and that is followed by an <code>UPDATE</code>, Oracle LogMiner will safely deduce the right table and column names from the old and the new object ids and versions. This means that Debezium will be able to safely ingest that change event as one might expect.<br> <br> The unfortunate pain point of this strategy is that it&#8217;s an expensive step at each log switch.<br> <br> First, it requires that the connector append a copy of the data dictionary periodically to the redo logs and Oracle performs a full log switch (all log groups perform a switch) after writing the dictionary. This means that more archive logs will be generated than when using the online catalog strategy we&#8217;ll discuss momentarily.<br> <br> Secondly, it also requires that when a LogMiner process begins to mine the redo logs, it must first read and prepare a section of the SGA with all the dictionary metadata so that resolution of table and column names can happen properly. Depending on the size of the redo logs, and more appropriately the dictionary segment of the logs, this can take upwards of several minutes to prepare. So you can probably guess that when you combine this strategy with a poorly sized redo logs, this can easily create a performance bottleneck.</p> </dd> </dl> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>It is <strong>not recommended</strong> to ever deploy multiple Oracle connectors using this strategy, but instead use a single Oracle connector.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1">Online Catalog Strategy</dt> <dd> <p>The online catalog mining strategy is used when specifying the <code>log.mining.strategy</code> connector configuration property with the value <code>online_catalog</code>. Unlike the default mining strategy, this strategy does not write any additional data to the redo logs, but instead, relies on the current data dictionary to resolve table and column names.<br> <br> The benefit to this strategy is that since we are not writing any dictionary details to the redo logs, redo logs will only transition to archive logs based on existing database activity. In short, Debezium will not influence this frequency beyond the additional supplemental logging configuration required, making it easier to manage the volume of archive logs created. Secondly, because no dictionary details are written to the logs and the number of log switches remains constant to existing behavior, a mining session starts nearly instantaneously and there is no need for LogMiner to prepare any dictionary metadata as the existing data dictionary satisfies that requirement as-is.<br> <br> Unfortunately, this strategy does have a single restriction and that is schema changes are not observed seamlessly. In other words, if a redo entry refers to an object id/version that does not match the object id/version in the online data dictionary, Oracle LogMiner is incapable of reconstructing the SQL for that operation.<br> <br> However, schema changes can be handled with this strategy, but it requires doing schema changes in a lock-step fashion. In other words, you would you halt changes on the table, wait until the last change for the table has been captured by Debezium, apply the schema change, wait for the schema change to be emitted by Debezium, and finally resume allowing changes to the data in the table.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This strategy provides the optimal performance gain both for Oracle and the connector.</p> </div> <div class="paragraph"> <p>The only requirement is that if a table&#8217;s schema isn&#8217;t static, and you may have changes to it periodically, if you can perform the schema changes in lock-step as described above, you can safely perform schema changes using this strategy; otherwise schema changes should be avoided on the table(s) being captured.</p> </div> <div class="paragraph"> <p>Finally, this strategy should be used if deploying multiple Oracle connectors on the same Oracle database.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>In conclusion, the mining strategy chosen can have significant impacts on the performance of the database as well as the ingestion rate of the Debezium Oracle connector. It&#8217;s important to weigh the benefits and consequences of this choice based on what is possible given your environment.</p> </div> <div class="admonitionblock important"> <table> <tr> <td class="icon"> <i class="fa icon-important" title="Important"></i> </td> <td class="content"> <div class="paragraph"> <p>There is an effort underway to bridge these two strategies and deliver a solution that provides all the performance benefits of the online catalog strategy and the seamless schema management provided by the default mining strategy. The progress for this effort can be found in <a href="https://issues.redhat.com/browse/DBZ-3401">DBZ-3401</a>.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="performance-batch-size">Batch size</h3> <div class="paragraph"> <p>The Debezium Oracle connector uses an adaptive batch size algorithm to determine the number of rows that will be fetched per database call. The algorithm is controlled by the following configuration properties:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>log.mining.batch.size.default</code></dt> <dd> <p>This specifies the default number of rows that will be fetched per database call.</p> </dd> <dt class="hdlist1"><code>log.mining.batch.size.min</code></dt> <dd> <p>This specifies the minimum number of database rows that will be fetched per database call.</p> </dd> <dt class="hdlist1"><code>log.mining.batch.size.max</code></dt> <dd> <p>This specifies the maximum number of database rows that will be fetched per database call.</p> </dd> </dl> </div> <div class="paragraph"> <p>These settings give the connector the ability to read more data and reduce network latency when the connector has fallen behind or observed a large transaction in the logs at the expense of consuming more SGA and JVM memory temporarily and using less SGA and JVM memory when the connector has caught up to near real-time changes.</p> </div> <div class="paragraph"> <p>The connector defaults for these are great starting points, but depending on your change event volume, it may be wise to increase or even shrink these settings based on your environment to improve performance.</p> </div> </div> <div class="sect2"> <h3 id="performance-query-filter-mode">Query filter mode</h3> <div class="paragraph"> <p>Any developer who has ever worked on a SQL-based application will tell you that just because a query performs well in one environment or at one point in time doesn&#8217;t mean that the same query will be as efficient in another environment or even the future as the data set changes. That&#8217;s why with Debezium 2.3, we added a new feature called <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-property-log-mining-query-filter-mode">log.mining.query.filter.mode</a>. Through various discussions with Oracle community users with various installations, volume sizes, and integrations, we concluded that the LogMiner query used by the Oracle connector simply cannot be a one-size fits all solution.</p> </div> <div class="paragraph"> <p>In order to be the most efficient, we needed to provide the user community with a way they can tune the LogMiner query that best satisfies their configuration and their environment. There are currently three options for how the LogMiner query is constructed and each influence how the query&#8217;s where-clause is generated.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>none</code></dt> <dd> <p>Specifies that no additional predicates are added to the LogMiner query.<br> <br> Instead, all filtering is delegated primarily to the Oracle connector&#8217;s Java runtime and not the database query. This has the highest network bandwidth usage of all the options and can have the highest throughput depending on the volume and the data-set of redo entries. For lower volume installations, this can easily perform the fastest but does not scale well as the volume of redo entries increases or if the data-set of interest is smaller than the total of the data-set.</p> </dd> <dt class="hdlist1"><code>in</code></dt> <dd> <p>Specifies that the schema and table include/exclude filters are applied using a SQL in-clause.<br> <br> By default, the include/exclude configuration options support comma-separated lists of regular expressions; however, if you elect to avoid the use of regular expressions, you can apply database-level filters to the LogMiner query more efficiently by using this query filter mode. An in-clause is much more efficient over using disjunctions or Oracle&#8217;s <code>REGEXP_LIKE</code> operator which we&#8217;ll discuss with the next option. This also performs extremely well if you have a lot of schema or table include/exclude list options defined in your configuration. And finally, because this choice performs database-level filtering, this reduces the network latency and only returns the necessary rows to the connector.</p> </dd> <dt class="hdlist1"><code>regex</code></dt> <dd> <p>Specifies that the schema and table include/exclude filters are applied using the SQL operator <code>REGEXP_LIKE</code>.<br> <br> Since the include/exclude configuration options support comma-separated lists of regular expressions, must be used instead of <code>in</code> when using regular expressions. While this option performs database-level filtering much like the in-clause choice, the use of regular expressions degrades in performance as more include/exclude options are specified in the connector configuration. Therefore, in order to maximize performance, its generally best when using regular expressions to write as few expressions that match the most tables or schemas as possible to reduce the number of predicates appended to the query.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>As of Debezium 2.3, the default is <code>none</code>, so you can gain additional performance by specifically configuring the <code>log.mining.query.filter.mode</code> to use <code>in</code> ir <code>regex</code> depending on the values provided in your include/exclude list configuration properties.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="debugging">Debugging</h2> <div class="sectionbody"> <div class="paragraph"> <p>As much as I would like to think Software Engineering is butterflies and flowers; it&#8217;s far from the truth and managing an environment that runs software is no different. When problems occur, it&#8217;s important to have the knowledge to self-diagnose and get back to a running state as quickly as possible. So we&#8217;re going down the rabbit-hole and discuss a number of common errors that we&#8217;ve seen, how do you debug those errors, and what might be the potential fixes.</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">None of the log files contains offset SCN, re-snapshot is required</dt> <dd> <p>I&#8217;m fairly certain that at some point most Oracle connector users have seen this error in the logs, whether during PoC design or testing, but hopefully not production. The error message itself is relatively clear, but what is often not clear is "why did this happen".<br> <br> For other databases, their transaction logs only contain committed changes that are then consumed by Debezium. Unfortunately, Oracle does not do this and instead writes every single change to the transaction logs, even if the change is later rolled back due to a constraint violation or an explicit user or system rollback. This means that reading the changes from the Oracle redo and archive logs isn&#8217;t as simple as reading from position X until the end of the file and then repeat with the next log in sequence. Instead, the connector must maintain what we call a low and high watermark SCN, or if you&#8217;re familiar with the connector&#8217;s offsets these are represented as <code>scn</code> and <code>commit_scn</code>.<br> <br> The low watermark or <code>scn</code> represents the safe resume point in the redo logs. Generally this points to the position in the logs where the oldest in-progress transaction started. The high watermark or <code>commit_scn</code> represents the position in the logs where we last emitted a transaction batch for a given redo thread. This means that the changes in between these two values are a mixture of uncommitted changes, committed changes, or rolled back changes.<br> <br> When the connector starts, the low watermark or <code>scn</code> read from the offsets is compared to the oldest available archive log in Oracle. If the archive log begins with a system change number that comes after this <code>scn</code> value, this error will occur.<br> <br> Long-running transactions directly impact the low watermark or <code>scn</code> position. If a transaction remains active for longer than your archive log retention policy and the connector is restarted due to a re-balance or failure, this error can occur. If you suspect long-running transactions, you can configure the <code>log.mining.transaction.retention.ms</code> property in order to discard a transaction that lives longer than the specified value. While this does cause data loss as that transaction&#8217;s changes are discarded, it does allow the low watermark to safely advance forward at a reasonable pace even when long-running transactions occur. You should set the transaction retention period to a value less than your archive log retention period.<br> <br> Another use case that can raise this error is if you are capturing changes from an Oracle database with a low volume of changes. In particular, if you are using an older version of Debezium where the LogMiner query applied database-level filters or you&#8217;ve configured the new query filter mode to apply database-level filters, it&#8217;s possible that the connector may go extended periods of time without observing a single change event. Since offset data only synchronizes when the connector sends an event to Kafka, low volumes of changes in a large window of time can mean those Kafka offsets become stale and if a restart occurs, this error could happen. In this use case, configuring both the <code>heartbeat.internval.ms</code> and <code>heartbeat.action.query</code> connector properties is a great way to guarantee that there is some activity flowing to Kafka to keep those offsets from becoming stale.</p> </dd> <dt class="hdlist1">ORA-01555: snapshot too old</dt> <dd> <p>This specific error is most commonly observed during the connector&#8217;s initial snapshot. The Oracle connector relies on what is called flashback queries during the initial snapshot phase.<br> <br> A flashback query is a standard SELECT statement that uses a system change number in order to generate a result-set based on the state of the data at that given point in the database&#8217;s lifetime. This can be useful for a variety of reasons, including being able to restore objects without the need for media recovery because Oracle is capable of retaining that previous state for a certain period of time. The data returned by these queries use the Automatic Undo Management (AUM) subsystem and rely on the undo data area where transactions are recorded and retained for a given period of time, configurable based on the database parameter <code>UNDO_RETENTION</code>.<br> <br> If the SCN used for the flashback query becomes too old and the undo retention no longer maintains historical data for that system change number, Oracle will report an ORA-01555 error that the snapshot is now too old. When this happens during your initial snapshot, the snapshot will need to be retaken from the beginning and unless you reconfigure Oracle&#8217;s undo retention period to allow for a longer retention time, rerunning the snapshot on the same data set will result in the same outcome.<br> <br> So either a) have your DBA increase the <code>UNDO_RETENTION</code> database parameter temporarily or b) use a schema-only snapshot and then rely on incremental snapshots to generate the initial data set from your existing table data.</p> </dd> <dt class="hdlist1">Streaming changes takes several minutes to appear</dt> <dd> <p>Occasionally users will notice there is a latency when the connector first starts or at specific periods during the connector&#8217;s lifetime. One of the best ways to identify what is going on is to coordinate with your DBA and to take a close look at your database&#8217;s alert log, which records all the LogMiner and XStream interactions that Debezium performs with your database. But most often this latency is quite common for users who use the default log mining strategy.<br> <br> The default mining strategy that we covered earlier performs a what is called a data dictionary build step and depending on your database, this can take some time to be written to your redo logs and then parsed by the LogMiner process. It&#8217;s not uncommon for this to take 30 seconds up to several minutes to complete, and when using the default mining strategy, this process occurs on each log switch.<br> <br> So we normally suggest that if you experience this latency frequently that you check the frequency of your log switches. If your database is performing excessive log switches within a small window that does not adhere to Oracle&#8217;s guidelines, your DBA may need to tune the database accordingly. Reducing the frequency of log switches increases the time that Debzium can reuse the same log file for mining and therefore reduces the need to build and parse the data dictionary.<br> <br> If your table schema does not change often or won&#8217;t change at all, you can reconfigure the connector to use the <code>online_catalog</code> mining strategy as an alternative. This avoids the writing the data dictionary to the redo logs and the parse phase performed by LogMiner, greatly increasing the speed at which a mining session begins at both connector start-up and at each log switch interval.</p> </dd> <dt class="hdlist1">How do you know if an archive log can be deleted?</dt> <dd> <p>Database administrators typically keep archive logs on the database server for a short period of time before they&#8217;re removed. This interval varies and depends on a number of factors including the frequency the logs are created, their respective sizes, and the physical space available on the server. What is most important is that if Debezium requires a specific archive log that it remains available until it&#8217;s no longer needed for CDC.<br> <br> The easiest way to determine what logs are needed is via JMX metrics, looking at the field <code>OffsetScn</code>. This field references the system change number where the connector will resume from in case of a restart, and so any archive or redo log that contains this system change number or comes after this change number must remain available.<br> <br> Debezium 2.4 will add another JMX metric that will provide the cut-off timestamp for archive logs based on this <code>OffsetScn</code>. This means that you will be able to use this timestamp directly in shell scripts to compare the filesystem&#8217;s timestamp with the one from JMX metrics, safely knowing which logs can must be retained and which can safely be removed via RMAN.</p> </dd> <dt class="hdlist1">Memory footprint, how do you manage it efficiently?</dt> <dd> <p>Due to the nature of how transaction data is written to the Oracle archive and redo logs, a buffer of the transaction state must be managed by the connector. Under ideal circumstances, this buffer maintains short-lived data, the transaction starts, we buffer its relevant changes, and we observe the rollback or commit, and the data managed in the buffer is handled and the buffer cleared.<br> <br> Because the connector buffers transactions, it&#8217;s extremely important that you have some prior knowledge of your environment&#8217;s transaction patterns. If this information can vary and cannot be predicted, you may want to consider using an alternative buffer type than the default Heap (memory) based buffer as this can easily lead to <code>OutOfMemory</code> exceptions under these circumstances when memory is configured too low.<br> <br> Please refer to the documentation about <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-event-buffering">Event Buffering</a>. The Oracle connector offers two Infinispan based solutions that allow the connector store the buffer off-heap, reducing the connector&#8217;s memory footprint and being capable of dealing with very large transactions seamlessly.</p> </dd> </dl> </div> <div class="paragraph"> <p>Lastly, we have composed a collection of <a href="https://debezium.io/documentation/reference/stable/connectors/oracle.html#oracle-frequently-asked-questions">frequently asked questions</a> in the documentation. We generally do our best to keep the most commonly asked questions there as a reference. Please take a moment and read through those and if you suspect anything may be missing, please open a <a href="https://issues.redhat.com/projects/DBZ">Jira issue</a>.</p> </div> </div> </div> <div class="sect1"> <h2 id="wrap-up">Wrapping up</h2> <div class="sectionbody"> <div class="paragraph"> <p>I really hope this series on the Oracle connector has been helpful and informative. We&#8217;ve covered topics ranging from installation, configuration, deploying the connector, performance optimizations, and how to debug or evaluate specific common use cases we hear from the community.</p> </div> <div class="paragraph"> <p>As I mentioned earlier, Oracle is unlike most other database platforms and requires a bit more care and precision to maximize it&#8217;s potential. When evaluating the Oracle connector, it is important that you coordinate with an Oracle administrator professional to make sure that you&#8217;re maximizing the connector&#8217;s potential, particularly if performance is a critical metric in your evaluation.</p> </div> <div class="paragraph"> <p>As always, if you have questions about anything related to the content in this post or about something you observe in your environment, the team will do their best to provide you with the answers you need either by using our <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or reaching out to us on our <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">chat</a>.</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="debezium"/><category term="oracle"/><category term="examples"/><summary type="html"><![CDATA[This post is the final part of a 3-part series to explore using Debezium to ingest changes from an Oracle database using Oracle LogMiner. In case you missed it, the first installment of this series is found here and the second installment is found here. In this third and final installment, we are going to build on what we have done in the previous two posts, focusing on the following areas: Performance optimizations How to debug and assess problems and how to resolve them]]></summary></entry><entry><title type="html">Debezium signaling and notifications - Part 1</title><link href="https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications/" rel="alternate" type="text/html" title="Debezium signaling and notifications - Part 1"/><published>2023-06-27T10:11:11+00:00</published><updated>2023-06-27T10:11:11+00:00</updated><id>https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications</id><content type="html" xml:base="https://debezium.io/blog/2023/06/27/Debezium-signaling-and-notifications/"><![CDATA[<div class="paragraph"> <p>Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the first installment in the series, where we will introduce the signaling and notification features offered by Debezium and discuss the available channels for interacting with the platform.</p> </div> <div class="paragraph"> <p>In the subsequent parts of this series, we will delve deeper into customizing signaling channels and explore additional topics such as JMX signaling and notifications.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>In today&#8217;s interconnected software applications and systems, seamlessly integrating with other products is crucial for building robust and efficient solutions. One of the key challenges in building integrated software ecosystems is establishing effective communication channels between different components and services.</p> </div> <div class="paragraph"> <p>Debezium introduces a comprehensive signaling and notification system providing seamless integration with any third-party solution. Combining the signal and notification systems, you can now orchestrate complex yet efficient pipelines to stay informed about the status and progress of the state managed by Debezium.</p> </div> <div class="paragraph"> <p>The new signaling system has been re-designed to be extensible, providing a simplified approach to adding new, custom signal channels on top of the various implementations available. Let&#8217;s explore the different signals and notification channels, covering their functionality and describing their benefits.</p> </div> <div class="sect1"> <h2 id="signaling_in_debezium">Signaling in Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Signaling in Debezium refers to the mechanism through which users can trigger specific actions within the product. It allows users to interact with Debezium and control some behaviors. One notable use case of signaling is the execution of incremental snapshots, which permits to start an ad-hoc snapshot of database.</p> </div> <div class="paragraph"> <p>Debezium provides different signaling channels through which users can send signals to the platform. Let&#8217;s explore the available signaling channels:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Signaling through Database Tables</dt> <dd> <p>In earlier versions of Debezium, signaling required a dedicated database table where you would insert specific records to trigger Debezium to execute a task. For example, an inserted row could initiate an incremental snapshot, prompting the connector to run a snapshot operation based on the given signal criteria.</p> </dd> <dt class="hdlist1">Signaling through Kafka Topics</dt> <dd> <p>In earlier versions of Debezium, you could send signals via a configured Kafka topic; however, this was only available for MySQL using read-only access and global transaction identifiers (GTID) behavior. With Debezium 2.3, the Kafka signal channel is available to all connectors.<br> <br> This enhancement provides a simplified integration approach and a unified and consistent approach for signals across all supported Debezium databases. You can send signals to a specific Kafka topic, and Debezium will consume and process that signal as though it originated from the signal table itself.<br> <br> Using a Kafka topic for signals provides several advantages. First, it aligns with event-driven design, making it a natural fit with change data capture and Debezium. Additionally, it provides a secure way to send signals to Debezium without necessarily providing the user with direct access to the underlying database.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Even when using the Kafka signal approach, the incremental snapshot feature still requires the presence and use of the signaling table to manage some bookkeeping needed for the incremental snapshot process. You can only omit the signal table when using MySQL in a read-only way with global transaction identifiers (GTIDs) enabled.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1">Signaling through File</dt> <dd> <p>You can trigger signals from the file system thanks to a contribution by the Debezium user community. This approach is an excellent alternative when not relying on messaging infrastructures like Kafka combined with Debezium Server or the embedded engine or when you cannot use the database for signals.</p> </dd> <dt class="hdlist1">Signaling through Java Management Extensions (JMX)</dt> <dd> <p>With this channel, you can send signals calling the signal operation exposed through a dedicated MBean. You can do it by connecting to the MBeanServer with your preferred client. A dedicated post about this feature will follow.</p> </dd> <dt class="hdlist1">Signaling through custom channel</dt> <dd> <p>We also redesigned the signaling mechanism to be extensible. You can provide your implementations and use it all via configuration. A dedicated post about this feature will follow.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="notification_in_debezium">Notification in Debezium:</h2> <div class="sectionbody"> <div class="paragraph"> <p>Notifications focus on retrieving status and progress information about Debezium and its internal processes. It provides insights into the execution and completion of tasks such as initial snapshots or incremental snapshots. Debezium generates notifications to keep users informed about the progress of these tasks, facilitating monitoring and decision-making based on real-time information. By leveraging the notification functionality, users can track the state of Debezium and take appropriate actions based on the received notifications.</p> </div> <div class="paragraph"> <p>Debezium provides a variety of ways to obtain notifications. Let&#8217;s explore the out-of-the-box channels:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Notification through Sink channel</dt> <dd> <p>This implementation sends notifications through the Connect API to a configured topic. Users can enable this channel and specify the topic name. By publishing notifications to the specified topic, Debezium allows users to consume and process notifications in a way that suits their needs. Since <code>Sink</code> is agnostic (i.e. not necessarily Kafka), this integrates seamlessly with Debezium Server to deliver notifications to any of its sink adapters.</p> </dd> <dt class="hdlist1">Notification through logs</dt> <dd> <p>This channel appends notifications directly to the end of the connector&#8217;s log. This approach provides convenient access for monitoring, debugging, and analyzing notification details.</p> </dd> <dt class="hdlist1">Notifications through Java Management Extensions (JMX)</dt> <dd> <p>This notification channel exposes a dedicated Debezium MBean with several attributes that contain a list of generated notifications. This channel combines common industry standard monitoring techniques like JMX to react and consume Debezium notifications. A dedicated post will follow to discuss this in more detail.</p> </dd> <dt class="hdlist1">Notification through custom channel</dt> <dd> <p>Debezium&#8217;s notification mechanism is extensible, allowing users to implement custom channels to deliver notifications using means that best fit their needs. We will cover how to provide a custom notification channel in a dedicated post.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="conclusion">Conclusion</h2> <div class="sectionbody"> <div class="paragraph"> <p>Signals and Notifications are now foundational features in Debezium. Signaling empowers users to interact with Debezium and trigger actions, while notification provides valuable information about Debezium&#8217;s state and progress.</p> </div> <div class="paragraph"> <p>In previous versions of Debezium, initiating an incremental snapshot was only possible using a signaling table. Users had to configure a specific table as a signaling mechanism to trigger the incremental snapshot. However, for MySQL with GTIDs enabled, it was possible to utilize Kafka for signaling purposes.</p> </div> <div class="paragraph"> <p>With the release of Debezium version 2.3, we have introduced significant improvements to both the signal and notification subsystems. We&#8217;ve unified several behaviors across connectors and made the entire system extensible, aiming to simplify both custom and future contributed implementations. We hope this enhances the overall experience of working with Debezium and provides a way to integrate Debezium with other third-party applications and tools seamlessly. These new and improved features allow you to maximize the capabilities of the Debezium change data capture platform in limitless ways.</p> </div> <div class="paragraph"> <p>Stay tuned for Part 2, where we will discuss how to customize signaling and notification channels in Debezium.</p> </div> </div> </div>]]></content><author><name>Fiore Mario Vitale</name></author><category term="debezium"/><category term="features"/><category term="notifications"/><category term="signaling"/><category term="integration"/><summary type="html"><![CDATA[Welcome to this series of articles dedicated to signaling and notifications in Debezium! This post serves as the first installment in the series, where we will introduce the signaling and notification features offered by Debezium and discuss the available channels for interacting with the platform. In the subsequent parts of this series, we will delve deeper into customizing signaling channels and explore additional topics such as JMX signaling and notifications.]]></summary></entry><entry><title type="html">Towards Debezium exactly-once delivery</title><link href="https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery/" rel="alternate" type="text/html" title="Towards Debezium exactly-once delivery"/><published>2023-06-22T01:20:00+00:00</published><updated>2023-06-22T01:20:00+00:00</updated><id>https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery</id><content type="html" xml:base="https://debezium.io/blog/2023/06/22/towards-exactly-once-delivery/"><![CDATA[<div class="paragraph"> <p>Every now and then there is a questions in the Debezium <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">chat</a> or on the <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> how to ensure exactly-once delivery of the records produced by Debezium. So far Debezium aimed only for at-least-once delivery. This means Debezium guarantees every single change will be delivered and there is no missing or skipped change event. However, in case of failures, restarts or DB connection drops, the same event can be delivered more than once. Typical scenario is that the event is delivered twice - once before failure/restart and second time after that. Exactly-once delivery (or semantic) provides stronger guarantee - every single message will be delivered and at the same time there won&#8217;t be any duplicates, every single message will be delivered exactly once. So far our answer was that the users have to implement their own deduplication system if they need exactly-once delivery. However, with Kafka Connect support for exactly-once delivery, it seems we can provide exactly-once delivery for Debezium connectors out-of-the-box, only with a little configuration change.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="kafka_connect_exactly_once_semantics">Kafka Connect exactly-once semantics</h2> <div class="sectionbody"> <div class="paragraph"> <p>Kafka itself provides transaction support, a building block for exactly-once delivery, as well as exactly-once delivery itself for quite some time (since Kafka 0.11). What was missing was support for exactly-once delivery in Kafka Connect. This has change in Kafka 3.3.0, which has added support for exactly-once delivery for the source connectors, see <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors">KIP-618</a> and related <a href="https://issues.apache.org/jira/browse/KAFKA-10000">Jira issue</a>. From the high-level perspective, events produced by the source connector and written during uncommitted transactions are not visible for the consumer and become visible only once a transaction is committed, which also includes a commit of the offset corresponding to these events. The source connector itself doesn&#8217;t create the transaction, it&#8217;s handled by Kafka Connect. Connector only defines transactional boundaries if needed and in many cases even this is not needed.</p> </div> <div class="sect2"> <h3 id="kafka_connect_configuration">Kafka Connect configuration</h3> <div class="paragraph"> <p>Exactly-once semantics (EOS) is currently supported only with Kafka Connect in distributed mode. The only thing the user has to do is to enable exactly once delivery on all Kafka Connect workers. The worker can enable exactly-once support by setting <code>exactly.once.source.support=enabled</code>. If you want to do a rolling update on the cluster without shut down, you can first update all the workers to <code>exactly.once.source.support=preparing</code> and then gradually switch the workers to <code>exactly.once.source.support=enabled</code>.</p> </div> <div class="paragraph"> <p>Source connector which requires exactly-once delivery needs to be configured in a similar way by setting <code>exactly.once.support</code>. You can set it either to <code>requested</code> or <code>required</code>:</p> </div> <div class="ulist"> <ul> <li> <p><code>required</code> - Kafka Connect checks that the source connector explicitly supports exactly-once semantics by implementing the <code>SourceConnector::exactlyOnceSupport</code> method. If the connector doesn&#8217;t implement this method, the start of the connector would fail.</p> </li> <li> <p><code>requested</code> - skips the check if the source connector explicitly supports exactly-once delivery or not. In such cases the documentation or the source code of the connector needs to be carefully checked if it can provide EOS or not.</p> </li> </ul> </div> <div class="paragraph"> <p>To be sure EOS works properly, it&#8217;s always better to use <code>exactly.once.support=required</code> when exactly-once delivery is required by the application and <code>exactly.once.support=requested</code> use only for testing purposes.</p> </div> <div class="paragraph"> <p>Another related source connector configuration option is <code>transaction.boundary</code>. It determines the span on the transaction. Possibles values are:</p> </div> <div class="ulist"> <ul> <li> <p><code>poll</code> - all event return in single <code>SourceTask::poll</code> method call will be wrapped into a single transaction,</p> </li> <li> <p><code>interval</code> - events will be chunked into the transactions based on the specified time interval, which can be configured via <code>transaction.boundary.interval.ms</code> option,</p> </li> <li> <p><code>connector</code> - definition of transaction boundaries is delegated to the given source connector.</p> </li> </ul> </div> <div class="paragraph"> <p>The default value is <code>poll</code> option.</p> </div> <div class="paragraph"> <p>To sum up, if you want to enable EOS and your source connector supports it, you need to configure <code>exactly.once.source.support=enabled</code> on all workers and update connector configurations with <code>exactly.once.support=required</code>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="exactly_once_delivery_and_debezium_connectors">Exactly once delivery and Debezium connectors</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium has two phases, initial snapshotting phase and streaming phase. We have a hard requirement that the initial snapshot has to finish successfully, otherwise it has to be repeated. If there is any failure during the snapshot phase, after the next start or restart of the connector the snapshot will be taken again and therefore there will be duplicates. Repeating the whole snapshot after the next start makes sense as the data may change until next start or during the restart. As the snapshot should reflect the exact data in time of taking the snapshot, thus in case of failure we have to start from scratch. There would probably be ways to avoid duplicate events in the initial snapshot phase, but for now let&#8217;s focus only on the streaming phase.</p> </div> <div class="paragraph"> <p>In the case of streaming the data, the situation is on the other hand quite easy. We store the events obtained from the database in Debezium internal queue and on each Kafka Connect <code>poll()</code> method call we dump the queue and update Kafka offset. Therefore the default transaction boundary, which wraps the <code>poll()</code> method, is a perfect fit for Debezium and Debezium doesn&#8217;t have to define any custom transaction boundaries.</p> </div> <div class="paragraph"> <p>It seems Debezium connectors can work with Kafka Connect exactly-once delivery out-of-the-box without any further modifications. However, we have to develop more robust tests for testing EOS and test all the connectors first. For connectors which we would test at least to some extent, we will add implementation of <code>SourceConnector::exactlyOnceSupport</code> method. In the meantime, if you want to test the EOS on your tests or stage environment on your own, you can use <code>exactly.once.support=requested</code>.</p> </div> <div class="sect2"> <h3 id="simple_test_of_exactly_once_delivery_when_db_connection_breaks">Simple test of exactly once delivery when DB connection breaks</h3> <div class="paragraph"> <p>Let&#8217;s show the simple test of EOS. We will try to break the Debezium connection to the database while the connector is running and data is continuously stored into the database. In such a case Debezium will throw retriable exception, Kafka connect would restart the connector and we would be interested if the connector really started from the point where it stopped the last time and doesn&#8217;t send any event twice.</p> </div> <div class="paragraph"> <p>The scenario can look like this:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>start Kafka Connect</p> </li> <li> <p>deploy the Debezium connector</p> </li> <li> <p>start a custom loader which will continuously insert data into the captured table in the database</p> </li> <li> <p>kill Debezium connection to the database</p> </li> <li> <p>let Kafka Connect restart Debezium connector</p> </li> <li> <p>stop the loader</p> </li> <li> <p>check, if there are any duplicates in the related Kafka topic.</p> </li> </ol> </div> <div class="paragraph"> <p>Here is a concrete example for Postgres database and Debezium Postgres connector.</p> </div> <div class="paragraph"> <p>Let&#8217;s create a simple test table which would contain ID, some integer value (can be the same as ID) and let&#8217;s say timestamp if we want to check when the record was created:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">DROP</span> <span class="type">TABLE</span> <span class="keyword">IF</span> <span class="keyword">EXISTS</span> public.eos_test;  <span class="comment">-- remove the table first if exists</span>
<span class="class">CREATE</span> <span class="type">TABLE</span> public.eos_test(id SERIAL <span class="keyword">NOT</span> <span class="predefined-constant">NULL</span> <span class="directive">PRIMARY</span> <span class="type">KEY</span>, val SERIAL, t <span class="predefined-type">TIMESTAMP</span> <span class="directive">DEFAULT</span> <span class="predefined">now</span>());</code></pre> </div> </div> <div class="paragraph"> <p>Don&#8217;t forget to update Kafka Connect with <code>exactly.once.source.support=enabled</code>. Also, Kafka Connect has to run in distributed mode so you need to modify <code>config/connect-distributed.properties</code> and run the worker with these properties. Once you have Kafka Connect running, you can start Debezium Postgres connector:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">eos</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">localhost</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5432</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span> : <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">topic.prefix</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">eos</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">table.include.list</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.eos_test</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">unwrap</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.unwrap.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.ExtractNewRecordState</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">exactly.once.support</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">requested</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> <div class="paragraph"> <p>To simplify reading of the topic, here we turned off adding schema into the records and also added Debezium SMT for extracting new record state so that only the new value is stored into the Kafka topic.</p> </div> <div class="paragraph"> <p>Now, you can start loading the data into the Postgres test table using e.g. this Python script:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="comment">#!/usr/bin/env python3</span>

<span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Loads data into Postgres EOS test table.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

<span class="keyword">import</span> <span class="include">psycopg2</span>

DROP_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">DROP TABLE IF EXISTS public.eos_test;</span><span class="delimiter">&quot;</span></span>
CREATE_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">CREATE TABLE public.eos_test(id SERIAL NOT NULL PRIMARY KEY,</span><span class="delimiter">&quot;</span></span> \
               <span class="string"><span class="delimiter">&quot;</span><span class="content">val SERIAL, t TIMESTAMP DEFAULT now());</span><span class="delimiter">&quot;</span></span>
INSERT_INTO_TABLE = <span class="string"><span class="delimiter">&quot;</span><span class="content">INSERT INTO public.eos_test(val) VALUES (%s)</span><span class="delimiter">&quot;</span></span>

<span class="keyword">try</span>:
    connection = psycopg2.connect(
        user=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        password=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
        host=<span class="string"><span class="delimiter">&quot;</span><span class="content">127.0.0.1</span><span class="delimiter">&quot;</span></span>,
        port=<span class="string"><span class="delimiter">&quot;</span><span class="content">5432</span><span class="delimiter">&quot;</span></span>,
        database=<span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>)
    cursor = connection.cursor()

    <span class="comment"># cursor.execute(DROP_TABLE)</span>
    <span class="comment"># cursor.execute(CREATE_TABLE)</span>
    <span class="comment"># connection.commit()</span>

    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(<span class="integer">1</span>, <span class="integer">50000</span>):
        cursor.execute(INSERT_INTO_TABLE, (i,))
        connection.commit()
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">wrote {i}</span><span class="delimiter">&quot;</span></span>)
<span class="keyword">finally</span>:
    <span class="keyword">if</span> connection:
        cursor.close()
        connection.close()</code></pre> </div> </div> <div class="paragraph"> <p>It will load 50,000 records into the test table. This should last long enough to check that the data is captured by the Debezium connector and switch to another window and kill the Debezium connection to the database. Depending on the speed of your test environment, you can possibly manage to kill the database connection several times during one run. You can eventually increase/decrease the number of loaded events.</p> </div> <div class="paragraph"> <p>Once out loader script loads the data into the database, kill the Debezium connection to the database, e.g. by connecting to Postgres database and running this command:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span> pg_terminate_backend(pid) <span class="keyword">FROM</span> pg_stat_activity <span class="keyword">WHERE</span> pid &lt;&gt; pg_backend_pid() <span class="keyword">AND</span> datname = <span class="string"><span class="delimiter">'</span><span class="content">postgres</span><span class="delimiter">'</span></span> <span class="keyword">AND</span> query <span class="keyword">like</span> <span class="string"><span class="delimiter">'</span><span class="content">START_REPLICATION SLOT %</span><span class="delimiter">'</span></span>;</code></pre> </div> </div> <div class="paragraph"> <p>This will terminate Debezium connection to the database, which will subsequently cause throwing Postgres driver <code>PSQLException</code> and Kafka Connect <code>RetriableException</code>. As the exception is retriable, Kafka Connect will restart the connector automatically and it will continue with capturing the data from the test table. You can check that all the data has arrived into the Kafka topic using <code>kafka-console-consumer</code> utility:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --property print.key=true --topic eos.public.eos_test</code></pre> </div> </div> <div class="paragraph"> <p>Once you are sure all the records are in the Kafka topic, you can finally run the checker script which would search for duplicate events. The script (again in Python) can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="comment">#!/usr/bin/env python3</span>

<span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">Check Kafka topic for records with duplicate key/value.</span><span class="content">
</span><span class="delimiter">&quot;&quot;&quot;</span></span>

<span class="keyword">import</span> <span class="include">json</span>
<span class="keyword">import</span> <span class="include">kafka</span>

UNIQUE_COLUMN = <span class="string"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>


<span class="keyword">def</span> <span class="function">deserialize</span>(rec):
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Deserialize JSON payload and extract value number.</span><span class="content">
</span><span class="content">    Ignore transaction boundaries markers.</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>
    <span class="keyword">try</span>:
        value = json.loads(rec.decode(<span class="string"><span class="delimiter">&quot;</span><span class="content">utf-8</span><span class="delimiter">&quot;</span></span>))
        <span class="keyword">if</span> UNIQUE_COLUMN <span class="keyword">in</span> value:
            <span class="keyword">return</span> value[UNIQUE_COLUMN]
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: no value in record {rec}</span><span class="delimiter">&quot;</span></span>)
    <span class="keyword">except</span>:
        print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: cannot deserialize record {rec}</span><span class="delimiter">&quot;</span></span>)

    <span class="keyword">return</span> <span class="predefined-constant">None</span>


<span class="keyword">def</span> <span class="function">check_topic</span>():
    <span class="docstring"><span class="delimiter">&quot;&quot;&quot;</span><span class="content">
</span><span class="content">    Check Kafka topic for duplicates and prin statistics, including skipped</span><span class="content">
</span><span class="content">    records.</span><span class="content">
</span><span class="content">    </span><span class="delimiter">&quot;&quot;&quot;</span></span>
    consumer = kafka.KafkaConsumer(
        <span class="string"><span class="delimiter">&quot;</span><span class="content">eos.public.eos_test</span><span class="delimiter">&quot;</span></span>,
        bootstrap_servers=[<span class="string"><span class="delimiter">&quot;</span><span class="content">localhost:9092</span><span class="delimiter">&quot;</span></span>],
        auto_offset_reset=<span class="string"><span class="delimiter">&quot;</span><span class="content">earliest</span><span class="delimiter">&quot;</span></span>,
        consumer_timeout_ms=<span class="integer">1000</span>,
        enable_auto_commit=<span class="predefined-constant">True</span>,
        group_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">eos-group</span><span class="delimiter">&quot;</span></span>,
        value_deserializer=deserialize)

    vals = []
    items = <span class="integer">0</span>
    duplicates = <span class="integer">0</span>
    skipped = <span class="integer">0</span>
    <span class="keyword">for</span> msg <span class="keyword">in</span> consumer:
        val = msg.value
        <span class="keyword">if</span> <span class="keyword">not</span> val:
            print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">WARN: skipping None value, items: {items}</span><span class="delimiter">&quot;</span></span>)
            skipped = skipped + <span class="integer">1</span>
            <span class="keyword">continue</span>

        items = items + <span class="integer">1</span>
        <span class="keyword">if</span> val <span class="keyword">in</span> vals:
            print(f<span class="string"><span class="delimiter">&quot;</span><span class="content">Duplicate value: {val}</span><span class="delimiter">&quot;</span></span>)
            duplicates = duplicates + <span class="integer">1</span>
        <span class="keyword">else</span>:
            vals.append(val)

    print(
        f<span class="string"><span class="delimiter">&quot;</span><span class="content">Found {duplicates} duplicates in {items} items (unique values: </span><span class="delimiter">&quot;</span></span> \
        f<span class="string"><span class="delimiter">&quot;</span><span class="content">{len(vals)}, skipped values: {skipped})</span><span class="delimiter">&quot;</span></span>)


check_topic()</code></pre> </div> </div> <div class="paragraph"> <p>Tail of the output can look as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">....
WARN: skipping None value, items: 46297
WARN: skipping None value, items: 48345
WARN: cannot deserialize record b'\x00\x00\x00\x00\x00\x00'
WARN: skipping None value, items: 49999
Found 0 duplicates in 49999 items (unique values: 49999, skipped values: 54)</code></pre> </div> </div> <div class="paragraph"> <p>In this case there are no duplicate records, so everything looks good. The only question is what are those skipped events. These are transaction boundaries markers. Python Kafka client for some reason cannot cope with them and fails to deserialize them, so we skip them. Java client should recognize these records and handle them without any issue.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="summary_and_the_next_steps">Summary and the next steps</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this blog post we have shown how to configure exactly once semantics for Kafka Connect source connectors and how to use it with Debezium Postgres connector. So far, it seems that there are no issues and at least Debezium Postgres connector can work fine with the exactly-once semantics.</p> </div> <div class="paragraph"> <p>However, not finding an issue of course doesn&#8217;t imply that there are no issues. Therefore, as a next step we would like to develop a more rigorous test framework for testing data consistency and exactly-once delivery. We would like to write the tests using the famous <a href="https://github.com/jepsen-io/jepsen">Jepsen</a> framework. If we succeed in writing the test, we will share the results in a follow-up blog post. In the meantime we would like to encourage you to test exactly-once delivery also in your environments and deployments to increase the chance to discover any potential bugs. If you run any such test, we would very appreciate if you share the results with us, negative ones when you find a bug as well as positive ones when everything passes.</p> </div> </div> </div>]]></content><author><name>Vojtch Jurnek</name></author><category term="apache-kafka"/><category term="exactly-once-semantics"/><category term="transactions"/><category term="deduplication"/><summary type="html"><![CDATA[Every now and then there is a questions in the Debezium chat or on the mailing list how to ensure exactly-once delivery of the records produced by Debezium. So far Debezium aimed only for at-least-once delivery. This means Debezium guarantees every single change will be delivered and there is no missing or skipped change event. However, in case of failures, restarts or DB connection drops, the same event can be delivered more than once. Typical scenario is that the event is delivered twice - once before failure/restart and second time after that. Exactly-once delivery (or semantic) provides stronger guarantee - every single message will be delivered and at the same time there won&#8217;t be any duplicates, every single message will be delivered exactly once. So far our answer was that the users have to implement their own deduplication system if they need exactly-once delivery. However, with Kafka Connect support for exactly-once delivery, it seems we can provide exactly-once delivery for Debezium connectors out-of-the-box, only with a little configuration change.]]></summary></entry><entry><title type="html">Debezium 2.3.0.Final Released</title><link href="https://debezium.io/blog/2023/06/21/debezium-2-3-final-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Final Released"/><published>2023-06-21T00:00:00+00:00</published><updated>2023-06-21T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/21/debezium-2-3-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/06/21/debezium-2-3-final-released/"><![CDATA[<div class="paragraph"> <p>The team has been quite busy these last couple months preparing for a condensed release timeline for Debezium 2.3, and I am thrilled to announce that the next installment has arrived, Debezium <strong>2.3.0.Final</strong> is now available!</p> </div> <div class="paragraph"> <p>Despite a condensed release schedule, this release is packed with tons of new features and improvements. Debezium 2.3 includes a brand-new notification subsystem, a rewrite of the signal subsystem to support additional means to send signals to Debezium connectors, the introduction of the JDBC storage module to maintain Debezium state in relational databases, parallelization of Vitess shard processing, PostgreSQL connector replica identity improvements, a brand-new Debezium Server operator for Kubernetes, support for exactly-once semantics for the PostgreSQL connector, and improvements to the Oracle connector.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and talk about any breaking changes and all these improvements and why they&#8217;re so important!</p> </div> <div class="ulist"> <ul> <li> <p><a href="#breaking-changes">Breaking changes</a></p> </li> <li> <p><a href="#new-features">New features and improvements</a></p> </li> <li> <p><a href="#other-fixes">Other changes</a></p> </li> <li> <p><a href="#whats-next">What&#8217;s next</a></p> </li> </ul> </div> <div class="paragraph"> <p>You can also find a full list of changes in Debezium 2.3&#8217;s <a href="https://debezium.io/releases/2.3/release-notes">release notes</a>.</p> </div> <div class="sect1"> <h2 id="breaking-changes">Breaking changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>We generally attempt to avoid as many breaking changes across minor releases as possible, but unfortunately sometimes such changes are inevitable, particularly if a feature may have been introduced as <em>experimental</em>. Debezium 2.3 introduces two breaking changes depending on whether you are upgrading from a preview release or an earlier stable version:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#postgresql-mysql-secure-connection-changes">PostgreSQL / MySQL secure connection changes</a></p> </li> <li> <p><a href="#jdbc-storage-encoding-changes">JDBC storage encoding changes</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="postgresql-mysql-secure-connection-changes">PostgreSQL / MySQL secure connection changes</h3> <div class="paragraph"> <p>Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-property-database-sslmode"><code>database.sslmode</code></a> while for MySQL this can be done with <a href="https://debezium.io/documentation/reference/2.3/connectors/mysql.html#mysql-property-database-ssl-mode"><code>database.ssl.mode</code></a>.</p> </div> <div class="paragraph"> <p>With Debezium 2.3, this configuration option no longer defaults to <code>disable</code> (PostgreSQL) or <code>disabled</code> (MySQL) but instead defaults to <code>prefer</code> (PostgreSQL) and <code>preferred</code> (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise.</p> </div> </div> <div class="sect2"> <h3 id="jdbc-storage-encoding-changes">JDBC storage encoding changes</h3> <div class="paragraph"> <p>Debezium 2.3.0.alpha1 introduced the new <em>experimental</em> JDBC storage module. This storage module defaulted to using UTF-16 as it&#8217;s default encoding; however, most databases prefer UTF-8 as a default. If you are upgrading from Debezium 2.3.0.Alpha1, Debezium 2.3.0.Beta1 and later now use UTF-8 when storing data using the JDBC storage module to align with typical database defaults.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="new-features">New features and improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 includes a plethora of new features improvements, which include the following:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#debezium-server-operator">Debezium Server Operator for Kubernetes</a></p> </li> <li> <p><a href="#notifications">New notification subsystem</a></p> </li> <li> <p><a href="#signals">New extensible signal subsystem with new channels</a></p> </li> <li> <p><a href="#jmx-signals-notifications">JMX signal and notification integration</a></p> </li> <li> <p><a href="#jdbc-storage">New JDBC storage module</a></p> </li> <li> <p><a href="#exactly-once-delivery">Exactly once delivery for PostgreSQL streaming</a></p> </li> <li> <p><a href="#replica-identity">PostgreSQL replica identity changes</a></p> </li> <li> <p><a href="#rac-specify-sid">Oracle RAC improvements</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="debezium-server-operator">Debezium Server Operator for Kubernetes</h3> <div class="paragraph"> <p>This release introduces a <em>preview</em> version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments.</p> </div> <div class="paragraph"> <p>Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this in upcoming release cycles. You can find a <a href="https://github.com/debezium/debezium-operator#quickstart-example">deployment example</a> and basic description of the <a href="https://github.com/debezium/debezium-operator#debeziumserverspec-reference">custom resource specification</a> in the github repository that you can use as a reference for the short-term.</p> </div> <div class="paragraph"> <p>We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs.</p> </div> </div> <div class="sect2"> <h3 id="notifications">New notification subsystem</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle.</p> </div> <div class="paragraph"> <p>Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event.</p> </div> <div class="listingblock"> <div class="title">Example Notification Event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">c485ccc3-16ff-47cc-b4e8-b56a57c3bad2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Started</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">additional_data</span><span class="delimiter">&quot;</span></span>: {
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>Each notification event consists of an <code>id</code> field, a UUID to identify the notification, an <code>aggregate_type</code> field to which the notification is related based on the concept of domain-driven design, a <code>type</code> field that is mean to given more detail about the aggregate type itself, and an optional <code>additional_data</code> field which consists of a map of string-based key/value pairs with additional information about the event.</p> </div> <div class="paragraph"> <p>At this time, there are two notification event types supported by Debezium:</p> </div> <div class="ulist"> <ul> <li> <p>Status of the initial snapshot</p> </li> <li> <p>Monitoring of the incremental snapshot</p> </li> </ul> </div> <div class="sect3"> <h4 id="initial_snapshot_notifications">Initial Snapshot Notifications</h4> <div class="paragraph"> <p>An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of <code>"Initial Snapshot"</code> and the type of event will consist of one of three logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>SKIPPED</code></dt> <dd> <p>Represents the initial snapshot was skipped.</p> </dd> <dt class="hdlist1"><code>ABORTED</code></dt> <dd> <p>Represents the initial snapshot was aborted.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Represents the initial snapshot has concluded successfully.</p> </dd> </dl> </div> <div class="paragraph"> <p>The following is an example of a notification about the completion of the initial snapshot:</p> </div> <div class="listingblock"> <div class="title">Example snapshot completed event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5563ae14-49f8-4579-9641-c1bbc2d76f99</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Initial Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">COMPLETED</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect3"> <h4 id="incremental_snapshot_notifications">Incremental Snapshot Notifications</h4> <div class="paragraph"> <p>An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of <code>"Incremental Snapshot"</code> and the type will consist of one of several logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>STARTED</code></dt> <dd> <p>Indicates an incremental snapshot has started.</p> </dd> <dt class="hdlist1"><code>PAUSED</code></dt> <dd> <p>Indicates an incremental snapshot has been temporarily paused.</p> </dd> <dt class="hdlist1"><code>RESUMED</code></dt> <dd> <p>Indicates an incremental snapshot that had been paused has now resumed.</p> </dd> <dt class="hdlist1"><code>STOPPED</code></dt> <dd> <p>Indicates an incremental snapshot has stopped.</p> </dd> <dt class="hdlist1"><code>IN_PROGRESS</code></dt> <dd> <p>Indicates an incremental snapshot is in-progress.</p> </dd> <dt class="hdlist1"><code>TABLE_SCAN_COMPLETED</code></dt> <dd> <p>Indicates an incremental snapshot has concluded for a given table.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Indicates that an incremental snapshot has concluded for all tables.</p> </dd> </dl> </div> </div> <div class="sect3"> <h4 id="configuring_notifications">Configuring Notifications</h4> <div class="paragraph"> <p>Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels.</p> </div> <div class="listingblock"> <div class="title">Using a Kafka Topic</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">sink</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.sink.topic.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_notifications</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Using the connector logs</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">log</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> <div class="sect2"> <h3 id="signals">New extensible signal subsystem</h3> <div class="paragraph"> <p>Debezium has supported the concept of a signal since the introduction of the Incremental Snapshot feature well back in Debezium 1.x. Signals are important as it allows you to provide metadata to instruct Debezium to perform a given task, whether that task is to write an entry to the connector log or perform an ad-hoc incremental snapshot.</p> </div> <div class="paragraph"> <p>In Debezium 2.3, the signal subsystem was reworked to introduce the concept called <em>channel</em>, which represents a medium for which Debezium watches or listens and reacts to signals. In previous versions, there was one channel supported universally across connectors, which was the database signal table. In this release, these channels have been unified and the following are available out of the box:</p> </div> <div class="ulist"> <ul> <li> <p>Database signal table</p> </li> <li> <p>Kafka signal topic</p> </li> <li> <p>Filesystem</p> </li> <li> <p>JMX</p> </li> </ul> </div> <div class="paragraph"> <p>But that&#8217;s not all, the signal channel contract is extensible, allowing you to write a custom implementation and make that available to your connectors with ease.</p> </div> </div> <div class="sect2"> <h3 id="jmx-signals-notifications">JMX signals and notifications integration</h3> <div class="paragraph"> <p>Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX.</p> </div> <div class="sect3"> <h4 id="sending_signals">Sending signals</h4> <div class="paragraph"> <p>In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the <code>jconsole</code> window, you can now see there are two new subsections for a connector, a notifications and signal section shown below:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-signal-notifications.png" class="responsive-image" alt="JConsole JMX metrics"> </div> </div> <div class="paragraph"> <p>The new <code>signals</code> section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-send-signal.png" class="responsive-image" alt="JConsole Sending Signals"> </div> </div> </div> <div class="sect3"> <h4 id="receiving_notifications">Receiving notifications</h4> <div class="paragraph"> <p>The new <code>notifications</code> section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-receive-notification.png" class="responsive-image" alt="JConsole Receive Notifications"> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the <code>reset</code> method on the notifications bean once you&#8217;ve processed all notifications.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX.</p> </div> </div> </div> <div class="sect2"> <h3 id="jdbc-storage">New JDBC storage subsystem</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new module, the following dependency must be added to your project or application:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>debezium-storage-jdbc<span class="tag">&lt;/artifactId&gt;</span>
    <span class="tag">&lt;version&gt;</span>2.3.0.Final<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>The following examples show how to configure Offset or Schema History storage via the JDBC storage module:</p> </div> <div class="listingblock"> <div class="title">Configuration example for Offset JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.offset_table_name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_offset_storage</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Configuration example for Schema History JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.schema.history.table.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_database_history</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="exactly-once-delivery">Exactly once delivery for PostgreSQL streaming</h3> <div class="paragraph"> <p>Debezium has traditionally been an at-least-once delivery solution, guaranteeing that no change is ever missed. Exactly-Once is a proposal by the Apache Kafka community as a part of <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors">KIP-618</a>. This proposal aims to address a common problem with producers (source connectors) when a producer retries, it may re-send a batch of events to the Kafka broker even if that batch had already been committed by the broker. This means there are situations where duplicate events may be sent and not every consumer (sink connector) may be capable of handling such situations easily.</p> </div> <div class="paragraph"> <p>Debezium plans to roll out exactly-once delivery semantics in a phased style. There are specific corner cases around snapshotting and streaming and these can vary by connector. And with that, Debezium 2.3 starts out by specifically only adding support for exactly-once semantics for the PostgreSQL during its <em>streaming phase</em> only!</p> </div> <div class="paragraph"> <p>In order to take advantage of exactly-once delivery, there is no connector configuration changes required; however, your Kafka Connect worker configuration will need to be adjusted to enable this feature. You can find a reference to the new configuration properties in <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-618%3A+Exactly-Once+Support+for+Source+Connectors#KIP618:ExactlyOnceSupportforSourceConnectors-Newproperties">KIP-618</a>.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>If you are mixing connectors on the same connect cluster that do and do not support exactly once delivery, you will not be able to set <code>exactly.once.support</code> in the worker configuration to <code>required</code> as this will cause connectors that do not support this feature to fail validation at start-up.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="replica-identity">PostgreSQL replica identity changes</h3> <div class="paragraph"> <p>Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity".</p> </div> <div class="paragraph"> <p>Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up.</p> </div> <div class="paragraph"> <p>The new configuration option, <code>replica.identity.autoset.values</code>, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-replica-identity">documentation</a>.</p> </div> <div class="paragraph"> <p>When specifying the <code>replica.identity.autoset.values</code>, the value is a comma-separated list of values where each element uses the format of <code>&lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;</code>. An example is shown below where two tables are configured to have full replica identity:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">replica.identity.autoset.values</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.table1:FULL,public.table2:FULL</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="rac-specify-sid">Oracle RAC improvements</h3> <div class="paragraph"> <p>When connecting to an Oracle RAC installation, you must specify a <code>rac.nodes</code> configuration property with a minimum of the host or IP address of all individual nodes across the cluster. Older versions of the connector also supported a varied format, allowing the inclusion of a port for each node as not every node on the cluster may use the same port.</p> </div> <div class="paragraph"> <p>Debezium 2.3 improves the Oracle RAC support by also acknowledging that each node may not necessarily use the same Oracle Site Identifier (SID), and therefore, the SID can also now be included in the <code>rac.nodes</code> configuration property to support such installations.</p> </div> <div class="paragraph"> <p>The following example illustrates connecting to two Oracle RAC nodes, each using different ports and SID parameters:</p> </div> <div class="listingblock"> <div class="title">An example configuration</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.oracle.OracleConnector</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">rac.nodes</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">host1.domain.com:1521/ORCLSID1,host2.domain.com:1522/ORCLSID2</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium Server stops sending events to Google Cloud Pub/Sub <a href="https://issues.redhat.com/browse/DBZ-5175">DBZ-5175</a></p> </li> <li> <p>Toasted varying character array and date array are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6122">DBZ-6122</a></p> </li> <li> <p>Upgrade to Infinispan 14.0.11.Final to fix CVE-2022-45047 <a href="https://issues.redhat.com/browse/DBZ-6193">DBZ-6193</a></p> </li> <li> <p>Introduce LogMiner query filtering modes <a href="https://issues.redhat.com/browse/DBZ-6254">DBZ-6254</a></p> </li> <li> <p>Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed <a href="https://issues.redhat.com/browse/DBZ-6256">DBZ-6256</a></p> </li> <li> <p>Ensure that the connector can start from a stale timestamp more than one hour into the past <a href="https://issues.redhat.com/browse/DBZ-6307">DBZ-6307</a></p> </li> <li> <p>The rs_id field is null in Oracle change event source information block <a href="https://issues.redhat.com/browse/DBZ-6329">DBZ-6329</a></p> </li> <li> <p>Add JWT authentication to HTTP Client <a href="https://issues.redhat.com/browse/DBZ-6348">DBZ-6348</a></p> </li> <li> <p>Using pg_replication_slot_advance which is not supported by PostgreSQL10. <a href="https://issues.redhat.com/browse/DBZ-6353">DBZ-6353</a></p> </li> <li> <p>log.mining.transaction.retention.hours should reference last offset and not sysdate <a href="https://issues.redhat.com/browse/DBZ-6355">DBZ-6355</a></p> </li> <li> <p>Support multiple tasks when streaming shard list <a href="https://issues.redhat.com/browse/DBZ-6365">DBZ-6365</a></p> </li> <li> <p>Code Improvements for skip.messages.without.change <a href="https://issues.redhat.com/browse/DBZ-6366">DBZ-6366</a></p> </li> <li> <p>Kinesis Sink - AWS Credentials Provider <a href="https://issues.redhat.com/browse/DBZ-6372">DBZ-6372</a></p> </li> <li> <p>Toasted hstore are not correctly processed <a href="https://issues.redhat.com/browse/DBZ-6379">DBZ-6379</a></p> </li> <li> <p>Oracle DDL shrink space for table partition can not be parsed <a href="https://issues.redhat.com/browse/DBZ-6386">DBZ-6386</a></p> </li> <li> <p>__source_ts_ms r (read) operation date is set to future for SQL Server <a href="https://issues.redhat.com/browse/DBZ-6388">DBZ-6388</a></p> </li> <li> <p>PostgreSQL connector task fails to resume streaming because replication slot is active <a href="https://issues.redhat.com/browse/DBZ-6396">DBZ-6396</a></p> </li> <li> <p>Date and Time values without timezones are not persisted correctly based on database.time_zone <a href="https://issues.redhat.com/browse/DBZ-6399">DBZ-6399</a></p> </li> <li> <p>MongoDB connector crashes on invalid resume token <a href="https://issues.redhat.com/browse/DBZ-6402">DBZ-6402</a></p> </li> <li> <p>Snapshot step 5 - Reading structure of captured tables time too long <a href="https://issues.redhat.com/browse/DBZ-6439">DBZ-6439</a></p> </li> <li> <p>NPE on read-only MySQL connector start up <a href="https://issues.redhat.com/browse/DBZ-6440">DBZ-6440</a></p> </li> <li> <p>Oracle parallel snapshots do not properly set PDB context when using multitenancy <a href="https://issues.redhat.com/browse/DBZ-6457">DBZ-6457</a></p> </li> <li> <p>[MariaDB] Add support for userstat plugin keywords <a href="https://issues.redhat.com/browse/DBZ-6459">DBZ-6459</a></p> </li> <li> <p>Debezium Server cannot recover from Google Pub/Sub errors <a href="https://issues.redhat.com/browse/DBZ-6461">DBZ-6461</a></p> </li> <li> <p>"Ignoring invalid task provided offset" <a href="https://issues.redhat.com/browse/DBZ-6463">DBZ-6463</a></p> </li> <li> <p>Oracle snapshot.include.collection.list should be prefixed with databaseName in documentation. <a href="https://issues.redhat.com/browse/DBZ-6474">DBZ-6474</a></p> </li> <li> <p>Db2 connector can fail with NPE on notification sending <a href="https://issues.redhat.com/browse/DBZ-6485">DBZ-6485</a></p> </li> <li> <p>ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception <a href="https://issues.redhat.com/browse/DBZ-6486">DBZ-6486</a></p> </li> <li> <p>BigDecimal fails when queue memory size limit is in place <a href="https://issues.redhat.com/browse/DBZ-6490">DBZ-6490</a></p> </li> <li> <p>Allow schema to be specified in the Debezium Sink Connector configuration <a href="https://issues.redhat.com/browse/DBZ-6491">DBZ-6491</a></p> </li> <li> <p>ORACLE table can not be captured, got runtime.NoViableAltException <a href="https://issues.redhat.com/browse/DBZ-6492">DBZ-6492</a></p> </li> <li> <p>Signal poll interval has incorrect default value <a href="https://issues.redhat.com/browse/DBZ-6496">DBZ-6496</a></p> </li> <li> <p>Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone <a href="https://issues.redhat.com/browse/DBZ-6502">DBZ-6502</a></p> </li> <li> <p>Alpine postgres images should use llvm/clang 15 explicitly <a href="https://issues.redhat.com/browse/DBZ-6506">DBZ-6506</a></p> </li> <li> <p>FileSignalChannel is not loaded <a href="https://issues.redhat.com/browse/DBZ-6509">DBZ-6509</a></p> </li> <li> <p>Utilize event.processing.failure.handling.mode in Vitess replication connection <a href="https://issues.redhat.com/browse/DBZ-6510">DBZ-6510</a></p> </li> <li> <p>MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization <a href="https://issues.redhat.com/browse/DBZ-6511">DBZ-6511</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6512">DBZ-6512</a></p> </li> <li> <p>Error value of negative seconds in convertOracleIntervalDaySecond <a href="https://issues.redhat.com/browse/DBZ-6513">DBZ-6513</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6515">DBZ-6515</a></p> </li> <li> <p>Only use error processing mode on certain errors <a href="https://issues.redhat.com/browse/DBZ-6523">DBZ-6523</a></p> </li> <li> <p>[PostgreSQL] LTree data is not being captured by streaming <a href="https://issues.redhat.com/browse/DBZ-6524">DBZ-6524</a></p> </li> <li> <p>Oracle Connector: Snapshot fails with specific combination <a href="https://issues.redhat.com/browse/DBZ-6528">DBZ-6528</a></p> </li> <li> <p>Use better hashing function for PartitionRouting <a href="https://issues.redhat.com/browse/DBZ-6529">DBZ-6529</a></p> </li> <li> <p>Table order is incorrect on snapshots <a href="https://issues.redhat.com/browse/DBZ-6533">DBZ-6533</a></p> </li> <li> <p>Start publishing nightly images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6541">DBZ-6541</a></p> </li> <li> <p>Start releasing images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6542">DBZ-6542</a></p> </li> <li> <p>Unhandled NullPointerException in PartitionRouting will crash the whole connect plugin <a href="https://issues.redhat.com/browse/DBZ-6543">DBZ-6543</a></p> </li> <li> <p>Bug in field.name.adjustment.mode Property <a href="https://issues.redhat.com/browse/DBZ-6559">DBZ-6559</a></p> </li> <li> <p>Operator sets incorrect value of transformation.predicate when no predicate is specified <a href="https://issues.redhat.com/browse/DBZ-6560">DBZ-6560</a></p> </li> <li> <p>Upgrade MySQL JDBC driver to 8.0.33 <a href="https://issues.redhat.com/browse/DBZ-6563">DBZ-6563</a></p> </li> <li> <p>Upgrade Google Cloud BOM to 26.17.0 <a href="https://issues.redhat.com/browse/DBZ-6570">DBZ-6570</a></p> </li> <li> <p>Kubernetes-Config extension interferes with SSL tests due to k8 devservice starting up <a href="https://issues.redhat.com/browse/DBZ-6574">DBZ-6574</a></p> </li> <li> <p>MySQL read-only connector with Kafka signals enabled fails on start up <a href="https://issues.redhat.com/browse/DBZ-6579">DBZ-6579</a></p> </li> <li> <p>Redis schema history can fail upon startup <a href="https://issues.redhat.com/browse/DBZ-6580">DBZ-6580</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(2.3.0.Alpha1%2C%202.3.0.Beta1%2C%202.3.0.CR1%2C%202.3.0.Final)%20ORDER%20BY%20component%20ASC">138 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/isacandrei">Andrei Isac</a>, <a href="https://github.com/angsdey2">Angshuman Dey</a>, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/bpaquet">Bertrand Paquet</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/cjmencias">Christian Jacob Mencias</a>, <a href="https://github.com/TechIsCool">David Beck</a>, <a href="https://github.com/fredlegaucher">Frederic Laurent</a>, <a href="https://github.com/ddsr-ops">Gong Chang Hua</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/Hidetomi">Hidetomi Umaki</a>, <a href="https://github.com/uidoyen">Hussain Ansari</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jehrenzweig-pi">Jesse Ehrenzweig</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/miguelbirdie">Miguel Angel Sotomayor</a>, <a href="https://github.com/nancyxu123">Nancy Xu</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/zalmane">Oren Elias</a>, <a href="https://github.com/rnowling-memphis">RJ Nowling</a>, <a href="https://github.com/rk3rn3r">Ren Kerner</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/ShuranZhang">Shuran Zhang</a>, <a href="https://github.com/sclarkson-zoomcare">Stephen Clarkson</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/tommyk-gears">Tommy Karlsson</a>, <a href="https://github.com/imtj1">Tony Joseph</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/caicancai"></a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 out, our major focus will be coordinating bugfixes to any reports for Debezium 2.3, but primarily on the upcoming new preview release for Debezium 2.4.</p> </div> <div class="paragraph"> <p>Debezium 2.4 is planned with a ton of changes, and we intend to tackle these changes in a bucketed fashion, with each bucket being assigned a priority. As with any schedule, priorities are subject to change but the following is an outline of what to expect:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Priority 1</dt> <dd> <div class="ulist"> <ul> <li> <p>[Core] TimescaleDB single message transformation support</p> </li> <li> <p>[Core] Timezone single message transformation to ease usages with Debezium temporal types</p> </li> <li> <p>[Core] Initial snapshot notifications</p> </li> <li> <p>[MongoDB] Database-wide change stream support</p> </li> <li> <p>[MongoDB] Multi-task deployment metrics support</p> </li> <li> <p>[Oracle] OpenLogReplicator adapter support</p> </li> <li> <p>[Oracle] XML, LONG, RAW, and LONG RAW data type support</p> </li> <li> <p>[Universal] Exactly-Once semantics support for other connectors</p> </li> <li> <p>[Dependencies] Apache Kafka 3.5.x support</p> </li> </ul> </div> </dd> <dt class="hdlist1">Priority 2</dt> <dd> <div class="ulist"> <ul> <li> <p>[Operator] Next steps for Debezium operator</p> </li> <li> <p>[Core] Ad-hoc blocking snapshot</p> </li> <li> <p>[Dependencies] Use OpenTelemetry</p> </li> </ul> </div> </dd> <dt class="hdlist1">Priority 3</dt> <dd> <div class="ulist"> <ul> <li> <p>[Embedded Engine] Parallelization support</p> </li> <li> <p>[MongoDB] Parallel incremental snapshots support</p> </li> <li> <p>[MySQL] Parallel schema snapshots support</p> </li> </ul> </div> </dd> </dl> </div> <div class="paragraph"> <p>This is not an exhaustive list and its quite ambitious, but given the shortened time with Debezium 2.3, we hope the extra few weeks on Debezium 2.4 will make this next minor release possible with all these features and much more. The roadmap will be updated this week to align with the above for Debezium 2.4 and the future, so please be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions.</p> </div> <div class="paragraph"> <p>Otherwise, I expect with summer in full swing for those of us in the north, holidays and much-deserved time-off will be normal for the next several months. To those who travel or intend to take some time for yourselves and family, enjoy and be safe!</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="caassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[The team has been quite busy these last couple months preparing for a condensed release timeline for Debezium 2.3, and I am thrilled to announce that the next installment has arrived, Debezium 2.3.0.Final is now available! Despite a condensed release schedule, this release is packed with tons of new features and improvements. Debezium 2.3 includes a brand-new notification subsystem, a rewrite of the signal subsystem to support additional means to send signals to Debezium connectors, the introduction of the JDBC storage module to maintain Debezium state in relational databases, parallelization of Vitess shard processing, PostgreSQL connector replica identity improvements, a brand-new Debezium Server operator for Kubernetes, support for exactly-once semantics for the PostgreSQL connector, and improvements to the Oracle connector. Let&#8217;s take a few moments and talk about any breaking changes and all these improvements and why they&#8217;re so important! Breaking changes New features and improvements Other changes What&#8217;s next You can also find a full list of changes in Debezium 2.3&#8217;s release notes. Breaking changes We generally attempt to avoid as many breaking changes across minor releases as possible, but unfortunately sometimes such changes are inevitable, particularly if a feature may have been introduced as experimental. Debezium 2.3 introduces two breaking changes depending on whether you are upgrading from a preview release or an earlier stable version: PostgreSQL / MySQL secure connection changes JDBC storage encoding changes PostgreSQL / MySQL secure connection changes Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring database.sslmode while for MySQL this can be done with database.ssl.mode. With Debezium 2.3, this configuration option no longer defaults to disable (PostgreSQL) or disabled (MySQL) but instead defaults to prefer (PostgreSQL) and preferred (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise. JDBC storage encoding changes Debezium 2.3.0.alpha1 introduced the new experimental JDBC storage module. This storage module defaulted to using UTF-16 as it&#8217;s default encoding; however, most databases prefer UTF-8 as a default. If you are upgrading from Debezium 2.3.0.Alpha1, Debezium 2.3.0.Beta1 and later now use UTF-8 when storing data using the JDBC storage module to align with typical database defaults. New features and improvements Debezium 2.3 includes a plethora of new features improvements, which include the following: Debezium Server Operator for Kubernetes New notification subsystem New extensible signal subsystem with new channels JMX signal and notification integration New JDBC storage module Exactly once delivery for PostgreSQL streaming PostgreSQL replica identity changes Oracle RAC improvements Debezium Server Operator for Kubernetes This release introduces a preview version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments. Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this in upcoming release cycles. You can find a deployment example and basic description of the custom resource specification in the github repository that you can use as a reference for the short-term. We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs. New notification subsystem Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle. Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event. Example Notification Event { &quot;id&quot;: &quot;c485ccc3-16ff-47cc-b4e8-b56a57c3bad2&quot;, &quot;aggregate_type&quot;: &quot;Snapshot&quot;, &quot;type&quot;: &quot;Started&quot;, &quot;additional_data&quot;: { ... } } Each notification event consists of an id field, a UUID to identify the notification, an aggregate_type field to which the notification is related based on the concept of domain-driven design, a type field that is mean to given more detail about the aggregate type itself, and an optional additional_data field which consists of a map of string-based key/value pairs with additional information about the event. At this time, there are two notification event types supported by Debezium: Status of the initial snapshot Monitoring of the incremental snapshot Initial Snapshot Notifications An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of "Initial Snapshot" and the type of event will consist of one of three logical values: SKIPPED Represents the initial snapshot was skipped. ABORTED Represents the initial snapshot was aborted. COMPLETED Represents the initial snapshot has concluded successfully. The following is an example of a notification about the completion of the initial snapshot: Example snapshot completed event { &quot;id&quot;: &quot;5563ae14-49f8-4579-9641-c1bbc2d76f99&quot;, &quot;aggregate_type&quot;: &quot;Initial Snapshot&quot;, &quot;type&quot;: &quot;COMPLETED&quot; } Incremental Snapshot Notifications An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of "Incremental Snapshot" and the type will consist of one of several logical values: STARTED Indicates an incremental snapshot has started. PAUSED Indicates an incremental snapshot has been temporarily paused. RESUMED Indicates an incremental snapshot that had been paused has now resumed. STOPPED Indicates an incremental snapshot has stopped. IN_PROGRESS Indicates an incremental snapshot is in-progress. TABLE_SCAN_COMPLETED Indicates an incremental snapshot has concluded for a given table. COMPLETED Indicates that an incremental snapshot has concluded for all tables. Configuring Notifications Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels. Using a Kafka Topic { &quot;notification.enable.channels&quot;: &quot;sink&quot;, &quot;notification.sink.topic.name&quot;: &quot;debezium_notifications&quot;, ... } Using the connector logs { &quot;notification.enable.channels&quot;: &quot;log&quot; } New extensible signal subsystem Debezium has supported the concept of a signal since the introduction of the Incremental Snapshot feature well back in Debezium 1.x. Signals are important as it allows you to provide metadata to instruct Debezium to perform a given task, whether that task is to write an entry to the connector log or perform an ad-hoc incremental snapshot. In Debezium 2.3, the signal subsystem was reworked to introduce the concept called channel, which represents a medium for which Debezium watches or listens and reacts to signals. In previous versions, there was one channel supported universally across connectors, which was the database signal table. In this release, these channels have been unified and the following are available out of the box: Database signal table Kafka signal topic Filesystem JMX But that&#8217;s not all, the signal channel contract is extensible, allowing you to write a custom implementation and make that available to your connectors with ease. JMX signals and notifications integration Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX. Sending signals In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the jconsole window, you can now see there are two new subsections for a connector, a notifications and signal section shown below: The new signals section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from jconsole: Receiving notifications The new notifications section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from jconsole: The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the reset method on the notifications bean once you&#8217;ve processed all notifications. We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX. New JDBC storage subsystem Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform. In order to take advantage of this new module, the following dependency must be added to your project or application: Maven coordinates &lt;dependency&gt; &lt;groupId&gt;io.debezium&lt;/groupId&gt; &lt;artifactId&gt;debezium-storage-jdbc&lt;/artifactId&gt; &lt;version&gt;2.3.0.Final&lt;/version&gt; &lt;/dependency&gt; The following examples show how to configure Offset or Schema History storage via the JDBC storage module: Configuration example for Offset JDBC storage { &quot;offset.storage.jdbc.url&quot;: &quot;&lt;jdbc-connection-url&gt;&quot;, &quot;offset.storage.jdbc.user&quot;: &quot;dbuser&quot;, &quot;offset.storage.jdbc.password&quot;: &quot;secret&quot;, &quot;offset.storage.jdbc.offset_table_name&quot;: &quot;debezium_offset_storage&quot; } Configuration example for Schema History JDBC storage { &quot;schema.history.internal.jdbc.url&quot;: &quot;&lt;jdbc-connection-url&gt;&quot;, &quot;schema.history.internal.jdbc.user&quot;: &quot;dbuser&quot;, &quot;schema.history.internal.jdbc.password&quot;: &quot;secret&quot;, &quot;schema.history.internal.jdbc.schema.history.table.name&quot;: &quot;debezium_database_history&quot; } Exactly once delivery for PostgreSQL streaming Debezium has traditionally been an at-least-once delivery solution, guaranteeing that no change is ever missed. Exactly-Once is a proposal by the Apache Kafka community as a part of KIP-618. This proposal aims to address a common problem with producers (source connectors) when a producer retries, it may re-send a batch of events to the Kafka broker even if that batch had already been committed by the broker. This means there are situations where duplicate events may be sent and not every consumer (sink connector) may be capable of handling such situations easily. Debezium plans to roll out exactly-once delivery semantics in a phased style. There are specific corner cases around snapshotting and streaming and these can vary by connector. And with that, Debezium 2.3 starts out by specifically only adding support for exactly-once semantics for the PostgreSQL during its streaming phase only! In order to take advantage of exactly-once delivery, there is no connector configuration changes required; however, your Kafka Connect worker configuration will need to be adjusted to enable this feature. You can find a reference to the new configuration properties in KIP-618. If you are mixing connectors on the same connect cluster that do and do not support exactly once delivery, you will not be able to set exactly.once.support in the worker configuration to required as this will cause connectors that do not support this feature to fail validation at start-up. PostgreSQL replica identity changes Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity". Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up. The new configuration option, replica.identity.autoset.values, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the documentation. When specifying the replica.identity.autoset.values, the value is a comma-separated list of values where each element uses the format of &lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;. An example is shown below where two tables are configured to have full replica identity: { &quot;replica.identity.autoset.values&quot;: &quot;public.table1:FULL,public.table2:FULL&quot; } Oracle RAC improvements When connecting to an Oracle RAC installation, you must specify a rac.nodes configuration property with a minimum of the host or IP address of all individual nodes across the cluster. Older versions of the connector also supported a varied format, allowing the inclusion of a port for each node as not every node on the cluster may use the same port. Debezium 2.3 improves the Oracle RAC support by also acknowledging that each node may not necessarily use the same Oracle Site Identifier (SID), and therefore, the SID can also now be included in the rac.nodes configuration property to support such installations. The following example illustrates connecting to two Oracle RAC nodes, each using different ports and SID parameters: An example configuration { &quot;connector.class&quot;: &quot;io.debezium.connector.oracle.OracleConnector&quot;, &quot;rac.nodes&quot;: &quot;host1.domain.com:1521/ORCLSID1,host2.domain.com:1522/ORCLSID2&quot;, ... } Other fixes There were quite a number of bugfixes and stability changes in this release, some noteworthy are: Debezium Server stops sending events to Google Cloud Pub/Sub DBZ-5175 Toasted varying character array and date array are not correcly processed DBZ-6122 Upgrade to Infinispan 14.0.11.Final to fix CVE-2022-45047 DBZ-6193 Introduce LogMiner query filtering modes DBZ-6254 Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed DBZ-6256 Ensure that the connector can start from a stale timestamp more than one hour into the past DBZ-6307 The rs_id field is null in Oracle change event source information block DBZ-6329 Add JWT authentication to HTTP Client DBZ-6348 Using pg_replication_slot_advance which is not supported by PostgreSQL10. DBZ-6353 log.mining.transaction.retention.hours should reference last offset and not sysdate DBZ-6355 Support multiple tasks when streaming shard list DBZ-6365 Code Improvements for skip.messages.without.change DBZ-6366 Kinesis Sink - AWS Credentials Provider DBZ-6372 Toasted hstore are not correctly processed DBZ-6379 Oracle DDL shrink space for table partition can not be parsed DBZ-6386 __source_ts_ms r (read) operation date is set to future for SQL Server DBZ-6388 PostgreSQL connector task fails to resume streaming because replication slot is active DBZ-6396 Date and Time values without timezones are not persisted correctly based on database.time_zone DBZ-6399 MongoDB connector crashes on invalid resume token DBZ-6402 Snapshot step 5 - Reading structure of captured tables time too long DBZ-6439 NPE on read-only MySQL connector start up DBZ-6440 Oracle parallel snapshots do not properly set PDB context when using multitenancy DBZ-6457 [MariaDB] Add support for userstat plugin keywords DBZ-6459 Debezium Server cannot recover from Google Pub/Sub errors DBZ-6461 "Ignoring invalid task provided offset" DBZ-6463 Oracle snapshot.include.collection.list should be prefixed with databaseName in documentation. DBZ-6474 Db2 connector can fail with NPE on notification sending DBZ-6485 ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception DBZ-6486 BigDecimal fails when queue memory size limit is in place DBZ-6490 Allow schema to be specified in the Debezium Sink Connector configuration DBZ-6491 ORACLE table can not be captured, got runtime.NoViableAltException DBZ-6492 Signal poll interval has incorrect default value DBZ-6496 Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone DBZ-6502 Alpine postgres images should use llvm/clang 15 explicitly DBZ-6506 FileSignalChannel is not loaded DBZ-6509 Utilize event.processing.failure.handling.mode in Vitess replication connection DBZ-6510 MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization DBZ-6511 Debezium incremental snapshot chunk size documentation unclear or incorrect DBZ-6512 Error value of negative seconds in convertOracleIntervalDaySecond DBZ-6513 Debezium incremental snapshot chunk size documentation unclear or incorrect DBZ-6515 Only use error processing mode on certain errors DBZ-6523 [PostgreSQL] LTree data is not being captured by streaming DBZ-6524 Oracle Connector: Snapshot fails with specific combination DBZ-6528 Use better hashing function for PartitionRouting DBZ-6529 Table order is incorrect on snapshots DBZ-6533 Start publishing nightly images for Debezium Operator DBZ-6541 Start releasing images for Debezium Operator DBZ-6542 Unhandled NullPointerException in PartitionRouting will crash the whole connect plugin DBZ-6543 Bug in field.name.adjustment.mode Property DBZ-6559 Operator sets incorrect value of transformation.predicate when no predicate is specified DBZ-6560 Upgrade MySQL JDBC driver to 8.0.33 DBZ-6563 Upgrade Google Cloud BOM to 26.17.0 DBZ-6570 Kubernetes-Config extension interferes with SSL tests due to k8 devservice starting up DBZ-6574 MySQL read-only connector with Kafka signals enabled fails on start up DBZ-6579 Redis schema history can fail upon startup DBZ-6580 Altogether, 138 issues were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Andrei Isac, Angshuman Dey, Anil Dasari, Anisha Mohanty, Bertrand Paquet, Bob Roldan, Breno Moreira, Chris Cranford, Christian Jacob Mencias, David Beck, Frederic Laurent, Gong Chang Hua, Harvey Yue, Hidetomi Umaki, Hussain Ansari, Indra Shukla, Ismail Simsek, Jakub Cechacek, Jesse Ehrenzweig, Jiri Pechanec, Jochen Schalanda, Kanthi Subramanian, Katerina Galieva, Mario Fiore Vitale, Martin Medek, Miguel Angel Sotomayor, Nancy Xu, Nir Levy, Ondrej Babec, Oren Elias, RJ Nowling, Ren Kerner, Robert Roldan, Ronak Jain, Sergey Eizner, Shuran Zhang, Stephen Clarkson, Thomas Thornton, Tommy Karlsson, Tony Joseph, Vojtech Juranek, and ! What&#8217;s next? With Debezium 2.3 out, our major focus will be coordinating bugfixes to any reports for Debezium 2.3, but primarily on the upcoming new preview release for Debezium 2.4. Debezium 2.4 is planned with a ton of changes, and we intend to tackle these changes in a bucketed fashion, with each bucket being assigned a priority. As with any schedule, priorities are subject to change but the following is an outline of what to expect: Priority 1 [Core] TimescaleDB single message transformation support [Core] Timezone single message transformation to ease usages with Debezium temporal types [Core] Initial snapshot notifications [MongoDB] Database-wide change stream support [MongoDB] Multi-task deployment metrics support [Oracle] OpenLogReplicator adapter support [Oracle] XML, LONG, RAW, and LONG RAW data type support [Universal] Exactly-Once semantics support for other connectors [Dependencies] Apache Kafka 3.5.x support Priority 2 [Operator] Next steps for Debezium operator [Core] Ad-hoc blocking snapshot [Dependencies] Use OpenTelemetry Priority 3 [Embedded Engine] Parallelization support [MongoDB] Parallel incremental snapshots support [MySQL] Parallel schema snapshots support This is not an exhaustive list and its quite ambitious, but given the shortened time with Debezium 2.3, we hope the extra few weeks on Debezium 2.4 will make this next minor release possible with all these features and much more. The roadmap will be updated this week to align with the above for Debezium 2.4 and the future, so please be sure to get in touch with us on the mailing list or our chat if you have any ideas or suggestions. Otherwise, I expect with summer in full swing for those of us in the north, holidays and much-deserved time-off will be normal for the next several months. To those who travel or intend to take some time for yourselves and family, enjoy and be safe! Until next time&#8230;&#8203;]]></summary></entry><entry><title type="html">Debezium 2.3.0.CR1 Released</title><link href="https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.CR1 Released"/><published>2023-06-12T00:00:00+00:00</published><updated>2023-06-12T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/06/12/debezium-2-3-cr1-released/"><![CDATA[<div class="paragraph"> <p>It is my pleasure to announce the next Debezium 2.3 release, <strong>2.3.0.CR1</strong>!</p> </div> <div class="paragraph"> <p>The main focus of this release is to stabilize the Debezium 2.3 release in preparation for a final release in the coming weeks, which typically means we&#8217;re focusing on bugfixes; however, this release includes two new features. Lets take a moment and dive into these new features and any bug fixes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>This release includes no breaking changes, so upgrading from Debezium 2.3.0.Beta1 to 2.3.0.CR1 should be a simple drop-in replacement.</p> </div> <div class="paragraph"> <p>If you are upgrading from a prior version, please review the <a href="https://debezium.io/releases/2.3/release-notes">release notes</a> for any migration steps that may be necessary.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_server_kubernetes_operator">Debezium Server Kubernetes Operator</h2> <div class="sectionbody"> <div class="paragraph"> <p>This release introduces a <em>preview</em> version of the new Debezium Operator, providing the ability to deploy and manage Debezium Server instances within Kubernetes. Debezium Server allows you to stream change events from your data sources to a wide variety of messaging infrastructures. Our goal is to provide a Kafka-less alternative for the Debezium community who wish to utilize Kubernetes for scalability and high availability deployments.</p> </div> <div class="paragraph"> <p>Presently, the documentation is sparse as the operator is in early incubation stages; however, we do intend to improve upon this throughout the remainder of the 2.3 as well into the Debezium 2.4 release cycles. You can find a <a href="https://github.com/debezium/debezium-operator#quickstart-example">deployment example</a> and basic description of the <a href="https://github.com/debezium/debezium-operator#debeziumserverspec-reference">custom resource specification</a> in the github repository that you can use as a reference for the short-term.</p> </div> <div class="paragraph"> <p>We do not recommend a production deployment of this component at this time; however, we encourage users to provide community feedback. The feedback will be valuable in evaluating if the component is feature ready or if there are still areas of improvement to meet everyone&#8217;s needs.</p> </div> </div> </div> <div class="sect1"> <h2 id="jmx_signals_and_notifications">JMX signals and notifications</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 previously introduced both a new signal channel and notification feature. This feature allows external applications to easily integrate with Debezium, sending signals to perform various tasks such as ad-hoc incremental snapshots, and to receive notifications about the progress of such tasks. This release builds on top of that functionality to allow the ability to send signals and receive notifications via JMX.</p> </div> <div class="sect2"> <h3 id="sending_signals">Sending signals</h3> <div class="paragraph"> <p>In this release, the signal channel subsystem has been improved to support sending signals via JMX. From the <code>jconsole</code> window, you can now see there are two new subsections for a connector, a notifications and signal section shown below:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-signal-notifications.png" class="responsive-image" alt="JConsole JMX metrics"> </div> </div> <div class="paragraph"> <p>The new <code>signals</code> section allows you to invoke an operation on the JMX bean in order to transmit a signal to Debezium. This signal resembles the logical signal table structure where it accepts 3 parameters, a unique identifier, the signal type, and finally the signal payload. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-send-signal.png" class="responsive-image" alt="JConsole Sending Signals"> </div> </div> </div> <div class="sect2"> <h3 id="receiving_notifications">Receiving notifications</h3> <div class="paragraph"> <p>The new <code>notifications</code> section allows you to receive and react to notifications captured by the JMX bean from Debezium. The Debezium JMX bean will buffer all notifications to ensure that no notification is missed. The following illustrates what this looks like from <code>jconsole</code>:</p> </div> <div class="exampleblock imageblock centered-image"> <div class="content"> <img src="/assets/images/jconsole-receive-notification.png" class="responsive-image" alt="JConsole Receive Notifications"> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The JMX bean does not automatically clear the notification queue. In order to avoid memory concerns, be sure to invoke the <code>reset</code> method on the notifications bean once you&#8217;ve processed all notifications.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>We look forward to your feedback on this new way to integrate signals and notifications with Debezium over JMX.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="ulist"> <ul> <li> <p>Code Improvements for skip.messages.without.change <a href="https://issues.redhat.com/browse/DBZ-6366">DBZ-6366</a></p> </li> <li> <p>FileSignalChannel is not loaded <a href="https://issues.redhat.com/browse/DBZ-6509">DBZ-6509</a></p> </li> <li> <p>Utilize event.processing.failure.handling.mode in Vitess replication connection <a href="https://issues.redhat.com/browse/DBZ-6510">DBZ-6510</a></p> </li> <li> <p>MySqlReadOnlyIncrementalSnapshotChangeEventSource enforces Kafka dependency during initialization <a href="https://issues.redhat.com/browse/DBZ-6511">DBZ-6511</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6512">DBZ-6512</a></p> </li> <li> <p>Debezium incremental snapshot chunk size documentation unclear or incorrect <a href="https://issues.redhat.com/browse/DBZ-6515">DBZ-6515</a></p> </li> <li> <p>Only use error processing mode on certain errors <a href="https://issues.redhat.com/browse/DBZ-6523">DBZ-6523</a></p> </li> <li> <p>[PostgreSQL] LTree data is not being captured by streaming <a href="https://issues.redhat.com/browse/DBZ-6524">DBZ-6524</a></p> </li> <li> <p>Use better hashing function for PartitionRouting <a href="https://issues.redhat.com/browse/DBZ-6529">DBZ-6529</a></p> </li> <li> <p>Start publishing nightly images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6541">DBZ-6541</a></p> </li> <li> <p>Start releasing images for Debezium Operator <a href="https://issues.redhat.com/browse/DBZ-6542">DBZ-6542</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.CR1%20ORDER%20BY%20component%20ASC">24 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jehrenzweig-pi">Jesse Ehrenzweig</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/tommyk-gears">Tommy Karlsson</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 quickly approaching a Final release, Debezium 2.4 is just around the corner.</p> </div> <div class="paragraph"> <p>Debezium 2.4 planning is underway, and we would love to have your feedback. Please reach out to us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions. The roadmap for Debezium 2.4 and beyond will be updated in conjunction with Debezium 2.3 final in the coming week.</p> </div> <div class="paragraph"> <p>And until next time, stay cool as summer approaches!</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="caassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It is my pleasure to announce the next Debezium 2.3 release, 2.3.0.CR1! The main focus of this release is to stabilize the Debezium 2.3 release in preparation for a final release in the coming weeks, which typically means we&#8217;re focusing on bugfixes; however, this release includes two new features. Lets take a moment and dive into these new features and any bug fixes that are noteworthy!]]></summary></entry><entry><title type="html">Debezium 2.3.0.Beta1 Released</title><link href="https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Beta1 Released"/><published>2023-05-29T00:00:00+00:00</published><updated>2023-05-29T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/05/29/debezium-2-3-beta1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce the next release of the Debezium 2.3 series, <strong>2.3.0.Beta1</strong>!</p> </div> <div class="paragraph"> <p>While this release focuses primarily on bug fixes and stability improvements, there are some new improvements with the PostgreSQL connector and the new notification and channels subsystem. In addition, there are also some compatibility breaking changes.</p> </div> <div class="paragraph"> <p>This release contains changes for <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Beta1%20ORDER%20BY%20issuetype%20DESC">22 issues</a>, so lets take a moment and dive into the new features and any potential bug fixes or breaking changes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium recently introduced the JDBC storage module that allows you to store offsets and schema history data inside a relational database. The JDBC storage module used UTF-16 as its default encoding; however, most databases use UTF-8. This release of Debezium aligns the JDBC storage module&#8217;s encoding to use UTF-8 moving forward.</p> </div> </div> </div> <div class="sect1"> <h2 id="postgresql_replica_identity_changes">PostgreSQL Replica Identity Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new PostgreSQL connector feature called "Autoset Replica Identity".</p> </div> <div class="paragraph"> <p>Replica identity is PostgreSQL&#8217;s way to identify what columns are captured in the database transaction logs for inserts, updates, and deletes. This new feature allows configuring a table&#8217;s replica identity via connector configuration and delegating the responsibility of setting this configuration to the connector at start-up.</p> </div> <div class="paragraph"> <p>The new configuration option, <code>replica.identity.autoset.values</code>, specifies a comma-separated list of table and replica identity tuples. If the table already has a given replica identity, the identity will be overwritten to match what is specified in this configuration if the table is included. PostgreSQL supports several replica identity types, more information on these can be found in the <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-replica-identity">documentation</a>.</p> </div> <div class="paragraph"> <p>When specifying the <code>replica.identity.autoset.values</code>, the value is a comma-separated list of values where each element uses the format of <code>&lt;fully-qualified-table-name&gt;:&lt;replica-identity&gt;</code>. An example is shown below where two tables are configured to have full replica identity:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">replica.identity.autoset.values</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public.table1:FULL,public.table2:FULL</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Be mindful that if the user account used by the connector does not have the appropriate database permissions to set a table&#8217;s replica identity, the use of this feature will result in a failure. In the event of a failure due to permissions, you must make sure the proper replica identity is set manually using a database account with the right permissions.</p> </div> </div> </div> <div class="sect1"> <h2 id="correlate_incremental_snapshot_notification_ids">Correlate Incremental Snapshot notification ids</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new notification and channels subsystem. This subsystem allows you to send a signal via a variety of channels that include the filesystem, Kafka topic, and database table out of the box; however, the feature is extendable. In addition, this subsystem also includes the ability to send notifications about the status of the initial snapshots and incremental snapshots if they&#8217;re used. These notifications can help facilitate an easier line of communication between Debezium and other third-party systems that may need to know when an incremental or traditional snapshot has finished and whether it finished successfully or not.</p> </div> <div class="paragraph"> <p>In this release, the notification and channels subsystem has been improved to correlate the signal to the notification. So when you send a signal and it is consumed by Debezium, any notification that is raised will contain a reference to the signal, allowing any third-party or external process to know precisely which signal the notification references.</p> </div> <div class="paragraph"> <p>This should help close the gap in distribution communications across applications or processes relying on the new notification and channel subsystem.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Debezium Server stops sending events to Google Cloud Pub/Sub <a href="https://issues.redhat.com/browse/DBZ-5175">DBZ-5175</a></p> </li> <li> <p>Snapshot step 5 - Reading structure of captured tables time too long <a href="https://issues.redhat.com/browse/DBZ-6439">DBZ-6439</a></p> </li> <li> <p>Oracle parallel snapshots do not properly set PDB context when using multitenancy <a href="https://issues.redhat.com/browse/DBZ-6457">DBZ-6457</a></p> </li> <li> <p>[MariaDB] Add support for userstat plugin keywords <a href="https://issues.redhat.com/browse/DBZ-6459">DBZ-6459</a></p> </li> <li> <p>Debezium Server cannot recover from Google Pub/Sub errors <a href="https://issues.redhat.com/browse/DBZ-6461">DBZ-6461</a></p> </li> <li> <p>Db2 connector can fail with NPE on notification sending <a href="https://issues.redhat.com/browse/DBZ-6485">DBZ-6485</a></p> </li> <li> <p>BigDecimal fails when queue memory size limit is in place <a href="https://issues.redhat.com/browse/DBZ-6490">DBZ-6490</a></p> </li> <li> <p>ORACLE table can not be captrued, got runtime.NoViableAltException <a href="https://issues.redhat.com/browse/DBZ-6492">DBZ-6492</a></p> </li> <li> <p>Signal poll interval has incorrect default value <a href="https://issues.redhat.com/browse/DBZ-6496">DBZ-6496</a></p> </li> <li> <p>Oracle JDBC driver 23.x throws ORA-18716 - not in any time zone <a href="https://issues.redhat.com/browse/DBZ-6502">DBZ-6502</a></p> </li> <li> <p>Alpine postgres images should use llvm/clang 15 explicitly <a href="https://issues.redhat.com/browse/DBZ-6506">DBZ-6506</a></p> </li> <li> <p>ExtractNewRecordState SMT in combination with HeaderToValue SMT results in Unexpected field name exception <a href="https://issues.redhat.com/browse/DBZ-6486">DBZ-6486</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Beta1%20ORDER%20BY%20component%20ASC">22 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/angsdey2">Angshuman Dey</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 being released under a condensed schedule, you can expect the next CR1 release within the next 1-2 weeks. The plan is to release Debezium 2.3.0.Final in the middle of June and for the team to begin preparation on Debezium 2.4.</p> </div> <div class="paragraph"> <p>As we begin to prepare to move toward Debezium 2.4, we would love to hear your feedback or suggestions. The roadmap will be updated in the coming week, so please be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if you have any ideas or suggestions.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce the next release of the Debezium 2.3 series, 2.3.0.Beta1! While this release focuses primarily on bug fixes and stability improvements, there are some new improvements with the PostgreSQL connector and the new notification and channels subsystem. In addition, there are also some compatibility breaking changes. This release contains changes for 22 issues, so lets take a moment and dive into the new features and any potential bug fixes or breaking changes that are noteworthy!]]></summary></entry><entry><title type="html">Debezium 2.3.0.Alpha1 Released</title><link href="https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.3.0.Alpha1 Released"/><published>2023-05-15T00:00:00+00:00</published><updated>2023-05-15T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/05/15/debezium-2-3-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce the first release of the Debezium 2.3 series, <strong>2.3.0.Alpha1</strong>!</p> </div> <div class="paragraph"> <p>This release brings many new and exciting features as well as bug fixes, including Debezium status notifications, storage of Debezium state into a JDBC data store, configurable signaling channels, the ability to edit connector configurations via Debezium UI, the parallelization of Vitess shards processing, and much more.</p> </div> <div class="paragraph"> <p>This release contains changes for <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Alpha1%20ORDER%20BY%20issuetype%20DESC">59 issues</a>, so lets take a moment and dive into several of these new features and any potential bug fixes or breaking changes that are noteworthy!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium for PostgreSQL and MySQL can be configured to use a secured SSL connection. For PostgreSQL, this can be done by configuring <a href="https://debezium.io/documentation/reference/2.3/connectors/postgresql.html#postgresql-property-database-sslmode"><code>database.sslmode</code></a> while for MySQL this can be done with <a href="https://debezium.io/documentation/reference/2.3/connectors/mysql.html#mysql-property-database-ssl-mode"><code>database.ssl.mode</code></a>.</p> </div> <div class="paragraph"> <p>With Debezium 2.3, this configuration option no longer defaults to <code>disable</code> (PostgreSQL) or <code>disabled</code> (MySQL) but instead defaults to <code>prefer</code> (PostgreSQL) and <code>preferred</code> (MySQL). This means that when attempting to connect using an encrypted, secure connection is unavailable, the connector will fallback to using an unsecured connection by default unless configured otherwise.</p> </div> </div> </div> <div class="sect1"> <h2 id="status_notifications">Status Notifications</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a brand-new feature called notifications, allowing Debezium to emit events that can be consumed by any external system to know the status of various stages of Debezium&#8217;s lifecycle.</p> </div> <div class="paragraph"> <p>Notification events are represented as a series of key/value tuples, with a structure that contains several out-of-the-box fields. The following is an example of a simple notification event.</p> </div> <div class="listingblock"> <div class="title">Example Notification Event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">c485ccc3-16ff-47cc-b4e8-b56a57c3bad2</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Started</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">additional_data</span><span class="delimiter">&quot;</span></span>: {
    <span class="error">.</span><span class="error">.</span><span class="error">.</span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>Each notification event consists of an <code>id</code> field, a UUID to identify the notification, an <code>aggregate_type</code> field to which the notification is related based on the concept of domain-driven design, a <code>type</code> field that is mean to given more detail about the aggregate type itself, and an optional <code>additional_data</code> field which consists of a map of string-based key/value pairs with additional information about the event.</p> </div> <div class="paragraph"> <p>At this time, there are two notification event types supported by Debezium:</p> </div> <div class="ulist"> <ul> <li> <p>Status of the initial snapshot</p> </li> <li> <p>Monitoring of the incremental snapshot</p> </li> </ul> </div> <div class="sect2"> <h3 id="initial_snapshot_notifications">Initial Snapshot Notifications</h3> <div class="paragraph"> <p>An initial snapshot is the consistent capture of the existing data when a connector first starts. An initial snapshot event will have an aggregate type with the value of <code>"Initial Snapshot"</code> and the type of event will consist of one of three logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>SKIPPED</code></dt> <dd> <p>Represents the initial snapshot was skipped.</p> </dd> <dt class="hdlist1"><code>ABORTED</code></dt> <dd> <p>Represents the initial snapshot was aborted.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Represents the initial snapshot has concluded successfully.</p> </dd> </dl> </div> <div class="paragraph"> <p>The following is an example of a notification about the completion of the initial snapshot:</p> </div> <div class="listingblock"> <div class="title">Example snapshot completed event</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">5563ae14-49f8-4579-9641-c1bbc2d76f99</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">aggregate_type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">Initial Snapshot</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">COMPLETED</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="incremental_snapshot_notifications">Incremental Snapshot Notifications</h3> <div class="paragraph"> <p>An incremental snapshot is a capture of the existing data from a configured set of tables while the connector is actively streaming changes. An incremental snapshot event will have an aggregate type with the value of <code>"Incremental Snapshot"</code> and the type will consist of one of several logical values:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>STARTED</code></dt> <dd> <p>Indicates an incremental snapshot has started.</p> </dd> <dt class="hdlist1"><code>PAUSED</code></dt> <dd> <p>Indicates an incremental snapshot has been temporarily paused.</p> </dd> <dt class="hdlist1"><code>RESUMED</code></dt> <dd> <p>Indicates an incremental snapshot that had been paused has now resumed.</p> </dd> <dt class="hdlist1"><code>STOPPED</code></dt> <dd> <p>Indicates an incremental snapshot has stopped.</p> </dd> <dt class="hdlist1"><code>IN_PROGRESS</code></dt> <dd> <p>Indicates an incremental snapshot is in-progress.</p> </dd> <dt class="hdlist1"><code>TABLE_SCAN_COMPLETED</code></dt> <dd> <p>Indicates an incremental snapshot has concluded for a given table.</p> </dd> <dt class="hdlist1"><code>COMPLETED</code></dt> <dd> <p>Indicates that an incremental snapshot has concluded for all tables.</p> </dd> </dl> </div> </div> <div class="sect2"> <h3 id="configuring_notifications">Configuring Notifications</h3> <div class="paragraph"> <p>Debezium notifications are configured via the connector&#8217;s configuration. The following examples show how to configure the out-of-the-box Kafka Topic or Log based channels.</p> </div> <div class="listingblock"> <div class="title">Using a Kafka Topic</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">sink</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.sink.topic.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_notifications</span><span class="delimiter">&quot;</span></span>,
  <span class="error">.</span><span class="error">.</span><span class="error">.</span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Using the connector logs</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">notification.enable.channels</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">log</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="jdbc_storage_module">JDBC Storage Module</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.3 introduces a new storage module implementation supporting the persistence of schema history and offset data in a datastore via JDBC. For environments where you may not have easy access to persistent filesystems, this offers yet another alternative for storage via a remote, persistent storage platform.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new module, the following dependency must be added to your project or application:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>debezium-storage-jdbc<span class="tag">&lt;/artifactId&gt;</span>
    <span class="tag">&lt;version&gt;</span>2.3.0.Alpha1<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="paragraph"> <p>The following examples show how to configure Offset or Schema History storage via the JDBC storage module:</p> </div> <div class="listingblock"> <div class="title">Configuration example for Offset JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">offset.storage.jdbc.offset_table_name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_offset_storage</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> <div class="listingblock"> <div class="title">Configuration example for Schema History JDBC storage</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;jdbc-connection-url&gt;</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbuser</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">secret</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.history.internal.jdbc.schema.history.table.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_database_history</span><span class="delimiter">&quot;</span></span>
}</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Toasted varying character array and date array are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6122">DBZ-6122</a></p> </li> <li> <p>Introduce LogMiner query filtering modes <a href="https://issues.redhat.com/browse/DBZ-6254">DBZ-6254</a></p> </li> <li> <p>Lock contention on LOG_MINING_FLUSH table when multiple connectors deployed <a href="https://issues.redhat.com/browse/DBZ-6256">DBZ-6256</a></p> </li> <li> <p>Ensure that the connector can start from a stale timestamp more than one hour into the past <a href="https://issues.redhat.com/browse/DBZ-6307">DBZ-6307</a></p> </li> <li> <p>The rs_id field is null in Oracle change event source information block <a href="https://issues.redhat.com/browse/DBZ-6329">DBZ-6329</a></p> </li> <li> <p>Add JWT authentication to HTTP Client <a href="https://issues.redhat.com/browse/DBZ-6348">DBZ-6348</a></p> </li> <li> <p>Using pg_replication_slot_advance which is not supported by PostgreSQL10. <a href="https://issues.redhat.com/browse/DBZ-6353">DBZ-6353</a></p> </li> <li> <p>log.mining.transaction.retention.hours should reference last offset and not sysdate <a href="https://issues.redhat.com/browse/DBZ-6355">DBZ-6355</a></p> </li> <li> <p>Support multiple tasks when streaming shard list <a href="https://issues.redhat.com/browse/DBZ-6365">DBZ-6365</a></p> </li> <li> <p>Kinesis Sink - AWS Credentials Provider <a href="https://issues.redhat.com/browse/DBZ-6372">DBZ-6372</a></p> </li> <li> <p>Toasted hstore are not correcly processed <a href="https://issues.redhat.com/browse/DBZ-6379">DBZ-6379</a></p> </li> <li> <p>Oracle DDL shrink space for table partition can not be parsed <a href="https://issues.redhat.com/browse/DBZ-6386">DBZ-6386</a></p> </li> <li> <p>__source_ts_ms r (read) operation date is set to future for SQL Server <a href="https://issues.redhat.com/browse/DBZ-6388">DBZ-6388</a></p> </li> <li> <p>PostgreSQL connector task fails to resume streaming because replication slot is active <a href="https://issues.redhat.com/browse/DBZ-6396">DBZ-6396</a></p> </li> <li> <p>MongoDB connector crashes on invalid resume token <a href="https://issues.redhat.com/browse/DBZ-6402">DBZ-6402</a></p> </li> <li> <p>NPE on read-only MySQL connector start up <a href="https://issues.redhat.com/browse/DBZ-6440">DBZ-6440</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.3.0.Alpha1%20ORDER%20BY%20component%20ASC">59 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/bpaquet">Bertrand Paquet</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/brenoavm">Breno Moreira</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/fredlegaucher">Frederic Laurent</a>, <a href="https://github.com/ddsr-ops">Gong Chang Hua</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/Hidetomi">Hidetomi Umaki</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/subkanthi">Kanthi Subramanian</a>, <a href="https://github.com/kgalieva">Katerina Galieva</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/miguelbirdie">Miguel Angel Sotomayor</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/zalmane">Oren Elias</a>, <a href="https://github.com/rnowling-memphis">RJ Nowling</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/eizners">Sergey Eizner</a>, <a href="https://github.com/sclarkson-zoomcare">Stephen Clarkson</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, and <a href="https://github.com/caicancai"></a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>With Debezium 2.3 underway, I do expect a rather quick cycle of alpha, beta, and final releases over the next six weeks. We still have a lot to do in this time period that we hope to get into this release, so stay tuned. As we get closer to the end of June, we&#8217;ll begin our planning for Debezium 2.4!</p> </div> <div class="paragraph"> <p>Also, Red Hat Summit 2023 is next week in Boston. There will be a break-out session where Hugo and Chris will be discussing the new Debezium JDBC sink connector. If you&#8217;re able to attend, we&#8217;d love to have an opportunity to chat with you before or after the session.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><category term="spanner"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce the first release of the Debezium 2.3 series, 2.3.0.Alpha1! This release brings many new and exciting features as well as bug fixes, including Debezium status notifications, storage of Debezium state into a JDBC data store, configurable signaling channels, the ability to edit connector configurations via Debezium UI, the parallelization of Vitess shards processing, and much more. This release contains changes for 59 issues, so lets take a moment and dive into several of these new features and any potential bug fixes or breaking changes that are noteworthy!]]></summary></entry><entry><title type="html">Image classification with Debezium and TensorFlow</title><link href="https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification/" rel="alternate" type="text/html" title="Image classification with Debezium and TensorFlow"/><published>2023-05-02T11:20:00+00:00</published><updated>2023-05-02T11:20:00+00:00</updated><id>https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification</id><content type="html" xml:base="https://debezium.io/blog/2023/05/02/tensorflow-mnist-classification/"><![CDATA[<div class="paragraph"> <p>With the recent success of ChatGPT, we can observe another wave of interest in the AI field and machine learning in general. The previous wave of interest in this field was, at least to a certain extent, caused by the fact that excellent ML frameworks like <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://pytorch.org/">PyTorch</a> or general data processing frameworks like <a href="https://spark.apache.org/">Spark</a> became available and made the writing of ML models much more straightforward. Since that time, these frameworks have matured, and writing models are even more accessible, as you will see later in this blog. However, data set preparation and gathering data from various sources can sometimes take time and effort. Creating a complete pipeline that would pull existing or newly created data, adjust it, and ingest it into selected ML libraries can be challenging. Let&#8217;s investigate if Debezium can help with this task and explore how we can leverage Debezium&#8217;s capabilities to make it easier.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="change_data_capture_and_debezium_in_ml_pipelines">Change data capture and Debezium in ML pipelines</h2> <div class="sectionbody"> <div class="paragraph"> <p>Change data capture (CDC) can be a compelling concept in machine learning, especially in online machine learning. However, using pre-trained models, CDC can also be an essential part of the pipeline. We can use CDC to deliver new data immediately into a pre-trained model, which can evaluate it, and other parts of the pipeline can take any action based on the model output in real-time.</p> </div> <div class="paragraph"> <p>Besides these use cases, Debezium is a perfect fit for any pipeline, including loading data from databases. Debezium can capture existing data as well as stream any newly created data. Another vital feature of Debezium is support for single message transforms. We can adjust the data at the very beginning of the whole pipeline. When applying transformations or filters, we can restrict data transmission over the wire to only that is of interest, saving bandwidth and speed within the pipeline. Additionally, Debezium can deliver records to several message brokers, and more brokers are being added (several new ones are available in the recent 2.2.0 release). These continued improvements increase the opportunity to integrate Debezium with other toolchains or data pipelines. The possibilities are endless, and Debezium&#8217;s common connector framework could allow for CDC beyond just databases.</p> </div> <div class="paragraph"> <p>So, this is the theory. Now let&#8217;s explore how it works in reality. This blog post will look at how to stream data into TensorFlow. Based on the interest from the community, this may result in a series of blog posts where we explore possible integrations with other ML libraries and frameworks.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_and_tensorflow_integration">Debezium and TensorFlow integration</h2> <div class="sectionbody"> <div class="paragraph"> <p>TensorFlow is one of the most popular machine learning frameworks. It provides a comprehensive platform for building, training, and deploying machine learning models across various applications.</p> </div> <div class="paragraph"> <p>To keep things simple, we will implement a model for recognizing handwritten digits, which is more or less the Hello World equivalent in the neural networks field. The ultimate goal of this demo is to use Debezium to load MNIST data samples from Postgres that are continuously stored, pass it to our model implemented in Tensorflow for training, and use this trained model for real-time classification of images</p> </div> <div class="paragraph"> <p>The diagram below depicts the complete pipeline:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/pipeline.png" class="responsive-image" alt="Debezium TensorFlow integration pipeline"> </div> <div class="paragraph"> <p>All the code mentioned later in this blog is available as a Debezium example in <a href="https://github.com/debezium/debezium-examples/tree/main/tensorflow-mnist">Debezium example repository</a>.</p> </div> <div class="sect2"> <h3 id="the_data_sample">The data sample</h3> <div class="paragraph"> <p>We will use <a href="http://yann.lecun.com/exdb/mnist/">MNIST data sample</a>. The training sample contains 60,000 images with handwritten digits from 0 to 9 and the same amount of labels with corresponding digits. The test sample contains 1,000 images. The samples are available as gzip binaries. As we assume a use case where the data of interest are in the database, we need to load the data into the database first.</p> </div> <div class="paragraph"> <p>We need to generate two SQL files, one for the train data set, <code>mnist_train.sql</code>, and one for a test data sample, <code>mnist_test.sql</code>. Each file would contain SQL commands for creating a table with two columns: <code>pixels</code> column of type <code>BYTEA</code>, which would contain raw image bytes, and <code>labels</code> column of type <code>SMALLINT</code>, which would contain digit corresponding to the image in given table row. The rest of the file would contain commands for populating the table. Image bytes can be decoded as a HEX string.</p> </div> <div class="paragraph"> <p>As we will show how to leverage Debezium for data streaming later in this post, we will initially load the training data set into the database. The SQL file with training data will be used directly by the Postgres container - when it starts, it will load this data into the training table. We will use the test data SQL file later. However, the preparation of the data is the same for training as well as test samples, and we can prepare both of them in one go.</p> </div> <div class="paragraph"> <p>To prepare these SQL files, you can use <code>mnist2sql.py</code> script from <a href="https://github.com/debezium/debezium-examples/tree/main/tensorflow-mnist">Debezium tensorflow-mnist example</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ ./mnist2sql.py --download</code></pre> </div> </div> <div class="paragraph"> <p>The script assumes MNIST data sets are available in the <code>postgres</code> directory. When using the <code>--download</code> parameter, the script first downloads MNIST data samples into the <code>postgres</code> directory. The <code>postgres</code> directory will contain the resulting SQL files.</p> </div> </div> <div class="sect2"> <h3 id="loading_streamed_data_into_tensorflow">Loading streamed data into Tensorflow</h3> <div class="paragraph"> <p>The most common Debezium usage is the streaming of records to Kafka. TensorFlow provides <a href="https://www.tensorflow.org/io">TensorFlow I/O</a> module for loading data from various sources. Besides other sources, it also allows loading the data from Kafka. There are several ways to do it. <a href="https://www.tensorflow.org/io/api_docs/python/tfio/IODataset#from_kafka">IODataset.from_kafka()</a> method loads only existing data from specified Kafka topics. Two experimental classes support streaming data, <a href="https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaBatchIODataset">KafkaBatchIODataset</a> and <a href="https://www.tensorflow.org/io/api_docs/python/tfio/experimental/streaming/KafkaGroupIODataset">KafkaGroupIODataset</a>. Both are very similar and allow them to work with streaming data, i.e., they not only read the existing data from a Kafka topic but also wait for new data and eventually pass new records into the TensorFlow. Streaming concludes when there are no new events within a specified time frame.</p> </div> <div class="paragraph"> <p>In all cases, a <a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset">Dataset</a> represents all loaded records in Tensorflow. This Tensorflow data structure provides convenience for building <a href="https://www.tensorflow.org/guide/data">data pipelines</a>, which may include further data transformations or preprocessing.</p> </div> <div class="paragraph"> <p>This sounds great. However, the most significant caveat is the representation of records within the Dataset. These Kafka loaders completely ignore the schema of the records provided by Kafka, meaning that keys and values are raw bytes of data. Additionally, the ingestion pipeline complicates the process by converting these into strings (i.e., <code>toString()</code> on the object called). So if you pass, e.g., raw image bytes via Kafka, using Kafka <code>BYTES_SCHEMA</code>, it would result in something like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>&lt;tf.Tensor: shape=(64,), dtype=string, numpy=
 array([b'[B@418b353d', b'[B@6aa28a4c', b'[B@b626485', b'[B@6d7491cd',
        b'[B@13fa86c5', b'[B@7c3bc352', b'[B@64e5d61c', b'[B@2dd6d9b4',
        b'[B@6addae65', b'[B@48ded13f', b'[B@2c1bb0e', b'[B@19c1d99b',
        b'[B@1ee8f240', b'[B@20019f8b', b'[B@2f17494e', b'[B@380d4036',
        b'[B@61aecf85', b'[B@4d7fe9fc', b'[B@58b79424', b'[B@ae963f4',
        b'[B@1dac57cb', b'[B@2fae7d8b', b'[B@4b5ccaee', b'[B@aebf6b2',
        b'[B@7506ea2b', b'[B@29989325', b'[B@43e2742', b'[B@51350f11',
        b'[B@13a0f0ae', b'[B@7e4c4844', b'[B@b3d64f8', b'[B@7209bf09',
        b'[B@66380466', b'[B@7aaa7e8d', b'[B@1ad0cf84', b'[B@259eca20',
        b'[B@3a3f1c1', b'[B@36e4ff1f', b'[B@6578fc29', b'[B@79c924be',
        b'[B@765b7f70', b'[B@67567aa3', b'[B@456d4bd4', b'[B@75317b13',
        b'[B@58bc3a3a', b'[B@c6bc0ec', b'[B@2377095e', b'[B@5de017c0',
        b'[B@64b48bac', b'[B@360a5b76', b'[B@2d2c9910', b'[B@70afd562',
        b'[B@3006c930', b'[B@54b3e5ad', b'[B@1d1e0232', b'[B@1394d036',
        b'[B@155dd43d', b'[B@5e88d5b6', b'[B@33ea53c7', b'[B@64a30ec',
        b'[B@7dcdf024', b'[B@6570bf4e', b'[B@4e5bc4c', b'[B@537f216c'],
       dtype=object)&gt;,</code></pre> </div> </div> <div class="paragraph"> <p>Instead of getting a batch of raw image bytes which you can further transform in TensorFlow, you get only string representation of Java byte arrays, which is not very useful.</p> </div> <div class="paragraph"> <p>The most straightforward solution would be to convert the raw image bytes into numbers before sending them to Kafka to mitigate the problem. As TensorFlow provides methods for parsing CSV input, we can convert each image into one CSV line of numbers. Since Tensorflow primarily works with numbers, we would be required to convert the images to numbers regardless. We can pass the number on the image as a message key. Now, a single message transform supported by Debezium comes in handy. The transformation can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java">    <span class="annotation">@Override</span>
    <span class="directive">public</span> R apply(R r) {
        <span class="directive">final</span> <span class="predefined-type">Struct</span> value = (<span class="predefined-type">Struct</span>) r.value();
        <span class="predefined-type">String</span> key = value.getInt16(labelFieldName).toString();

        <span class="predefined-type">StringBuilder</span> builder = <span class="keyword">new</span> <span class="predefined-type">StringBuilder</span>();
        <span class="keyword">for</span> (<span class="type">byte</span> pixel : value.getBytes(pixlesFieldName)) {
            builder.append(pixel &amp; <span class="hex">0xFF</span>).append(<span class="string"><span class="delimiter">&quot;</span><span class="content">,</span><span class="delimiter">&quot;</span></span>);
        }
        <span class="keyword">if</span> (builder.length() &gt; <span class="integer">0</span>) {
            builder.deleteCharAt(builder.length() - <span class="integer">1</span>);
        }
        <span class="predefined-type">String</span> newValue = builder.toString();

        <span class="keyword">return</span> r.newRecord(r.topic(), r.kafkaPartition(), <span class="predefined-type">Schema</span>.STRING_SCHEMA, key, <span class="predefined-type">Schema</span>.STRING_SCHEMA, newValue, r.timestamp());
    }</code></pre> </div> </div> <div class="paragraph"> <p>On the TensorFlow side, we must convert bytes obtained from Kafka messages into numbers. The following illustrates a map function to handle this easily:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">decode_kafka_record</span>(record):
    img_int = tf.io.decode_csv(record.message, [[<span class="float">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / <span class="float">255.</span>
    label_int = tf.strings.to_number(record.key, out_type=tf.dtypes.int32)
    <span class="keyword">return</span> (img_norm, label_int)</code></pre> </div> </div> <div class="paragraph"> <p>Here we parse CSV lines, potentially provided as the raw bytes, and immediately scale the numbers within the &lt;0, 1&gt; interval, which is convenient for training our model later. Loading the data and creating data batches is very straightforward:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">train_ds = tfio.IODataset.from_kafka(KAFKA_TRAIN_TOPIC, partition=<span class="integer">0</span>, offset=<span class="integer">0</span>, servers=KAFKA_SERVERS)
train_ds = train_ds.map(decode_kafka_record)
train_ds = train_ds.batch(BATCH_SIZE)</code></pre> </div> </div> <div class="paragraph"> <p>Here we use <code>IODataset.from_kafka()</code> for loading existing data from the Kafka topic, use our map function to convert bytes into numbers, and scale the numbers. As a last step, we create batches from the data set for more efficient processing. Parameters of <code>tfio.IODataset.from_kafka()</code> are self-explanatory and probably don&#8217;t need further comments.</p> </div> <div class="paragraph"> <p>As a result, we have a data set formed by two-dimensional tensors. The first dimension is a vector of floats representing the image, while the second dimension is a single number (scalar) describing the number on the picture. Once we have prepared our training data set, we can define our neural network model.</p> </div> </div> <div class="sect2"> <h3 id="defining_the_model">Defining the model</h3> <div class="paragraph"> <p>To keep things simple, as the main goal of this post is not to show the best handwritten digit classifier, but to show how to create the data pipeline, let&#8217;s use a very simple model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(<span class="integer">128</span>, activation=<span class="string"><span class="delimiter">'</span><span class="content">relu</span><span class="delimiter">'</span></span>),
    tf.keras.layers.Dense(<span class="integer">10</span>)
])</code></pre> </div> </div> <div class="paragraph"> <p>This model contains only two layers. Although this model is really simple, it still does a pretty good job in recognition of handwritten digits. Probably more interesting than the model itself is how easy it is to write a mode in TensorFlow (or actually <a href="https://keras.io/">Keras</a>, but it&#8217;s now part of TensorFlow).</p> </div> <div class="paragraph"> <p>Similarly easy is to define model optimizer and the loss function:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.compile(
    optimizer=tf.keras.optimizers.Adam(<span class="float">0.001</span>),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="predefined-constant">True</span>),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],
)</code></pre> </div> </div> <div class="paragraph"> <p>It&#8217;s outside of this post&#8217;s scope to explain these functions, and you can check almost any machine learning online course or textbook on this topic for a detailed explanation.</p> </div> <div class="paragraph"> <p>Once we have our model ready, we can train it on the trained dataset prepared in the previous section:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.fit(train_ds,epochs=MAX_EPOCHS)</code></pre> </div> </div> <div class="paragraph"> <p>This step may take quite some time to finish. However, once finished, our model is ready to recognize handwritten digits!</p> </div> </div> <div class="sect2"> <h3 id="streaming_the_data_into_the_model">Streaming the data into the model</h3> <div class="paragraph"> <p>Let&#8217;s see how good our model is in digit recognition. But as our primary goal here is to explore the means how to ingest data into TensorFlow, we will start model evaluation on an empty (or, more accurately, even non-existing) Kafka topic and see if we will be able to evaluate the data on the fly as they will pop-up first in the database and then in the corresponding Kafka topic. For this purpose, we can use one of the streaming classes mentioned above:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">test_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)</code></pre> </div> </div> <div class="paragraph"> <p>Again, arguments are mostly self-explanatory. Two things may need further explanation: <code>stream_timeout</code> and <code>configuration</code> parameters. <code>stream_timeout</code> determines the interval of inactivity (in milliseconds) after which the streaming would terminate. <code>configuration</code> is <a href="https://github.com/confluentinc/librdkafka/blob/master/CONFIGURATION.md">librdkafka configuration</a>. It&#8217;s a configuration of the Kafka client; you should configure at least the session timeout (<code>session.timeout.ms</code>), and it&#8217;s poll interval (<code>max.poll.interval.ms</code>). The values of these parameters should be higher than the value of <code>stream_timeout</code>.</p> </div> <div class="paragraph"> <p>The dataset this loader provides is slightly different - instead of providing a single record containing the message and its key, we get the key and message already split. Therefore, we have to define a slightly modified map function with two arguments:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">decode_kafka_stream_record</span>(message, key):
    img_int = tf.io.decode_csv(message, [[<span class="float">0.0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="predefined">range</span>(NUM_COLUMNS)])
    img_norm = tf.cast(img_int, tf.float32) / <span class="float">255.</span>
    label_int = tf.strings.to_number(key, out_type=tf.dtypes.int32)
    <span class="keyword">return</span> (img_norm, label_int)</code></pre> </div> </div> <div class="paragraph"> <p>With this function, we can adjust the dataset and create batches as before:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">test_ds = test_ds.map(decode_kafka_stream_record)
test_ds = test_ds.batch(BATCH_SIZE)</code></pre> </div> </div> <div class="paragraph"> <p>and evaluate the model:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">model.evaluate(test_ds)</code></pre> </div> </div> <div class="paragraph"> <p>You can execute a cell with model evaluation in the Jupyter notebook. The execution will wait because there is no such topic in Kafka and no table with test data in the database. The streaming timeout is 9 seconds, so data must be provided within this time frame after launching the model evaluation. At the start of this demo, we created a SQL file in the <code>postgres</code> directory called <code>mnist_test.sql</code>, which can generate the test data we need:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ export PGPASSWORD=postgres
$ psql -h localhost -U postgres -f postgres/mnist_test.sql</code></pre> </div> </div> <div class="paragraph"> <p>After a short while, you should see in the Jupyter notebook output that some data arrived into the model and, a few moments later final evaluation of the model.</p> </div> <div class="paragraph"> <p>To make the results closer to humans, let&#8217;s define an image manually and serve it to the model. We can also easily show the image in the Jypiter notebook. The function for plotting the images and providing model predictions as a plot title can look like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python"><span class="keyword">def</span> <span class="function">plot_and_predict</span>(pixels):
    test = tf.constant([pixels])
    tf.shape(test)
    test_norm = tf.cast(test, tf.float32) / <span class="float">255.</span>

    prediction = model.predict(test_norm)
    number = tf.nn.softmax(prediction).numpy().argmax()

    pixels_array = np.asarray(pixels)
    raw_img = np.split(pixels_array, <span class="integer">28</span>)
    plt.imshow(raw_img)
    plt.title(number)
    plt.axis(<span class="string"><span class="delimiter">&quot;</span><span class="content">off</span><span class="delimiter">&quot;</span></span>)</code></pre> </div> </div> <div class="paragraph"> <p>Probably the only cryptic line in this function is the one containing the <code>softmax()</code> function. This function converts the resulting vector into a vector of probabilities. Elements of this vector express the probability that the number on a given position is the one on the image. Therefore, the position with the highest probability is the model&#8217;s prediction, where <code>argmax()</code> is derived.</p> </div> <div class="paragraph"> <p>We can try it, e.g., for this image, which contains the handwritten number 3:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>pixels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,108,43,6,6,6,6,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,84,248,254,254,254,254,254,241,45,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,90,254,254,254,223,173,173,173,253,156,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,79,157,228,245,251,188,63,17,0,0,54,252,132,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,254,254,254,244,131,0,0,0,0,13,220,254,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,254,225,160,47,0,0,0,0,59,211,254,206,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,21,14,0,0,0,2,17,146,245,250,194,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,140,140,171,254,254,254,203,55,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,254,254,254,179,211,254,254,202,171,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,167,233,193,69,16,3,9,16,107,231,248,195,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,73,229,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,99,252,254,146,0,0,0,0,0,0,0,0,79,142,0,0,0,0,0,0,0,0,0,26,28,116,147,247,254,239,150,22,0,0,0,0,0,0,0,0,175,230,174,155,66,66,132,174,174,174,174,250,255,254,192,189,99,36,0,0,0,0,0,0,0,0,0,0,106,226,254,254,254,254,254,254,254,254,217,151,80,43,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,7,114,114,114,46,5,5,5,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]
plot_and_predict(pixels)</code></pre> </div> </div> <div class="paragraph"> <p>The result would be as follows:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_digit.png" class="responsive-image" alt="TensorFlow digit recognition"> </div> <div class="paragraph"> <p>You can do the same by reading from a Kafka stream, and we can reuse existing topics for this purpose. As we already read all records from the test stream, we need to change the Kafka consumer group if we want to reread it using streaming <code>KafkaGroupIODataset</code>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[KAFKA_TEST_TOPIC],
    group_id=<span class="string"><span class="delimiter">&quot;</span><span class="content">mnistcg2</span><span class="delimiter">&quot;</span></span>,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)</code></pre> </div> </div> <div class="paragraph"> <p>If you want to create a new stream and verify that our model can provide prediction as the new data arrives, you can easily do so:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="bash">$ head -5 mnist_test.sql | sed s/test/manual/ &gt; mnist_manual.sql
$ psql -h localhost -U postgres -f postgres/mnist_manual.sql</code></pre> </div> </div> <div class="paragraph"> <p>In such case you don&#8217;t need to change Kafka consumer group, but you have to change the Kafka topic:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="python">manual_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=[<span class="string"><span class="delimiter">&quot;</span><span class="content">tf.public.mnist_manual</span><span class="delimiter">&quot;</span></span>],
    group_id=KAFKA_CONSUMER_GROUP,
    servers=KAFKA_SERVERS,
    stream_timeout=<span class="integer">9000</span>,
    configuration=[
        <span class="string"><span class="delimiter">&quot;</span><span class="content">session.timeout.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">max.poll.interval.ms=10000</span><span class="delimiter">&quot;</span></span>,
        <span class="string"><span class="delimiter">&quot;</span><span class="content">auto.offset.reset=earliest</span><span class="delimiter">&quot;</span></span>
    ],
)

manual_ds = manual_ds.map(decode_kafka_stream_record)</code></pre> </div> </div> <div class="paragraph"> <p>In either case, the result should look like this:</p> </div> <div class="imageblock centered-image"> <img src="/assets/images/2023-04-21-tensorflow-mnist-classification/tensorflow_mnist_streaming.png" class="responsive-image" alt="TensorFlow digit recognition from streaming"> </div> </div> </div> </div> <div class="sect1"> <h2 id="conclusions">Conclusions</h2> <div class="sectionbody"> <div class="paragraph"> <p>In this demo, we have shown how to load existing data from the database, transform it on the fly, ingest it into the TensorFlow model via Kafka, and use it for model training. Later on, we ingested newly created data into this pre-trained model using CDC and data streaming and obtained meaningful results. Debezium can provide valuable service not only for use cases like the one described in this post but can also play a key role in ingesting data to online machine learning pipelines.</p> </div> <div class="paragraph"> <p>While the whole pipeline is relatively easy to implement, some areas can be improved to improve the user experience and/or make the entire pipeline more smooth. As our (Debezium developers) background is not primarily in machine learning and data science, we would appreciate any input from the community on how Debezium can aid machine learning pipelines (or is already used, if there are any such cases) and where are the rooms for improvements. We would also appreciate any new ideas on how Debezium, or in general, change data capture, can be helpful in this area. These ideas further reveal Debezium&#8217;s potential to ingest data into machine learning pipelines and contribute to better user experience in the whole process. In case you have any input any this regard, don&#8217;t hesitate to reach out to us on the <a href="http://debezium.zulipchat.com/">Zulip chat</a>, <a href="https://groups.google.com/forum/#!forum/debezium">mailing list</a> or you can transform your ideas directly into <a href="http://issues.redhat.com/projects/DBZ/issues">Jira feature requests</a>.</p> </div> </div> </div>]]></content><author><name>Vojtch Jurnek</name></author><category term="machine-learning"/><category term="tensorflow"/><category term="examples"/><category term="apache-kafka"/><summary type="html"><![CDATA[With the recent success of ChatGPT, we can observe another wave of interest in the AI field and machine learning in general. The previous wave of interest in this field was, at least to a certain extent, caused by the fact that excellent ML frameworks like TensorFlow, PyTorch or general data processing frameworks like Spark became available and made the writing of ML models much more straightforward. Since that time, these frameworks have matured, and writing models are even more accessible, as you will see later in this blog. However, data set preparation and gathering data from various sources can sometimes take time and effort. Creating a complete pipeline that would pull existing or newly created data, adjust it, and ingest it into selected ML libraries can be challenging. Let&#8217;s investigate if Debezium can help with this task and explore how we can leverage Debezium&#8217;s capabilities to make it easier.]]></summary></entry><entry><title type="html">New Debezium images will be available only Quay.io in the future</title><link href="https://debezium.io/blog/2023/04/25/container-images-quayio/" rel="alternate" type="text/html" title="New Debezium images will be available only Quay.io in the future"/><published>2023-04-25T10:11:11+00:00</published><updated>2023-04-25T10:11:11+00:00</updated><id>https://debezium.io/blog/2023/04/25/container-images-quayio</id><content type="html" xml:base="https://debezium.io/blog/2023/04/25/container-images-quayio/"><![CDATA[<div class="paragraph"> <p>As you may have noticed, the Docker company recently announced a reduction of the free organization accounts offering. The Docker company wanted to provide for free organization accounts only for <a href="https://www.docker.com/community/open-source/application/">Docker-Sponsored Open Source (DSOS) projects</a>. Debezium project doesn&#8217;t meet their definition of open source project as we have a pathway to commercialization. As the accounts ought to be terminated in 30 days, we immediately started to work on moving out the Debezium project from Docker Hub.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Based on the feedback from the community, the Docker company later on <a href="https://www.docker.com/blog/no-longer-sunsetting-the-free-team-plan/">re-evaluated their decision</a> and Free Team plan is still available as before. However, the whole story, and especially the initial intention to give projects which don&#8217;t meet DSOS conditions only 30 days for migration, undermined our trust in the Docker company and raised a question of what will come in the future. As a result we decided <strong>not to publish Debezium images on Docker Hub</strong> in the future.</p> </div> <div class="paragraph"> <p>For quite some time we already publish all Debezium images into two container image registries:</p> </div> <div class="ulist"> <ul> <li> <p>on the <a href="https://hub.docker.com/u/debezium">Docker Hub</a></p> </li> <li> <p>on the <a href="https://quay.io/organization/debezium/">Quay.io</a></p> </li> </ul> </div> <div class="paragraph"> <p>Upcomming 2.2 release and previews of 2.3 (including CR releases), will be still available on the Docker Hub, but starting 2.3.0.Final release, we will stop publishing images there. Images of Debezium 2.3.0.Final and subsequent releases will be available only on the <a href="https://quay.io/organization/debezium/">Quay.io</a>. Older, already published, images will be of course still available through Docker Hub (unless Docker company changes the conditions which would prevent it in the future). Older Debezium images can be found also on <a href="https://quay.io/organization/debezium/">Quay.io</a>.</p> </div> <div class="paragraph"> <p><a href="https://quay.io">Quay.io</a> is a mature container registry service, which provides additional features like e.g. vulnerability scans. As the <a href="https://quay.io">Quay.io</a> is run and sponsored by Red Hat, and we already publish the image there, it was a natural choice for us to move to this container registry.</p> </div> <div class="paragraph"> <p>How to migrate to <a href="https://quay.io/organization/debezium/">Quay.io</a>? It&#8217;s very simple - just add <code>quay.io/</code> prefix to the container image name, e.g. instead of running</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker pull debezium/connect:latest</code></pre> </div> </div> <div class="paragraph"> <p>you run</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code>docker pull quay.io/debezium/connect:latest</code></pre> </div> </div> <div class="paragraph"> <p>and similar for any other images or commands.</p> </div> <div class="paragraph"> <p>If you have any questions or issues with using <a href="https://quay.io">Quay.io</a> images, don&#8217;t hesitate to reach to us and raise your questions or concerns in our <a href="https://debezium.zulipchat.com/#narrow/stream/302529-users">user chat room</a>.</p> </div>]]></content><author><name>Vojtch Jurnek</name></author><category term="news"/><category term="docker"/><category term="containers"/><summary type="html"><![CDATA[As you may have noticed, the Docker company recently announced a reduction of the free organization accounts offering. The Docker company wanted to provide for free organization accounts only for Docker-Sponsored Open Source (DSOS) projects. Debezium project doesn&#8217;t meet their definition of open source project as we have a pathway to commercialization. As the accounts ought to be terminated in 30 days, we immediately started to work on moving out the Debezium project from Docker Hub.]]></summary></entry><entry><title type="html">Debezium 2.2.0.Final Released</title><link href="https://debezium.io/blog/2023/04/20/debezium-2-2-final-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Final Released"/><published>2023-04-20T00:00:00+00:00</published><updated>2023-04-20T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/04/20/debezium-2-2-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/04/20/debezium-2-2-final-released/"><![CDATA[<div class="paragraph"> <p>Today, it&#8217;s with great joy that we can announce the availability of Debezium <strong>2.2.0.Final</strong>!</p> </div> <div class="paragraph"> <p>Many of you may have noticed, this release cadence took a bit longer than our traditional three-months. While we normally prefer to keep to our usual cadence, this shift gives us a unique opportunity to ship Debezium 2.2 with tons of new features and bug fixes, but also major upgrades to several core components.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Table of Contents:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#debezium-2-2-breaking-changes">Breaking changes</a></p> </li> <li> <p><a href="#debezium-2-2-whats-new">What&#8217;s new?</a></p> </li> <li> <p><a href="#other-fixes-improvements">Other fixes &amp; improvements</a></p> </li> <li> <p><a href="#whats-next">What&#8217;s Next?</a></p> </li> </ul> </div> <div class="sect1"> <h2 id="debezium-2-2-breaking-changes">Breaking changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>Before we dive into what&#8217;s new and changed, lets take a moment and discuss several breaking changes that took place in the Debezium 2.2 release:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#breaking-change-zonedtimestamp-truncation">ZonedTimestamp truncation</a></p> </li> <li> <p><a href="#breaking-change-topic-and-schema-naming-changes">Topic and Schema naming changes</a></p> </li> <li> <p><a href="#breaking-change-oracle-source-info-block-changes">Oracle source-information block changes</a></p> </li> <li> <p><a href="#breaking-change-debezium-server-repository">Debezium Server moved to new repository</a></p> </li> <li> <p><a href="#breaking-change-sunset-dockerio">Sunset container image publication to docker.io</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="breaking-change-zonedtimestamp-truncation">ZonedTimestamp truncation</h3> <div class="paragraph"> <p>An edge case was reported in <a href="https://issues.redhat.com/browse/issues/DBZ-5996">DBZ-5996</a> where if a temporal column used <code>ZonedTimestamp</code> and if the column&#8217;s value had <code>0</code> micro or nanoseconds, rather than emitting the value as <code>2023-01-19T12:30:00.123000Z</code>, the value would be emitted in a truncated way as <code>2023-01-19T12:30:00.123Z</code>. This could lead to other issues with converters used in the event pipeline when the output from that column could be formatted inconsistently.</p> </div> <div class="paragraph"> <p>In order to remedy the edge case, the <code>ZonedTimestamp</code> implementation will now pad the fraction-based seconds value of the column&#8217;s value to the length/scale of the source database column. Using the example above of a <code>TIMESTAMP(6)</code> MySQL column type, the emitted value will now properly reflect a value of <code>2023-01-19T12:30:00.123000Z</code>.</p> </div> <div class="paragraph"> <p>While this change in behavior is likely to have minimal impact to most users, we wanted to bring attention to it in the event that you&#8217;ve perhaps used other means to handle this edge case in your pipelines. If you have, you should be able to rely on Debezium to emit the value consistently, even when the fraction-based seconds is <code>0</code>.</p> </div> </div> <div class="sect2"> <h3 id="breaking-change-topic-and-schema-naming-changes">Topic and Schema naming changes</h3> <div class="paragraph"> <p>Debezium previously sanitized topic and schema names by using an underscore (<code>_</code>) to replace non-ASCII characters that would lead to unsupported topic or schema names when using schema registries. However, if this non-ASCII character was the only difference between two similar topics or schema names that otherwise only varied by case, this would lead to other problems.</p> </div> <div class="paragraph"> <p>In order to address this in the most compatible way, Debezium now uses a strategy-based approach to map characters uniquely. As a side effect of this change, the <code>sanitize.field.names</code> configuration property has been retired and replaced by this new strategy-based approach.</p> </div> <div class="paragraph"> <p>Each connector supports two configuration properties to control this behavior:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.name.adjustment.mode</code></dt> <dd> <p>Specifies how schema names should be adjusted for compatibility with the message converter.</p> </dd> <dt class="hdlist1"><code>field.name.adjustment.mode</code></dt> <dd> <p>Specifies how field names should be adjusted for compatibility with the message converter.</p> </dd> </dl> </div> <div class="paragraph"> <p>These two connector configuration properties support three modes:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>none</code></dt> <dd> <p>No adjustment is made to the schema or field names, passed as-is.</p> </dd> <dt class="hdlist1"><code>avro</code></dt> <dd> <p>Replaces characters that cannot be used in Avro with an underscore (<code>_</code>).</p> </dd> <dt class="hdlist1"><code>avro_unicode</code></dt> <dd> <p>Replaces underscores (<code>_</code>) and characters that cannot be used in Avro with unicode-based characters.</p> </dd> </dl> </div> <div class="paragraph"> <p>This now allows you to pick the most appropriate strategy based on your table or collection naming convention.</p> </div> </div> <div class="sect2"> <h3 id="breaking-change-oracle-source-info-block-changes">Oracle source-information block changes</h3> <div class="paragraph"> <p>All Debezium change events related to inserts, updates, and deletes contain a <code>source</code> info block in the event&#8217;s payload. For the Oracle connector, this block contains a special field called <code>ssn</code> that represents the SQL sequence number for this change.</p> </div> <div class="paragraph"> <p>It has been identified that there were corner cases where the value sourced from the database for this field could exceed the maximum value of <code>2,147,483,647</code>, or the maximum value of an <code>INT32</code> data type. To fix this corner case, we&#8217;ve changed the data type from <code>INT32</code> to <code>INT64</code>, which allows up to a maximum value of <code>9,223,372,036,854,775,807</code>.</p> </div> <div class="paragraph"> <p>This change should be entirely non-invasive, but we wanted to bring attention to this should you have pipelines that could be storing this value in a sink system or if you are using a schema registry.</p> </div> </div> <div class="sect2"> <h3 id="breaking-change-debezium-server-repository">Debezium Server moved to new repository</h3> <div class="paragraph"> <p>Debezium Server is a standalone Quarkus-based runtime for Debezium source connectors enabling the integration with various platforms like EventHubs, PubSub, Pulsar, Redis, and Kafka, to name a few. With this release, we have moved the code related to Debezium Server to its own <a href="https://www.github.com/debezium/debezium-server">GitHub repository</a>.</p> </div> <div class="paragraph"> <p>This change was required in order to support building Debezium Server to include connectors that are not part of the main Debezium repository, connectors such as Db2, Google Spanner, Cassandra 4, and Vitess. Therefore, this means that starting with this release, Debezium Server now ships with all connectors (excluding Cassandra 3) by default.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Cassandra 3 is excluded due to some technical limitations with class loading that creates conflicts with Cassandra 4. We are aware of this and plan to deliver a solution to include Cassandra 3 in the future.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="breaking-change-sunset-dockerio">Sunset container image publication to docker.io</h3> <div class="paragraph"> <p>Debezium intends to sunset publishing container images to <code>docker.io</code> in June 2023. Some may be aware of recent policy changes at Docker around the reduction of their free organization plans, a plan that is used by a number of open-source projects including Debezium.</p> </div> <div class="paragraph"> <p>While Docker walked back their decision, this does raise a question about whether this could happen in the future. Debezium has been dual publishing container artifacts to <code>docker.io</code> and <code>quay.io</code> for quite some time, and we plan to continue doing so throughout this upcoming quarter with the <strong>preview</strong> releases of Debezium 2.3.</p> </div> <div class="paragraph"> <p>However, effective with the release of <strong>Debezium 2.3.0.Final</strong> at the end of June 2023, Debezium will cease publishing container artifacts to <code>docker.io</code> and will only be publishing container images moving forward to <code>quay.io</code>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="debezium-2-2-whats-new">What&#8217;s new?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.2 is packed with a plethora of new features, most notable are:</p> </div> <div class="ulist"> <ul> <li> <p>Containers</p> <div class="ulist"> <ul> <li> <p><a href="#new-jolokia">Jolokia support</a></p> </li> </ul> </div> </li> <li> <p>Core</p> <div class="ulist"> <ul> <li> <p><a href="#new-database-connection-retries-startup">Database connections retried on connector start-up</a></p> </li> <li> <p><a href="#new-extract-changed-record-state-smt">ExtractNewRecordState single message transformation</a></p> </li> <li> <p><a href="#new-drop-fields-extract-new-record-state-smt">Drop fields using ExtractNewRecordState single message transformation</a></p> </li> <li> <p><a href="#new-parallel-snapshots">Parallel Snapshots</a></p> </li> <li> <p><a href="#new-incremental-snapshots-surrogate-key">Incremental snapshots using surrogate key</a></p> </li> <li> <p><a href="#new-quarkus-3">Quarkus 3 support</a></p> </li> </ul> </div> </li> <li> <p>Connectors</p> <div class="ulist"> <ul> <li> <p>JDBC</p> <div class="ulist"> <ul> <li> <p><a href="#new-jdbc-sink">JDBC Sink connector</a></p> </li> </ul> </div> </li> <li> <p>Oracle</p> <div class="ulist"> <ul> <li> <p><a href="#new-oracle-logical-standby">Ingest changes from Oracle Logical standby instances</a></p> </li> </ul> </div> </li> <li> <p>Spanner</p> <div class="ulist"> <ul> <li> <p><a href="#new-spanner-postgresql-dialect">Google Spanner PostgreSQL dialect support</a></p> </li> </ul> </div> </li> </ul> </div> </li> <li> <p>Debezium Server</p> <div class="ulist"> <ul> <li> <p><a href="#new-infinispan-sink">Infinispan sink adapter</a></p> </li> <li> <p><a href="#new-rabbitmq-sink">RabbitMQ sink adapter</a></p> </li> <li> <p><a href="#new-rocketmq-sink">RocketMQ sink adapter</a></p> </li> <li> <p><a href="#new-pulsar-async-delivery">Pulsar asynchronous event delivery</a></p> </li> </ul> </div> </li> <li> <p>Outbox Quarkus Extension</p> <div class="ulist"> <ul> <li> <p><a href="#new-outbox-reactive-extension">Reactive Quarkus Outbox extension</a></p> </li> </ul> </div> </li> <li> <p>Storage API</p> <div class="ulist"> <ul> <li> <p><a href="#new-storage-amazon-s3-bucket">Amazon S3 bucket storage support</a></p> </li> <li> <p><a href="#new-storage-rocketmq">RocketMQ storage support</a></p> </li> </ul> </div> </li> </ul> </div> <div class="sect2"> <h3 id="new-jolokia">Jolokia support</h3> <div class="paragraph"> <p>Jolokia is a JMX-HTTP bridge that provides an alternative to using JSR-160 to gather metrics. It is an agent based approach that improves traditional JMX by introducing unique features like bulk requests and fine-grained security policies.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, the <code>debezium/connect</code> image now ships with Jolokia, but this agent isn&#8217;t enabled by default. In order to enable Jolokia support, the container must be started with <code>ENABLE_JOLOKIA</code> set to <code>true</code>. By default, Jolokia will bind to port 8778 when enabled.</p> </div> <div class="paragraph"> <p>In the event that a different port is required, Jolokia will need to be enabled differently. For example, in order to enable Jolokia using port 9779, do not set the <code>ENABLE_JOLOKIA</code> but instead configure the <code>KAFKA_OPTS</code> environment variable as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="propreties">-e KAFKA_OPTS=&quot;-javaagent:$(ls &quot;$KAFKA_HOME&quot;/libs/jolokia-jvm-*.jar)=port=9779,host=*&quot;</code></pre> </div> </div> <div class="paragraph"> <p>By specifying the above environment variable, Jolokia&#8217;s JMX-HTTP bridge will be available on port 9779 of the container.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Do not forget to add the Jolokia port to the container&#8217;s list of exposed ports when starting.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="new-database-connection-retries-startup">Database connections retried on connector start-up</h3> <div class="paragraph"> <p>In previous releases of Debezium, the connector start-up phase used a fail-fast strategy. Simply put, this meant that if we couldn&#8217;t connect, authenticate, or performs any of the start-up phase steps required by the connector, the connector would enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>One specific problem area for users is if the connector gracefully starts, runs for a period of time, and then eventually encounters some fatal error. If the error is related to a resource that wasn&#8217;t accessed during the connector&#8217;s start-up lifecycle, the connector would typically gracefully restart just fine. However, the situation is different if the problem was related to the database&#8217;s availability and the database was still unavailable during the connector&#8217;s start-up phase. In this situation, the connector would fail-fast, and would enter a <code>FAILED</code> state, requiring manual intervention.</p> </div> <div class="paragraph"> <p>The fail-fast approach served Debezium well over the years, but in a world where a resource can come and go without warning, it became clear that changes were needed to improve Debezium&#8217;s reliability and resiliency. While the Kafka Connect&#8217;s retry/back-off framework has helped in this regard, that doesn&#8217;t address the concerns with start-up resources being unavailable with how the code is currently written.</p> </div> <div class="paragraph"> <p>Debezium 2.2 changes this landscape, shifting how we integrate with Kafka Connect&#8217;s source connector API slightly. Instead of accessing potentially unavailable resources during the start-up lifecycle, we moved that access to a later phase in the connector&#8217;s lifecycle. In effect, the Debezium start-up code is executed lazily that accesses potentially unavailable resources, which allows us to take advantage of the Kafka Connect retry/back-off framework even during our start-up code. In short, if the database is still unavailable during the connector&#8217;s start-up, the connector will continue to retry/back-off if Kafka Connect retries are enabled. Only once the maximum number of retry attempts has been reached or a non-retriable error occurs will the connector task enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>We hope this brings more reliability and resiliency for the Debezium experience, improving how errors are handled in an ever-changing landscape, and provides a solid foundation to manage connector lifecycles.</p> </div> </div> <div class="sect2"> <h3 id="new-extract-changed-record-state-smt">ExtractNewRecordState single message transformation</h3> <div class="paragraph"> <p>We have heard from the community on several occasions that it would great to have an out-of-the-box way to determine what values have changed in a Debezium change event. The new single message transform (SMT) <code>ExtractChangedRecordState</code> aims to deliver on this request by adding metadata to the event identifying which fields changed or were unchanged.</p> </div> <div class="paragraph"> <p>In order to get started with this new transformation, configure it as part of your connector configuration:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">transforms=changes
transforms.changes.type=io.debezium.transforms.ExtractChangedRecordState
transforms.changes.header.changed=ChangedFields
transforms.changes.header.unchanged=UnchangedFields</code></pre> </div> </div> <div class="paragraph"> <p>This transformation can be configured to disclose either what fields changed by setting <code>header.changed</code>, what fields are unchanged by setting <code>header.unchanged</code>, or both by setting both properties as shown above. The transformation will add a new header with the specified name, and it&#8217;s value will include a collection of field names based on whether you&#8217;ve configured changes, non-changes, or both.</p> </div> </div> <div class="sect2"> <h3 id="new-drop-fields-extract-new-record-state-smt">Drop fields using ExtractNewRecordState single message transformation</h3> <div class="paragraph"> <p>The <code>ExtractNewRecordState</code> single message transformation is extremely useful in situations where you need to consume the Debezium change event in a <em>flattened</em> format. This SMT has been changed in this release to add the ability to drop fields from the payload and the message key of the event.</p> </div> <div class="paragraph"> <p>This new feature introduces three new configuration properties for the transformation:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>drop.fields.header.name</code></dt> <dd> <p>The Kafka message header name to use for listing field names in the source message that are to be dropped.</p> </dd> <dt class="hdlist1"><code>drop.fields.from.key</code></dt> <dd> <p>Specifies whether to remove fields also from the key, defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>drop.fields.keep.schema.compatible</code></dt> <dd> <p>Specifies whether to remove fields that are only optional, defaults to <code>true</code>.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>When using Avro, schema compatibility is extremely important. This is why we opted to enforce schema compatibility by default. If a field is configured to be dropped but it is non-optional, the field will not be removed from the key nor the payload unless schema compatibility is disabled.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>These new configuration options allow for some exciting ways to manipulate change events. For example, to emit events with only changed fields, pairing the <code>ExtractNewRecordState</code> with the new <code>ExtractChangedRecordState</code> transformation makes this extremely simple and straightforward. An example configuration to only emit changed columns would look like the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">transforms=changes,extract
transforms.changes.type=io.debezium.transforms.ExtractChangedRecordState
transforms.changes.header.unchanged=UnchangedFields
transforms.extract.type=io.debezium.transforms.ExtractNewRecordState
transforms.extract.drop.fields.header.name=UnchangedFields</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration will explicitly not include unchanged fields from the event&#8217;s payload value. If a field in the key did not change, it will be unaffected because <code>drop.fields.from.key</code> was left as its default of <code>false</code>. And finally, if a field in the event&#8217;s payload is to be dropped because it did not change, but it&#8217;s not optional, it will continue to be included in the transformation&#8217;s output event to comply with schema compatibility.</p> </div> </div> <div class="sect2"> <h3 id="new-parallel-snapshots">Parallel Snapshots</h3> <div class="paragraph"> <p>Debezium&#8217;s relational database initial snapshot process has always been single-threaded. This limitation primarily stems from the complexities of ensuring data consistency across multiple transactions.</p> </div> <div class="paragraph"> <p>Starting in Debezium 2.2, we&#8217;re adding a new and initially optional way to utilize multiple threads to perform consistent database snapshot for a connector. This implementation uses these multiple threads to execute table-level snapshots in parallel.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new feature, specify <code>snapshot.max.threads</code> in your connector&#8217;s configuration and when this property has a value greater than <code>1</code>, parallel snapshots will be used.</p> </div> <div class="listingblock"> <div class="title">Example configuration using parallel snapshots</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">snapshot.max.threads=4</code></pre> </div> </div> <div class="paragraph"> <p>In the example above, if the connector needs to snapshot more than 4 tables, there will be at most 4 tables being snapshot in parallel. When one thread finishes processing a table, it will get a new table to snapshot from the queue and the process continues until all tables have been snapshot.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This feature is considered <em>incubating</em>, but we strongly suggest that new connector deployments give this feature a try. We would welcome any and all feedback on how to improve this going forward.</p> </div> </td> </tr> </table> </div> </div> <div class="sect2"> <h3 id="new-incremental-snapshots-surrogate-key">Incremental snapshots using surrogate key</h3> <div class="paragraph"> <p>Debezium&#8217;s incremental snapshot feature has been a tremendous success. It provides an efficient way to perform a consist snapshot of data that can be resumed, which is critical when the snapshot consists of large volumes of data.</p> </div> <div class="paragraph"> <p>However, incremental snapshots do have specific requirements that must be met before the feature can be used. One of those requirements is all tables being snapshot must use a primary key. You may ask, why does a table have no primary key, and we aren&#8217;t going to debate that here today; however, suffice to say this occurs more often than you may think.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, incremental snapshots can be performed on key-less tables as long as there is one column that is unique and can be considered a "surrogate key" for incremental snapshot purposes.</p> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>The surrogate key feature is not supported by MongoDB; only relational connectors.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>To provide the surrogate key column data in an incremental snapshot signal, the signal&#8217;s payload must include the new surrogate key attribute, <code>surrogate-key</code>.</p> </div> <div class="listingblock json"> <div class="title">An example incremental snapshot signal payload specifying a surrogate key</div> <div class="content"> <pre class="CodeRay highlight"><code>{
  "data-collections": [ "public.mytab" ],
  "surrogate-key": "customer_ref"
}</code></pre> </div> </div> <div class="paragraph"> <p>In the above example, an incremental snapshot will be started for table <code>public.mytab</code> and the incremental snapshot will use the <code>customer_ref</code> column as the primary key for generating the snapshot windows.</p> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>A surrogate key cannot be defined using multiple columns, only a <strong>single</strong> column.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>However, the surrogate key feature isn&#8217;t just applicable for tables with no primary keys. There are a series of advantages when using this feature with tables that have primary keys:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>One clear advantage is when the table&#8217;s primary key consists of multiple columns. The query generates a disjunction predicate for each column in the primary key, and it&#8217;s performance is highly dependent on the environment. Reducing the number of columns down to a single column often performs universally.</p> </li> <li> <p>Another advantage is when the surrogate key is based on a numeric data type while the primary key column is based on a character-based data type. Relational databases generally perform predicate evaluation more efficiently with numeric comparisons rather than character comparisons. By adjusting the query to use a numeric data type in this case, query performance could be better.</p> </li> </ol> </div> </div> <div class="sect2"> <h3 id="new-quarkus-3">Quarkus 3 support</h3> <div class="paragraph"> <p>Quarkus is a Kubernetes Native Java stack that combines the best Java libraries to create fast, low footprint applications. The Debezium Server runtime is based on Quarkus as well as part of Debezium UI. Additionally, the Debezium Outbox extension is also based on the Quarkus platform.</p> </div> <div class="paragraph"> <p>The upgrade to Quarkus 3 introduces a number of improvements, including using the latest stable releases of a plethora of Java libraries, including the migration from Java EE to Jakarta EE. If you are not familiar with this migration, previously most Java EE platform classes were bundled in the package <code>javax.*</code>. Over the past year or two, more applications have started the move from JavaEE or J2EE to Jakarta EE, and Quarkus 3 marks this transition era. Overall, the only real change is that classes that previously resided in <code>javax.*</code> now are placed in <code>jakarta.*</code>.</p> </div> <div class="paragraph"> <p>If your application makes use of the Debezium Quarkus Outbox extension, be aware that in order to use Debezium 2.2 with Quarkus, you will need to migrate to Quarkus 3. This also means that if you want to take advantage of the Outbox extension for Reactive data sources, you will be required to use Quarkus 3 as well.</p> </div> <div class="paragraph"> <p>Finally, if you are developing or maintaining sink adapters for Debezium Server, you will also need to make adjustments to using the new Jakarta EE annotations rather than the older Java EE annotations.</p> </div> </div> <div class="sect2"> <h3 id="new-jdbc-sink">JDBC Sink connector</h3> <div class="paragraph"> <p>The Debezium 2.2 release ushers in a new era for Debezium which has had a longstanding focus purely on providing a set of source connectors for relational and non-relational databases. This release alters that landscape, introducing a new JDBC sink connector implementation.</p> </div> <div class="paragraph"> <p>The Debezium JDBC sink connector is quite different from other vendor implementations in that it is capable of ingesting change events emitted by Debezium connectors without the need for event flattening. This has the potential to reduce the processing footprint in your pipeline, simplifies the pipeline&#8217;s configuration, and allows Debezium&#8217;s JDBC sink connector to take advantage of numerous Debezium source connector features such as column type propagation and much more.</p> </div> <div class="paragraph"> <p>Getting started with the Debezium JDBC sink connector is quite simple, lets take a look at an example.</p> </div> <div class="paragraph"> <p>Let&#8217;s say we have a Kafka topic called <code>orders</code> that contains Debezium change events that were created without using the <code>ExtractNewRecordState</code> transformation from MySQL. A simple configuration to ingest these change events into a PostgreSQL database might look this the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mysql-to-postgres-pipeline</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector_class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.jdbc.JdbcSinkConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">topics</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">orders</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">jdbc://postgresql://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;username&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;password&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">insert.mode</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">upsert</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">delete.enabled</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">primary.key.mode</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">record_key</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.evolution</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">basic</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>In this example, we&#8217;ve specified a series of <code>connection.*</code> properties that define the connection string and credentials for accessing the destination PostgreSQL database. Additionally, records will use <em>UPSERT</em> semantics when writing to the destination database, choosing to use an insert if the record doesn&#8217;t exist or updating the record if it does. We have also enabled schema evolution and specified that a table&#8217;s key columns should be derived from the event&#8217;s primary key.</p> </div> <div class="paragraph"> <p>The JDBC sink connector presently has support for the following relational databases:</p> </div> <div class="ulist"> <ul> <li> <p>Db2</p> </li> <li> <p>MySQL</p> </li> <li> <p>Oracle</p> </li> <li> <p>PostgreSQL</p> </li> <li> <p>SQL Server</p> </li> </ul> </div> <div class="paragraph"> <p>We do intend to add additional dialects in the future, and if there one you&#8217;d like to see, please get in touch with us either on our mailing list, in chat, or opening a Jira enhancement.</p> </div> </div> <div class="sect2"> <h3 id="new-oracle-logical-standby">Ingest changes from Oracle Logical standby instances</h3> <div class="paragraph"> <p>The Debezium for Oracle connector normally manages what is called a <em>flush table</em>, which is an internal table used to manage the flush cycles used by the Oracle Log Writer Buffer (LGWR) process. This flushing process requires that the user account the connector uses to have permission to create and write to this table. Logical stand-by databases often have more restrictive rules about data manipulation and may even be read-only, therefore, writing to the database is unfavorable or even not permissible.</p> </div> <div class="paragraph"> <p>To support an Oracle read-only logical stand-by database, we introduced a flag to disable the creation and management of this <em>flush table</em>. This feature can be used with both Oracle Standalone and Oracle RAC installations, and is currently considered incubating, meaning its subject to change in the future.</p> </div> <div class="paragraph"> <p>In order to enable Oracle read-only logical stand-by support, add the following connector option:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">internal.log.mining.read.only=true</code></pre> </div> </div> <div class="paragraph"> <p>In a future version, we plan to add support for an Oracle read-only physical stand-by database.</p> </div> </div> <div class="sect2"> <h3 id="new-spanner-postgresql-dialect">Google Spanner PostgreSQL dialect support</h3> <div class="paragraph"> <p>Google&#8217;s Cloud Spanner platform supports a PostgreSQL interface, which combines the scalability and reliability of the Google Spanner platform with the familiarity and portability of PostgreSQL. When operating Google Spanner with this PostgreSQL interface, metadata of columns and tables is different than when using the standard GoogleSQL dialect.</p> </div> <div class="paragraph"> <p>This release extends the Debezium Spanner connector support not only for the GoogleSQL dialect but also for users that use the Spanner PostgreSQL dialect feature. This means regardless of which dialect your spanner environment relies on, you will be able to capture change events from Spanner using the Debezium Spanner connector seamlessly.</p> </div> </div> <div class="sect2"> <h3 id="new-infinispan-sink">Infinispan sink adapter</h3> <div class="paragraph"> <p><a href="https://infinispan.org">Infinispan</a> is an in-memory, distributed data store that offers flexible deployment options with robust capabilities to store, manage, and process data. Infinispan is based on the notion of a key-value store that allows storing any data type. In order to integrate Debezium Server with Infinispan, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=infinispan
debezium.sink.infinispan.server.host=&lt;hostname&gt;
debezium.sink.infinispan.server.port=&lt;port&gt;
debezium.sink.infinispan.cache=&lt;cache-name&gt;
debezium.sink.infinispan.user=&lt;user&gt;
debezium.sink.infinispan.password=&lt;password&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>infinispan</code>, which enables the use of the Infinispan module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.server.host</code></dt> <dd> <p>Specifies the host name of one of the servers in the Infinispan cluster. This configuration option can also supply a comma-separated list of hostnames as well, such as <code>hostname1,hostname2</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.server.port</code></dt> <dd> <p>Specifies the port of the Infinispan cluster. Defaults to <code>11222</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.cache</code></dt> <dd> <p>Specifies the name of the Infinispan cache to write change events.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The Infinispan sink requires that the cache be created manually ahead of time. This enables the ability to create the cache with any variable configuration needed to fit your requirements.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.user</code></dt> <dd> <p>An optional configuration to specify the user to authenticate with, if authentication is required.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.password</code></dt> <dd> <p>An optional configuration to specify the password for the authenticating user, if authentication is required.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with Infinispan, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_infinispan">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="new-rabbitmq-sink">RabbitMQ sink adapter</h3> <div class="paragraph"> <p>Debezium 2.2 introduces a new sink adapter to the Debezium Server portfolio, allowing Debezium users to send change events to RabbitMQ. The following configuration shows a simple example of how easy it is to configure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=rabbitmq

# Connection details
debezium.sink.rabbitmq.connection.host=&lt;hostname&gt;
debezium.sink.rabbitmq.connection.port=&lt;port&gt;

# The routing key specifies an override of where events are published
debezium.sink.rabbitmq.routingKey=&lt;routing-key&gt;

# The default is 30 seconds, specified in milliseconds
debezium.sink.rabbitmq.ackTimeout=30000</code></pre> </div> </div> <div class="paragraph"> <p>The <code>debezium.sink.rabbitmq.connection.*</code> properties are required while the latter two properties for <code>routingKey</code> and <code>ackTimeout</code> are optional or have preset defaults that should be sufficient for most use cases.</p> </div> </div> <div class="sect2"> <h3 id="new-rocketmq-sink">RocketMQ sink adapter</h3> <div class="paragraph"> <p><a href="https://rocketmq.apache.org">Apache RocketMQ</a> is a cloud-native messaging, eventing, and streaming real-time data processing platform that covers cloud-edge-device collaboration scenarios. In order to integrate Debezium Server with RocketMQ, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=rocketmq
debezium.sink.rocketmq.producer.name.srv.addr=&lt;hostname&gt;:&lt;port&gt;
debezium.sink.rocketmq.producer.group=debezuim-group
debezium.sink.rocketmq.producer.max.message.size=4194304
debezium.sink.rocketmq.producer.send.msg.timeout=3000
debezium.sink.rocketmq.producer.acl.enabled=false
debezium.sink.rocketmq.producer.access.key=&lt;access-key&gt;
debezium.sink.rocketmq.producer.secret.key=&lt;secret-key&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>rocketmq</code>, which enables the use of the RocketMQ module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.name.srv.addr</code></dt> <dd> <p>Specifies the host and port where Apache RocketMQ is available.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.group</code></dt> <dd> <p>Specifies the name associated with the Apache RocketMQ producer group.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.max.message.size</code></dt> <dd> <p>(Optional) Specifies the maximum number of bytes a message can be. Defaults to <code>4193404</code> (4MB).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.send.msg.timeout</code></dt> <dd> <p>(Optional) Specifies the timeout in milliseconds when sending messages. Defaults to <code>3000</code> (3 seconds).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.acl.enabled</code></dt> <dd> <p>(Optional) Controls whether access control lists are enabled. Defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.access.key</code></dt> <dd> <p>(Optional) The access key used for connecting to the Apache RocketMQ cluster.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.secret.key</code></dt> <dd> <p>(Optional) The access secret used for connecting to the Apache RocketMQ cluster.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with RocketMQ, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_apache_rocketmq">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="new-pulsar-async-delivery">Pulsar asynchronous event delivery</h3> <div class="paragraph"> <p>In prior versions of the Debezium Server Pulsar sink, the adapter leveraged the <code>send()</code> method to deliver messages in a synchronous way. While this works for sending one-off messages, this has the potential to introduce connector latency as the method waits an acknowledgement of send operation sequentially. Since the Debezium Server sink adapters are provided a collection of events to deliver, the synchronous nature just does not perform well.</p> </div> <div class="paragraph"> <p>Starting Debezium 2.2, the Pulsar sink will now use <code>sendAsync()</code> to asynchronously deliver the batch of events to Pulsar, netting a substantial increase in overall throughput. While each event within the batch is delivered asynchronously, the adapter will only proceed to the next batch once the current batch is acknowledged in entirety.</p> </div> </div> <div class="sect2"> <h3 id="new-outbox-reactive-extension">Reactive Quarkus Outbox extension</h3> <div class="paragraph"> <p>The <a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">outbox pattern</a> is an approach that many microservices leverage to share data across microservice boundaries. We introduced the Debezium Outbox Quarkus Extension in Debezium 1.1 back in early 2020, and it has allowed Quarkus users to leverage the outbox pattern with ease using Debezium.</p> </div> <div class="paragraph"> <p>Thanks to <a href="https://github.com/ingmarfjolla">Ingmar Fjolla</a>, Debezium 2.2 includes a new reactive-based implementation of the Debezium Outbox Quarkus Extension. This new implementation is based on Vert.x and Hibernate Reactive, providing a fully asynchronous solution to the outbox pattern using Debezium.</p> </div> <div class="paragraph"> <p>This new extension is included in the Quarkus 3 platform released later this month. However if you want to get started with it today, you can easily drop it directly into your project&#8217;s configuration using the following coordinates:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
  <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
  <span class="tag">&lt;artifactId&gt;</span>debezium-quarkus-outbox-reactive<span class="tag">&lt;/artifactId&gt;</span>
  <span class="tag">&lt;version&gt;</span>2.2.0.Final<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="listingblock"> <div class="title">Gradle coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">io.debezium:debezium-quarkus-outbox-reactive:2.2.0.Final</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="new-storage-amazon-s3-bucket">Amazon S3 bucket storage support</h3> <div class="paragraph"> <p>Debezium provides a Storage API framework that enables connectors to store offset and schema history state in a variety of persistence datastores. Moreover, the framework enables contributors to extend the API by adding new storage implementations with ease. Currently, the Storage API framework supports the local FileSystem, a Kafka Topic, or Redis datastores.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, we&#8217;re pleased to add Amazon S3 buckets as part of that framework, allowing the schema history to be persisted to an S3 bucket. An example connector configuration using S3 might look like the following:</p> </div> <div class="listingblock properties"> <div class="content"> <pre class="CodeRay highlight"><code>...
schema.history.internal=io.debezium.storage.s3.history
schema.history.internal.s3.access.key.id=aa
schema.history.internal.s3.secret.access.key=bb
schema.history.internal.s3.region.name=aws-global
schema.history.internal.s3.bucket.name=debezium
schema.history.internal.s3.object.name=db-history.log
schema.history.internal.s3.endpoint=http://&lt;server&gt;:&lt;port&gt;</code></pre> </div> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.history.internal.s3.access.key.id</code></dt> <dd> <p>Specifies the access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.secret.access.key</code></dt> <dd> <p>Specifies the secret access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.region.name</code></dt> <dd> <p>Specifies the region where the S3 bucket is available.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.bucket.name</code></dt> <dd> <p>Specifies the name of the S3 bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.object.name</code></dt> <dd> <p>Specifies the object name in the bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.endpoint</code></dt> <dd> <p>Specifies the S3 endpoint with the format of <code><a href="http://&lt;server&gt;:&lt;port&gt" class="bare">http://&lt;server&gt;:&lt;port&gt</a>;</code>.</p> </dd> </dl> </div> </div> <div class="sect2"> <h3 id="new-storage-rocketmq">RocketMQ storage support</h3> <div class="paragraph"> <p>Debezium&#8217;s new storage API has been a huge success over this past year. We initially started with our original file and Kafka based implementations for offset and schema history storage, but that has since grown to support storing schema history on other platforms such as Amazon S3 and Redis.</p> </div> <div class="paragraph"> <p>This release continues to expand on this by adding a new schema history storage implementation for Rocket MQ. In order to get started with storing your schema history into Rocket MQ, the <code>debezium-storage-rocketmq</code> dependency must first be on the classpath and accessible by the connector runtime.</p> </div> <div class="paragraph"> <p>Once the dependency exists, the only remaining step will be configuring the schema history connector configuration. The following example shows basic usage of the Rocket MQ schema history:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">schema.history.internal.rocketmq.topic=schema-history
schema.history.internal.rocketmq.name.srv.addr=172.17.15.2
schema.history.internal.rocketmq.acl.enabled=true
schema.history.internal.rocketmq.access.key=&lt;rocketmq-access-key&gt;
schema.history.internal.rocketmq.secret.key=&lt;rocketmq-secret-key&gt;
schema.history.internal.rocketmq.recovery.attempts=5
schema.history.internal.rocketmq.recovery.poll.interval.ms=1000
schema.history.internal.rocketmq.store.record.timeout.ms=2000</code></pre> </div> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.history.internal.rocketmq.topic</code></dt> <dd> <p>Specifies the topic name where the schema history will be stored.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.name.srv.addr</code></dt> <dd> <p>Specifies the service discovery service nameserver for Rocket MQ.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.acl.enabled</code></dt> <dd> <p>Specifies whether access control lists (ACLs) are enabled, defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.access.key</code></dt> <dd> <p>Specifies the Rocket MQ access key, required only if ACLs are enabled.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.secret.key</code></dt> <dd> <p>Specifies the Rocket MQ secret key, required only if ACLs are enabled.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.recovery.attempts</code></dt> <dd> <p>Specifies the number of sequential attempts that no data is returned before recovery completes.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.recovery.poll.interval.ms</code></dt> <dd> <p>Specifies the number of milliseconds for each poll attempt to recover the history.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.store.record.timeout.ms</code></dt> <dd> <p>Specifies the number of milliseconds for a write to Rocket MQ to complete before timing out.</p> </dd> </dl> </div> </div> </div> </div> <div class="sect1"> <h2 id="other-fixes-improvements">Other fixes &amp; improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were many bugfixes, stability changes, and improvements throughout the development of Debezium 2.2. Altogether, a total of <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(2.2.0.Alpha1%2C%202.2.0.Alpha2%2C%202.2.0.Alpha3%2C%202.2.0.Beta1%2C%202.2.0.CR1%2C%202.2.0.Final)%20ORDER%20BY%20component%20ASC">228 issues</a> were fixed for this release.</p> </div> <div class="paragraph"> <p>A big thank you to all the contributors from the community who worked on this release: Akshansh Jain,  Ngc Sn, <a href="https://github.com/AnatolyPopov">Anatolii Popov</a>, <a href="https://github.com/ggaborg">Gabor Andras</a> <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/akanimesh7">Animesh Kumar</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/btiernay">Bobby Tiernay</a>, <a href="https://github.com/bruth">Byron Ruth</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/erdinctaskin">Erdin Takn</a>, <a href="https://github.com/EugeneAbramchuk">Eugene Abramchuk</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/govi20">Govinda Sakhare</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/henkosch">Henrik Schnell</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/ingmarfjolla">Ingmar Fjolla</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jbarrieault">Jacob Barrieault</a>, <a href="https://github.com/sugarcrm-jgminder">Jacob Gminder</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jakzal">Jakub Zalas</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/echatman-ias">Liz Chatman</a>, <a href="https://github.com/lokesh1729">Lokesh Sanapalli</a>, <a href="https://github.com/Lucascanna">Luca Scannapieco</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/alwaysbemark">Mark Bereznitsky</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/MehmetFiratKomurcu">Mehmet Firat Komurcu</a>, <a href="https://github.com/MyLanPangzi">My Lang Pangzi</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/olivierboudet">Olivier Boudet</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/smallYellowCat">Pengwei Dou</a>, <a href="https://github.com/PlugaruT">Plugaru Tudor</a>, <a href="https://github.com/rnowling-memphis">RJ Nowling</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/ironakj">Ronak Jain</a>, <a href="https://github.com/Apteryx0">Russell Mora</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/subodh1810">Subodh Kant Chaturvedi</a>, <a href="https://github.com/sunxiaojian">Sun Xiao Jian</a>, <a href="https://github.com/twthorn">Thomas Thornton</a>, <a href="https://github.com/chtitux">Thophile Helleboid</a>, <a href="https://github.com/Tideri-Tim2">Tim Loes</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/vjuranek">Vojtch Jurnek</a>, <a href="https://github.com/xinbinhuang">Xinbin Huang</a>, <a href="https://github.com/y5w">Yang Wu</a>, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, <a href="https://github.com/zzzming">ming luo</a>, <a href="https://github.com/imtj1">tony joseph</a>, <a href="https://github.com/yoheimuta">yohei yoshimuta</a>, and <a href="https://github.com/caicancai"></a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats-next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We began pre-planning Debezium 2.3 several weeks ago and with 2.2 shipped, our focus will now be on the next minor release. With Debezium 2.2 release cycle being a tad longer than normal, the release cycle for 2.3 will be condensed as we want to return to our end-of-quarter release cadence. In order to achieve that goal, we&#8217;ve chosen to focus on the following features for the next minor release:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1">Configurable Signal Channels</dt> <dd> <p>The goal of this change is to provide a way in which signals can be sent to a connector from a variety of sources, including things like the filesystem, Kafka topic, database table, etc.</p> </dd> <dt class="hdlist1">Exactly once delivery semantics</dt> <dd> <p>Debezium currently only guarantees at-least-once delivery semantics, meaning that a change event could be written to a topic more than once in the case of unsafe shutdowns or failures of a connector. Kafka and by extension Kafka Connect, now support exactly-once delivery and we want to explore this feature as part of Debezium. The goal is to focus adding this to at least once connector as a proof of concept and based on feedback, extend this to all connectors.</p> </dd> <dt class="hdlist1">Kubernetes operator for Debezium Server</dt> <dd> <p>Debezium Server has gained quite a bit of exposure in recent months, both with new sink adapters and just general usage by the community. We want to bring the power of Kubernetes to Debezium Server, introducing an operator that you can deploy in order to manage the full lifecycle of a Debezium Server deployment.</p> </dd> <dt class="hdlist1">Ingestion from Oracle using OpenLogReplicator</dt> <dd> <p>The Debezium Oracle connector presents supports ingestion of changes using XStream or LogMiner. We want to build a proof-of-concept using OpenLogReplicator, a native application that is capable of reading the Oracle redo and archive logs directly from the file system. We do not intend to replace either of the existing adapters with this new approach, but to instead extend the connector&#8217;s functionality to offer alternatives to data ingestion that may have less overhead.</p> </dd> <dt class="hdlist1">Debezium UI Enhancements</dt> <dd> <p>We believe there is a lot of unlocked potential with Debezium UI, so this release will focus on improving that overall user experience by adding new features like starting/stopping ad-hoc snapshots, editing connector deployments, and displaying critical connector metrics.</p> </dd> </dl> </div> <div class="paragraph"> <p>While the team intends to focus on the above improvements, we would really like your feedback or suggestions. If you have anything that you&#8217;d like to share, be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="mongodb"/><category term="spanner"/><summary type="html"><![CDATA[Today, it&#8217;s with great joy that we can announce the availability of Debezium 2.2.0.Final! Many of you may have noticed, this release cadence took a bit longer than our traditional three-months. While we normally prefer to keep to our usual cadence, this shift gives us a unique opportunity to ship Debezium 2.2 with tons of new features and bug fixes, but also major upgrades to several core components.]]></summary></entry><entry><title type="html">Debezium 2.2.0.CR1 Released</title><link href="https://debezium.io/blog/2023/04/17/debezium-2-2-cr1-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.CR1 Released"/><published>2023-04-17T00:00:00+00:00</published><updated>2023-04-17T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/04/17/debezium-2-2-cr1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/04/17/debezium-2-2-cr1-released/"><![CDATA[<div class="paragraph"> <p>The Debezium team is excited to announce the first release candidate of Deebzium 2.2, Debezium <strong>2.2.0.CR1</strong>.</p> </div> <div class="paragraph"> <p>This release primarily focuses on stability improvements and bug fixes; however, there are a number of new features and breaking changes. In this release, Debezium migrated to Quarkus 3.0.0.Final, there are performance improvements to Debezium Server Pulsar sink, Jolokia can be enabled inside Debezium&#8217;s Kafka Connect container image, incubating support for incremental snapshots on MongoDB multi-replica and sharded clusters, and the deprecation usage of Docker Hub for images.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a moment and dive into several of these and what it means moving forward!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="upgrade_to_quarkus_3">Upgrade to Quarkus 3</h2> <div class="sectionbody"> <div class="paragraph"> <p>Quarkus is a Kubernetes Native Java stack that combines the best Java libraries to create fast, low footprint applications. The Debezium Server runtime is based on Quarkus as well as part of Debezium UI. Additionally, the Debezium Outbox extension is also based on the Quarkus platform.</p> </div> <div class="paragraph"> <p>The upgrade to Quarkus 3 introduces a number of improvements, including using the latest stable releases of a plethora of Java libraries, including the migration from Java EE to Jakarta EE. If you are not familiar with this migration, previously most Java EE platform classes were bundled in the package <code>javax.*</code>. Over the past year or two, more applications have started the move from JavaEE or J2EE to Jakarta EE, and Quarkus 3 marks this transition era. Overall, the only real change is that classes that previously resided in <code>javax.*</code> now are placed in <code>jakarta.*</code>.</p> </div> <div class="paragraph"> <p>If your application makes use of the Debezium Quarkus Outbox extension, be aware that in order to use Debezium 2.2 with Quarkus, you will need to migrate to Quarkus 3. This also means that if you want to take advantage of the Outbox extension for Reactive data sources, you will be required to use Quarkus 3 as well.</p> </div> <div class="paragraph"> <p>Finally, if you are developing or maintaining sink adapters for Debezium Server, you will also need to make adjustments to using the new Jakarta EE annotations rather than the older Java EE annotations.</p> </div> </div> </div> <div class="sect1"> <h2 id="debezium_server_pulsar_changes">Debezium Server Pulsar Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>In prior versions of the Debezium Server Pulsar sink, the adapter leveraged the <code>send()</code> method to deliver messages in a synchronous way. While this works for sending one-off messages, this has the potential to introduce connector latency as the method waits an acknowledgement of send operation sequentially. Since the Debezium Server sink adapters are provided a collection of events to deliver, the synchronous nature just does not perform well.</p> </div> <div class="paragraph"> <p>Starting Debezium 2.2, the Pulsar sink will now use <code>sendAsync()</code> to asynchronously deliver the batch of events to Pulsar, netting a substantial increase in overall throughput. While each event within the batch is delivered asynchronously, the adapter will only proceed to the next batch once the current batch is acknowledged in entirety.</p> </div> </div> </div> <div class="sect1"> <h2 id="jolokia_support">Jolokia support</h2> <div class="sectionbody"> <div class="paragraph"> <p>Jolokia is a JMX-HTTP bridge that provides an alternative to using JSR-160 to gather metrics. It is an agent based approach that improves traditional JMX by introducing unique features like bulk requests and fine-grained security policies.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, the <code>debezium/connect</code> image now ships with Jolokia, but this agent isn&#8217;t enabled by default. In order to enable Jolokia support, the container must be started with <code>ENABLE_JOLOKIA</code> set to <code>true</code>. By default, Jolokia will bind to port 8778 when enabled.</p> </div> <div class="paragraph"> <p>In the event that a different port is required, Jolokia will need to be enabled differently. For example, in order to enable Jolokia using port 9779, do not set the <code>ENABLE_JOLOKIA</code> but instead configure the <code>KAFKA_OPTS</code> environment variable as follows:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="propreties">-e KAFKA_OPTS=&quot;-javaagent:$(ls &quot;$KAFKA_HOME&quot;/libs/jolokia-jvm-*.jar)=port=9779,host=*&quot;</code></pre> </div> </div> <div class="paragraph"> <p>By specifying the above environment variable, Jolokia&#8217;s JMX-HTTP bridge will be available on port 9779 of the container.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Do not forget to add the Jolokia port to the container&#8217;s list of exposed ports when starting.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="deprecation_of_docker_hub">Deprecation of Docker Hub</h2> <div class="sectionbody"> <div class="paragraph"> <p>Docker recently announced a reduction in their free organization account offerings, which is leveraged by a number of open-source communities, including Debezium. Unfortunately, Debezium does not qualify according to their rules.</p> </div> <div class="paragraph"> <p>Despite that Docker chose to walk back their decision, the Debezium team believes that we need a solid path forward to guarantee the availability of our images to our community without disruption. Debezium has been dual-publishing to both <code>docker.io</code> and <code>quay.io</code> for quite some time, and we will continue this for Debezium 2.2.0.Final and for all Debezium 2.3 <strong>preview</strong> releases.</p> </div> <div class="paragraph"> <p>We plan to publish a blog post going into more detail in the coming days with more details. In the meantime, all users should plan to migrate to <code>quay.io</code> as soon as possible to avoid disruption of fetching newer versions of Debezium.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of other improvements, bug fixes, and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Upgrade dependencies (Quarkus, etc) of Debezium UI <a href="https://issues.redhat.com/browse/DBZ-4109">DBZ-4109</a></p> </li> <li> <p>Failed retriable operations are retried infinitely <a href="https://issues.redhat.com/browse/DBZ-4488">DBZ-4488</a></p> </li> <li> <p>UI- Add the UI to configure the additional properties for a connector <a href="https://issues.redhat.com/browse/DBZ-5365">DBZ-5365</a></p> </li> <li> <p>Capture events in order across mongodb shards <a href="https://issues.redhat.com/browse/DBZ-5590">DBZ-5590</a></p> </li> <li> <p>DDL events not stored in schema history topic for excluded tables <a href="https://issues.redhat.com/browse/DBZ-6070">DBZ-6070</a></p> </li> <li> <p>Oracle path used current batchSize to calculate end scn is wrong, need to use min batch size <a href="https://issues.redhat.com/browse/DBZ-6155">DBZ-6155</a></p> </li> <li> <p>Upgrade UI build to use Debezium 2.2 or latest <a href="https://issues.redhat.com/browse/DBZ-6173">DBZ-6173</a></p> </li> <li> <p>Oracle-Connector dbz##user needs more rights <a href="https://issues.redhat.com/browse/DBZ-6198">DBZ-6198</a></p> </li> <li> <p>Make quay.io primary image repository <a href="https://issues.redhat.com/browse/DBZ-6216">DBZ-6216</a></p> </li> <li> <p>Multiplatform build of example-postres fails <a href="https://issues.redhat.com/browse/DBZ-6258">DBZ-6258</a></p> </li> <li> <p>Add protoc version property to postgres connector pom.xml <a href="https://issues.redhat.com/browse/DBZ-6261">DBZ-6261</a></p> </li> <li> <p>Pass through configurations for kafka topics/configuration <a href="https://issues.redhat.com/browse/DBZ-6262">DBZ-6262</a></p> </li> <li> <p>Postgres connector doesn&#8217;t need logical WAL level when snapshotting only <a href="https://issues.redhat.com/browse/DBZ-6265">DBZ-6265</a></p> </li> <li> <p>Update config properties in RHEL deployment instructions <a href="https://issues.redhat.com/browse/DBZ-6266">DBZ-6266</a></p> </li> <li> <p>MySQL connector doesn&#8217;t need to query binlog when snapshotting only <a href="https://issues.redhat.com/browse/DBZ-6271">DBZ-6271</a></p> </li> <li> <p>Table names with spaces are not correctly deserialized when using an Infinispan cache as the transaction buffer <a href="https://issues.redhat.com/browse/DBZ-6273">DBZ-6273</a></p> </li> <li> <p>Infinispan cache configuration used by Oracle tests are not compatible with Infinispan 14.0.2 <a href="https://issues.redhat.com/browse/DBZ-6274">DBZ-6274</a></p> </li> <li> <p>Transaction buffer state can become corrupted when using Infinispan cache with LOBs <a href="https://issues.redhat.com/browse/DBZ-6275">DBZ-6275</a></p> </li> <li> <p>Enable the docker tag to be configurable in the Spanner connector <a href="https://issues.redhat.com/browse/DBZ-6302">DBZ-6302</a></p> </li> <li> <p>Upgrade MySQL JDBC driver to 8.0.32 <a href="https://issues.redhat.com/browse/DBZ-6304">DBZ-6304</a></p> </li> <li> <p>Allow specifying docker image reference in MongoDB testcontainers implementation <a href="https://issues.redhat.com/browse/DBZ-6305">DBZ-6305</a></p> </li> <li> <p>Use <strong>MongoDbContainer</strong> instead of <strong>MongoDBContainer</strong> test containers class in ConnectorConfiguration class <a href="https://issues.redhat.com/browse/DBZ-6306">DBZ-6306</a></p> </li> <li> <p>DDL statement couldn&#8217;t be parsed - Oracle connector 2.1.3.Final <a href="https://issues.redhat.com/browse/DBZ-6314">DBZ-6314</a></p> </li> <li> <p>Unparsable DDL statements (MySQL/MariaDB) <a href="https://issues.redhat.com/browse/DBZ-6316">DBZ-6316</a></p> </li> <li> <p>Remove outdated information about SYS user accounts with Oracle <a href="https://issues.redhat.com/browse/DBZ-6318">DBZ-6318</a></p> </li> <li> <p>Cassandra 3 cannot be built using JDK20 <a href="https://issues.redhat.com/browse/DBZ-6320">DBZ-6320</a></p> </li> <li> <p>Bundle Jolokia with Debezium connect image <a href="https://issues.redhat.com/browse/DBZ-6323">DBZ-6323</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.CR1%20ORDER%20BY%20component%20ASC">34 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/sugarcrm-jgminder">Jacob Gminder</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/alwaysbemark">Mark Bereznitsky</a>, <a href="https://github.com/obabec">Ondrej Babec</a>, <a href="https://github.com/smallYellowCat">Pengwei Dou</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, and <a href="https://github.com/vjuranek">Vojtech Juranek</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We are approaching the end of the Debezium 2.2 release cycle, with a final release expected this week. If there are any outstanding bugs or issues, please get in touch with us so that can be addressed prior to the final release.</p> </div> <div class="paragraph"> <p>The Debezium team is shifting their focus on Debezium 2.3. The Debezium 2.3 release will be a much more condensed and focused release, as our goal is to release it in late June. The Debezium roadmap has been updated and the following features are planned for this quarter:</p> </div> <div class="ulist"> <ul> <li> <p>Support configurable signaling channels</p> </li> <li> <p>Support exactly once delivery semantics (phase 1)</p> </li> <li> <p>Kubernetes operator for Debezium Server</p> </li> <li> <p>Oracle OpenLogReplicator adapter proof-of-concept / incubating implementation</p> </li> <li> <p>Debezium UI improvements</p> </li> </ul> </div> <div class="paragraph"> <p>We would like to hear your feedback or suggestions, so if you have anything you&#8217;d like to share be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> <div class="paragraph"> <p>Until next time, let the changes continue to stream&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[The Debezium team is excited to announce the first release candidate of Deebzium 2.2, Debezium 2.2.0.CR1. This release primarily focuses on stability improvements and bug fixes; however, there are a number of new features and breaking changes. In this release, Debezium migrated to Quarkus 3.0.0.Final, there are performance improvements to Debezium Server Pulsar sink, Jolokia can be enabled inside Debezium&#8217;s Kafka Connect container image, incubating support for incremental snapshots on MongoDB multi-replica and sharded clusters, and the deprecation usage of Docker Hub for images. Let&#8217;s take a moment and dive into several of these and what it means moving forward!]]></summary></entry><entry><title type="html">Debezium 2.2.0.Beta1 Released</title><link href="https://debezium.io/blog/2023/04/03/debezium-2-2-beta1-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Beta1 Released"/><published>2023-04-03T00:00:00+00:00</published><updated>2023-04-03T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/04/03/debezium-2-2-beta1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/04/03/debezium-2-2-beta1-released/"><![CDATA[<div class="paragraph"> <p>The team is excited to announce the first beta release of the Debezium 2.2 release stream, Debezium <strong>2.2.0.Beta1</strong>.</p> </div> <div class="paragraph"> <p>This release includes a plethora of bug fixes, improvements, and a number of new features including, but not limited to, a new JDBC sink connector implementation, MongoDB sharded cluster improvements, Google Spanner PostgreSQL dialect support, and a RabbitMQ sink implementation for Debezium Server to just name a few.</p> </div> <div class="paragraph"> <p>Let&#8217;s take moment and dive into what&#8217;s new!</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="jdbc_sink_connector">JDBC Sink Connector</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium 2.2 release ushers in a new era for Debezium which has had a longstanding focus purely on providing a set of source connectors for relational and non-relational databases. This release alters that landscape, introducing a new JDBC sink connector implementation.</p> </div> <div class="paragraph"> <p>The Debezium JDBC sink connector is quite different from other vendor implementations in that it is capable of ingesting change events emitted by Debezium connectors without the need for event flattening. This has the potential to reduce the processing footprint in your pipeline, simplifies the pipeline&#8217;s configuration, and allows Debezium&#8217;s JDBC sink connector to take advantage of numerous Debezium source connector features such as column type propagation and much more.</p> </div> <div class="paragraph"> <p>Getting started with the Debezium JDBC sink connector is quite simple, lets take a look at an example.</p> </div> <div class="paragraph"> <p>Let&#8217;s say we have a Kafka topic called <code>orders</code> that contains Debezium change events that were created without using the <code>ExtractNewRecordState</code> transformation from MySQL. A simple configuration to ingest these change events into a PostgreSQL database might look this the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mysql-to-postgres-pipeline</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector_class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.jdbc.JdbcSinkConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">topics</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">orders</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.url</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">jdbc://postgresql://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;username&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">&lt;password&gt;</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">insert.mode</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">upsert</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">delete.enabled</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">primary.key.mode</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">record_key</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">schema.evolution</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">basic</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>In this example, we&#8217;ve specified a series of <code>connection.*</code> properties that define the connection string and credentials for accessing the destination PostgreSQL database. Additionally, records will use <em>UPSERT</em> semantics when writing to the destination database, choosing to use an insert if the record doesn&#8217;t exist or updating the record if it does. We have also enabled schema evolution and specified that a table&#8217;s key columns should be derived from the event&#8217;s primary key.</p> </div> <div class="paragraph"> <p>The JDBC sink connector presently has support for the following relational databases:</p> </div> <div class="ulist"> <ul> <li> <p>Db2</p> </li> <li> <p>MySQL</p> </li> <li> <p>Oracle</p> </li> <li> <p>PostgreSQL</p> </li> <li> <p>SQL Server</p> </li> </ul> </div> <div class="paragraph"> <p>We do intend to add additional dialects in the future, and if there one you&#8217;d like to see, please get in touch with us either on our mailing list, in chat, or opening a Jira enhancement.</p> </div> </div> </div> <div class="sect1"> <h2 id="mongodb_sharded_cluster_improvements">MongoDB Sharded Cluster Improvements</h2> <div class="sectionbody"> <div class="paragraph"> <p>When using the Debezium for MongoDB connector in a sharded cluster deployment, the connector opens a connection with each of the shard&#8217;s replica sets directly. This is not a recommended approach and instead MongoDB suggests that the connector <a href="https://www.mongodb.com/docs/manual/sharding/#connecting-to-a-sharded-cluster">open a connection with the mongos instance</a> (the router) instead.</p> </div> <div class="paragraph"> <p>This release aligns with this recommended strategy and users should be prepared to adjust their configurations slightly and when using the connector in such a deployment, point the connector as the <code>mongos</code> instance instead. There should be be other changes required.</p> </div> </div> </div> <div class="sect1"> <h2 id="spanner_postgresql_dialect_support">Spanner PostgreSQL Dialect Support</h2> <div class="sectionbody"> <div class="paragraph"> <p>Google&#8217;s Cloud Spanner platform supports a PostgreSQL interface, which combines the scalability and reliability of the Google Spanner platform with the familiarity and portability of PostgreSQL. When operating Google Spanner with this PostgreSQL interface, metadata of columns and tables is different than when using the standard GoogleSQL dialect.</p> </div> <div class="paragraph"> <p>This release extends the Debezium Spanner connector support not only for the GoogleSQL dialect but also for users that use the Spanner PostgreSQL dialect feature. This means regardless of which dialect your spanner environment relies on, you will be able to capture change events from Spanner using the Debezium Spanner connector seamlessly.</p> </div> <div class="paragraph"> <p>So if you&#8217;re using Spanner&#8217;s PostgreSQL dialect, upgrade to Debezium 2.2.0.Beta1 or later and start capturing changes!</p> </div> </div> </div> <div class="sect1"> <h2 id="rabbitmq_debezium_server_sink">RabbitMQ Debezium Server Sink</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium Server is a ready-made Quarkus-based runtime for Debezium source and sink connectors. Debezium Server provides the capability to send Debezium change events from any source connector to a variety of messaging infrastructure platforms, particularly for users who would prefer something other than Apache Kafka.</p> </div> <div class="paragraph"> <p>In this release, a new sink adapter has been added to the Debezium Server portfolio, allowing Debezium users to send change events to RabbitMQ. The following configuration shows a simple example of how easy it is to configure:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=rabbitmq

# Connection details
debezium.sink.rabbitmq.connection.host=&lt;hostname&gt;
debezium.sink.rabbitmq.connection.port=&lt;port&gt;

# The routing key specifies an override of where events are published
debezium.sink.rabbitmq.routingKey=&lt;routing-key&gt;

# The default is 30 seconds, specified in milliseconds
debezium.sink.rabbitmq.ackTimeout=30000</code></pre> </div> </div> <div class="paragraph"> <p>The <code>debezium.sink.rabbitmq.connection.*</code> properties are required while the latter two properties for <code>routingKey</code> and <code>ackTimeout</code> are optional or have preset defaults that should be sufficient for most use cases.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of other improvements, bug fixes, and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Create an endpoint to update a connector <a href="https://issues.redhat.com/browse/DBZ-5314">DBZ-5314</a></p> </li> <li> <p>Refactor snapshotting to use change streams instead of oplog <a href="https://issues.redhat.com/browse/DBZ-5987">DBZ-5987</a></p> </li> <li> <p>Update the design for Debezium based connectors Filter step <a href="https://issues.redhat.com/browse/DBZ-6060">DBZ-6060</a></p> </li> <li> <p>NPE when setting schema.history.internal.store.only.captured.tables.ddl=true <a href="https://issues.redhat.com/browse/DBZ-6072">DBZ-6072</a></p> </li> <li> <p>Postgres connector stuck when replication slot does not have confirmed_flush_lsn <a href="https://issues.redhat.com/browse/DBZ-6092">DBZ-6092</a></p> </li> <li> <p>java.lang.NullPointerException in MySQL connector with max.queue.size.in.bytes <a href="https://issues.redhat.com/browse/DBZ-6104">DBZ-6104</a></p> </li> <li> <p>debezium-connector-mysql failed to parse serveral DDLs of 'CREATE TABLE' <a href="https://issues.redhat.com/browse/DBZ-6124">DBZ-6124</a></p> </li> <li> <p>Connect and stream from sharded clusters through mongos instances <a href="https://issues.redhat.com/browse/DBZ-6170">DBZ-6170</a></p> </li> <li> <p>Support Azure blob storage as Debezium history storage <a href="https://issues.redhat.com/browse/DBZ-6180">DBZ-6180</a></p> </li> <li> <p>Zerofill property failed for different int types <a href="https://issues.redhat.com/browse/DBZ-6185">DBZ-6185</a></p> </li> <li> <p>GRANT DELETE HISTORY couldn&#8217;t be parsed in mariadb <a href="https://issues.redhat.com/browse/DBZ-6186">DBZ-6186</a></p> </li> <li> <p>ddl parse failed for key partition table <a href="https://issues.redhat.com/browse/DBZ-6188">DBZ-6188</a></p> </li> <li> <p>Config options internal.schema.history.internal.ddl.filter not working <a href="https://issues.redhat.com/browse/DBZ-6190">DBZ-6190</a></p> </li> <li> <p>Support Database role in Connector Config. <a href="https://issues.redhat.com/browse/DBZ-6192">DBZ-6192</a></p> </li> <li> <p>Use CHARSET for alterByConvertCharset clause <a href="https://issues.redhat.com/browse/DBZ-6194">DBZ-6194</a></p> </li> <li> <p>Remove duplicated createDdlFilter method from historized connector config <a href="https://issues.redhat.com/browse/DBZ-6197">DBZ-6197</a></p> </li> <li> <p>Create new SMT to copy/move header to record value <a href="https://issues.redhat.com/browse/DBZ-6201">DBZ-6201</a></p> </li> <li> <p>Data loss upon connector restart <a href="https://issues.redhat.com/browse/DBZ-6204">DBZ-6204</a></p> </li> <li> <p>ParsingException: DDL statement couldn&#8217;t be parsed <a href="https://issues.redhat.com/browse/DBZ-6217">DBZ-6217</a></p> </li> <li> <p>The CHARACTER/CHARACTER(p)/CHARACTER VARYING(p) data types not recognized as JDBC type CHAR <a href="https://issues.redhat.com/browse/DBZ-6221">DBZ-6221</a></p> </li> <li> <p>MySQL treats the BOOLEAN synonym differently when processed in snapshot vs streaming phases. <a href="https://issues.redhat.com/browse/DBZ-6225">DBZ-6225</a></p> </li> <li> <p>MySQL treats REAL synonym differently when processed in snapshot vs streaming phases. <a href="https://issues.redhat.com/browse/DBZ-6226">DBZ-6226</a></p> </li> <li> <p>Spanner Connector - Deadlock in BufferedPublisher when publish gives exception <a href="https://issues.redhat.com/browse/DBZ-6227">DBZ-6227</a></p> </li> <li> <p>Publish of sync event fails when message becomes very large. <a href="https://issues.redhat.com/browse/DBZ-6228">DBZ-6228</a></p> </li> <li> <p>MySQL treats NCHAR/NVARCHAR differently when processed in snapshot vs streaming phases. <a href="https://issues.redhat.com/browse/DBZ-6231">DBZ-6231</a></p> </li> <li> <p>Add support for columns of type "bytea[]" - array of bytea (byte array) <a href="https://issues.redhat.com/browse/DBZ-6232">DBZ-6232</a></p> </li> <li> <p>MySQL singleDeleteStatement parser does not support table alias <a href="https://issues.redhat.com/browse/DBZ-6243">DBZ-6243</a></p> </li> <li> <p>Support ImageFromDockerfile with Debezium&#8217;s testcontainers suite <a href="https://issues.redhat.com/browse/DBZ-6244">DBZ-6244</a></p> </li> <li> <p>Testcontainers MongoDbReplicaSetTest failing with MongoDB 4.2 <a href="https://issues.redhat.com/browse/DBZ-6247">DBZ-6247</a></p> </li> <li> <p>Expose EmbeddedEngine configurations <a href="https://issues.redhat.com/browse/DBZ-6248">DBZ-6248</a></p> </li> <li> <p>Wrong error thrown when snapshot.custom_class=custom and no snapshot.custom.class <a href="https://issues.redhat.com/browse/DBZ-6249">DBZ-6249</a></p> </li> <li> <p>Missing GEOMETRY keyword which can be used as column name <a href="https://issues.redhat.com/browse/DBZ-6250">DBZ-6250</a></p> </li> <li> <p>Postgres connector stuck trying to fallback to restart_lsn when replication slot confirmed_flush_lsn is null. <a href="https://issues.redhat.com/browse/DBZ-6251">DBZ-6251</a></p> </li> <li> <p>MariaDB&#8217;s UUID column type cannot be parsed when scheme is loaded <a href="https://issues.redhat.com/browse/DBZ-6255">DBZ-6255</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.Beta1%20ORDER%20BY%20component%20ASC">52 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/sondn"> Ngc Sn</a>, <a href="https://github.com/AnatolyPopov">Anatolii Popov</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/nirolevy">Nir Levy</a>, <a href="https://github.com/PlugaruT">Plugaru Tudor</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/Apteryx0">Russell Mora</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/vjuranek">Vojtch Jurnek</a>, and <a href="https://github.com/imtj1">tony joseph</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As we approach the end of the Debezium 2.2 development cycle, with a final release expected in the next two weeks, we&#8217;re going to begin to turn our attention toward Debezium 2.3. The Debezium 2.3 release will be a much more condensed and focused release, as our goal is to release it in late June.</p> </div> <div class="paragraph"> <p>We will be refining our <a href="https://debezium.io/roadmap">roadmap</a> in the coming days, so I would pay close attention to this to get an understanding of what lies ahead in the near future for Debezium 2.3. We would like to hear your feedback or suggestions, so if you have anything you&#8217;d like to share be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a>.</p> </div> <div class="paragraph"> <p>DevNexus 2023 is also underway this week, from April 4th until April 6th and I will be presenting a talk on CDC Patterns with Distributed Systems using Debezium. If you&#8217;re in the Atlanta area and plan to attend DevNexus on Thursday, April 6th, drop me a line.</p> </div> <div class="paragraph"> <p>Until next time, let the changes continue to stream&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[The team is excited to announce the first beta release of the Debezium 2.2 release stream, Debezium 2.2.0.Beta1. This release includes a plethora of bug fixes, improvements, and a number of new features including, but not limited to, a new JDBC sink connector implementation, MongoDB sharded cluster improvements, Google Spanner PostgreSQL dialect support, and a RabbitMQ sink implementation for Debezium Server to just name a few. Let&#8217;s take moment and dive into what&#8217;s new!]]></summary></entry><entry><title type="html">Hello Debezium Team!</title><link href="https://debezium.io/blog/2023/03/09/hello-debezium/" rel="alternate" type="text/html" title="Hello Debezium Team!"/><published>2023-03-09T10:11:11+00:00</published><updated>2023-03-09T10:11:11+00:00</updated><id>https://debezium.io/blog/2023/03/09/hello-debezium</id><content type="html" xml:base="https://debezium.io/blog/2023/03/09/hello-debezium/"><![CDATA[<div class="paragraph"> <p>Hi everyone, my name is Mario Fiore Vitale and I recently joined Red Hat and the Debezium team.</p> </div> <div class="paragraph"> <p>I am a very curious person that follows a continuous learning approach, I like to keep growing my skills. I care about code quality and readability.</p> </div> <div class="paragraph"> <p>I have about 9+ years of experience and have worked for consultancy, startup, and enterprise product companies in different sectors. In my previously experience I had the chance to work on architecture re-design project to split a monolith into a microservices application. During this experience I gained experience with different technologies such as Kafka, Elasticsearch, Redis, Kubernetes, VictoriaMetrics, Spring Framework, and a bit of Cassandra.</p> </div> <div class="paragraph"> <p>Why Am I here?</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>First of all, I have always been fascinated by OSS and the power of the "community". During my career I have used a lot of OSS and when I had the chance to give back to the community I didn&#8217;t back down.</p> </div> <div class="paragraph"> <p>In recent years data are becoming more and more important, in terms of their role for companies but also in terms of quantity. The way we manage these data is now crucial. Data comes from different source, asynchronously and must be shared with different consumers. So we need to continuously process incoming data, and this is where Event Stream Processing comes in. Debezium can act as a facilitator for Stream processing, enabling a lot of useful use cases for modern software architecture. This is why I like Debezium.</p> </div> <div class="paragraph"> <p>I&#8217;m thrilled to be here and looking forward to working with this amazing community.</p> </div> <div class="paragraph"> <p>Onwards,</p> </div> <div class="paragraph"> <p>--Mario</p> </div>]]></content><author><name>Fiore Mario Vitale</name></author><category term="community"/><category term="news"/><summary type="html"><![CDATA[Hi everyone, my name is Mario Fiore Vitale and I recently joined Red Hat and the Debezium team. I am a very curious person that follows a continuous learning approach, I like to keep growing my skills. I care about code quality and readability. I have about 9+ years of experience and have worked for consultancy, startup, and enterprise product companies in different sectors. In my previously experience I had the chance to work on architecture re-design project to split a monolith into a microservices application. During this experience I gained experience with different technologies such as Kafka, Elasticsearch, Redis, Kubernetes, VictoriaMetrics, Spring Framework, and a bit of Cassandra. Why Am I here?]]></summary></entry><entry><title type="html">Debezium 2.2.0.Alpha3 Released</title><link href="https://debezium.io/blog/2023/03/08/debezium-2-2-alpha3-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Alpha3 Released"/><published>2023-03-08T00:00:00+00:00</published><updated>2023-03-08T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/03/08/debezium-2-2-alpha3-released</id><content type="html" xml:base="https://debezium.io/blog/2023/03/08/debezium-2-2-alpha3-released/"><![CDATA[<div class="paragraph"> <p>Today, I am pleased to announce the third alpha release in the 2.2 release stream, Debezium <strong>2.2.0.Alpha3</strong>.</p> </div> <div class="paragraph"> <p>This release includes a plethora of bug fixes, improvements, breaking changes, and a number of new features including, but not limited to, optional parallel snapshots, server-side MongoDB change stream filtering, surrogate keys for incremental snapshots, a new Cassandra connector for Cassandra Enterprise, much more.</p> </div> <div class="paragraph"> <p>Let&#8217;s take moment and dive into some of these new features, improvements, and breaking changes.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>We typically try to avoid any breaking changes, even during minor releases such as this; however, sometimes breaking changes are inevitable given the circumstances. Debezium 2.2.0.Alpha3 includes one breaking change:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#zoned-datetime-truncation">PostgreSQL zoned date-time data types truncated</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="zoned-datetime-truncation">PostgreSQL zoned date-time data types truncated</h3> <div class="paragraph"> <p>It was identified (<a href="https://issues.redhat.com/browse/DBZ-6163">DBZ-6163</a>) that PostgreSQL timezone based column values that had a value of zero (<code>0</code>) for milli and micro second parts of a timezone based column were being serialized incorrectly where the string did not include neither the millisecond nor the microsecond portions of the time using zeroes.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This <strong>does not</strong> create any data loss!</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>What&#8217;s important to note is that prior to this release, when evaluating the values of such columns, consumers must be prepared to parse these string-based time values without the presence of a milli or microsecond value. In effect, this means events have an inconsistent pattern where some will have the milli and microsecond portions and others may not if their source value had 0 milliseconds or 0 microseconds.</p> </div> <div class="paragraph"> <p>These string-based time values will be be emitted consistently, padded with zeroes (<code>0</code>) for the milli and microsecond parts of the string-based time, even when the source value has neither milli nor microseconds.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="optional_parallel_snapshots">Optional parallel snapshots</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium&#8217;s relational database initial snapshot process has always been single-threaded. This limitation primarily stems from the complexities of ensuring data consistency across multiple transactions.</p> </div> <div class="paragraph"> <p>Starting in Debezium 2.2, we&#8217;re adding a new and initially optional way to utilize multiple threads to perform consistent database snapshot for a connector. This implementation uses these multiple threads to execute table-level snapshots in parallel.</p> </div> <div class="paragraph"> <p>In order to take advantage of this new feature, specify <code>snapshot.max.threads</code> in your connector&#8217;s configuration and when this property has a value greater than <code>1</code>, parallel snapshots will be used.</p> </div> <div class="listingblock"> <div class="title">Example configuration using parallel snapshots</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">snapshot.max.threads=4</code></pre> </div> </div> <div class="paragraph"> <p>In the example above, if the connector needs to snapshot more than 4 tables, there will be at most 4 tables being snapshot in parallel. When one thread finishes processing a table, it will get a new table to snapshot from the queue and the process continues until all tables have been snapshot.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This feature is considered <em>incubating</em>, but we strongly suggest that new connector deployments give this feature a try. We would welcome any and all feedback on how to improve this going forward.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="mongodb_server_side_change_stream_filtering">MongoDB server-side change stream filtering</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium presently subscribes to the MongoDB change stream and evaluates whether an event is of relevance or not on the connector side. On the surface, there is nothing technically wrong with this approach, it has worked well; however, a recent contributor explained how this decision impacts them.</p> </div> <div class="paragraph"> <p>Overall, the current process effectively serializes across the network all changes from MongoDB to the connector. If you have a lower volume of changes, you likely don&#8217;t see any issue with this approach; however, in a high volume scenario, especially when you&#8217;re only interested in a subset of the data generated by change streams, you quickly begin to see how this approach is inefficient. Furthermore, if you&#8217;re running the connector in a cloud environment like AWS, you&#8217;ll likely see in a high volume scenario where utilization costs could be impacted.</p> </div> <div class="paragraph"> <p>By moving where the include/exclude list filters are evaluated from the connector to the MongoDB server&#8217;s change stream subscription, this adds a number of advantages for all MongoDB connector users.</p> </div> <div class="paragraph"> <p>By reducing the number of events seen by connector, this impacts both network and CPU utilization. When events are sent that the connector simply discards due to include/exclude filters, this leads to network usage that could be avoided. When the connector is configured with full document or pre-image settings, this adds even more utilization to the network that is entirely unnecessary. Furthermore, by receiving more events than the connector configuration is interested in, this leads to the connector doing more processing, raising CPU utilization.</p> </div> <div class="paragraph"> <p>While network and CPU utilization are critical regardless of one&#8217;s environment, these are often more scrutinized when operating a cloud-based environments as these two metrics directly impact the operating budget. Users should see an overall lower network and CPU utilization with Debezium MongoDB 2.2 connectors.</p> </div> <div class="paragraph"> <p>We hope to share more details the benefits of this change in a future blog post, so stay tuned!</p> </div> </div> </div> <div class="sect1"> <h2 id="incremental_snapshot_surrogate_key_support">Incremental snapshot surrogate key support</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium&#8217;s incremental snapshot feature has been a tremendous success. It provides an efficient way to perform a consist snapshot of data that can be resumed, which is critical when the snapshot consists of large volumes of data.</p> </div> <div class="paragraph"> <p>However, incremental snapshots do have specific requirements that must be met before the feature can be used. One of those requirements is all tables being snapshot must use a primary key. You may ask, why does a table have no primary key, and we aren&#8217;t going to debate that here today; however, suffice to say this occurs more often than you may think.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, incremental snapshots can be performed on key-less tables as long as there is one column that is unique and can be considered a "surrogate key" for incremental snapshot purposes.</p> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>The surrogate key feature is not supported by MongoDB; only relational connectors.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>To provide the surrogate key column data in an incremental snapshot signal, the signal&#8217;s payload must include the new surrogate key attribute, <code>surrogate-key</code>.</p> </div> <div class="listingblock json"> <div class="title">An example incremental snapshot signal payload specifying a surrogate key</div> <div class="content"> <pre class="CodeRay highlight"><code>{
  "data-collections": [ "public.mytab" ],
  "surrogate-key": "customer_ref"
}</code></pre> </div> </div> <div class="paragraph"> <p>In the above example, an incremental snapshot will be started for table <code>public.mytab</code> and the incremental snapshot will use the <code>customer_ref</code> column as the primary key for generating the snapshot windows.</p> </div> <div class="admonitionblock warning"> <table> <tr> <td class="icon"> <i class="fa icon-warning" title="Warning"></i> </td> <td class="content"> <div class="paragraph"> <p>A surrogate key cannot be defined using multiple columns, only a <strong>single</strong> column.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>However, the surrogate key feature isn&#8217;t just applicable for tables with no primary keys. There are a series of advantages when using this feature with tables that have primary keys:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>One clear advantage is when the table&#8217;s primary key consists of multiple columns. The query generates a disjunction predicate for each column in the primary key, and it&#8217;s performance is highly dependent on the environment. Reducing the number of columns down to a single column often performs universally.</p> </li> <li> <p>Another advantage is when the surrogate key is based on a numeric data type while the primary key column is based on a character-based data type. Relational databases generally perform predicate evaluation more efficiently with numeric comparisons rather than character comparisons. By adjusting the query to use a numeric data type in this case, query performance could be better.</p> </li> </ol> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of other improvements, bug fixes, and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>When using <code>snapshot.collection.include.list</code>, relational schema isn&#8217;t populated correctly <a href="https://issues.redhat.com/browse/DBZ-3594">DBZ-3594</a></p> </li> <li> <p>Debezium UI should use fast-jar again with Quarkus 2.x <a href="https://issues.redhat.com/browse/DBZ-4621">DBZ-4621</a></p> </li> <li> <p>Create a Datastax connector based on Cassandra connector <a href="https://issues.redhat.com/browse/DBZ-5951">DBZ-5951</a></p> </li> <li> <p>Add support for honouring MongoDB read preference in change stream after promotion <a href="https://issues.redhat.com/browse/DBZ-5953">DBZ-5953</a></p> </li> <li> <p>Add support for header to all Debezium Server sinks <a href="https://issues.redhat.com/browse/DBZ-6017">DBZ-6017</a></p> </li> <li> <p>GCP Spanner connector start failing when there are multiple indexes on a single column <a href="https://issues.redhat.com/browse/DBZ-6101">DBZ-6101</a></p> </li> <li> <p>Negative remaining attempts on MongoDB reconnect case <a href="https://issues.redhat.com/browse/DBZ-6113">DBZ-6113</a></p> </li> <li> <p>Support String type for key in Mongo incremental snapshot <a href="https://issues.redhat.com/browse/DBZ-6116">DBZ-6116</a></p> </li> <li> <p>Tables with spaces or non-ASCII characters in their name are not captured by Oracle because they must be quoted. <a href="https://issues.redhat.com/browse/DBZ-6120">DBZ-6120</a></p> </li> <li> <p>Offsets are not advanced in a CDB deployment with low frequency of changes to PDB <a href="https://issues.redhat.com/browse/DBZ-6125">DBZ-6125</a></p> </li> <li> <p>Allow TestContainers test framework to expose ConnectorConfiguration as JSON <a href="https://issues.redhat.com/browse/DBZ-6136">DBZ-6136</a></p> </li> <li> <p>Oracle TIMESTAMP WITH TIME ZONE is emitted as GMT during snapshot rather than the specified TZ <a href="https://issues.redhat.com/browse/DBZ-6143">DBZ-6143</a></p> </li> <li> <p>Upgrade impsort-maven-plugin from 1.7.0 to 1.8.0 <a href="https://issues.redhat.com/browse/DBZ-6144">DBZ-6144</a></p> </li> <li> <p>Debezium UI E2E Frontend build failing randomly with corrupted Node 16 tar file <a href="https://issues.redhat.com/browse/DBZ-6146">DBZ-6146</a></p> </li> <li> <p>Debezium UI SQL Server tests randomly fail due to slow agent start-up <a href="https://issues.redhat.com/browse/DBZ-6149">DBZ-6149</a></p> </li> <li> <p>Upgrade Quarkus dependencies to 2.16.3.Final <a href="https://issues.redhat.com/browse/DBZ-6150">DBZ-6150</a></p> </li> <li> <p>Remove hardcoded list of system database exclusions that are not required for change streaming <a href="https://issues.redhat.com/browse/DBZ-6152">DBZ-6152</a></p> </li> <li> <p>RelationalSnapshotChangeEventSource swallows exception generated during snapshot <a href="https://issues.redhat.com/browse/DBZ-6179">DBZ-6179</a></p> </li> <li> <p>Create SSL scenarios for integration tests for MySQL connector <a href="https://issues.redhat.com/browse/DBZ-6184">DBZ-6184</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.Alpha3%20ORDER%20BY%20component%20ASC">33 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/btiernay">Bobby Tiernay</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/EugeneAbramchuk">Eugene Abramchuk</a>, <a href="https://github.com/ggaborg">Gabor Andras</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/MehmetFiratKomurcu">Mehmet Firat Komurcu</a>, <a href="https://github.com/PlugaruT">Plugaru Tudor</a>, <a href="https://github.com/smiklosovic">Stefan Miklosovic</a>, <a href="https://github.com/subodh1810">Subodh Kant Chaturvedi</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, and <a href="https://github.com/xinbinhuang">Xinbin Huang</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook &amp; What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>In addition, we are nearing the end of the Debezium 2.2 development cycle. Assuming no unexpected problems, we do intend to release Beta1 next week, followed by a release candidate two weeks thereafter. Our goal is to finalize the Debezium 2.2 release in late March or early April at the latest.</p> </div> <div class="paragraph"> <p>We would love to hear your feedback or suggestions about our roadmap, changes in this release, or any that are outstanding or that we may haven&#8217;t mentioned. Be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if there is.</p> </div> <div class="paragraph"> <p>Also, the DevNexus 2023 conference is coming up in early April in Atlanta, and I have the privilege to be a guest speaker discussing Debezium and CDC patterns. Be sure to check out that talk in person if you have an opportunity!</p> </div> <div class="paragraph"> <p>And finally, be on the lookout for our first installment of our 2023 Newsletter later this month. I also will be wrapping up the blog series, "Debezium for Oracle" where I cover performance, debugging, and frequently asked questions about the Oracle connector.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[Today, I am pleased to announce the third alpha release in the 2.2 release stream, Debezium 2.2.0.Alpha3. This release includes a plethora of bug fixes, improvements, breaking changes, and a number of new features including, but not limited to, optional parallel snapshots, server-side MongoDB change stream filtering, surrogate keys for incremental snapshots, a new Cassandra connector for Cassandra Enterprise, much more. Let&#8217;s take moment and dive into some of these new features, improvements, and breaking changes.]]></summary></entry><entry><title type="html">Debezium 2.2.0.Alpha2 Released</title><link href="https://debezium.io/blog/2023/02/16/debezium-2-2-alpha2-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Alpha2 Released"/><published>2023-02-16T00:00:00+00:00</published><updated>2023-02-16T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/02/16/debezium-2-2-alpha2-released</id><content type="html" xml:base="https://debezium.io/blog/2023/02/16/debezium-2-2-alpha2-released/"><![CDATA[<div class="paragraph"> <p>Today, I am pleased to announce the second alpha release in the 2.2 release stream, Debezium <strong>2.2.0.Alpha2</strong>. This release includes a plethora of bug fixes, improvements, breaking changes, and a number of new features including, but not limited to, a new <code>ExtractRecordChanges</code> single message transformation, a Reactive-based implementation of the Debezium Outbox extension for Quarkus, a Debezium Storage module for Apache RocketMQ, and much more. Let&#8217;s take moment and dive into these new features, improvements, and breaking changes.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_changes">Breaking Changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>We typically try to avoid any breaking changes, even during minor releases such as this; however, sometimes breaking changes are inevitable given the circumstances. Debezium 2.2.0.Alpha2 includes three breaking changes:</p> </div> <div class="ulist"> <ul> <li> <p><a href="#topic-schema-naming-changes">Topic and schema naming changes</a></p> </li> <li> <p><a href="#source-info-block-changes-oracle">Source info block changed for Oracle connector</a></p> </li> <li> <p><a href="#debezium-server-moved-to-new-repository">Debezium Server source code in new repository</a></p> </li> </ul> </div> <div class="sect2"> <h3 id="topic-schema-naming-changes">Topic / Schema naming changes</h3> <div class="paragraph"> <p>Debezium previously sanitized topic and schema names by using an underscore (<code>_</code>) to replace non-ASCII characters that would lead to unsupported topic or schema names when using schema registries. However, if this non-ASCII character was the only difference between two similar topics or schema names that otherwise only varied by case, this would lead to other problems.</p> </div> <div class="paragraph"> <p>In order to address this in the most compatible way, Debezium now uses a strategy-based approach to map characters uniquely. As a side effect of this change, the <code>sanitize.field.names</code> configuration property has been retired and replaced by this new strategy-based approach.</p> </div> <div class="paragraph"> <p>Each connector supports two configuration properties to control this behavior:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.name.adjustment.mode</code></dt> <dd> <p>Specifies how schema names should be adjusted for compatibility with the message converter.</p> </dd> <dt class="hdlist1"><code>field.name.adjustment.mode</code></dt> <dd> <p>Specifies how field names should be adjusted for compatibility with the message converter.</p> </dd> </dl> </div> <div class="paragraph"> <p>These two connector configuration properties support three modes:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>none</code></dt> <dd> <p>No adjustment is made to the schema or field names, passed as-is.</p> </dd> <dt class="hdlist1"><code>avro</code></dt> <dd> <p>Replaces characters that cannot be used in Avro with an underscore (<code>_</code>).</p> </dd> <dt class="hdlist1"><code>avro_unicode</code></dt> <dd> <p>Replaces underscores (<code>_</code>) and characters that cannot be used in Avro with unicode-based characters.</p> </dd> </dl> </div> <div class="paragraph"> <p>This now allows you to pick the most appropriate strategy based on your table or collection naming convention.</p> </div> </div> <div class="sect2"> <h3 id="source-info-block-changes-oracle">Source info block changes with Oracle connector</h3> <div class="paragraph"> <p>All Debezium change events related to inserts, updates, and deletes contain a <code>source</code> info block in the event&#8217;s payload. For the Oracle connector, this block contains a special field called <code>ssn</code> that represents the SQL sequence number for this change.</p> </div> <div class="paragraph"> <p>It has been identified that there were corner cases where the value sourced from the database for this field could exceed the maximum value of <code>2,147,483,647</code>, or the maximum value of an <code>INT32</code> data type. To fix this corner case, we&#8217;ve changed the data type from <code>INT32</code> to <code>INT64</code>, which allows up to a maximum value of <code>9,223,372,036,854,775,807</code>.</p> </div> <div class="paragraph"> <p>This change should be entirely non-invasive, but we wanted to bring attention to this should you have pipelines that could be storing this value in a sink system or if you are using a schema registry.</p> </div> </div> <div class="sect2"> <h3 id="debezium-server-moved-to-new-repository">Debezium Server moved to new repository</h3> <div class="paragraph"> <p>Debezium Server is a standalone Quarkus-based runtime for Debezium source connectors enabling the integration with various platforms like EventHubs, PubSub, Pulsar, Redis, and Kafka, to name a few. With this release, we have moved the code related to Debezium Server to its own <a href="https://www.github.com/debezium/debezium-server">GitHub repository</a>.</p> </div> <div class="paragraph"> <p>This change was required in order to support building Debezium Server to include connectors that are not part of the main Debezium repository, connectors such as Db2, Google Spanner, Cassandra 4, and Vitess. Therefore, this means that starting with this release, Debezium Server now ships with all connectors (excluding Cassandra 3) by default.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>Cassandra 3 is excluded due to some technical limitations with class loading that creates conflicts with Cassandra 4. We are aware of this and plan to deliver a solution to include Cassandra 3 in the future.</p> </div> </td> </tr> </table> </div> </div> </div> </div> <div class="sect1"> <h2 id="new_extractchangedrecordstate_smt">New ExtractChangedRecordState SMT</h2> <div class="sectionbody"> <div class="paragraph"> <p>We have heard from the community on several occasions that it would great to have an out-of-the-box way to determine what values have changed in a Debezium change event. The new single message transform (SMT) <code>ExtractChangedRecordState</code> aims to deliver on this request by adding metadata to the event identifying which fields changed or were unchanged.</p> </div> <div class="paragraph"> <p>In order to get started with this new transformation, configure it as part of your connector configuration:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">transforms=changes
transforms.changes.type=io.debezium.transforms.ExtractChangedRecordState
transforms.changes.header.changed=ChangedFields
transforms.changes.header.unchanged=UnchangedFields</code></pre> </div> </div> <div class="paragraph"> <p>This transformation can be configured to disclose either what fields changed by setting <code>header.changed</code>, what fields are unchanged by setting <code>header.unchanged</code>, or both by setting both properties as shown above. The transformation will add a new header with the specified name and it&#8217;s value will include a collection of field names based on whether you&#8217;ve configured changes, non-changes, or both.</p> </div> </div> </div> <div class="sect1"> <h2 id="drop_fields_using_extractnewrecordstate_smt">Drop fields using ExtractNewRecordState SMT</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <code>ExtractNewRecordState</code> single message transformation is extremely useful in situations where you need to consume the Debezium change event in a <em>flattened</em> format. This SMT has been changed in this release to add the ability to drop fields from the payload and the message key of the event.</p> </div> <div class="paragraph"> <p>This new feature introduces three new configuration properties for the transformation:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>drop.fields.header.name</code></dt> <dd> <p>The Kafka message header name to use for listing field names in the source message that are to be dropped.</p> </dd> <dt class="hdlist1"><code>drop.fields.from.key</code></dt> <dd> <p>Specifies whether to remove fields also from the key, defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>drop.fields.keep.schema.compatible</code></dt> <dd> <p>Specifies whether to remove fields that are only optional, defaults to <code>true</code>.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>When using Avro, schema compatibility is extremely important. This is why we opted to enforce schema compatibility by default. If a field is configured to be dropped but it is non-optional, the field will not be removed from the key nor the payload unless schema compatibility is disabled.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>These new configuration options allow for some exciting ways to manipulate change events. For example, to emit events with only changed fields, pairing the <code>ExtractNewRecordState</code> with the new <code>ExtractChangedRecordState</code> transformation makes this extremely simple and straightforward. An example configuration to only emit changed columns would look like the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">transforms=changes,extract
transforms.changes.type=io.debezium.transforms.ExtractChangedRecordState
transforms.changes.header.unchanged=UnchangedFields
transforms.extract.type=io.debezium.transforms.ExtractNewRecordState
transforms.extract.drop.fields.header.name=UnchangedFields</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration will explicitly not include unchanged fields from the event&#8217;s payload value. If a field in the key did not change, it will be unaffected because <code>drop.fields.from.key</code> was left as its default of <code>false</code>. And finally, if a field in the event&#8217;s payload is to be dropped because it did not change, but it&#8217;s not optional, it will continue to be included in the transformation&#8217;s output event to comply with schema compatibility.</p> </div> </div> </div> <div class="sect1"> <h2 id="reactive_debezium_outbox_quarkus_extension">Reactive Debezium Outbox Quarkus Extension</h2> <div class="sectionbody"> <div class="paragraph"> <p>The <a href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/">outbox pattern</a> is an approach that many microservices leverage to share data across microservice boundaries. We introduced the Debezium Outbox Quarkus Extension in Debezium 1.1 back in early 2020, and it has allowed Quarkus users to leverage the outbox pattern with ease using Debezium.</p> </div> <div class="paragraph"> <p>Thanks to <a href="https://github.com/ingmarfjolla">Ingmar Fjolla</a>, Debezium 2.2.0.Alpha2 includes a new reactive-based implementation of the Debezium Outbox Quarkus Extension. This new implementation is based on Vert.x and Hibernate Reactive, providing a fully asynchronous solution to the outbox pattern using Debezium.</p> </div> <div class="paragraph"> <p>This new extension will be included in the Quarkus Platform releases latter this quarter or early Q2, however, if you want to get started with it today, you can easily drop it directly into your project&#8217;s configuration using the following coordinates:</p> </div> <div class="listingblock"> <div class="title">Maven coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="xml"><span class="tag">&lt;dependency&gt;</span>
  <span class="tag">&lt;groupId&gt;</span>io.debezium<span class="tag">&lt;/groupId&gt;</span>
  <span class="tag">&lt;artifactId&gt;</span>debezium-quarkus-outbox-reactive<span class="tag">&lt;/artifactId&gt;</span>
  <span class="tag">&lt;version&gt;</span>2.2.0.Alpha2<span class="tag">&lt;/version&gt;</span>
<span class="tag">&lt;/dependency&gt;</span></code></pre> </div> </div> <div class="listingblock"> <div class="title">Gradle coordinates</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">io.debezium:debezium-quarkus-outbox-reactive:2.2.0.Alpha2</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="new_rocket_mq_schema_history_storage">New Rocket MQ Schema History Storage</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium&#8217;s new storage API has been a huge success over this past year. We initially started with our original file and Kafka based implementations for offset and schema history storage, but that has since grown to support storing schema history on other platforms such as Amazon S3 and Redis.</p> </div> <div class="paragraph"> <p>This release continues to expand on this by adding a new schema history storage implementation for Rocket MQ. In order to get started with storing your schema history into Rocket MQ, the <code>debezium-storage-rocketmq</code> dependency must first be on the classpath and accessible by the connector runtime.</p> </div> <div class="paragraph"> <p>Once the dependency exists, the only remaining step will be configuring the schema history connector configuration. The following example shows basic usage of the Rocket MQ schema history:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">schema.history.internal.rocketmq.topic=schema-history
schema.history.internal.rocketmq.name.srv.addr=172.17.15.2
schema.history.internal.rocketmq.acl.enabled=true
schema.history.internal.rocketmq.access.key=&lt;rocketmq-access-key&gt;
schema.history.internal.rocketmq.secret.key=&lt;rocketmq-secret-key&gt;
schema.history.internal.rocketmq.recovery.attempts=5
schema.history.internal.rocketmq.recovery.poll.interval.ms=1000
schema.history.internal.rocketmq.store.record.timeout.ms=2000</code></pre> </div> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.history.internal.rocketmq.topic</code></dt> <dd> <p>Specifies the topic name where the schema history will be stored.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.name.srv.addr</code></dt> <dd> <p>Specifies the service discovery service nameserver for Rocket MQ.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.acl.enabled</code></dt> <dd> <p>Specifies whether access control lists (ACLs) are enabled, defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.access.key</code></dt> <dd> <p>Specifies the Rocket MQ access key, required only if ACLs are enabled.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.secret.key</code></dt> <dd> <p>Specifies the Rocket MQ secret key, required only if ACLs are enabled.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.recovery.attempts</code></dt> <dd> <p>Specifies the number of sequential attempts that no data is returned before recovery completes.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.recovery.poll.interval.ms</code></dt> <dd> <p>Specifies the number of milliseconds for each poll attempt to recover the history.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.rocketmq.store.record.timeout.ms</code></dt> <dd> <p>Specifies the number of milliseconds for a write to Rocket MQ to complete before timing out.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of other improvements, bug fixes, and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Better control on debezium GTID usage <a href="https://issues.redhat.com/browse/DBZ-2296">DBZ-2296</a></p> </li> <li> <p>Data type conversion failed for mysql bigint <a href="https://issues.redhat.com/browse/DBZ-5798">DBZ-5798</a></p> </li> <li> <p>ActivateTracingSpan wrong timestamps reported <a href="https://issues.redhat.com/browse/DBZ-5827">DBZ-5827</a></p> </li> <li> <p>Unable to specify column or table include list if name contains a backslash \ <a href="https://issues.redhat.com/browse/DBZ-5917">DBZ-5917</a></p> </li> <li> <p>debezium-connector-cassandra 2.1.0.Alpha2 plugin can no longer run "out of the box" <a href="https://issues.redhat.com/browse/DBZ-5925">DBZ-5925</a></p> </li> <li> <p>MongoDB Incremental Snapshot not Working <a href="https://issues.redhat.com/browse/DBZ-5973">DBZ-5973</a></p> </li> <li> <p>Nullable columns marked with "optional: false" in DDL events <a href="https://issues.redhat.com/browse/DBZ-6003">DBZ-6003</a></p> </li> <li> <p>Upgrade to Quarkus 2.16.0.Final <a href="https://issues.redhat.com/browse/DBZ-6005">DBZ-6005</a></p> </li> <li> <p>Vitess: Handle the shard list difference between current db shards and persisted shards <a href="https://issues.redhat.com/browse/DBZ-6011">DBZ-6011</a></p> </li> <li> <p>Offsets are not flushed on connect offsets topic when encountering an error on Postgres connector <a href="https://issues.redhat.com/browse/DBZ-6026">DBZ-6026</a></p> </li> <li> <p>Unexpected format for TIME column: 8:00 <a href="https://issues.redhat.com/browse/DBZ-6029">DBZ-6029</a></p> </li> <li> <p>Oracle does not support compression/logging clauses after an LOB storage clause <a href="https://issues.redhat.com/browse/DBZ-6031">DBZ-6031</a></p> </li> <li> <p>debezium-server Pulsar support non-default tenant and namespace <a href="https://issues.redhat.com/browse/DBZ-6033">DBZ-6033</a></p> </li> <li> <p>Debezium is logging the full message along with the error <a href="https://issues.redhat.com/browse/DBZ-6037">DBZ-6037</a></p> </li> <li> <p>Improve resilience during internal schema history recovery from Kafka <a href="https://issues.redhat.com/browse/DBZ-6039">DBZ-6039</a></p> </li> <li> <p>Vitess: Support Mapping unsigned bigint mysql column type to long <a href="https://issues.redhat.com/browse/DBZ-6043">DBZ-6043</a></p> </li> <li> <p>Incremental snapshot sends the events from signalling DB to Kafka <a href="https://issues.redhat.com/browse/DBZ-6051">DBZ-6051</a></p> </li> <li> <p>Upgrade Kafka to 3.3.2 <a href="https://issues.redhat.com/browse/DBZ-6054">DBZ-6054</a></p> </li> <li> <p>Mask password in log statement <a href="https://issues.redhat.com/browse/DBZ-6064">DBZ-6064</a></p> </li> <li> <p>Loading Custom offset storage fails with Class not found error <a href="https://issues.redhat.com/browse/DBZ-6075">DBZ-6075</a></p> </li> <li> <p>Increase query.fetch.size default to something sensible above zero <a href="https://issues.redhat.com/browse/DBZ-6079">DBZ-6079</a></p> </li> <li> <p>SQL Server tasks fail if the number of databases is smaller than maxTasks <a href="https://issues.redhat.com/browse/DBZ-6084">DBZ-6084</a></p> </li> <li> <p>When using LOB support, an UPDATE against multiple rows can lead to inconsistent event data <a href="https://issues.redhat.com/browse/DBZ-6107">DBZ-6107</a></p> </li> <li> <p>Expose sequence field in CloudEvents message id <a href="https://issues.redhat.com/browse/DBZ-6089">DBZ-6089</a></p> </li> <li> <p>Reduce verbosity of skipped transactions if transaction has no events relevant to captured tables <a href="https://issues.redhat.com/browse/DBZ-6094">DBZ-6094</a></p> </li> <li> <p>Upgrade Kafka client to 3.4.0 <a href="https://issues.redhat.com/browse/DBZ-6102">DBZ-6102</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.Alpha2%20ORDER%20BY%20component%20ASC">53 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/bruth">Byron Ruth</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/gunnarmorling">Gunnar Morling</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/ingmarfjolla">Ingmar Fjolla</a>, <a href="https://github.com/ismailsimsek">Ismail Simsek</a>, <a href="https://github.com/jbarrieault">Jacob Barrieault</a>, <a href="https://github.com/sugarcrm-jgminder">Jacob Gminder</a>, <a href="https://github.com/jcechace">Jakub Cechacek</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/echatman-ias">Liz Chatman</a>, <a href="https://github.com/lokesh1729">Lokesh Sanapalli</a>, <a href="https://github.com/MartinMedek">Martin Medek</a>, <a href="https://github.com/roldanbob">Robert Roldan</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/sunxiaojian">Sun Xiao Jian</a>, <a href="https://github.com/chtitux">Thophile Helleboid</a>, <a href="https://github.com/Tideri-Tim2">Tim Loes</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/y5w">Yang Wu</a>, and <a href="https://github.com/zzzming">ming luo</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>We&#8217;re still very early in the development cycle of Debezium 2.2 and many other features are still in development, including:</p> </div> <div class="ulist"> <ul> <li> <p>Configurable signal channels, enabling users to send signals not only from a database table or a Kafka topic, but also from other means such as an HTTP endpoint, the file system, etc.</p> </li> <li> <p>The Debezium JDBC sink connector that supports native Debezium change events out-of-the-box, without requiring the use of the Event Flattening transformation.</p> </li> <li> <p>And a plethora of Debezium UI enhancements</p> </li> </ul> </div> <div class="paragraph"> <p>We are about middle way through the quarter and Debezium 2.2 will begin to enter beta phase very soon. We would love to hear your feedback or suggestions regarding the roadmap, changes in this release, those that are outstanding, or anything we haven&#8217;t mentioned. Be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if there is.</p> </div> <div class="paragraph"> <p>Also be on the lookout for our first installment of our 2023 Newsletter as well as the upcoming and conclusion to the blog series, "Debezium for Oracle" where I cover performance, debugging, and frequently asked questions about the Oracle connector.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[Today, I am pleased to announce the second alpha release in the 2.2 release stream, Debezium 2.2.0.Alpha2. This release includes a plethora of bug fixes, improvements, breaking changes, and a number of new features including, but not limited to, a new ExtractRecordChanges single message transformation, a Reactive-based implementation of the Debezium Outbox extension for Quarkus, a Debezium Storage module for Apache RocketMQ, and much more. Let&#8217;s take moment and dive into these new features, improvements, and breaking changes.]]></summary></entry><entry><title type="html">DDD Aggregates via CDC-CQRS Pipeline using Kafka &amp;amp; Debezium</title><link href="https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/" rel="alternate" type="text/html" title="DDD Aggregates via CDC-CQRS Pipeline using Kafka &amp;amp; Debezium"/><published>2023-02-04T00:00:00+00:00</published><updated>2023-02-04T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium</id><content type="html" xml:base="https://debezium.io/blog/2023/02/04/ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/"><![CDATA[<div class="paragraph"> <p>In this post, we are going to talk about a CDC-CQRS pipeline between a normalized relational database, MySQL, as the command database and a de-normalized NoSQL database, MongoDB, as the query database resulting in the creation of DDD Aggregates via Debezium &amp; Kafka-Streams.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>You can find the complete source code of the example <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline">here</a>. Refer to the <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline/blob/master/README.md">README.md</a> for details on building and running the example code.</p> </div> <div class="paragraph"> <p>The example is centered around three microservices: <code>order-write-service</code>, <code>order-aggregation-service</code> and <code>order-read-service</code>. These services are implemented as Spring-Boot applications in Java.</p> </div> <div class="paragraph"> <p>The <code>order-write-service</code> exposes two REST endpoints which persist shipping-details and item-details in their respective tables on MySQL database. Debezium tails the MySQL bin logs to capture any events in both these tables and publishes messages to Kafka topics. These topics are consumed by <code>order-aggregation-service</code> which is a Kafka-Streams application that joins data from both of these topics to create an Order-Aggregate object which is then published to a third topic. This topic is consumed by MongoDB Sink Connector and the data is persisted in MongoDB which is served by <code>order-read-service</code>.</p> </div> <div class="paragraph"> <p>The overall architecture of the solution can be seen in the following diagram:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/design_overview.png" style="max-width:90%;" class="responsive-image"> </div> </div> <div class="sect1"> <h2 id="rest_application_order_write_service">REST Application: order-write-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The first component that triggers the workflow starts is the <code>order-write-service</code>. This has been implemented as a Spring-Boot application and exposes two REST end-points:</p> </div> <div class="ulist"> <ul> <li> <p>POST: <code>api/shipping-details</code> to persist shipping details in the MySQL database</p> </li> <li> <p>POST: <code>api/item-details</code> to persist item details in the MySQL database</p> </li> </ul> </div> <div class="paragraph"> <p>Both of these endpoints persist their data in their respective tables in the MySQL database.</p> </div> </div> </div> <div class="sect1"> <h2 id="command_database_mysql">Command Database: MySQL</h2> <div class="sectionbody"> <div class="paragraph"> <p>The backend processing of the above-mentioned REST endpoints culminates in persisting the data in their respective tables in MySQL.</p> </div> <div class="paragraph"> <p>Shipping details are stored in a table called <code>SHIPPING_DETAILS</code>. And Item details are stored in a table called <code>ITEM_DETAILS</code>.</p> </div> <div class="paragraph"> <p>Here is the data-model of <code>SHIPPING_DETAILS</code> table, the column <code>ORDER_ID</code> is its primary key:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/shipping_details_data_model.png" style="max-width:100%;" class="responsive-image"> </div> </div> <div class="paragraph"> <p>Here is the data-model of <code>ITEM_DETAILS</code> table, the column <code>ORDER_ID</code> + <code>ITEM_ID</code> is its primary key:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/item_details_data_model.png" style="max-width:100%;" class="responsive-image"> </div> </div> </div> </div> <div class="sect1"> <h2 id="kafka_connect_source_connector_mysql_cdc_debezium">Kafka-Connect Source Connector: MySQL CDC Debezium</h2> <div class="sectionbody"> <div class="paragraph"> <p>Change Data Capture (CDC) is a solution that captures change events from a database transaction log (called BinLogs in the case of MySQL) and forwards those events to downstream consumers ex. Kafka topic.</p> </div> <div class="paragraph"> <p>Debezium is a platform that provides a low latency data streaming platform for change data capture (CDC) and is built on top of Apache Kafka. It allows database row-level changes to be captured as events and published to Apache Kafka topics. We setup and configure Debezium to monitor our databases, and then our applications consume events for each row-level change made to the database.</p> </div> <div class="paragraph"> <p>In our case, we will be using Debezium MySQL Source connector to capture any new events in the aforementioned tables and relay them to Apache Kafka. To achieve this, we will be registering our connecter by POST-ing the following JSON request to the REST API of Kafka Connect:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db-connector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.mysql.MySqlConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mysql_db_server</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">3306</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">custom_mysql_user</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">custom_mysql_user_password</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.id</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">184054</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-server</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.whitelist</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">table.whitelist</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mysql-db.shipping_details,app-mysql-db.item_details</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.bootstrap.servers</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">kafka_server:29092</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database.history.kafka.topic</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbhistory.app-mysql-db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">include.schema.changes</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">unwrap</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.unwrap.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.transforms.ExtractNewRecordState</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The above configuration is based on Debezium 1.9.5.Final. Be aware that if you attempt to use the demo with Debezium 2.0+, a number of the above configuration properties have new names and the configuration will require some adjustments.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>This sets up an instance of <code>io.debezium.connector.mysql.MySqlConnector</code>, capturing changes from the specified MySQL instance. Note that by means of a table include list, only changes from the <code>SHIPPING_DETAILS</code> and <code>ITEM_DETAILS</code> tables are captured. It also applies a single message transform (SMT) named <code>ExtractNewRecordState</code> which extracts the <code>after</code> field from a Debezium change event in a Kafka record. The SMT replaces the original change event with only its <code>after</code> field to create a simple Kafka record.</p> </div> <div class="paragraph"> <p>By default, the Kafka topic name is serverName.schemaName.tableName which as per our connector configuration translates to:</p> </div> <div class="ulist"> <ul> <li> <p><code>app-mysql-server.app-mysql-db.item_details</code></p> </li> <li> <p><code>app-mysql-server.app-mysql-db.shipping_details</code></p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="kafka_streams_application_order_aggregation_service">Kafka-Streams Application: order-aggregation-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Kafka-Streams application, namely <code>order-aggregation-service</code>, is going to process data from the two Kafka cdc-topics. These topics receive CDC events based on the shipping-details and item-details relations found in MySQL.</p> </div> <div class="paragraph"> <p>With that in place, the KStreams topology to create and maintain DDD order-aggregates on-the-fly can be built as follows.</p> </div> <div class="paragraph"> <p>The application reads the data from the shipping-details-cdc-topic. Since the Kafka topic records are in Debezium JSON format with unwrapped envelopes we need to parse the order-id and the shipping-details from it to create a KTable with order-id as the key and shipping-details as the value.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Shipping Details Read</span>
KStream&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; shippingDetailsSourceInputKStream = streamsBuilder.stream(shippingDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE));

<span class="comment">// Change the Json value of the message to ShippingDetailsDto</span>
KStream&lt;<span class="predefined-type">String</span>, ShippingDetailsDto&gt; shippingDetailsDtoWithKeyAsOrderIdKStream = shippingDetailsSourceInputKStream
                        .map((orderIdJson, shippingDetailsJson) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(parseOrderId(orderIdJson), parseShippingDetails(shippingDetailsJson)));

<span class="comment">// Convert KStream to KTable</span>
KTable&lt;<span class="predefined-type">String</span>, ShippingDetailsDto&gt; shippingDetailsDtoWithKeyAsOrderIdKTable = shippingDetailsDtoWithKeyAsOrderIdKStream.toTable(
                        Materialized.&lt;<span class="predefined-type">String</span>, ShippingDetailsDto, KeyValueStore&lt;Bytes, <span class="type">byte</span><span class="type">[]</span>&gt;&gt;as(SHIPPING_DETAILS_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(SHIPPING_DETAILS_DTO_SERDE));</code></pre> </div> </div> <div class="paragraph"> <p>Similarly, the application reads the data from the item-details-cdc-topic and parses the order-id and the item from each individual message to group-by all the items pertaining to the same order-id in one list which is then aggregated to a KTable with order-id as key and the list of items pertaining to that specific order-id as value.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Item Details Read</span>
KStream&lt;<span class="predefined-type">String</span>, <span class="predefined-type">String</span>&gt; itemDetailsSourceInputKStream = streamsBuilder.stream(itemDetailsTopicName, Consumed.with(STRING_SERDE, STRING_SERDE));

<span class="comment">// Change the Key of the message from ItemId + OrderId to only OrderId and parse the Json value to ItemDto</span>
KStream&lt;<span class="predefined-type">String</span>, ItemDto&gt; itemDtoWithKeyAsOrderIdKStream = itemDetailsSourceInputKStream
                        .map((itemIdOrderIdJson, itemDetailsJson) -&gt; <span class="keyword">new</span> KeyValue&lt;&gt;(parseOrderId(itemIdOrderIdJson), parseItemDetails(itemDetailsJson)));

<span class="comment">// Group all the ItemDtos for each OrderId</span>
KGroupedStream&lt;<span class="predefined-type">String</span>, ItemDto&gt; itemDtoWithKeyAsOrderIdKGroupedStream = itemDtoWithKeyAsOrderIdKStream.groupByKey(Grouped.with(STRING_SERDE, ITEM_DTO_SERDE));

<span class="comment">// Aggregate all the ItemDtos pertaining to each OrderId in a list</span>
KTable&lt;<span class="predefined-type">String</span>, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;&gt; itemDtoListWithKeyAsOrderIdKTable = itemDtoWithKeyAsOrderIdKGroupedStream.aggregate(
                (Initializer&lt;<span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;&gt;) <span class="predefined-type">ArrayList</span>::<span class="keyword">new</span>,
                (orderId, itemDto, itemDtoList) -&gt; addItemToList(itemDtoList, itemDto),
                Materialized.&lt;<span class="predefined-type">String</span>, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;, KeyValueStore&lt;Bytes, <span class="type">byte</span><span class="type">[]</span>&gt;&gt;as(ITEM_DTO_STATE_STORE).withKeySerde(STRING_SERDE).withValueSerde(ITEM_DTO_ARRAYLIST_SERDE));</code></pre> </div> </div> <div class="paragraph"> <p>With both the KTables having order-id as the key, its easy enough to join them using order-id to create an aggregate called Order-Aggregate. Order-Aggregate is a composite object created by assimilating data from both the shipping-details as well as the item-details. This Order-Aggregate is then written to an order-aggregate Kafka topic.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="java"><span class="comment">// Joining the two tables: shippingDetailsDtoWithKeyAsOrderIdKTable and itemDtoListWithKeyAsOrderIdKTable</span>
ValueJoiner&lt;ShippingDetailsDto, <span class="predefined-type">ArrayList</span>&lt;ItemDto&gt;, OrderAggregate&gt; shippingDetailsAndItemListJoiner = (shippingDetailsDto, itemDtoList) -&gt; instantiateOrderAggregate(shippingDetailsDto, itemDtoList);
KTable&lt;<span class="predefined-type">String</span>, OrderAggregate&gt; orderAggregateKTable = shippingDetailsDtoWithKeyAsOrderIdKTable.join(itemDtoListWithKeyAsOrderIdKTable, shippingDetailsAndItemListJoiner);

<span class="comment">// Outputting to Kafka Topic</span>
orderAggregateKTable.toStream().to(orderAggregateTopicName, Produced.with(STRING_SERDE, ORDER_AGGREGATE_SERDE));</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="kafka_connect_sink_connector_mongodb_connector">Kafka-Connect Sink Connector: MongoDB Connector</h2> <div class="sectionbody"> <div class="paragraph"> <p>The sink connector is a Kafka Connect connector that reads data from Apache Kafka and writes data to some data-store. Using a MongoDB sink connector, it is easy to have the DDD aggregates written into MongoDB. All it needs is a configuration which can be posted to the REST API of Kafka Connect in order to run the connector.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">app-mongo-sink-connector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">com.mongodb.kafka.connect.MongoSinkConnector</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">topics</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order_aggregate</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">connection.uri</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">mongodb://root_mongo_user:root_mongo_user_password@mongodb_server:27017</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">key.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.storage.StringConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.json.JsonConverter</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">value.converter.schemas.enable</span><span class="delimiter">&quot;</span></span>: <span class="value">false</span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">database</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order_db</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">collection</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">document.id.strategy.overwrite.existing</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">true</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">document.id.strategy</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">com.mongodb.kafka.connect.sink.processor.id.strategy.ProvidedInKeyStrategy</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">hk,hv</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hk.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.transforms.HoistField$Key</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hk.field</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">_id</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hv.type</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">org.apache.kafka.connect.transforms.HoistField$Value</span><span class="delimiter">&quot;</span></span>,
        <span class="key"><span class="delimiter">&quot;</span><span class="content">transforms.hv.field</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">order</span><span class="delimiter">&quot;</span></span>
    }
}</code></pre> </div> </div> </div> </div> <div class="sect1"> <h2 id="query_database_mongodb">Query Database: MongoDB</h2> <div class="sectionbody"> <div class="paragraph"> <p>The DDD aggregate is written to the database <code>order_db</code> in the collection <code>order</code> on MongoDB. The order-id becomes the <code>_id</code> of the table and the <code>order</code> column stores the order-aggregate as JSON.</p> </div> </div> </div> <div class="sect1"> <h2 id="rest_application_order_read_service">REST Application: order-read-service</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Order Aggregate persisted in MongoDB is served via a REST endpoint in <code>order-read-service</code>.</p> </div> <div class="ulist"> <ul> <li> <p>GET: <code>api/order/{order-id}</code> to retrieve the order from the MongoDB database</p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="execution_instructions">Execution Instructions</h2> <div class="sectionbody"> <div class="paragraph"> <p>The complete source code for this blog post is provided <a href="https://github.com/purnima-jain/cdc-cqrs-pipeline">here</a> in Github. Begin by cloning this repository and changing into the <code>cdc-cqrs-pipeline</code> directory. The project provides a Docker Compose file with services for all the components:</p> </div> <div class="ulist"> <ul> <li> <p>MySQL</p> </li> <li> <p>Adminer (formerly known as phpMinAdmin), to manage MySQL via browser</p> </li> <li> <p>MongoDB</p> </li> <li> <p>Mongo Express, to manage MongoDB via browser</p> </li> <li> <p>Zookeeper</p> </li> <li> <p>Confluent Kafka</p> </li> <li> <p>Kafka Connect</p> </li> </ul> </div> <div class="paragraph"> <p>Once all services have started, register an instance of the Debezium MySQL connector &amp; MongoDB Connector by executing the <code>Create-MySQL-Debezium-Connector</code> and <code>Create-MongoDB-Sink-Connector</code> request respectively from <code>cdc-cqrs-pipeline.postman_collection.json</code>. Execute the request <code>Get-All-Connectors</code> to verify that the connectors have been properly created.</p> </div> <div class="paragraph"> <p>Change into the individual directories and spin-up the three Spring-Boot applications:</p> </div> <div class="ulist"> <ul> <li> <p><code>order-write-service</code>: runs on port no <code>8070</code></p> </li> <li> <p><code>order-aggregation-service</code>: runs on port no <code>8071</code></p> </li> <li> <p><code>order-read-service</code>: runs on port no <code>8072</code></p> </li> </ul> </div> <div class="paragraph"> <p>With this, our setup is complete.</p> </div> <div class="paragraph"> <p>To test the application, execute the request <code>Post-Shipping-Details</code> from the postman collection to insert shipping-details and <code>Post-Item-Details</code> to insert item-details for a particular order id.</p> </div> <div class="paragraph"> <p>Finally, execute the <code>Get-Order-By-Order-Id</code> request in the postman collection to retrieve the complete Order Aggregate.</p> </div> </div> </div> <div class="sect1"> <h2 id="summary">Summary</h2> <div class="sectionbody"> <div class="paragraph"> <p>Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services. Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services. If for instance single components of the solution fail or are not available for some time, events will simply be processed later on: after a restart, the Debezium connector will continue to tail the relevant tables from the point where it left off before. Similarly, any consumer will continue to process topics from its previous offset. By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.</p> </div> <div class="paragraph"> <p>Naturally, such event pipeline between different services is eventually consistent, i.e. consumers such as the order-read-service may lag a bit behind producers such as the order-write-service. Usually, thats just fine, though, and can be handled in terms of the applications business logic. Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range), thanks to log-based change data capture which allows for emission of events in near-realtime.</p> </div> </div> </div>]]></content><author><name>Purnima Jain</name></author><category term="ddd"/><category term="cdc"/><category term="cqrs"/><category term="debezium"/><category term="kafka"/><summary type="html"><![CDATA[In this post, we are going to talk about a CDC-CQRS pipeline between a normalized relational database, MySQL, as the command database and a de-normalized NoSQL database, MongoDB, as the query database resulting in the creation of DDD Aggregates via Debezium &amp; Kafka-Streams.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://debezium.io/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/design_overview.png"/><media:content medium="image" url="https://debezium.io/assets/images/2023-02-04-ddd-aggregates-via-cdc-cqrs-pipeline-using-kafka-and-debezium/design_overview.png" xmlns:media="http://search.yahoo.com/mrss/"/></entry><entry><title type="html">Debezium 2.1.2.Final Released</title><link href="https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/" rel="alternate" type="text/html" title="Debezium 2.1.2.Final Released"/><published>2023-01-26T00:00:00+00:00</published><updated>2023-01-26T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released</id><content type="html" xml:base="https://debezium.io/blog/2023/01/26/debezium-2-1-2-final-released/"><![CDATA[<div class="paragraph"> <p>The Debezium release cadence is in full swing as I&#8217;m excited to announce Debezium <strong>2.1.2.Final</strong>!</p> </div> <div class="paragraph"> <p>This release focuses primarily on bug fixes and stability; and it is the recommended update for all users from earlier versions. This release contains <a href="https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+2.1.2.Final">28 resolved issues</a>, so let&#8217;s take a moment and discuss a critical breaking change.</p> </div> <div class="paragraph"> <p></p> </div> <div class="sect1"> <h2 id="breaking_change">Breaking Change</h2> <div class="sectionbody"> <div class="paragraph"> <p>An edge case was reported in <a href="https://issues.redhat.com/browse/issues/DBZ-5996">DBZ-5996</a> where if a temporal column used <code>ZonedTimestamp</code> and if the column&#8217;s value had <code>0</code> micro or nanoseconds, rather than emitting the value as <code>2023-01-19T12:30:00.123000Z</code>, the value would be emitted in a truncated way as <code>2023-01-19T12:30:00.123Z</code>. This could lead to other issues with converters used in the event pipeline when the output from that column could be formatted inconsistently.</p> </div> <div class="paragraph"> <p>In order to remedy the edge case, the <code>ZonedTimestamp</code> implementation will now pad the fraction-based seconds value of the column&#8217;s value to the length/scale of the source database column. Using the example above of a <code>TIMESTAMP(6)</code> MySQL column type, the emitted value will now properly reflect a value of <code>2023-01-19T12:30:00.123000Z</code>.</p> </div> <div class="paragraph"> <p>While this change in behavior is likely to have minimal impact to most users, we wanted to bring attention to it in the event that you&#8217;ve perhaps used other means to handle this edge case in your pipelines. If you have, you should be able to rely on Debezium to emit the value consistently, even when the fraction-based seconds is <code>0</code>.</p> </div> </div> </div> <div class="sect1"> <h2 id="other_changes">Other changes</h2> <div class="sectionbody"> <div class="paragraph"> <p>A few noteworthy bug fixes and stability improvements include:</p> </div> <div class="ulist"> <ul> <li> <p>Data type conversion failed for mysql bigint <a href="https://issues.redhat.com/browse/DBZ-5798">DBZ-5798</a></p> </li> <li> <p>Oracle cannot undo change <a href="https://issues.redhat.com/browse/DBZ-5907">DBZ-5907</a></p> </li> <li> <p>Truncate records incompatible with ExtractNewRecordState <a href="https://issues.redhat.com/browse/DBZ-5966">DBZ-5966</a></p> </li> <li> <p>Computed partition must not be negative <a href="https://issues.redhat.com/browse/DBZ-5967">DBZ-5967</a></p> </li> <li> <p>NPE in execute snapshot signal with exclude.tables config on giving wrong table name <a href="https://issues.redhat.com/browse/DBZ-5988">DBZ-5988</a></p> </li> <li> <p>There is a problem with postgresql connector parsing the boundary value of money type <a href="https://issues.redhat.com/browse/DBZ-5991">DBZ-5991</a></p> </li> <li> <p>Nullable columns marked with "optional: false" in DDL events <a href="https://issues.redhat.com/browse/DBZ-6003">DBZ-6003</a></p> </li> <li> <p>Vitess: Handle the shard list difference between current db shards and persisted shards <a href="https://issues.redhat.com/browse/DBZ-6011">DBZ-6011</a></p> </li> <li> <p>Postgres LSN check should honor event.processing.failure.handling.mode <a href="https://issues.redhat.com/browse/DBZ-6012">DBZ-6012</a></p> </li> <li> <p>Enhance the Spanner connector by adding features and/or solving bugs <a href="https://issues.redhat.com/browse/DBZ-6014">DBZ-6014</a></p> </li> <li> <p>DDL statement with TokuDB engine specific "CLUSTERING KEY" couldn&#8217;t be parsed <a href="https://issues.redhat.com/browse/DBZ-6016">DBZ-6016</a></p> </li> <li> <p>DDL parse fail for role revoke with "user-like" role name <a href="https://issues.redhat.com/browse/DBZ-6019">DBZ-6019</a></p> </li> <li> <p>DDL parse fail for ALTER USER x DEFAULT ROLE y; <a href="https://issues.redhat.com/browse/DBZ-6020">DBZ-6020</a></p> </li> <li> <p>Offsets are not flushed on connect offsets topic when encountering an error on Postgres connector <a href="https://issues.redhat.com/browse/DBZ-6026">DBZ-6026</a></p> </li> <li> <p>Unexpected format for TIME column: 8:00 <a href="https://issues.redhat.com/browse/DBZ-6029">DBZ-6029</a></p> </li> <li> <p>Oracle does not support compression/logging clauses after an LOB storage clause <a href="https://issues.redhat.com/browse/DBZ-6031">DBZ-6031</a></p> </li> <li> <p>Debezium is logging the full message along with the error <a href="https://issues.redhat.com/browse/DBZ-6037">DBZ-6037</a></p> </li> <li> <p>Improve resilience during internal schema history recovery from Kafka <a href="https://issues.redhat.com/browse/DBZ-6039">DBZ-6039</a></p> </li> </ul> </div> <div class="paragraph"> <p>Please refer to the <a href="/releases/2.1/release-notes#release-2.1.2-final">release notes</a> to learn more about all fixed bugs, update procedures, etc.</p> </div> <div class="paragraph"> <p>Many thanks to the following individuals from the community who contributed to Debezium 2.1.2.Final: Akshansh Jain, <a href="https://github.com/akanimesh7">Animesh Kumar</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/HenryCaiHaiying">Henry Cai</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/Lucascanna">Luca Scannapieco</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/morozov">Sergei Morozov</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, and <a href="https://github.com/yoheimuta">yohei yoshimuta</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="outlook_whats_next">Outlook, What&#8217;s next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium 2.1 will continue to receive bug fix and maintenance changes throughout this quarter. I expect there will be at least one additional release likely toward the middle or late February or March timeframe as we begin to wrap up up the work on Debezium 2.2.</p> </div> <div class="paragraph"> <p>Regarding Debezium 2.2, we intend to deliver another Alpha build in the coming weeks. We have lots of features still in the works, including the JDBC Sink Connector, configurable signal channels, new message transformations, and much more.</p> </div> <div class="paragraph"> <p>Stay tuned, and until then &#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[The Debezium release cadence is in full swing as I&#8217;m excited to announce Debezium 2.1.2.Final! This release focuses primarily on bug fixes and stability; and it is the recommended update for all users from earlier versions. This release contains 28 resolved issues, so let&#8217;s take a moment and discuss a critical breaking change.]]></summary></entry><entry><title type="html">We Are Hiring (Saga continues)</title><link href="https://debezium.io/blog/2023/01/24/we-are-hiring-2/" rel="alternate" type="text/html" title="We Are Hiring (Saga continues)"/><published>2023-01-24T00:00:00+00:00</published><updated>2023-01-24T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/24/we-are-hiring-2</id><content type="html" xml:base="https://debezium.io/blog/2023/01/24/we-are-hiring-2/"><![CDATA[<div class="paragraph"> <p>In November last year, we <a href="/blog/2022/11/15/filling-the-ranks/">announced</a> we were looking for reinforcements for the team. And I have two pieces of news for you today: a good one and an even better one.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>The good news is that we will have a new pair of hands joining the team soon. It is still early to share more details, but we are happy to see the new guy with us!</p> </div> <div class="paragraph"> <p>How about the better news? Well, we can have one more person! The process has changed a bit since the last time, but everything else holds! So if you are interested then please apply via the Red Hat <a href="https://global-redhat.icims.com/jobs/97420/principal-software-engineer---openshift-connectors/job">job portal</a>.</p> </div> <div class="paragraph"> <p>Dont be shy, and dont underestimate yourself. We would rather speak to more people than miss you!</p> </div>]]></content><author><name>Jiri Pechanec</name></author><category term="community"/><category term="hiring"/><summary type="html"><![CDATA[In November last year, we announced we were looking for reinforcements for the team. And I have two pieces of news for you today: a good one and an even better one.]]></summary></entry><entry><title type="html">Debezium 2.2.0.Alpha1 Released</title><link href="https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/" rel="alternate" type="text/html" title="Debezium 2.2.0.Alpha1 Released"/><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released</id><content type="html" xml:base="https://debezium.io/blog/2023/01/19/debezium-2-2-alpha1-released/"><![CDATA[<div class="paragraph"> <p>It&#8217;s my pleasure to announce not only the first release of the Debezium 2.2 series, but also the first release of Debezium in 2023, <strong>2.2.0.Alpha</strong>!</p> </div> <div class="paragraph"> <p>The Debezium 2.2.0.Alpha1 release includes some breaking changes, a number of bug fixes, and some noteworthy improvements and features, including but not limited to:</p> </div> <div class="ulist"> <ul> <li> <p>[Breaking Change] - <code>ZonedTimestamp</code> values will no longer truncate fractional seconds.</p> </li> <li> <p>[New] - Support ingesting changes from an Oracle logical stand-by database</p> </li> <li> <p>[New] - Support Amazon S3 buckets using the Debezium Storage API</p> </li> <li> <p>[New] - Support retrying database connections during connector start-up</p> </li> <li> <p>[New] - Debezium Server sink connector support for Apache RocketMQ and Infinispan</p> </li> </ul> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Let&#8217;s take a few moments and dive into some of the features in more detail!</p> </div> <div class="sect1"> <h2 id="breaking_change">Breaking Change</h2> <div class="sectionbody"> <div class="paragraph"> <p>An edge case was reported in <a href="https://issues.redhat.com/browse/issues/DBZ-5996">DBZ-5996</a> where if a temporal column used <code>ZonedTimestamp</code> and if the column&#8217;s value had <code>0</code> micro or nanoseconds, rather than emitting the value as <code>2023-01-19T12:30:00.123000Z</code>, the value would be emitted in a truncated way as <code>2023-01-19T12:30:00.123Z</code>. This could lead to other issues with converters used in the event pipeline when the output from that column could be formatted inconsistently.</p> </div> <div class="paragraph"> <p>In order to remedy the edge case, the <code>ZonedTimestamp</code> implementation will now pad the fraction-based seconds value of the column&#8217;s value to the length/scale of the source database column. Using the example above of a <code>TIMESTAMP(6)</code> MySQL column type, the emitted value will now properly reflect a value of <code>2023-01-19T12:30:00.123000Z</code>.</p> </div> <div class="paragraph"> <p>While this change in behavior is likely to have minimal impact to most users, we wanted to bring attention to it in the event that you&#8217;ve perhaps used other means to handle this edge case in your pipelines. If you have, you should be able to rely on Debezium to emit the value consistently, even when the fraction-based seconds is <code>0</code>.</p> </div> </div> </div> <div class="sect1"> <h2 id="ingesting_changes_from_oracle_logical_stand_bys">Ingesting changes from Oracle logical stand-bys</h2> <div class="sectionbody"> <div class="paragraph"> <p>The Debezium for Oracle connector normally manages what is called a <em>flush table</em>, which is an internal table used to manage the flush cycles used by the Oracle Log Writer Buffer (LGWR) process. This flushing process requires that the user account the connector uses to have permission to create and write to this table. Logical stand-by databases often have more restrictive rules about data manipulation and may even be read-only, therefore, writing to the database is unfavorable or even not permissible.</p> </div> <div class="paragraph"> <p>To support an Oracle read-only logical stand-by database, we introduced a flag to disable the creation and management of this <em>flush table</em>. This feature can be used with both Oracle Standalone and Oracle RAC installations, and is currently considered incubating, meaning its subject to change in the future.</p> </div> <div class="paragraph"> <p>In order to enable Oracle read-only logical stand-by support, add the following connector option:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">internal.log.mining.read.only=true</code></pre> </div> </div> <div class="paragraph"> <p>In a future version, we plan to add support for an Oracle read-only physical stand-by database.</p> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>This configuration option is prefixed with <code>internal.</code>, meaning that it&#8217;s considered an undocumented and experimental feature. The semantics and behavior of this option are subject to change in future versions that may not be guaranteed forward or backward compatible.</p> </div> </td> </tr> </table> </div> </div> </div> <div class="sect1"> <h2 id="using_amazon_s3_buckets_with_storage_api">Using Amazon S3 buckets with Storage API</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium provides a Storage API framework that enables connectors to store offset and schema history state in a variety of persistence datastores. Moreover, the framework enables contributors to extend the API by adding new storage implementations with ease. Currently, the Storage API framework supports the local FileSystem, a Kafka Topic, or Redis datastores.</p> </div> <div class="paragraph"> <p>With Debezium 2.2, we&#8217;re pleased to add Amazon S3 buckets as part of that framework, allowing the schema history to be persisted to an S3 bucket. An example connector configuration using S3 might look like the following:</p> </div> <div class="listingblock properties"> <div class="content"> <pre class="CodeRay highlight"><code>...
schema.history.internal=io.debezium.storage.s3.history
schema.history.internal.s3.access.key.id=aa
schema.history.internal.s3.secret.access.key=bb
schema.history.internal.s3.region.name=aws-global
schema.history.internal.s3.bucket.name=debezium
schema.history.internal.s3.object.name=db-history.log
schema.history.internal.s3.endpoint=http://&lt;server&gt;:&lt;port&gt;</code></pre> </div> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>schema.history.internal.s3.access.key.id</code></dt> <dd> <p>Specifies the access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.secret.access.key</code></dt> <dd> <p>Specifies the secret access key required to authenticate to S3.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.region.name</code></dt> <dd> <p>Specifies the region where the S3 bucket is available.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.bucket.name</code></dt> <dd> <p>Specifies the name of the S3 bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.object.name</code></dt> <dd> <p>Specifies the object name in the bucket where the schema history is to be persisted.</p> </dd> <dt class="hdlist1"><code>schema.history.internal.s3.endpoint</code></dt> <dd> <p>Specifies the S3 endpoint with the format of <code><a href="http://&lt;server&gt;:&lt;port&gt" class="bare">http://&lt;server&gt;:&lt;port&gt</a>;</code>.</p> </dd> </dl> </div> </div> </div> <div class="sect1"> <h2 id="retry_database_connections_on_start_up">Retry database connections on start-up</h2> <div class="sectionbody"> <div class="paragraph"> <p>In previous releases of Debezium, the connector start-up phase used a fail-fast strategy. Simply put, this meant that if we couldn&#8217;t connect, authenticate, or performs any of the start-up phase steps required by the connector, the connector would enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>One specific problem area for users is if the connector gracefully starts, runs for a period of time, and then eventually encounters some fatal error. If the error is related to a resource that wasn&#8217;t accessed during the connector&#8217;s start-up lifecycle, the connector would typically gracefully restart just fine. However, the situation is different if the problem was related to the database&#8217;s availability and the database was still unavailable during the connector&#8217;s start-up phase. In this situation, the connector would fail-fast, and would enter a <code>FAILED</code> state, requiring manual intervention.</p> </div> <div class="paragraph"> <p>The fail-fast approach served Debezium well over the years, but in a world where a resource can come and go without warning, it became clear that changes were needed to improve Debezium&#8217;s reliability and resiliency. While the Kafka Connect&#8217;s retry/back-off framework has helped in this regard, that doesn&#8217;t address the concerns with start-up resources being unavailable with how the code is currently written.</p> </div> <div class="paragraph"> <p>Debezium 2.2 changes this landscape, shifting how we integrate with Kafka Connect&#8217;s source connector API slightly. Instead of accessing potentially unavailable resources during the start-up lifecycle, we moved that access to a later phase in the connector&#8217;s lifecycle. In effect, the Debezium start-up code is executed lazily that accesses potentially unavailable resources, which allows us to take advantage of the Kafka Connect retry/back-off framework even during our start-up code. In short, if the database is still unavailable during the connector&#8217;s start-up, the connector will continue to retry/back-off if Kafka Connect retries are enabled. Only once the maximum number of retry attempts has been reached or a non-retriable error occurs will the connector task enter a <code>FAILED</code> state.</p> </div> <div class="paragraph"> <p>We hope this brings more reliability and resiliency for the Debezium experience, improving how errors are handled in an ever-changing landscape, and provides a solid foundation to manage connector lifecycles.</p> </div> </div> </div> <div class="sect1"> <h2 id="rocketmq_and_infinispan_support_in_debezium_server">RocketMQ and Infinispan support in Debezium Server</h2> <div class="sectionbody"> <div class="paragraph"> <p>Debezium Server is a Quarkus-based framework that allows executing a Debezium connector from the command line, without Kafka or Kafka Connect, allowing the delivery of Debezium change events to any destination framework. With Debezium 2.2, two new sink connectors have been added to Debezium Server to support sending change events to Apache RocketMQ and to Infinispan.</p> </div> <div class="sect2"> <h3 id="rocketmq">RocketMQ</h3> <div class="paragraph"> <p><a href="https://rocketmq.apache.org">Apache RocketMQ</a> is a cloud-native messaging, eventing, and streaming real-time data processing platform that covers cloud-edge-device collaboration scenarios. In order to integrate Debezium Server with RocketMQ, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=rocketmq
debezium.sink.rocketmq.producer.name.srv.addr=&lt;hostname&gt;:&lt;port&gt;
debezium.sink.rocketmq.producer.group=debezuim-group
debezium.sink.rocketmq.producer.max.message.size=4194304
debezium.sink.rocketmq.producer.send.msg.timeout=3000
debezium.sink.rocketmq.producer.acl.enabled=false
debezium.sink.rocketmq.producer.access.key=&lt;access-key&gt;
debezium.sink.rocketmq.producer.secret.key=&lt;secret-key&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>rocketmq</code>, which enables the use of the RocketMQ module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.name.srv.addr</code></dt> <dd> <p>Specifies the host and port where Apache RocketMQ is available.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.group</code></dt> <dd> <p>Specifies the name associated with the Apache RocketMQ producer group.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.max.message.size</code></dt> <dd> <p>(Optional) Specifies the maximum number of bytes a message can be. Defaults to <code>4193404</code> (4MB).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.send.msg.timeout</code></dt> <dd> <p>(Optional) Specifies the timeout in milliseconds when sending messages. Defaults to <code>3000</code> (3 seconds).</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.acl.enabled</code></dt> <dd> <p>(Optional) Controls whether access control lists are enabled. Defaults to <code>false</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.access.key</code></dt> <dd> <p>(Optional) The access key used for connecting to the Apache RocketMQ cluster.</p> </dd> <dt class="hdlist1"><code>debezium.sink.rocketmq.producer.secret.key</code></dt> <dd> <p>(Optional) The access secret used for connecting to the Apache RocketMQ cluster.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with RocketMQ, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_apache_rocketmq">documentation</a>.</p> </div> </div> <div class="sect2"> <h3 id="infinispan">Infinispan</h3> <div class="paragraph"> <p><a href="https://infinispan.org">Infinispan</a> is an in-memory, distributed data store that offers flexible deployment options with robust capabilities to store, manage, and process data. Infinispan is based on the notion of a key-value store that allows storing any data type. In order to integrate Debezium Server with Infinispan, the Debezium Server <code>application.properties</code> must be modified to include the following entries:</p> </div> <div class="listingblock"> <div class="title">application.properties</div> <div class="content"> <pre class="CodeRay highlight"><code data-lang="properties">debezium.sink.type=infinispan
debezium.sink.infinispan.server.host=&lt;hostname&gt;
debezium.sink.infinispan.server.port=&lt;port&gt;
debezium.sink.infinispan.cache=&lt;cache-name&gt;
debezium.sink.infinispan.user=&lt;user&gt;
debezium.sink.infinispan.password=&lt;password&gt;</code></pre> </div> </div> <div class="paragraph"> <p>The above configuration specifies that the sink type to be used is <code>infinispan</code>, which enables the use of the Infinispan module. The following is a description of each of the properties shown above:</p> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.server.host</code></dt> <dd> <p>Specifies the host name of one of the servers in the Infinispan cluster. This configuration option can also supply a comma-separated list of hostnames as well, such as <code>hostname1,hostname2</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.server.port</code></dt> <dd> <p>Specifies the port of the Infinispan cluster. Defaults to <code>11222</code>.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.cache</code></dt> <dd> <p>Specifies the name of the Infinispan cache to write change events.</p> </dd> </dl> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>The Infinispan sink requires that the cache be created manually ahead of time. This enables the ability to create the cache with any variable configuration needed to fit your requirements.</p> </div> </td> </tr> </table> </div> <div class="dlist"> <dl> <dt class="hdlist1"><code>debezium.sink.infinispan.user</code></dt> <dd> <p>An optional configuration to specify the user to authenticate with, if authentication is required.</p> </dd> <dt class="hdlist1"><code>debezium.sink.infinispan.password</code></dt> <dd> <p>An optional configuration to specify the password for the authenticating user, if authentication is required.</p> </dd> </dl> </div> <div class="paragraph"> <p>For more information on using Debezium Server with Infinispan, see the <a href="/documentation/reference/2.2/operations/debezium-server.html#_infinispan">documentation</a>.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="other_fixes">Other fixes</h2> <div class="sectionbody"> <div class="paragraph"> <p>There were quite a number of bugfixes and stability changes in this release, some noteworthy are:</p> </div> <div class="ulist"> <ul> <li> <p>Remove option for specifying driver class from MySQL Connector <a href="https://issues.redhat.com/browse/DBZ-4663">DBZ-4663</a></p> </li> <li> <p>Debezium is not working with Apicurio and custom truststores <a href="https://issues.redhat.com/browse/DBZ-5282">DBZ-5282</a></p> </li> <li> <p>Show/Hide password does not work on Connectors View details screen <a href="https://issues.redhat.com/browse/DBZ-5322">DBZ-5322</a></p> </li> <li> <p>Oracle cannot undo change <a href="https://issues.redhat.com/browse/DBZ-5907">DBZ-5907</a></p> </li> <li> <p>Postgresql Data Loss on restarts <a href="https://issues.redhat.com/browse/DBZ-5915">DBZ-5915</a></p> </li> <li> <p>Add support for Connect Headers to Debezium Server <a href="https://issues.redhat.com/browse/DBZ-5926">DBZ-5926</a></p> </li> <li> <p>Oracle Multithreading lost data <a href="https://issues.redhat.com/browse/DBZ-5945">DBZ-5945</a></p> </li> <li> <p>Spanner connector is missing JSR-310 dependency <a href="https://issues.redhat.com/browse/DBZ-5959">DBZ-5959</a></p> </li> <li> <p>Truncate records incompatible with ExtractNewRecordState <a href="https://issues.redhat.com/browse/DBZ-5966">DBZ-5966</a></p> </li> <li> <p>Computed partition must not be negative <a href="https://issues.redhat.com/browse/DBZ-5967">DBZ-5967</a></p> </li> <li> <p>Table size log message for snapshot.select.statement.overrides tables not correct <a href="https://issues.redhat.com/browse/DBZ-5985">DBZ-5985</a></p> </li> <li> <p>NPE in execute snapshot signal with exclude.tables config on giving wrong table name <a href="https://issues.redhat.com/browse/DBZ-5988">DBZ-5988</a></p> </li> <li> <p>There is a problem with postgresql connector parsing the boundary value of money type <a href="https://issues.redhat.com/browse/DBZ-5991">DBZ-5991</a></p> </li> <li> <p>Log statement for unparseable DDL statement in MySqlDatabaseSchema contains placeholder <a href="https://issues.redhat.com/browse/DBZ-5993">DBZ-5993</a></p> </li> <li> <p>Postgresql connector parses the null of the money type into 0 <a href="https://issues.redhat.com/browse/DBZ-6001">DBZ-6001</a></p> </li> <li> <p>Postgres LSN check should honor event.processing.failure.handling.mode <a href="https://issues.redhat.com/browse/DBZ-6012">DBZ-6012</a></p> </li> </ul> </div> <div class="paragraph"> <p>Altogether, <a href="https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%202.2.0.Alpha1%20ORDER%20BY%20component%20ASC">42 issues</a> were fixed for this release. A big thank you to all the contributors from the community who worked on this release: Akshansh Jain, Gabor, <a href="https://github.com/adasari">Anil Dasari</a>, <a href="https://github.com/akanimesh7">Animesh Kumar</a>, <a href="https://github.com/ani-sha">Anisha Mohanty</a>, <a href="https://github.com/roldanbob">Bob Roldan</a>, <a href="https://github.com/Naros">Chris Cranford</a>, <a href="https://github.com/erdinctaskin">Erdin Takn</a>, <a href="https://github.com/govi20">Govinda Sakhare</a>, <a href="https://github.com/harveyyue">Harvey Yue</a>, <a href="https://github.com/blcksrx">Hossein Torabi</a>, <a href="https://github.com/indraraj">Indra Shukla</a>, <a href="https://github.com/jakzal">Jakub Zalas</a>, <a href="https://github.com/jeremy-l-ford">Jeremy Ford</a>, <a href="https://github.com/jpechane">Jiri Pechanec</a>, <a href="https://github.com/joschi">Jochen Schalanda</a>, <a href="https://github.com/Lucascanna">Luca Scannapieco</a>, <a href="https://github.com/mfvitale">Mario Fiore Vitale</a>, <a href="https://github.com/dude0001">Mark Lambert</a>, <a href="https://github.com/rajdangwal">Rajendra Dangwal</a>, <a href="https://github.com/sunxiaojian">Sun Xiao Jian</a>, <a href="https://github.com/vjuranek">Vojtech Juranek</a>, <a href="https://github.com/yoheimuta">Yohei Yoshimuta</a>, and <a href="https://github.com/yoheimuta">yohei yoshimuta</a>!</p> </div> </div> </div> <div class="sect1"> <h2 id="whats_next">What&#8217;s Next?</h2> <div class="sectionbody"> <div class="paragraph"> <p>As the road to Debezium 2.2 is just starting, this initial release covers quite a lot of the features we&#8217;ve outlined our recent 2023 <a href="#/roadmap">road map</a> update. However, there are still a number of features that are still in active development, which include:</p> </div> <div class="ulist"> <ul> <li> <p>Configurable signal channels, enabling users to send signals not only from a database table or a Kafka topic, but also from other means such as an HTTP endpoint, the file system, etc.</p> </li> <li> <p>The Debezium JDBC sink connector that supports native Debezium change events out-of-the-box, without requiring the use of the Event Flattening transformation.</p> </li> <li> <p>A new single message transformation, <code>ExtractChangedRecordState</code>, that supports adding headers to the emitted event that describes that fields were changed or unchanged by the source event.</p> </li> <li> <p>And a plethora of enhancements to Debezium&#8217;s UI</p> </li> </ul> </div> <div class="paragraph"> <p>As we continue development on Debezium 2.2 and bugfixes to Debezium 2.1, we would love to hear your feedback or suggestions, whether it&#8217;s regarding our road map, the changes in this release, or something you&#8217;d like to see that we haven&#8217;t mentioned. Be sure to get in touch with us on the <a href="https://groups.google.com/g/debezium">mailing list</a> or our <a href="https://debezium.zulipchat.com/login/#narrow/stream/302529-users">chat</a> if there is. Or if you just want to stop by and give us a "Hello", we&#8217;d welcome that too.</p> </div> <div class="paragraph"> <p>Until next time&#8230;&#8203;</p> </div> </div> </div>]]></content><author><name>Chris Cranford</name></author><category term="releases"/><category term="mongodb"/><category term="mysql"/><category term="postgres"/><category term="sqlserver"/><category term="cassandra"/><category term="oracle"/><category term="db2"/><category term="vitess"/><category term="outbox"/><summary type="html"><![CDATA[It&#8217;s my pleasure to announce not only the first release of the Debezium 2.2 series, but also the first release of Debezium in 2023, 2.2.0.Alpha! The Debezium 2.2.0.Alpha1 release includes some breaking changes, a number of bug fixes, and some noteworthy improvements and features, including but not limited to: [Breaking Change] - ZonedTimestamp values will no longer truncate fractional seconds. [New] - Support ingesting changes from an Oracle logical stand-by database [New] - Support Amazon S3 buckets using the Debezium Storage API [New] - Support retrying database connections during connector start-up [New] - Debezium Server sink connector support for Apache RocketMQ and Infinispan]]></summary></entry><entry><title type="html">Change Data Capture with QuestDB and Debezium</title><link href="https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/" rel="alternate" type="text/html" title="Change Data Capture with QuestDB and Debezium"/><published>2023-01-06T00:00:00+00:00</published><updated>2023-01-06T00:00:00+00:00</updated><id>https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium</id><content type="html" xml:base="https://debezium.io/blog/2023/01/06/change-data-capture-with-questdb-and-debezium/"><![CDATA[<div class="paragraph"> <p>This tutorial was originally published by <a href="https://questdb.io/">QuestDB</a>, where guest contributor, <a href="https://yitaek.medium.com/">Yitaek Hwang</a>, shows us how to stream data into QuestDB with change data capture via Debezium and Kafka Connect.</p> </div> <div class="paragraph"> <p></p> </div> <div class="paragraph"> <p>Modern data architecture has largely shifted away from the <strong>ETL</strong> (Extract-Transform-Load) paradigm to <strong>ELT</strong> (Extract-Load-Transform) where raw data is first loaded into a data lake before transformations are applied (e.g., aggregations, joins) for further analysis. Traditional ETL pipelines were hard to maintain and relatively inflexible with changing business needs. As new cloud technologies promised cheaper storage and better scalability, data pipelines could move away from pre-built extractions and batch uploads to a more streaming architecture.</p> </div> <div class="paragraph"> <p><a href="https://en.wikipedia.org/wiki/Change_data_capture">Change data capture</a>(CDC) fits nicely into this paradigm shift where changes to data from one source can be streamed to other destinations. As the name implies, CDC tracks changes in data (usually a database) and provides plugins to act on those changes. For event-driven architectures, CDC is especially useful as a consistent data delivery mechanism between service boundaries (e.g., <a href="https://microservices.io/patterns/data/transactional-outbox.html">Outbox Pattern</a>). In a complex microservice environment, CDC helps to simplify data delivery logic by offloading the burden to the CDC systems.</p> </div> <div class="paragraph"> <p>To illustrate, let&#8217;s take a reference architecture to stream stock updates from PostgreSQL into QuestDB. A simple Java Spring App polls stock prices by ticker symbol and updates the current price to a PostgreSQL database. Then the updates are detected by Debezium and fed to a Kafka topic. Finally, the Kafka Connect QuestDB connector listens to that topic and streams changes into QuestDB for analysis.</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/overview.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Design overview</p> </div> </div> </div> <div class="paragraph"> <p>Structuring the data pipeline this way allows the application to be simple. The Java Spring App only needs to fetch the latest stock data and commit to PostgreSQL. Since PostgreSQL is an excellent OLTP (transactional) database, the app can rely on the ACID compliance to ensure that only the committed data will be seen by downstream services. The app developer does not need to worry about complicated retry logic or out-of-sync datasets. From the database standpoint, PostgreSQL can be optimized to do what it does best  transactional queries. Kafka can be leveraged to reliably feed data to other endpoints, and QuestDB can be used to store historical data to run analytical queries and visualization.</p> </div> <div class="paragraph"> <p>So without further ado, let&#8217;s get to the example:</p> </div> <div class="sect1"> <h2 id="prerequisites">Prerequisites</h2> <div class="sectionbody"> <div class="ulist"> <ul> <li> <p>Git</p> </li> <li> <p>Docker Engine: 20.10+</p> </li> </ul> </div> </div> </div> <div class="sect1"> <h2 id="setup">Setup</h2> <div class="sectionbody"> <div class="paragraph"> <p>To run the example locally, first clone the <a href="https://github.com/questdb/kafka-questdb-connector.git">QuestDG Kafka connector repo</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">$ git clone https://github.com/questdb/kafka-questdb-connector.git</code></pre> </div> </div> <div class="paragraph"> <p>Then, navigate to the stocks sample to build and run the Docker compose files:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">$ cd kafka-questdb-connector/kafka-questdb-connector-samples/stocks/
$ docker compose build
$ docker compose up</code></pre> </div> </div> <div class="admonitionblock note"> <table> <tr> <td class="icon"> <i class="fa icon-note" title="Note"></i> </td> <td class="content"> <div class="paragraph"> <p>In Linux or older versions of Docker, the <code>compose</code> subcommand might not be available. You can try to execute <code>docker-compose</code> instead of <code>docker compose</code>. If <code>docker-compose</code> is unavailable in your distribution, you can <a href="https://docs.docker.com/compose/install/other/">install it</a> manually.</p> </div> </td> </tr> </table> </div> <div class="paragraph"> <p>This will build the Dockerfile for the Java Spring App/Kafka Connector for QuestDB and pull down PostgreSQL (preconfigured with Debezium), Kafka/Zookeeper, QuestDB, and Grafana containers. Kafka and Kafka Connect take a bit to initialize. Wait for the logs to stop by inspecting the connect container.</p> </div> <div class="sect2"> <h3 id="start_the_debezium_connector">Start the Debezium connector</h3> <div class="paragraph"> <p>At this point, the Java App is continuously updating the stock table in PostgreSQL, but the connections have not been setup. Create the Debezium connector (i.e., PostgreSQL  Debezium  Kafka) by executing the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d  '{&quot;name&quot;:&quot;debezium_source&quot;,&quot;config&quot;:{&quot;tasks.max&quot;:1,&quot;database.hostname&quot;:&quot;postgres&quot;,&quot;database.port&quot;:5432,&quot;database.user&quot;:&quot;postgres&quot;,&quot;database.password&quot;:&quot;postgres&quot;,&quot;connector.class&quot;:&quot;io.debezium.connector.postgresql.PostgresConnector&quot;,&quot;database.dbname&quot;:&quot;postgres&quot;,&quot;database.server.name&quot;:&quot;dbserver1&quot;}} ' localhost:8083/connectors</code></pre> </div> </div> </div> <div class="sect2"> <h3 id="start-the-questdb-kafka-connect-sink">Start the QuestDB Kafka Connect sink</h3> <div class="paragraph"> <p>Finish the plumbing by creating the Kafka Connect side (i.e., Kafka  QuestDB sink):</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d '{&quot;name&quot;:&quot;questdb-connect&quot;,&quot;config&quot;:{&quot;topics&quot;:&quot;dbserver1.public.stock&quot;,&quot;table&quot;:&quot;stock&quot;, &quot;connector.class&quot;:&quot;io.questdb.kafka.QuestDBSinkConnector&quot;,&quot;tasks.max&quot;:&quot;1&quot;,&quot;key.converter&quot;:&quot;org.apache.kafka.connect.storage.StringConverter&quot;,&quot;value.converter&quot;:&quot;org.apache.kafka.connect.json.JsonConverter&quot;,&quot;host&quot;:&quot;questdb&quot;, &quot;transforms&quot;:&quot;unwrap&quot;, &quot;transforms.unwrap.type&quot;:&quot;io.debezium.transforms.ExtractNewRecordState&quot;, &quot;include.key&quot;: &quot;false&quot;, &quot;symbols&quot;: &quot;symbol&quot;, &quot;timestamp.field.name&quot;: &quot;last_update&quot;}}' localhost:8083/connectors</code></pre> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="final_result">Final result</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now all the updates written to the PostgreSQL table will also be reflected in QuestDB. To validate, navigate to <a href="http://localhost:19000">http://localhost:19000</a> and select from the stock table:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span> * <span class="keyword">FROM</span> stock;</code></pre> </div> </div> <div class="paragraph"> <p>You can also run aggregations for a more complex analysis:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"> <span class="class">SELECT</span>
   <span class="predefined-type">timestamp</span>,
   symbol,
   <span class="predefined">avg</span>(price),
   <span class="predefined">min</span>(price),
   <span class="predefined">max</span>(price)
 <span class="keyword">FROM</span> stock
   <span class="keyword">WHERE</span> symbol = <span class="string"><span class="delimiter">'</span><span class="content">IBM</span><span class="delimiter">'</span></span>
 SAMPLE <span class="keyword">BY</span> <span class="integer">1</span>m ALIGN <span class="keyword">TO</span> CALENDAR;</code></pre> </div> </div> <div class="paragraph"> <p>Finally, you can interact with a Grafana dashboard for visualization at <a href="http://localhost:3000/d/stocks/stocks?orgId=1&amp;refresh=5s&amp;viewPanel=2">http://localhost:3000/d/stocks/stocks?orgId=1&amp;refresh=5s&amp;viewPanel=2</a>.</p> </div> <div class="paragraph"> <p>The visualization is a candle chart composed of changes captured by Debezium; each candle shows the opening, closing, high, and low price, in a given time interval. The time interval can be changed by selecting the top-left 'Interval' option:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/screenshot.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Grafana candle chart</p> </div> </div> </div> </div> </div> <div class="sect1"> <h2 id="deep_dive">Deep dive</h2> <div class="sectionbody"> <div class="paragraph"> <p>Now that we have the sample application up and running, let&#8217;s take a deeper dive into each component in the <a href="https://github.com/questdb/kafka-questdb-connector/tree/main/kafka-questdb-connector-samples/stocks">stocks</a> example.</p> </div> <div class="paragraph"> <p>We will look at the following files:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code> kafka-questdb-connector/kafka-questdb-connector-samples/stocks/
    Dockerfile-App
|   |    -- The Dockerfile to package our Java App
|    Dockerfile-Connect
|   |    -- The Dockerfile to combine the Debezium container
|   |    -- image the with QuestDB Kafka connector
    src/main/resources/schema.sql
|   |    -- The SQL which creates the stock table in PostgreSQL
|   |    -- and populates it with initial data
    src/main/java/com/questdb/kafka/connector/samples/StocksApplication.java
|   |    -- The Java Spring App which updates the stock table in PostgreSQL
|   |    -- in regular intervals
...</code></pre> </div> </div> <div class="sect2"> <h3 id="producer_java_app">Producer (Java App)</h3> <div class="paragraph"> <p>The producer is a simple Java Spring Boot App. It has two components:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>The <a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/src/main/resources/schema.sql">schema.sql</a> file. This file is used to create the stock table in PostgreSQL and populate it with initial data. It&#8217;s picked up by the Spring Boot App and executed on startup.</p> <div class="literalblock"> <div class="content"> <pre>[source,sql]
----
CREATE TABLE IF NOT EXISTS stock (
    id serial primary key,
    symbol varchar(10) unique,
    price float8,
    last_update timestamp
);
INSERT INTO stock (symbol, price, last_update) VALUES ('AAPL', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('IBM', 50.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('MSFT', 100.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('GOOG', 1000.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('FB', 200.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('AMZN', 1000.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('TSLA', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('NFLX', 500.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('TWTR', 50.0, now()) ON CONFLICT DO NOTHING;
INSERT INTO stock (symbol, price, last_update) VALUES ('SNAP', 10.0, now()) ON CONFLICT DO NOTHING;
----</pre> </div> </div> <div class="literalblock"> <div class="content"> <pre>The `ON CONFLICT DO NOTHING` clause is used to avoid duplicate entries in the
table when the App is restarted.</pre> </div> </div> </li> <li> <p><a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/src/main/java/io/questdb/kafka/samples/StockService.java">Java code</a> to update prices and timestamps with a random value. The updates are not perfectly random, the application uses a very simple algorithm to generate updates which very roughly resembles stock price movements. In a real-life scenario, the application would fetch the price from some external source.</p> </li> </ol> </div> <div class="paragraph"> <p>The producer is packaged into a minimal Dockerfile, <a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/Dockerfile-App">Dockerfile-App</a>, and linked to PostgreSQL:</p> </div> <div class="listingblock"> <div class="content"> <pre>FROM maven:3.8-jdk-11-slim AS builder
COPY ./pom.xml /opt/stocks/pom.xml
COPY ./src ./opt/stocks/src
WORKDIR /opt/stocks
RUN mvn clean install -DskipTest
FROM azul/zulu-openjdk:11-latest
COPY --from=builder /opt/stocks/target/kafka-samples-stocks-*.jar /stocks.jar
CMD ["java", "-jar", "/stocks.jar"]</pre> </div> </div> </div> <div class="sect2"> <h3 id="kafka_connect_debezium_and_questdb_kafka_connector">Kafka Connect, Debezium, and QuestDB Kafka Connector</h3> <div class="paragraph"> <p>Before we dive into the Kafka Connect, Debezium, and the QuestDB Kafka connector configurations, let&#8217;s take a look at their relation with each other.</p> </div> <div class="paragraph"> <p>Kafka Connect is a framework for building connectors to move data between Kafka and other systems. It supports 2 classes of connectors:</p> </div> <div class="olist arabic"> <ol class="arabic"> <li> <p>Source connectors - read data from a source system and write it to Kafka</p> </li> <li> <p>Sink connectors - read data from Kafka and write it to a sink system</p> </li> </ol> </div> <div class="paragraph"> <p>Debezium is a Source connector for Kafka Connect that can monitor and capture the row-level changes in the databases. What does it mean? Whenever a row is inserted, updated, or deleted in a database, Debezium will capture the change and write it as an event to Kafka.</p> </div> <div class="paragraph"> <p>On a technical level, Debezium is a Kafka Connect connector running inside the Kafka Connect framework. This is reflected in the <a href="https://hub.docker.com/r/debezium/connect">Debezium container image</a>, which packages the Kafka Connect with Debezium connectors pre-installed.</p> </div> <div class="paragraph"> <p>QuestDB Kafka connector is also a Kafka Connect connector. It&#8217;s a Sink connector that reads data from Kafka and writes it to QuestDB. We add the QuestDB Kafka connector to the Debezium container image, and we get a Kafka Connect image that has both Debezium and QuestDB Kafka connector installed!</p> </div> <div class="paragraph"> <p>This is the Dockerfile we use to build the image:</p> </div> <div class="paragraph"> <p>(<a href="https://github.com/questdb/kafka-questdb-connector/blob/main/kafka-questdb-connector-samples/stocks/Dockerfile-Connect">Dockerfile-Connect</a>)</p> </div> <div class="listingblock"> <div class="content"> <pre>FROM ubuntu:latest AS builder
WORKDIR /opt
RUN apt-get update &amp;&amp; apt-get install -y curl wget unzip jq
RUN curl -s https://api.github.com/repos/questdb/kafka-questdb-connector/releases/latest | jq -r '.assets[]|select(.content_type == "application/zip")|.browser_download_url'|wget -qi -
RUN unzip kafka-questdb-connector-*-bin.zip

FROM debezium/connect:1.9.6.Final
COPY --from=builder /opt/kafka-questdb-connector/*.jar /kafka/connect/questdb-connector/</pre> </div> </div> <div class="paragraph"> <p>The Dockerfile downloads the latest release of the QuestDB Kafka connector, unzip it copies it to the Debezium container image. The resulting image has both Debezium and QuestDB Kafka connector installed:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/dockerfile-connect.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>Dockerfile-Coonnect adding the QuestDB Kafka Connector layer</p> </div> </div> </div> <div class="paragraph"> <p>The overall Kafka connector is completed with a Source connector and a Sink connector:</p> </div> <div class="exampleblock centered-image responsive-image"> <div class="content"> <img src="/assets/images/2023-01-06-change-data-capture-with-questdb-and-debezium/kafka-cluster.png" style="max-width:90%;" class="responsive-image"> <div class="paragraph"> <p>How the Source and Sink connector work with the Kafka cluster and the databases</p> </div> </div> </div> <div class="sect3"> <h4 id="debezium_connector">Debezium Connector</h4> <div class="paragraph"> <p>We already know that Debezium is a Kafka Connect connector that can monitor and capture the row-level changes in the databases. We also have a Docker image that has both Debezium and QuestDB Kafka connectors installed. However, at this point neither of the connectors is running. We need to configure and start them. This is done via CURL command that sends a POST request to the Kafka Connect REST API.</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="shell">curl -X POST -H &quot;Content-Type: application/json&quot; -d  '{&quot;name&quot;:&quot;debezium_source&quot;,&quot;config&quot;:{&quot;tasks.max&quot;:1,&quot;database.hostname&quot;:&quot;postgres&quot;,&quot;database.port&quot;:5432,&quot;database.user&quot;:&quot;postgres&quot;,&quot;database.password&quot;:&quot;postgres&quot;,&quot;connector.class&quot;:&quot;io.debezium.connector.postgresql.PostgresConnector&quot;,&quot;database.dbname&quot;:&quot;postgres&quot;,&quot;database.server.name&quot;:&quot;dbserver1&quot;}} ' localhost:8083/connectors</code></pre> </div> </div> <div class="paragraph"> <p>The request body contains the configuration for the Debezium connector, let&#8217;s break it down:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">debezium_source</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">config</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">tasks.max</span><span class="delimiter">&quot;</span></span>: <span class="integer">1</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.hostname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.port</span><span class="delimiter">&quot;</span></span>: <span class="integer">5432</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.user</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.password</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector.class</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">io.debezium.connector.postgresql.PostgresConnector</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.dbname</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">database.server.name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbserver1</span><span class="delimiter">&quot;</span></span>
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>It listens to changes in the PostgreSQL database and publishes to Kafka with the above configuration. The topic name defaults to <code>&lt;server-name&gt;.&lt;schema&gt;.&lt;table&gt;.</code> In our example, it is <code>dbserver1.public.stock</code>. Why? Because the database server name is <code>dbserver1</code>, the schema is <code>public</code> and the only table we have is <code>stock</code>.</p> </div> <div class="paragraph"> <p>So after we send the request, Debezium will start listening to changes in the <code>stock</code> table and publish them to the <code>dbserver1.public.stock</code> topic.</p> </div> </div> <div class="sect3"> <h4 id="questdb_kafka_connector">QuestDB Kafka Connector</h4> <div class="paragraph"> <p>At this point, we have a PostgreSQL table <code>stock</code> being populated with random stock prices and a Kafka topic <code>dbserver1.public.stock</code> that contains the changes. The next step is to configure the QuestDB Kafka connector to read from the <code>dbserver1.public.stock</code> topic and write the data to QuestDB.</p> </div> <div class="paragraph"> <p>Let&#8217;s take a deeper look at the configuration in the <a href="#start-the-questdb-kafka-connect-sink">start the QuestDB Kafka Connect sink</a>:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="jason">{
  &quot;name&quot;: &quot;questdb-connect&quot;,
  &quot;config&quot;: {
    &quot;topics&quot;: &quot;dbserver1.public.stock&quot;,
    &quot;table&quot;: &quot;stock&quot;,
    &quot;connector.class&quot;: &quot;io.questdb.kafka.QuestDBSinkConnector&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,
    &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,
    &quot;host&quot;: &quot;questdb&quot;,
    &quot;transforms&quot;: &quot;unwrap&quot;,
    &quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.ExtractNewRecordState&quot;,
    &quot;include.key&quot;: &quot;false&quot;,
    &quot;symbols&quot;: &quot;symbol&quot;,
    &quot;timestamp.field.name&quot;: &quot;last_update&quot;
  }
}</code></pre> </div> </div> <div class="paragraph"> <p>The important things to note here are:</p> </div> <div class="ulist"> <ul> <li> <p><code>table</code> and <code>topics</code>: The QuestDB Kafka connector will create a QuestDB table with the name <code>stock</code> and write the data from the <code>dbserver1.public.stock</code> topic to it.</p> </li> <li> <p><code>host</code>: The QuestDB Kafka connector will connect to QuestDB running on the <code>questdb</code> host. This is the name of the QuestDB container.</p> </li> <li> <p><code>connector.class</code>: The QuestDB Kafka connector class name. This tells Kafka Connect to use the QuestDB Kafka connector.</p> </li> <li> <p><code>value.converter</code>: The Debezium connector produces the data in JSON format. This is why we need to configure the QuestDB connector to use the JSON converter to read the data: <code>org.apache.kafka.connect.json.JsonConverter</code>.</p> </li> <li> <p><code>symbols</code>: Stock symbols are translated to <a href="https://questdb.io/docs/concept/symbol/">QuestDB symbol type</a>, used for string values with low cardinality (e.g., enums).</p> </li> <li> <p><code>timestamp.field.name</code>: Since QuestDB has great support for timestamp and partitioning based on that, we can specify the designated timestamp column.</p> </li> <li> <p><code>transforms</code>: unwrap field uses <code>io.debezium.transforms.ExtractNewRecordState</code> type to extract just the new data and not the metadata that Debezium emits. In other words, this is a filter to basically take the <code>payload.after</code> portion of the Debezium data on the Kafka topics. See its <a href="https://debezium.io/documentation/reference/1.9/transformations/event-flattening.html">documentation</a> for more details.</p> </li> </ul> </div> <div class="paragraph"> <p>The <code>ExtractNewRecordState</code> transform is probably the least intuitive part of the configuration. Let&#8217;s have a closer look at it: In short, for every change in the PostgreSQL table, the Debezium emits a JSON message to a Kafka topic such as the following:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">This JSON key contains Debezium message schema. It's not very relevant for this sample. Omitted for brevity.</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">payload</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">before</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">after</span><span class="delimiter">&quot;</span></span>: {
      <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="integer">8</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">symbol</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">NFLX</span><span class="delimiter">&quot;</span></span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">price</span><span class="delimiter">&quot;</span></span>: <span class="float">1544.3357414199545</span>,
      <span class="key"><span class="delimiter">&quot;</span><span class="content">last_update</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978269856</span>
    }
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">source</span><span class="delimiter">&quot;</span></span>: {
    <span class="key"><span class="delimiter">&quot;</span><span class="content">version</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">1.9.6.Final</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">connector</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgresql</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">name</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">dbserver1</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978272</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">snapshot</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">false</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">db</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">postgres</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">sequence</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">[</span><span class="char">\&quot;</span><span class="content">87397208</span><span class="char">\&quot;</span><span class="content">,</span><span class="char">\&quot;</span><span class="content">87397208</span><span class="char">\&quot;</span><span class="content">]</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">schema</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">public</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">table</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">stock</span><span class="delimiter">&quot;</span></span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">txId</span><span class="delimiter">&quot;</span></span>: <span class="integer">402087</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">lsn</span><span class="delimiter">&quot;</span></span>: <span class="integer">87397208</span>,
    <span class="key"><span class="delimiter">&quot;</span><span class="content">xmin</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
  },
  <span class="key"><span class="delimiter">&quot;</span><span class="content">op</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">u</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">ts_ms</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978637</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">transaction</span><span class="delimiter">&quot;</span></span>: <span class="value">null</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>Don&#8217;t get scared if you feel overwhelmed by the sheer size of this message. Most of the fields are metadata, and they are not relevant to this sample. See <a href="https://debezium.io/documentation/reference/1.9/connectors/postgresql.html#postgresql-events">Debezium documentation</a>, for more details. The important point is that we cannot push the whole JSON message to QuestDB and we do not want all the metadata in QuestDB. We need to extract the <code>payload.after</code> portion of the message and only then push it to QuestDB. This is exactly what the <code>ExtractNewRecordState</code> transform does: It transforms the big message into a smaller one that contains only the <code>payload.after</code> portion of the message. Hence, it is as if the message looked like this:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="json">{
  <span class="key"><span class="delimiter">&quot;</span><span class="content">id</span><span class="delimiter">&quot;</span></span>: <span class="integer">8</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">symbol</span><span class="delimiter">&quot;</span></span>: <span class="string"><span class="delimiter">&quot;</span><span class="content">NFLX</span><span class="delimiter">&quot;</span></span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">price</span><span class="delimiter">&quot;</span></span>: <span class="float">1544.3357414199545</span>,
  <span class="key"><span class="delimiter">&quot;</span><span class="content">last_update</span><span class="delimiter">&quot;</span></span>: <span class="integer">1666172978269856</span>
}</code></pre> </div> </div> <div class="paragraph"> <p>This is the message that we can push to QuestDB. The QuestDB Kafka connector will read this message and write it to the QuestDB table. The QuestDB Kafka connector will also create the QuestDB table if it does not exist. The QuestDB table will have the same schema as the JSON message - where each JSON field will be a column in the QuestDB table.</p> </div> </div> </div> <div class="sect2"> <h3 id="questdb_and_grafana">QuestDB and Grafana</h3> <div class="paragraph"> <p>Once the data is written to QuestDB tables, we can work with the time-series data easier. Since QuestDB is compatible with the PostgreSQL wire protocol, we can use the PostgreSQL data source on Grafana to visualize the data. The preconfigured dashboard is using the following query:</p> </div> <div class="listingblock"> <div class="content"> <pre class="CodeRay highlight"><code data-lang="sql"><span class="class">SELECT</span>
  <span class="error">$</span>__time(<span class="predefined-type">timestamp</span>),
  <span class="predefined">min</span>(price) <span class="keyword">as</span> low,
  <span class="predefined">max</span>(price) <span class="keyword">as</span> high,
  first(price) <span class="keyword">as</span> open,
  last(price) <span class="keyword">as</span> close
<span class="keyword">FROM</span>
  stock
<span class="keyword">WHERE</span>
  <span class="error">$</span>__timeFilter(<span class="predefined-type">timestamp</span>)
  <span class="keyword">and</span> symbol = <span class="string"><span class="delimiter">'</span><span class="content">$Symbol</span><span class="delimiter">'</span></span>
SAMPLE <span class="keyword">BY</span> <span class="error">$</span>Interval ALIGN <span class="keyword">TO</span> CALENDAR;</code></pre> </div> </div> <div class="paragraph"> <p>We have created a system that continuously tracks and stores the latest prices for multiple stocks in a PostgreSQL table. These prices are then fed as events to Kafka through Debezium, which captures every price change. The QuestDB Kafka connector reads these events from Kafka and stores each change as a new row in QuestDB, allowing us to retain a comprehensive history of stock prices. This history can then be analyzed and visualized using tools such as Grafana, as demonstrated by the candle chart.</p> </div> </div> </div> </div> <div class="sect1"> <h2 id="next_steps">Next steps</h2> <div class="sectionbody"> <div class="paragraph"> <p>This sample project is a foundational reference architecture to stream data from a relational database into an optimized time series database. For existing projects that are using PostgreSQL, Debezium can be configured to start streaming data to QuestDB and take advantage of time series queries and partitioning. For databases that are also storing raw historical data, adopting Debezium may need some architectural changes. However, this is beneficial as it is an opportunity to improve performance and establish service boundaries between a transactional database and an analytical, time-series database.</p> </div> <div class="paragraph"> <p>This reference architecture can also be extended to configure Kafka Connect to also stream to other data warehouses for long-term storage. After inspecting the data, QuestDB can also be configured to downsample the data for longer term storage or even <a href="https://questdb.io/blog/2022/11/02/data-lifecycle-questdb/">detach partitions to save space</a>.</p> </div> <div class="paragraph"> <p>Give this <a href="https://github.com/questdb/kafka-questdb-connector/issues/new">sample application</a> a try and join the <a href="https://slack.questdb.io/">QuestDB Slack community</a> if you have any questions.</p> </div> </div> </div>]]></content><author><name>Yitaek Hwang</name></author><category term="questdb"/><category term="kafka"/><category term="debezium"/><category term="time series"/><summary type="html"><![CDATA[This tutorial was originally published by QuestDB, where guest contributor, Yitaek Hwang, shows us how to stream data into QuestDB with change data capture via Debezium and Kafka Connect.]]></summary></entry></feed>