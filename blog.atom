<?xml version="1.0" encoding="utf-8" ?>
<feed xml:lang="en-US" xmlns="http://www.w3.org/2005/Atom">
<id>https://debezium.io/</id>
<title>Debezium Blog</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<link href="https://debezium.io/blog.atom" rel="self" type="application/atom+xml" />
<link href="https://debezium.io/" rel="alternate" type="text/html" />
<entry>
<id>https://debezium.io/blog/2020/01/22/outbox-quarkus-extension/</id>
<title>Outbox Event Router goes Supersonic!</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2020-01-22T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/01/22/outbox-quarkus-extension/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="outbox"></category>
<category term="quarkus"></category>
<summary>



Outbox as in that folder in my email client?
No, not exactly but there are some similarities!


The term outbox describes a pattern that allows independent components or services to perform read your own write semantics while concurrently providing a reliable, eventually consistent view to those writes across component or service boundaries.


You can read more about the Outbox pattern and how it applies to microservices in our blog post, Reliable Microservices Data Exchange With the Outbox Patttern.


So what exactly is an Outbox Event Router?


In Debezium version 0.9.3.Final, we introduced a ready-to-use Single Message Transform (SMT) that builds on the Outbox pattern to...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Outbox as in that folder in my email client?
No, not exactly but there are some similarities!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The term outbox describes a pattern that allows independent components or services to perform &lt;em&gt;read your own write&lt;/em&gt; semantics while concurrently providing a reliable, eventually consistent view to those writes across component or service boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can read more about the Outbox pattern and how it applies to microservices in our blog post, &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;Reliable Microservices Data Exchange With the Outbox Patttern&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what exactly is an Outbox Event Router?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Debezium version 0.9.3.Final, we introduced a ready-to-use &lt;a href=&quot;https://kafka.apache.org/documentation/#connect_transforms&quot;&gt;Single Message Transform&lt;/a&gt; (SMT) that builds on the Outbox pattern to propagate data change events using Debezium and Kafka.
Please see the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html&quot;&gt;documentation&lt;/a&gt; for details on how to use this transformation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;going_supersonic_with_quarkus&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#going_supersonic_with_quarkus&quot; /&gt;Going Supersonic with Quarkus!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;http://www.quarkus.io&quot;&gt;Quarkus&lt;/a&gt; is a Kubernetes Native Java framework that is tailored for GraalVM and HotSpot using the &lt;em&gt;best-of-breed&lt;/em&gt; Java technologies and standards.
Quarkus aims to offer developers a unified reactive and imperative programming model to address a wide range of application architectures.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what does all this mean exactly in laymen’s terms?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In short, the Debezium community can now leverage the Outbox pattern in a Quarkus-based application using a ready-to-use extension that works in parallel with your Debezium connector to emit change data events.
The Debezium Outbox extension for Quarkus can be used in both JVM or Native image modes in Quarkus.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;how_to_get_it&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#how_to_get_it&quot; /&gt;How to get it?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Currently the dependency must be manually added to your Quarkus application’s &lt;code&gt;pom.xml&lt;/code&gt; as shown below.
There are plans to make this extension available in the Quarkus extension catalogue as well as via Quarkus&#39; Maven plugin in a future release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;io.debezium.quarkus&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;debezium-quarkus-outbox&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;1.1.0.Alpha1&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the time of this blog, the extension was released as &lt;em&gt;1.1.0.Alpha1&lt;/em&gt;.&lt;br&gt;
A newer version of the extension may be available, see &lt;a href=&quot;https://debezium.io/releases/&quot;&gt;Releases&lt;/a&gt; for details.&lt;/br&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;using_the_extension&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_the_extension&quot; /&gt;Using the extension&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium Outbox extension uses the Observer pattern to monitor when the user application emits an object that implements the &lt;code&gt;io.debezium.outbox.quarkus.ExportedEvent&lt;/code&gt; interface.
This allows the Quarkus application behavior to be completely decoupled from that of the extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Lets walk through a simple example where a service is responsible for storing newly created orders and then emits an event that could be used to notify other interested services that an order has been created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So to get started, we’ll begin by first implementing &lt;code&gt;OrderCreatedEvent&lt;/code&gt;, an implementation of &lt;code&gt;ExportedEvent&lt;/code&gt;.
This event is used to signal when an &lt;code&gt;Order&lt;/code&gt; has been saved by the &lt;code&gt;OrderService&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class OrderCreatedEvent implements ExportedEvent&amp;lt;String, JsonNode&amp;gt; {
    private final long orderId;
    private final JsonNode payload;
    private final Instant created;

    public OrderCreatedEvent(Instant createdAt, Order order) {
        this.orderId = order.getId();
        this.payload = convertOrderToJsonNode(order);
        this.created = createdAt;
    }

    @Override
    public String getAggregateId() {
        return String.valueOf(orderId);
    }

    @Override
    public String getAggregateType() {
        return &quot;Order&quot;;
    }

    @Override
    public JsonNode getPayload() {
        return payload;
    }

    @Override
    public String getType() {
        return &quot;OrderCreated&quot;;
    }

    @Override
    public Instant getTimestamp() {
        return created;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is the contract that defines how a Quarkus application is to provide the extension with the data to persist to the outbox database table.
This contract exposes several different values discussed below:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;aggregate_id&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#aggregate_id&quot; /&gt;Aggregate Id&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aggregate id is used when emitting messages to Kafka as the message key to preserve message order.
In this example, the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; returns the order identifier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock tip&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is parameterized and the first argument of the parameter argument list allows the application to specify the return data type for the aggregate id.
While this example uses a &lt;code&gt;String&lt;/code&gt;, the value returned can be any persistable object type.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;aggregate_type&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#aggregate_type&quot; /&gt;Aggregate Type&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aggregate type is a string-based value that is used to append to the Kafka topic name and also assists in routing of the given message inside the Outbox Event Router SMT.
In this example, we use &lt;code&gt;Order&lt;/code&gt; and when using the default configuration of the SMT, messages would be found in the &lt;code&gt;outbox.event.Order&lt;/code&gt; topic.
Please see the &lt;code&gt;route.topic.replacement&lt;/code&gt; in the &lt;a href=&quot;documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;SMT configuration options&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;type&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#type&quot; /&gt;Type&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The message type is a string value that is emitted in the Kafka message’s envelope.
In this example, the value in the message envelope would be &lt;code&gt;OrderCreated&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;timestamp&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#timestamp&quot; /&gt;Timestamp&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By default, the Outbox Event Router SMT emits outbox events using the current timestamp when processing records but this may not always be sufficient for every use case.
This field allows the source application to specify an &lt;code&gt;Instant&lt;/code&gt; that can then be configured through the &lt;a href=&quot;documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;SMT configuration options&lt;/a&gt; to be used as the Kafka message timestamp instead.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect3&quot;&gt;
&lt;h4 id=&quot;payload&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#payload&quot; /&gt;Payload&lt;/h4&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The payload is the message content or value and is what is consumed by consumers of the Kafka topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock tip&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-tip&quot; title=&quot;Tip&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;ExportedEvent&lt;/code&gt; interface is parameterized and the second argument of the parameter argument list allows the application to specify the return data type for the payload.
While this example uses a &lt;code&gt;JsonNode&lt;/code&gt; to store a JSON representation of the &lt;code&gt;Order&lt;/code&gt;, the payload can be any persistable object type.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If multiple implementations of &lt;code&gt;ExportedEvent&lt;/code&gt; exist in a Quarkus application, they must all use the same signature.
If different signatures are required, the code should be split into different Quarkus applications because all &lt;code&gt;ExportedEvent&lt;/code&gt; implementations will be stored in the same database outbox table for a given Quarkus application.
We are currently investigating alternatives to loosen this restriction in a future release to allow multiple variants within the same application.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By itself, this &lt;code&gt;OrderCreatedEvent&lt;/code&gt; does nothing on its own.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next we want to implement an application component that is responsible for persisting the order to the database and then to emit the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; event.
The &lt;code&gt;OrderService&lt;/code&gt; class below uses JPA to persist the &lt;code&gt;Order&lt;/code&gt; entity and then &lt;code&gt;javax.enterprise.event.Event&amp;lt;T&amp;gt;&lt;/code&gt; to notify the outbox extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {
    @Inject
    EntityManager entityManager;

    @Inject
    Event&amp;lt;ExportedEvent&amp;lt;String, JsonNode&amp;gt;&amp;gt; event;

    @Transactional
    public Order addOrder(Order order) {
        entityManager.persist(order);
        event.fire(new OrderCreatedEvent(Instant.now(), order));
        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Before starting the application, certain configuration settings must be specified in &lt;code&gt;application.properties&lt;/code&gt;.
An example configuration might look like the following where we specify the database to connect to as well as how the persistence provider, Hibernate, is to operate.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.datasource.driver=org.postgresql.Driver
quarkus.datasource.url=jdbc:postgresql://order-db:5432/orderdb?currentSchema=orders
quarkus.datasource.username=user
quarkus.datasource.password=password
quarkus.hibernate-orm.database.generation=update
quarkus.hibernate-orm.dialect=org.hibernate.dialect.PostgreSQLDialect
quarkus.hibernate-orm.log.sql=true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By starting the application with this configuration the outbox table &lt;code&gt;OutboxEvent&lt;/code&gt; will be created in the &lt;code&gt;orders&lt;/code&gt; schema of the the &lt;code&gt;order-db&lt;/code&gt; database with the following layout:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;orderdb=# \d orders.outboxevent
                        Table &quot;orders.outboxevent&quot;
    Column     |            Type             | Collation | Nullable | Default
---------------+-----------------------------+-----------+----------+---------
 id            | uuid                        |           | not null |
 aggregatetype | character varying(255)      |           | not null |
 aggregateid   | character varying(255)      |           | not null |
 type          | character varying(255)      |           | not null |
 timestamp     | timestamp without time zone |           | not null |
 payload       | character varying(8000)     |           |          |
Indexes:
    &quot;outboxevent_pkey&quot; PRIMARY KEY, btree (id)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using &lt;code&gt;JsonNode&lt;/code&gt; as the payload return type, the extension uses a JPA attribute converter to store the contents as a string in the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Should the table or column names not fit your naming convention, they can be customized with several &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html#_build_time_configuration_options&quot;&gt;build-time configuration options&lt;/a&gt;.
For example, if you wanted the table to be named &lt;code&gt;outbox&lt;/code&gt; rather than &lt;code&gt;outboxevent&lt;/code&gt; add the following line to the &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.debezium-outbox.table-name=outbox&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you enabled SQL logging or check the row count of the outbox table, you might find it unusual that after saving the order that a record is inserted into the outbox table but then is immediately deleted.
This is the default behavior since rows are not required to be retained for Debezium to pick up the change.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If row retention is required, this can be configured using a &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html#_runtime_configuration_options&quot;&gt;run-time configuration option&lt;/a&gt;.
In order to enable row retention, add the following configuration to the &lt;code&gt;application.properties&lt;/code&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-properties&quot; data-lang=&quot;properties&quot;&gt;quarkus.debezium-outbox.remove-after-insert=false&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;setting_up_the_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#setting_up_the_connector&quot; /&gt;Setting up the connector&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Up to this point we’ve covered how to configure and use the extension in a Quarkus application to save events into the outbox database table.
The last step is to configure the Debezium connector to monitor the outbox and emit those records to Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re going to use the following connector configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;connector.class&quot;: &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
  &quot;tasks.max&quot;: &quot;1&quot;,
  &quot;database.hostname&quot;: &quot;order-db&quot;,
  &quot;database.port&quot;: &quot;5432&quot;,
  &quot;database.user&quot;: &quot;user&quot;,
  &quot;database.password&quot;: &quot;password&quot;,
  &quot;database.dbname&quot;: &quot;orderdb&quot;,
  &quot;database.server.name&quot;: &quot;dbserver1&quot;,
  &quot;schema.whitelist&quot; : &quot;orders&quot;,
  &quot;table.whitelist&quot;: &quot;orders.outboxevent&quot;,
  &quot;tombstones.on.delete&quot;: &quot;false&quot;,
  &quot;transforms&quot;: &quot;outbox&quot;,
  &quot;transforms.outbox.type&quot; : &quot;io.debezium.transforms.outbox.EventRouter&quot;,
  &quot;transforms.outbox.route.topic.replacement&quot;: &quot;${routedByValue}.events&quot;,
  &quot;transforms.outbox.table.field.event.timestamp&quot;: &quot;timestamp&quot;,
  &quot;transforms.outbox.table.fields.additional.placement&quot;: &quot;type:header:eventType&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A vast majority of this is standard Debezium connector configuration, but what is important are the last several lines that begin with &lt;strong&gt;transforms&lt;/strong&gt;.
These are configuration options that are used by Kafka Connect to configure and call the Outbox Event Router SMT.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This configuration uses a custom &lt;code&gt;route.topic.replacement&lt;/code&gt; configuration property.
This setting will instead route &lt;code&gt;OrderCreatedEvent&lt;/code&gt; rows from the outbox to the &lt;code&gt;Order.events&lt;/code&gt; topic rather than the default &lt;code&gt;outbox.events.Order&lt;/code&gt; topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This configuration also specifies the &lt;code&gt;field.event.timestamp&lt;/code&gt; configuration property.
This setting will instead populate the Kafka message time from the &lt;code&gt;timestamp&lt;/code&gt; field in the outbox database table rather than the current timestamp when processing the row.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html#configuration-options&quot;&gt;Outbox Event Router Configuration Options&lt;/a&gt; for details on how to configure the SMT.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the connector is running, the &lt;code&gt;Order.events&lt;/code&gt; topic will be populated with messages from the outbox table.
The following JSON example represents an &lt;code&gt;Order&lt;/code&gt; which gets saved by the &lt;code&gt;OrderService&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;customerId&quot; : &quot;123&quot;,
    &quot;orderDate&quot; : &quot;2019-01-31T12:13:01&quot;,
    &quot;lineItems&quot; : [
        {
            &quot;item&quot; : &quot;Debezium in Action&quot;,
            &quot;quantity&quot; : 2,
            &quot;totalPrice&quot; : 39.98
        },
        {
            &quot;item&quot; : &quot;Debezium for Dummies&quot;,
            &quot;quantity&quot; : 1,
            &quot;totalPrice&quot; : 29.99
        }
    ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When examining the &lt;code&gt;Order.events&lt;/code&gt; topic, the event emitted will look like the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;key&quot;: &quot;1&quot;,
  &quot;headers&quot;: &quot;id=cc74eac7-176b-44e7-8bda-413a5088ca66,eventType=OrderCreated&quot;
}
&quot;{\&quot;id\&quot;:1,\&quot;customerId\&quot;:123,\&quot;orderDate\&quot;:\&quot;2019-01-31T12:13:01\&quot;,\&quot;lineItems\&quot;:[{\&quot;id\&quot;:1,\&quot;item\&quot;:\&quot;Debezium in Action\&quot;,\&quot;quantity\&quot;:2,\&quot;totalPrice\&quot;:39.98,\&quot;status\&quot;:\&quot;ENTERED\&quot;},{\&quot;id\&quot;:2,\&quot;item\&quot;:\&quot;Debezium for Dummies\&quot;,\&quot;quantity\&quot;:1,\&quot;totalPrice\&quot;:29.99,\&quot;status\&quot;:\&quot;ENTERED\&quot;}]}&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;wrapping_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#wrapping_up&quot; /&gt;Wrapping up&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is really simple and easy to setup and use the Debezium Outbox extension.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We have a complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;example&lt;/a&gt; in our examples repository that uses the order service described here as well as a shipment service that consumes the events.
For more details on the extension, refer to the &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;Outbox Quarkus Extension&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;future_plans&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_plans&quot; /&gt;Future Plans&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The current implementation of the Debezium Outbox extension works quite well, but we acknowledge there is still room for improvement.
Some of the things we’ve already identified and have plans to include in future iterations of the extension are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Avro serialization support for event payload&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full outbox table column attribute control, e.g. definition, length, precision, scale, and converters.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Complete outbox table customization using a user-supplied entity class.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Allow varied signatures of &lt;code&gt;ExportedEvent&lt;/code&gt; within a single application.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are currently tracking all future changes to this extension in &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1711&quot;&gt;DBZ-1711&lt;/a&gt;.
As always we welcome any and all feedback, so feel free to let us know in that issue, on Gitter, or the mailing lists.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2020/01/16/debezium-1-1-alpha1-released/</id>
<title>Debezium 1.1.0.Alpha1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2020-01-16T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2020/01/16/debezium-1-1-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<summary>



Did you know January 16th is National Nothing Day?
It&#8217;s the one day in the year without celebrating, observing or honoring anything.


Well, normally, that is.
Because we couldn&#8217;t stop ourselves from sharing the news of the Debezium 1.1.0.Alpha1 release with you!
It&#8217;s the first release after Debezium 1.0,
and there are some really useful features coming with it.
Let&#8217;s take a closer look.




Quarkus Outbox Pattern Extension


The outbox pattern is a great way for letting services update their own database
(e.g. to persist a new purchase order) as well as emitting corresponding events to other services in a reliable and consistent way.
The pattern avoids unsafe dual writes,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Did you know January 16th is &lt;a href=&quot;https://en.wikipedia.org/wiki/National_Nothing_Day&quot;&gt;National Nothing Day&lt;/a&gt;?
It’s the one day in the year without celebrating, observing or honoring anything.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Well, normally, that is.
Because we couldn’t stop ourselves from sharing the news of the Debezium &lt;strong&gt;1.1.0.Alpha1&lt;/strong&gt; release with you!
It’s the first release after Debezium 1.0,
and there are some really useful features coming with it.
Let’s take a closer look.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;quarkus_outbox_pattern_extension&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#quarkus_outbox_pattern_extension&quot; /&gt;Quarkus Outbox Pattern Extension&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt; is a great way for letting services update their own database
(e.g. to persist a new purchase order) as well as emitting corresponding events to other services in a reliable and consistent way.
The pattern avoids unsafe dual writes, but also doesn’t require distributed transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium already supports this pattern via its &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/configuration/outbox-event-router.html&quot;&gt;outbox event router&lt;/a&gt;,
which can be used to stream events from one outbox table into different topics in Apache Kafka.
As we saw a growing adoption of the pattern and usage of this router,
we wanted to take things to the next level and also provide a component to simplify the creation of outbox events within your application itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For that purpose there’s now a &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/outbox.html&quot;&gt;brand-new extension&lt;/a&gt; for &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;, a &quot;Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards&quot;.
Using this extension it becomes as simple as firing plain CDI events to produce outbox events:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {

    @Inject
    Event&amp;lt;ExportedEvent&amp;lt;?, ?&amp;gt;&amp;gt; event;

    @Transactional
    public PurchaseOrder addOrder(PurchaseOrder order) {
        // process and persist the order...

        // create a corresponding outbox event to notify other services
        event.fire(OrderCreatedEvent.of(order));

        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the event has been persisted in the outbox table and the transaction has been committed,
Debezium will capture it and propagate it to downstream consumers via Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re planning to publish a detailed post with a complete end-to-end example for implementing the outbox pattern with this Debezium and this Quarkus extension next week, so stay tuned.
To read up on the outbox pattern in the meantime, please refer to the blog post linked above or to any of &lt;a href=&quot;https://dzone.com/articles/implementing-the-outbox-pattern&quot;&gt;these&lt;/a&gt; &lt;a href=&quot;https://medium.com/engineering-varo/event-driven-architecture-and-the-outbox-pattern-569e6fba7216&quot;&gt;great&lt;/a&gt; &lt;a href=&quot;https://thoughts-on-java.org/outbox-pattern-with-cdc-and-debezium/&quot;&gt;posts&lt;/a&gt; from the community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cloudevents_support&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cloudevents_support&quot; /&gt;CloudEvents Support&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://cloudevents.io/&quot;&gt;CloudEvents&lt;/a&gt; is a &quot;specification for describing event data in a common way&quot;, aiming at providing &quot;interoperability across services, platforms and systems&quot;.
By means of a new Kafka Connect message converter (&lt;code&gt;io.debezium.converters.CloudEventsConverter&lt;/code&gt;), Debezium now can &lt;a href=&quot;https://debezium.io/documentation/reference/1.1/integrations/cloudevents.html&quot;&gt;emit change events&lt;/a&gt; that adhere to the CloudEvents specification:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;{
  &quot;id&quot; : &quot;name:test_server;lsn:29274832;txId:565&quot;,
  &quot;source&quot; : &quot;/debezium/postgresql/test_server&quot;,
  &quot;specversion&quot; : &quot;1.0&quot;,
  &quot;type&quot; : &quot;io.debezium.postgresql.datachangeevent&quot;,
  &quot;time&quot; : &quot;2020-01-13T13:55:39.738Z&quot;,
  &quot;datacontenttype&quot; : &quot;application/json&quot;,
  &quot;iodebeziumop&quot; : &quot;r&quot;,
  &quot;iodebeziumversion&quot; : &quot;1.1.0.Alpha1&quot;,
  &quot;iodebeziumconnector&quot; : &quot;postgresql&quot;,
  &quot;iodebeziumname&quot; : &quot;test_server&quot;,
  &quot;iodebeziumtsms&quot; : &quot;1578923739738&quot;,
  &quot;iodebeziumsnapshot&quot; : &quot;true&quot;,
  &quot;iodebeziumdb&quot; : &quot;postgres&quot;,
  &quot;iodebeziumschema&quot; : &quot;s1&quot;,
  &quot;iodebeziumtable&quot; : &quot;a&quot;,
  &quot;iodebeziumtxId&quot; : &quot;565&quot;,
  &quot;iodebeziumlsn&quot; : &quot;29274832&quot;,
  &quot;iodebeziumxmin&quot; : null,
  &quot;data&quot; : {
    &quot;before&quot; : null,
    &quot;after&quot; : {
      &quot;pk&quot; : 1,
      &quot;name&quot; : &quot;Bob&quot;
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With CloudEvents, each event contains a few defined attributes such as &lt;code&gt;id&lt;/code&gt;, &lt;code&gt;source&lt;/code&gt; and &lt;code&gt;type&lt;/code&gt;.
The actual event payload can be found in the &lt;code&gt;data&lt;/code&gt; attribute, which in the case of Debezium is the structure of old and new state of the affected database record.
Most of the other Debezium change events attributes (&lt;code&gt;op&lt;/code&gt;, timestamp, source metadata) are mapped to custom attributes using the &lt;code&gt;iodebezium&lt;/code&gt; prefix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;data&lt;/code&gt; attribute as well as the entire event can be encoded using JSON or Avro.
Initially, only the &quot;structured mode&quot; of CloudEvents is supported, i.e. all the attributes are part of the event structure, which is the Kafka record value in this case.
In a future release we’ll also add support for the CloudEvents &quot;binary mode&quot;,
where only the &lt;code&gt;data&lt;/code&gt; attribute is part of the event structure, while all other attributes will be mapped the (Kafka) header attributes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;CloudEvents support is under active development, so details around the format likely will change in future versions as this feature matures.
We’d love to get your feedback on this and learn from your insights and experiences with CloudEvents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_changes&quot; /&gt;Further Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these two larger features, a number of smaller improvements and fixes has been done for Debezium 1.1 Alpha1:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Column value masking for Postgres, allowing to replace sensitive column values with asterisks (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1685&quot;&gt;DBZ-1685&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several fixes to the MySQL DDL parser related to trigger definitions (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1699&quot;&gt;DBZ-1699&lt;/a&gt;) and the &lt;code&gt;SIGNAL&lt;/code&gt; keyword (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1691&quot;&gt;DBZ-1691&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Two bugfixes around time and precision thereof (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1688&quot;&gt;DBZ-1688&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1707&quot;&gt;DBZ-1707&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Altogether, &lt;a href=&quot;https://debezium.io/releases/1.1/release-notes/#release-1.1.0-alpha1&quot;&gt;17 issues&lt;/a&gt; were fixed for this release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to &lt;a href=&quot;https://github.com/oscerd&quot;&gt;Andrea Cosentino&lt;/a&gt;, &lt;a href=&quot;https://github.com/vasilyulianko-visma&quot;&gt;Vasily Ulianko&lt;/a&gt;, &lt;a href=&quot;https://github.com/vedit&quot;&gt;Vedit Firat Arig&lt;/a&gt;, &lt;a href=&quot;https://github.com/liulangwa&quot;&gt;Yongjun Du&lt;/a&gt; and &lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Going forward, we’ll continue with further Debezium 1.1 preview releases every two to three weeks.
Take a look at the &lt;a href=&quot;https://debezium.io/roadmap/&quot;&gt;roadmap&lt;/a&gt; to see what’s coming up, or get in touch to tell us about your specific feature requirements!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/18/debezium-1-0-0-final-released/</id>
<title>Streaming Now: Debezium 1.0 Final Is Out</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-12-18T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/18/debezium-1-0-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="mongodb"></category>
<category term="cassandra"></category>
<category term="oracle"></category>
<summary>



Today it&#8217;s my great pleasure to announce the availability of Debezium 1.0.0.Final!


Since the initial commit in November 2015,
the Debezium community has worked tirelessly to realize the vision of building a comprehensive open-source low-latency platform for change data capture (CDC) for a variety of databases.


Within those four years, Debezium&#8217;s feature set has grown tremendously: stable, highly configurable CDC connectors for MySQL, Postgres, MongoDB and SQL Server, incubating connectors for Apache Cassandra and Oracle, facilities for transforming and routing change data events, support for design patterns such as the outbox pattern and much more.
A very active and welcoming community of users, contributors...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Today it’s my great pleasure to announce the availability of Debezium &lt;strong&gt;1.0.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Since the &lt;a href=&quot;https://github.com/debezium/debezium/commit/0a99ed67cd8f74d6f451b0a2d3809e23127e4698&quot;&gt;initial commit&lt;/a&gt; in November 2015,
the Debezium community has worked tirelessly to realize the vision of building a comprehensive open-source low-latency platform for change data capture (CDC) for a variety of databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Within those four years, Debezium’s feature set has grown tremendously: stable, highly configurable CDC connectors for MySQL, Postgres, MongoDB and SQL Server, incubating connectors for Apache Cassandra and Oracle, facilities for transforming and routing change data events, support for design patterns such as the outbox pattern and much more.
A very active and welcoming community of users, contributors and committers has formed around the project.
Debezium is deployed to production at lots of organizations from all kinds of industries,
some with huge installations, using hundreds of connectors to stream data changes out of thousands of databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 1.0 release marks an important milestone for the project:
based on all the production feedback we got from the users of the 0.x versions, we figured it’s about time to express the maturity of the four stable connectors in the version number, too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;why_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why_debezium&quot; /&gt;Why Debezium?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of the things making it so enjoyable to work on Debezium as a tool for change data capture is the variety of potential use cases.
When presenting the project at conferences,
it’s just great to see how people quickly get excited when they realize all the possibilities enabled by Debezium and CDC.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In a nutshell, Debezium is one big enabler for letting you react to changes in your data with a low latency.
Or, as one conference attendee recently put it, it’s &quot;like the observer pattern, but for your database&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here’s a few things we’ve seen Debezium being used for as a ingestion component in data streaming pipelines:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Replicating data from production databases to other databases and data warehouses&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Feeding data to search services like Elasticsearch or Apache Solr&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Updating or invalidating caches&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using Debezium with Apache Kafka and its rich ecosystem of sink connectors,
setting up such integrations can be done without any coding,
just by means of deploying and configuring connectors in Kafka Connect:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/debezium_kafka_pipeline.png&quot; style=&quot;max-width:95%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Data Streaming Pipeline With Debezium&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But there are many other use cases of CDC which go beyond just moving data from A to B.
When adding stream processing into the picture, e.g. via Kafka Streams or Apache Flink,
CDC enables you to run &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kstreams-live-update&quot;&gt;time-windowed streaming queries&lt;/a&gt;, continuously updated as your operational data changes
(&quot;what’s the aggregated order revenue per category within the last hour&quot;).
You can use CDC to &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;build audit logs&lt;/a&gt; of your data,
telling who changed which data items at what time.
Or update denormalized views of your data, for the sake of efficient data retrieval, adhering to the CQRS pattern (Command Query Responsibility Segregation).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, CDC can also play a vital role in microservices architectures;
exchanging data between services and keeping local views of data owned by other services achieves a higher independence, without having to rely on synchronous API calls.
One particularly interesting approach in this context is the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt;,
which is &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/configuration/outbox-event-router.html&quot;&gt;well supported&lt;/a&gt; by Debezium.
In case you don’t start on the green field (who ever does?),
CDC can be used to implement the &lt;a href=&quot;https://martinfowler.com/bliki/StranglerFigApplication.html&quot;&gt;strangler pattern&lt;/a&gt; for moving from a monolithic design to microservices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can learn more about change data capture use cases with Debezium and Apache Kafka in &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/practical-change-data-streaming-use-cases-with-apache-kafka-and-debezium-qcon-san-francisco-2019&quot;&gt;this presentation&lt;/a&gt; from QCon San Francisco.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But you don’t have to take our word for it:
you can find lots of blog posts, conference talks and examples by folks using Debezium in production in our compilation of &lt;a href=&quot;https://debezium.io/documentation/online-resources/&quot;&gt;resources&lt;/a&gt;.
If you’d like to get a glimpse of who else already is using Debezium,
see our rapidly growing &lt;a href=&quot;https://debezium.io/community/users/&quot;&gt;list of reference users&lt;/a&gt;
(or send us a &lt;a href=&quot;https://github.com/debezium/debezium.github.io/blob/develop/community/users.asciidoc&quot;&gt;pull request&lt;/a&gt; to get your name added if your organization already is running Debezium in production).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_1_0&quot; /&gt;Debezium 1.0&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, let’s talk a little bit about the contents of the 1.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This version continues the effort we began in &lt;a href=&quot;https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/&quot;&gt;0.10&lt;/a&gt; to make sure the emitted event structures and configuration options of the connectors are correct and consistent.
While we’ve always been very careful to ensure a smooth upgrading experience, you can expect even more stability in this regard going forward after the 1.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve expanded the test coverage of databases (Postgres 12, SQL Server 2019, MongoDB 4.2),
upgraded our container images to OpenJDK 11
and now build against the latest version of Apache Kafka
(2.4.0; earlier versions continue to be supported, too).
And last but not least, we’ve also fixed a large number of bugs.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(1.0.0.Beta1%2C%201.0.0.Beta2%2C%201.0.0.Beta3%2C%201.0.0.CR1%2C%201.0.0.Final)&quot;&gt;96 issues&lt;/a&gt; were addressed in Debezium 1.0 and its preview releases (&lt;a href=&quot;https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/&quot;&gt;Beta3&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/&quot;&gt;CR1&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’re on 0.10 right now, the upgrade is mostly a drop-in replacement.
When coming from earlier versions, please make sure to read the &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/&quot;&gt;migration notes&lt;/a&gt; to learn about deprecated options, upgrading procedures and more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_most_important_part_the_debezium_community&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_most_important_part_the_debezium_community&quot; /&gt;The Most Important Part: The Debezium Community&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium couldn’t exist without its community of contributors and users.
I can’t begin to express how grateful I am for having the chance to be a member of this fantastic community,
interacting and working with folks from around the world towards our joint goal of building the leading open-source solution for change data capture.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, about 150 people have contributed to the different Debezium code repositories (please let me know if I’ve missed anybody):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Aaron Rosenberg, Addison Higham, Adrian Kreuziger, Akshath Patkar, Alexander Kovryga, Amit Sela, Andreas Bergmeier, Andras Istvan Nagy, Andrew Garrett, Andrew Tongen, Andrey Pustovetov, Anton Martynov, Arkoprabho Chakraborti, artiship, Ashhar Hasan, Attila Szucs, Barry LaFond, Bartosz Miedlar, Ben Williams, Bin Li, Bingqin Zhou, Braden Staudacher, Brandon Brown, Brandon Maguire, Cheng Pan, Ching Tsai, Chris Cranford, Chris Riccomini, Christian Posta, Chuck Ha, Cliff Wheadon, Collin Van Dyck, Cyril Scetbon, David Chen, David Feinblum, David Leibovic, David Szabo, Deepak Barr, Denis Mikhaylov, Dennis Campagna, Dennis Persson, Duncan Sands, Echo Xu, Eero Koplimets, Emrul Islam, Eric S. Kreiseir, Ewen Cheslack-Postava, Felix Eckhardt, Gagan Agrawal, Grant Cooksey, Guillaume Rosauro, Gunnar Morling, Gurnaaz Randhawa, Grzegorz Kołakowski, Hans-Peter Grahsl, Henryk Konsek, Horia Chiorean, Ian Axelrod, Ilia Bogdanov, Ivan Kovbas, Ivan Lorenz, Ivan Luzyanin, Ivan San Jose, Ivan Vucina, Jakub Cechacek, Jaromir Hamala, Javier Holguera, Jeremy Finzel, Jiri Pechanec, Johan Venant, John Martin, Jon Casstevens, Jordan Bragg, Jork Zijlstra, Josh Arenberg, Josh Stanfield, Joy Gao, Jure Kajzer, Keith Barber, Kevin Pullin, Kewen Chao, Krizhan Mariampillai, Leo Mei, Lev Zemlyanov, Listman Gamboa, Liu Hanlin, Luis Garcés-Erice, Maciej Bryński, MaoXiang Pan, Mario Mueller, Mariusz Strzelecki, Matteo Capitanio, Mathieu Rozieres, Matthias Wessendorf, Mike Graham, Mincong Huang, Moira Tagle, Muhammad Sufyian, Navdeep Agarwal, Nikhil Benesch, Olavi Mustanoja, Oliver Weiler, Olivier Lemasle, Omar Al-Safi, Ori Popowski, Orr Ganani, Peng Lyu, Peter Goransson, Peter Larsson, Philip Sanetra, Pradeep Mamillapalli, Prannoy Mittal, Preethi Sadagopan, pushpavanthar, Raf Liwoch, Ram Satish, Ramesh Reddy, Randall Hauch, Renato Mefi, Roman Kuchar, Sagar Rao, René Kerner, Rich O’Connell, Robert Coup, Sairam Polavarapu, Sanjay Kr Singh, Sanne Grinovero, Satyajit Vegesna, Saulius Valatka, Scofield Xu, Sherafudheen PM, Shivam Sharma, Shubham Rawat, Stanley Shyiko, Stathis Souris, Stephen Powis, Steven Siahetiong, Syed Muhammad Sufyian, Tautvydas Januskevicius, Taylor Rolison, Theofanis Despoudis, Thomas Deblock, Tom Bentley, Tomaz Lemos Fernandes, Tony Rizko, Wang-Yu-Chao, Wei Wu, WenZe Hu, William Pursell, Willie Cheong, Wout Scheepers, Yang Yang, Zheng Wang&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You’re amazing, and I would like to wholeheartedly thank each and everyone of you!
I’m sure our community will continue to grow in the future — I’d love it if we hit the mark of 200 contributors in 2020.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Equally important are our users; interacting with you in the chat, on the mailing list or at conferences and meet-ups is what helps to drive the direction of the project:
learning about your specific requirements and use cases (or bugs you’ve run into) is vital for deciding where to put the focus next.
A big thank you to you, too!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some of you even have shared their experiences with Debezium in conference talks and blog posts.
Nothing beats hearing the war stories of others and being able to learn from their experiences,
so you speaking about your insights around Debezium and CDC is incredibly helpful and highly appreciated!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s Next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s wrap up this post with a look to see what’s next in store for Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After some long over-due holidays, we’re planning to begin the work on Debezium 1.1 in January.
Some of the potential features you can look forward to are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support for the CloudEvents specification as a portable event format&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A Quarkus extension for implementing the outbox pattern&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A stand-alone Debezium server which will let you stream data change events to messaging infrastructure such as Amazon Kinesis&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Means of exposing transactional boundaries on a separate topic, allowing to aggregate all the events originating from one source transaction and process them at once&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Further progression of the incubating community-led connectors for Oracle and Apache Cassandra&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Of course, this roadmap is strongly influenced by the community, i.e. you.
So if you would like to see any particular items here, please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We also have some exciting blog posts in the workings,
e.g. on how to combine Debezium with the brand-new Kafka Connect &lt;a href=&quot;https://camel.apache.org/blog/Camel-Kafka-connector-intro/&quot;&gt;connector for Apache Camel&lt;/a&gt; or how to use the recently added support for non-key joins in Kafka Streams (&lt;a href=&quot;https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=74684836&quot;&gt;KIP-213&lt;/a&gt;) with Debezium change events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One more thing I’m super-thrilled about is Debezium becoming a supported component of the &lt;a href=&quot;https://www.redhat.com/en/blog/whats-new-red-hat-integration&quot;&gt;Red Hat Integration&lt;/a&gt; product.
Part of the current release is a &lt;a href=&quot;https://developers.redhat.com/blog/2019/11/22/red-hat-advances-debezium-cdc-connectors-for-apache-kafka-support-to-technical-preview/&quot;&gt;Tech Preview&lt;/a&gt; for the change data capture connectors for MySQL, Postgres, SQL Server and MongoDB.
This is great news for folks who wish to have commercial support by Red Hat for their CDC connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For now, let’s celebrate the release of Debezium 1.0 and look forward to what’s coming in 2020.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Onwards and Upwards!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/13/externalized-secrets/</id>
<title>Secrets externalization with Debezium connectors</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-12-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/13/externalized-secrets/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="secrets"></category>
<category term="mysql"></category>
<category term="example"></category>
<summary>

When a Debezium connector is deployed to a Kafka Connect instance it is sometimes necessary to keep database credentials hidden from other users of the Connect API.


Let&#8217;s remind how a connector registration request looks like for the MySQL Debezium connector:



{
    "name": "inventory-connector",
    "config": {
        "connector.class": "io.debezium.connector.mysql.MySqlConnector",
        "tasks.max": "1",
        "database.hostname": "mysql",
        "database.port": "3306",
        "database.user": "debezium",
     ...
</summary>
<content type="html">
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a Debezium connector is deployed to a Kafka Connect instance it is sometimes necessary to keep database credentials hidden from other users of the Connect API.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s remind how a connector registration request looks like for the MySQL Debezium connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
        &quot;tasks.max&quot;: &quot;1&quot;,
        &quot;database.hostname&quot;: &quot;mysql&quot;,
        &quot;database.port&quot;: &quot;3306&quot;,
        &quot;database.user&quot;: &quot;debezium&quot;,
        &quot;database.password&quot;: &quot;dbz&quot;,
        &quot;database.server.id&quot;: &quot;184054&quot;,
        &quot;database.server.name&quot;: &quot;dbserver1&quot;,
        &quot;database.whitelist&quot;: &quot;inventory&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;username&lt;/code&gt; and &lt;code&gt;password&lt;/code&gt; are passed to the API as plain strings.
Worse yet, anybody who has access to the Kafka Connect cluster and its REST API can issue a &lt;code&gt;GET&lt;/code&gt; request to obtain a configuration of the connector including the database credentials:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -s http://localhost:8083/connectors/inventory-connector | jq .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;name&quot;: &quot;inventory-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.user&quot;: &quot;debezium&quot;,
    &quot;database.server.id&quot;: &quot;184054&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;database.hostname&quot;: &quot;mysql&quot;,
    &quot;database.password&quot;: &quot;dbz&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
    &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;database.server.name&quot;: &quot;dbserver1&quot;,
    &quot;database.whitelist&quot;: &quot;inventory&quot;,
    &quot;database.port&quot;: &quot;3306&quot;
  },
  &quot;tasks&quot;: [
    {
      &quot;connector&quot;: &quot;inventory-connector&quot;,
      &quot;task&quot;: 0
    }
  ],
  &quot;type&quot;: &quot;source&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If one Kafka Connect cluster is shared by multiple connectors/teams, then this behaviour can be undesiable for security reasons.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To solve the problem &lt;a href=&quot;https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations&quot;&gt;KIP-297&lt;/a&gt; (&quot;Externalizing Secrets for Connect Configurations&quot;) was implemented in Kafka 2.0.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The externalization expects there is at least one implementation class of the &lt;code&gt;org.apache.kafka.common.config.provider.ConfigProvider&lt;/code&gt; interface.
Kafka Connect provides the reference implementation &lt;code&gt;org.apache.kafka.common.config.provider.FileConfigProvider&lt;/code&gt; that reads secrets from a file.
Available config providers are configured at Kafka Connect worker level (e.g. in &lt;code&gt;connect-distributed.properties&lt;/code&gt;) and are referred to from the connector configuration.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example of worker configuration would be this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;config.providers=file
config.providers.file.class=org.apache.kafka.common.config.provider.FileConfigProvider&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;and the connector registration request will refer to it like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
        &quot;tasks.max&quot;: &quot;1&quot;,
        &quot;database.hostname&quot;: &quot;mysql&quot;,
        &quot;database.port&quot;: &quot;3306&quot;,
        &quot;database.user&quot;: &quot;${file:/secrets/mysql.properties:user}&quot;,
        &quot;database.password&quot;: &quot;${file:/secrets/mysql.properties:password}&quot;,
        &quot;database.server.id&quot;: &quot;184054&quot;,
        &quot;database.server.name&quot;: &quot;dbserver1&quot;,
        &quot;database.whitelist&quot;: &quot;inventory&quot;,
        &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
        &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the Placeholder &lt;code&gt;${file:/secrets/mysql.properties:user}&lt;/code&gt; says that the file config provider should be used, reading the property file &lt;code&gt;/secrets/mysql.properties&lt;/code&gt; and extracting the &lt;code&gt;user&lt;/code&gt; property from it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The file config provider is probably the simplest possible implementation, and it can be expected that other providers will appear that will integrate with secret repositories or identity management systems.
It should be noted though that the file config provider is satisfactory in Kubernetes/OpenShift deployments, as &lt;code&gt;secrets&lt;/code&gt; objects could be injected into cluster pods as files and thus consumed by it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve created a version of the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial&quot;&gt;tutorial example&lt;/a&gt;, which demonstrates a deployment of externalized secrets. Please note the two environment variables in the Docker Compose &lt;code&gt;connect&lt;/code&gt; service:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-[source&quot; data-lang=&quot;[source&quot;&gt;...
- CONNECT_CONFIG_PROVIDERS=file
- CONNECT_CONFIG_PROVIDERS_FILE_CLASS=org.apache.kafka.common.config.provider.FileConfigProvider
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These environment variables are directly mapped into Kafka Connect worker properties as a functionality of the &lt;code&gt;debezium/connect&lt;/code&gt; image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When you issue the REST call to get the connector configuration, you will see that the sensitive information is externalized and masked from unauthorized users:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;curl -s http://localhost:8083/connectors/inventory-connector | jq .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;name&quot;: &quot;inventory-connector&quot;,
  &quot;config&quot;: {
    &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
    &quot;database.user&quot;: &quot;${file:/secrets/mysql.properties:user}&quot;,
    &quot;database.server.id&quot;: &quot;184054&quot;,
    &quot;tasks.max&quot;: &quot;1&quot;,
    &quot;database.hostname&quot;: &quot;mysql&quot;,
    &quot;database.password&quot;: &quot;${file:/secrets/mysql.properties:password}&quot;,
    &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
    &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;,
    &quot;name&quot;: &quot;inventory-connector&quot;,
    &quot;database.server.name&quot;: &quot;dbserver1&quot;,
    &quot;database.whitelist&quot;: &quot;inventory&quot;,
    &quot;database.port&quot;: &quot;3306&quot;
  },
  &quot;tasks&quot;: [
    {
      &quot;connector&quot;: &quot;inventory-connector&quot;,
      &quot;task&quot;: 0
    }
  ],
  &quot;type&quot;: &quot;source&quot;
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/tutorial#using-externalized-secrets&quot;&gt;README&lt;/a&gt; of the tutorial example for complete instructions.&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/</id>
<title>Debezium 1.0.0.CR1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-12-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/12/12/debezium-1-0-0-cr1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<summary>



Did you know December 12th is National Ding-a-Ling Day?
It&#8217;s the day to call old friends you haven&#8217;t heard from in a while.
So we thought we&#8217;d get in touch (not that is has been that long) with our friends, i.e. you, and share the news about the release of Debezium 1.0.0.CR1!


It&#8217;s the first, and ideally only, candidate release; so Debezium 1.0 should be out very soon.
Quite a few nice features found their way into CR1:




A SerDe (serializer/deserializer) for working with JSON-formatted change data events in Kafka Streams (DBZ-1533)


Advanced type support for Postgres: the long awaited enum types (DBZ-920), domain types (DBZ-1413)...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Did you know December 12th is National Ding-a-Ling Day?
It’s the day to call old friends you haven’t heard from in a while.
So we thought we’d get in touch (not that is has been &lt;strong&gt;that&lt;/strong&gt; long) with our friends, i.e. you, and share the news about the release of Debezium &lt;strong&gt;1.0.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s the first, and ideally only, candidate release; so Debezium 1.0 should be out very soon.
Quite a few nice features found their way into CR1:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A SerDe (serializer/deserializer) for &lt;a href=&quot;https://debezium.io/documentation/reference/1.0/configuration/serdes.html&quot;&gt;working with JSON-formatted change data events&lt;/a&gt; in Kafka Streams (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1533&quot;&gt;DBZ-1533&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Advanced type support for Postgres: the long awaited enum types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-920&quot;&gt;DBZ-920&lt;/a&gt;), domain types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1413&quot;&gt;DBZ-1413&lt;/a&gt;) and arrays of UUIDs (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1637&quot;&gt;DBZ-1637&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Graceful handling of MongoDB 4.0 transaction events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1215&quot;&gt;DBZ-1215&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Full support for Java 11, including the upgrade to Java 11 in the Debezium container images for Apache Kafka and Connect (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-969&quot;&gt;DBZ-969&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1402&quot;&gt;DBZ-1402&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There was a number of bug fixes, too:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Graceful handling of an empty history topic for the MySQL connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1201&quot;&gt;DBZ-1201&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Correct column filtering for SQL Server connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1617&quot;&gt;DBZ-1617&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for &lt;code&gt;ALTER TABLE …​ RENAME …​&lt;/code&gt; in MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1645&quot;&gt;DBZ-1645&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As were’re approaching the 1.0 Final release, we also took the time to check the configuration options of the different connectors for consistency.
Things were in pretty good shape already due to previous work towards unification in Debezium 0.10.
Only for the SQL Server and Oracle connectors, the snapshot mode &quot;initial_schema_only&quot; has been deprecated and will be removed in a future version. Please use &quot;schema_only&quot; instead, as known from the MySQL connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-585&quot;&gt;DBZ-585&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-cr1&quot;&gt;24 changes&lt;/a&gt;.
As always, this release wouldn’t have been possible without the help from folks of the community:
&lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/collinvandyck&quot;&gt;Collin Van Dyck&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gnaazr95&quot;&gt;Gurnaaz Randhawa&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivansjg&quot;&gt;Ivan San Jose&lt;/a&gt;,
&lt;a href=&quot;https://github.com/theodesp&quot;&gt;Theofanis Despoudis&lt;/a&gt; and
&lt;a href=&quot;https://github.com/deblockt&quot;&gt;Thomas Deblock&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to you! In total, not less than 144 people have contributed to the Debezium main code repository at this point.
Perhaps we can bump this number to 200 in 2020?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Barring any unforeseen issues, this candidate release should be the only one for Debezium 1.0,
and the final version should be in your hands before Christmas.
So please give the CR a try and let us know how it works for you!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/</id>
<title>Debezium 1.0.0.Beta3 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-11-14T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/11/14/debezium-1-0-0-beta3-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



While fall weather is in full swing, the Debezium community is not letting the unusually low, frigid temperatures get the best of us.  It is my pleasure to announce the release of Debezium 1.0.0.Beta3!


This new Debezium release includes several notable new features, enhancements, and fixes:




Built against Kafka Connect 2.3.1 (DBZ-1612)


Renamed drop_on_stop configuration parameter to drop.on.stop (DBZ-1595)


Standardized source information for Cassandra connector (DBZ-1408)


Propagate MongoDB replicator exceptions so they are visible from Kafka Connect&#8217;s status endpoint (DBZ-1583)


Envelope methods should accept Instant rather than long values for timestamps (DBZ-1607)


Erroneously reporting no tables captured (DBZ-1519)


Avoid Oracle connector attempting to analyze tables (DBZ-1569)


Toasted columns...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While fall weather is in full swing, the Debezium community is not letting the unusually low, frigid temperatures get the best of us.  It is my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta3&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Built against Kafka Connect 2.3.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1612&quot;&gt;DBZ-1612&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Renamed &lt;code&gt;drop_on_stop&lt;/code&gt; configuration parameter to &lt;code&gt;drop.on.stop&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1595&quot;&gt;DBZ-1595&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Standardized source information for Cassandra connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1408&quot;&gt;DBZ-1408&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Propagate MongoDB replicator exceptions so they are visible from Kafka Connect’s status endpoint (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1583&quot;&gt;DBZ-1583&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Envelope methods should accept &lt;code&gt;Instant&lt;/code&gt; rather than &lt;code&gt;long&lt;/code&gt; values for timestamps (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1607&quot;&gt;DBZ-1607&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Erroneously reporting no tables captured (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1519&quot;&gt;DBZ-1519&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avoid Oracle connector attempting to analyze tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1569&quot;&gt;DBZ-1569&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Toasted columns should contain &lt;code&gt;null&lt;/code&gt; in &lt;em&gt;before&lt;/em&gt; rather than &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1570&quot;&gt;DBZ-1570&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support PostgreSQL 11+ &lt;code&gt;TRUNCATE&lt;/code&gt; operations using &lt;code&gt;pgoutput&lt;/code&gt; decoder (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1576&quot;&gt;DBZ-1576&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;PostgreSQL connector times out in schema discovery for databases with many tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1579&quot;&gt;DBZ-1579&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Value of &lt;code&gt;ts_ms&lt;/code&gt; is not correct duing snapshot processing (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1588&quot;&gt;DBZ-1588&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Heartbeats are not generated for non-whitelisted tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1592&quot;&gt;DBZ-1592&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally there were improvements to the Docker container images to reduce their overall size and some build infrastructure improvements to apply automatic code formatting rules.  Details about code formatting changes can be found in the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/CONTRIBUTE.md#code-formatting&quot;&gt;CONTRIBUTE.md&lt;/a&gt; file.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta3&quot;&gt;27 changes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/dvfeinblum&quot;&gt;David Feinblum&lt;/a&gt;,
&lt;a href=&quot;https://github.com/rk3rn3r&quot;&gt;René Kerner&lt;/a&gt;,
&lt;a href=&quot;https://github.com/lga-zurich&quot;&gt;Luis Garcés-Erice&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jfinzel&quot;&gt;Jeremy Finzel&lt;/a&gt;,
&lt;a href=&quot;https://github.com/datumgeek&quot;&gt;Mike Graham&lt;/a&gt;,
Yang Yang,
Addison Higham&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/</id>
<title>Debezium 1.0.0.Beta2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-24T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/24/debezium-1-0-0-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



It is my pleasure to announce the release of Debezium 1.0.0.Beta2!


This new Debezium release includes several notable new features, enhancements, and fixes:




Support PostgreSQL LTREE columns with a logical data type (DBZ-1336)


Support for PostgreSQL 12 (DBZ-1542)


Validate configured PostgreSQL replication slot not contains no invalid characters (DBZ-1525)


Add MySQL DDL parser support for index creation VISIBLE and INVISIBLE keywords (DBZ-1534)


Add MySQL DDL parser support for granting SESSION_VARIABLES_ADMIN (DBZ-1535)


Fix MongoDB collection source struct field when collection name contains a dot (DBZ-1563)


Close idle transactions after performing a PostgreSQL snapshot (DBZ-1564)




Additionally the PostgreSQL connector was improved to warn users of a common situation where their configuration...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support PostgreSQL &lt;code&gt;LTREE&lt;/code&gt; columns with a logical data type (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1336&quot;&gt;DBZ-1336&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for PostgreSQL 12 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1542&quot;&gt;DBZ-1542&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Validate configured PostgreSQL replication slot not contains no invalid characters (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1525&quot;&gt;DBZ-1525&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add MySQL DDL parser support for index creation &lt;code&gt;VISIBLE&lt;/code&gt; and &lt;code&gt;INVISIBLE&lt;/code&gt; keywords (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1534&quot;&gt;DBZ-1534&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add MySQL DDL parser support for granting &lt;code&gt;SESSION_VARIABLES_ADMIN&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1535&quot;&gt;DBZ-1535&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fix MongoDB &lt;code&gt;collection&lt;/code&gt; source struct field when collection name contains a dot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1563&quot;&gt;DBZ-1563&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Close idle transactions after performing a PostgreSQL snapshot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1564&quot;&gt;DBZ-1564&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally the PostgreSQL connector was improved to warn users of a common situation where their configuration does not enable heartbeats and the monitored table(s) change less frequent than tables that are not monitored.
This often lead to the write ahead logs (WAL) consuming additional disk space creating a WAL backlog as the connector only flushes LSN information to PostgreSQL if the log contains events for tables that are monitored.
Our hope is this will help automated tools identify this problem earlier while also giving hints on how to avoid it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta2&quot;&gt;13 fixes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/grantcooksey&quot;&gt;Grant Cooksey&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mincong-h&quot;&gt;Mingcong Huang&lt;/a&gt;,
&lt;a href=&quot;https://github.com/navdeep710&quot;&gt;navdeep710&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/22/audit-logs-with-kogito/</id>
<title>Admin Service for Audit Logs with Kogito</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-22T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/22/audit-logs-with-kogito/" rel="alternate" type="text/html" />
<author>
<name>Maciej Swiderski</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="apache-kafka"></category>
<category term="kafka-streams"></category>
<category term="kogito"></category>
<category term="featured"></category>
<summary>



As a follow up to the recent Building Audit Logs with Change Data Capture and Stream Processing blog post,
we’d like to extend the example with admin features to make it possible to capture and fix any missing transactional data.


In the above mentioned blog post, there is a log enricher service used to combine data inserted or updated in the Vegetable database table with transaction context data such as




Transaction id


User name who performed the work


Use case that was behind the actual change e.g. "CREATE VEGETABLE"




This all works well as long as all the changes are done via the vegetable service. But...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a follow up to the recent &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;Building Audit Logs with Change Data Capture and Stream Processing&lt;/a&gt; blog post,
we’d like to extend the example with admin features to make it possible to capture and fix any missing transactional data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the above mentioned blog post, there is a log enricher service used to combine data inserted or updated in the Vegetable database table with transaction context data such as&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Transaction id&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;User name who performed the work&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use case that was behind the actual change e.g. &quot;CREATE VEGETABLE&quot;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This all works well as long as all the changes are done via the vegetable service. But is this always the case?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What about maintenance activities or migration scripts executed directly on the database level?
There are still a lot of such activities going on, either on purpose or because that is our old habits we are trying to change…&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;maintenance_on_database_level&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#maintenance_on_database_level&quot; /&gt;Maintenance on database level&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So let’s assume there is a need to do some maintenance on the inventory database that will essentially make changes to the data stored in the vegetable table. To make it simple, let’s just add a new entry into the vegetable table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;insert into inventory.vegetable (id, name, description) values (106, ‘cucumber, &#39;excellent&#39;);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once that is added you will see that the log enricher service is starting to print out quite a few log messages… and it does it constantly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-plain&quot; data-lang=&quot;plain&quot;&gt;log-enricher_1        | 2019-10-11 10:30:46,099 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:30:46,106 WARN  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) No metadata found for transaction {&quot;transaction_id&quot;:611}
log-enricher_1        | 2019-10-11 10:30:46,411 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:30:46,415 WARN  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) No metadata found for transaction {&quot;transaction_id&quot;:611}
log-enricher_1        | 2019-10-11 10:30:46,921 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Looking at the logs you can identify that it actually refers to the entry we just inserted (id 106).
In addition to that, it refers to missing transaction context data that it cannot find. That is the
consequence of doing it manually on database level instead of going through the  vegetable service.
There is no corresponding data in the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; Kafka topic and thus the log enricher cannot
correlate and by that merge/enrich them.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;kogito_to_the_rescue&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kogito_to_the_rescue&quot; /&gt;Kogito to the rescue&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There would be a really good feature (or a neat feature as Gunnar said) if we could have some sort of admin service that
 could help in resolving this kind of problems. Mainly because if such entry is added it will block the entire
 enrichment activity as the first missing message will hold off all others.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And here comes &lt;a href=&quot;https://kogito.kie.org&quot;&gt;Kogito&lt;/a&gt; - a cloud native business automation toolkit to build intelligent
business applications based on battle tested capabilities. In other words, it brings business processes and rules
to solve particular business problems. In this case the business problem is blocked log enrichment which can lead to
some lost opportunities (of various types).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What Kogito helps us with is to define our logic to understand what might get wrong, what needs to be done to resolve
it and what are the conditions that can lead to both problem and resolution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this particular case we use both processes and rules to make sure we get the context right and react to the events
behind the vegetable service. To be able to spot the erroneous situations we need to monitor two topics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt; - vegetable data change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; - events from vegetable service with additional context data&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So for that we define two business processes where each will be started based on incoming messages - from individual
Kafka topics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-process1.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Vegetable events process definition&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-process2.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Transaction context data process definition&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As illustrated above, both processes are initiated based on an incoming message. Then the logic afterwards is significantly different.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;Transaction context data&quot; process is responsible for just retrieving the event and pushing it into processing phase - that
 essentially means to insert it into the so called &quot;working memory&quot; that is used for rule evaluation. And at that moment it’s done.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;Vegetable event&quot; process starts in a similar way… it retrieves the message and then (first ignore snapshot messages
in the same way as the log enricher service) will wait for a predefined amount of time (2 seconds) before matching
vegetable and transaction context events. Once there is a match it will simple finish its execution. But if there is
no match found it will create a user task (that’s a task that requires human actors to provide data before process
can move forward).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is done via admin user interface (&lt;a href=&quot;http://localhost:8085/&quot;&gt;http://localhost:8085/&lt;/a&gt;) that allows to easily spot such instance and
work on them to fix missing data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_kogito-ui.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Admin service UI for fixing missing transaction context data&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the &lt;code&gt;Use case&lt;/code&gt; and &lt;code&gt;User name&lt;/code&gt; attributes are provided, the process will create a new transaction context event,
push it to the Kafka topic and complete itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the missing transaction context data event has been put on the topic the log enricher will resume its operation
and you will be able to see the following lines in the log:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-plain&quot; data-lang=&quot;plain&quot;&gt;log-enricher_1        | 2019-10-11 10:31:00,385 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Processing buffered change event for key {&quot;id&quot;:106}
log-enricher_1        | 2019-10-11 10:31:00,389 INFO  [io.deb.dem.aud.enr.ChangeEventEnricher] (auditlog-enricher-c9e5d1bb-d953-42b4-8dc6-bbc328f5344f-StreamThread-1) Enriched change event for key {&quot;id&quot;:106}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With this you can easily administrate the audit logs to make sure any erroneous situations are resolved quickly
to not affect any other activities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And if you would like to see everything in action, just watch this video:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;responsive-video&quot;&gt;
&lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/BNcFaE0AVow&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Or try it yourself by running the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;audit log example&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/17/debezium-newsletter-02-2019/</id>
<title>Debezium&#8217;s Newsletter 02/2019</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-17T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/17/debezium-newsletter-02-2019/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="newsletter"></category>
<summary>



Welcome to the Debezium community newsletter in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.




Upcoming Events




JokerConf - Practical change data streaming uses cases with Apache Kafka and Debezium


QCon San Fransisco - Practical change data streaming use cases with Apache Kafka and Debezium






Articles


There have been quite a number of blog posts about Debezium lately; here are some of the latest ones that you should not miss:


This recent blog by Gunnar Morling discusses how Debezium combined with Kafka stream post processing can deliver an enriched stream of...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Welcome to the Debezium community newsletter in which we share all things CDC related including blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot; /&gt;Upcoming Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://jokerconf.com/en/2019/talks/6nkqqv8zj4gmujxmo64ov3/&quot;&gt;JokerConf - Practical change data streaming uses cases with Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://qconsf.com/sf2019/presentation/practical-change-data-streaming-use-cases-apache-kafka-debezium&quot;&gt;QCon San Fransisco - Practical change data streaming use cases with Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot; /&gt;Articles&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There have been quite a number of blog posts about Debezium lately; here are some of the latest ones that you should not miss:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/&quot;&gt;recent blog&lt;/a&gt; by Gunnar Morling discusses how Debezium combined with Kafka stream post processing can deliver an enriched stream of events suitable for even the most complicated of tasks like audit tracking.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Rod Shokrian from Varo &lt;a href=&quot;https://medium.com/engineering-varo/event-driven-architecture-and-the-outbox-pattern-569e6fba7216&quot;&gt;recently blogged&lt;/a&gt; about their CDC solution and experiences using Debezium in conjunction with the Outbox Pattern.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Community involvement is critical to each Open Source project and Debezium is no different.  Joy Gao &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1&quot;&gt;blogs&lt;/a&gt; about her experience at WePay where CDC innovation brought Cassandra and Debezium together.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://speakerdeck.com/jbfletch/using-kafka-to-discover-events-hidden-in-your-database&quot;&gt;slide deck&lt;/a&gt; by Anna McDonald showcases Debezium capturing database change events in complex architectures to emit enriched, derivative-based events across your enterprise.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are quite a number of applicable use cases for Debezium.
Dave Cramer from Crunchy Data recently &lt;a href=&quot;https://info.crunchydata.com/blog/postgresql-change-data-capture-with-debezium&quot;&gt;blogged&lt;/a&gt; about his experiences using Debezium to replicate data between both a source and sink PostgreSQL environment using CDC and Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Data analytics are vital across lots of industries.
This &lt;a href=&quot;https://medium.com/convoy-tech/logs-offsets-near-real-time-elt-with-apache-kafka-snowflake-473da1e4d776&quot;&gt;post&lt;/a&gt; by Adrian Kreuziger discusses how Convoy used Debezium and Apache Kafka to design a low-latency data warehouse solution for the trucking industry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also checkout our &lt;a href=&quot;https://debezium.io/documentation/online-resources&quot;&gt;online resources&lt;/a&gt; for more…​&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;examples&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#examples&quot; /&gt;Examples&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An example is often much like a picture, its worth a thousand words.
Debezium’s &lt;a href=&quot;https://github.com/debezium/debezium-examples&quot;&gt;examples repository&lt;/a&gt; has recently undergone changes introducing new examples and update existing ones:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;Build Audit Logs using Debezium and Kafka Streams&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[NEW] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/postgres-toast&quot;&gt;Dealing with PostgreSQL TOAST columns&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/end-to-end-demo&quot;&gt;End to End demo now supports propagating delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-smt&quot;&gt;Unwrap SMT example now propagates delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;[UPDATE] &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/unwrap-mongodb-smt&quot;&gt;MongoDB Unwrap SMT example now propagates delete events&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot; /&gt;Time to upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/&quot;&gt;0.10.0.Final&lt;/a&gt; was released at the beginning of October.
If you are using the 0.9 branch, we urge you to check out the latest major release.
For details on the bug fixes, enhancements, and improvements that spanned 8 preview releases, check out the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes/&quot;&gt;release-notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team has also begun active development on the next major version, 1.0.
If you want details on the bug fixes, enhancements, and improvements, you can view &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes&quot;&gt;release-notes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot; /&gt;Questions and answers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/58337376/debezium-postgres-error-parameter-include-unchanged-toast-was-deprecated&quot;&gt;Debezium postgres ERROR: Parameter &quot;include-unchanged-toast&quot; was deprecated&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/57095096/unable-to-register-debezium-kafka-connect-connector-in-ssl-enabled-kafka-clust&quot;&gt;Unable to register Debezium (Kafka-Connect) connector in SSL enabled Kafka cluster&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msg/debezium/1vKTWwcf71I/L98pV5nnBgAJ&quot;&gt;Debezium or database triggers&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;using_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#using_debezium&quot; /&gt;Using Debezium?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We recently added a &lt;a href=&quot;https://www.debezium.io/community/users&quot;&gt;community users&lt;/a&gt; page to &lt;a href=&quot;https://www.debezium.io&quot; class=&quot;bare&quot;&gt;https://www.debezium.io&lt;/a&gt;.
If you are a user of Debezium and would like to be included, please send us a GitHub pull request or reach out to us directly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;getting_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#getting_involved&quot; /&gt;Getting involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It can often be overwhelming when starting work on an existing code base.
We welcome community contributions and we want to make the process of getting started extremely easy.
Below is a list of open issues that are currently labeled with &lt;code&gt;easy-starter&lt;/code&gt; if you want to dive in quick.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Avro serialization automatically when detecting link to schema registry (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-59&quot;&gt;DBZ-59&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add tests for using fallback values with default REPLICA IDENTITY (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1158&quot;&gt;DBZ-1158&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add ability to insert fields from op field in ExtractNewRecordState SMT (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1452&quot;&gt;DBZ-1452&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support CREATE TABLE …​ LIKE syntax for blacklisted source table (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1496&quot;&gt;DBZ-1496&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provide change event JSON Serde for Kafka Streams (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1533&quot;&gt;DBZ-1533&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Explore SMT for Externalizing large column values (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1541&quot;&gt;DBZ-1541&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Whitespaces not stripped from table.whitelist (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1546&quot;&gt;DBZ-1546&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;opportunities&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#opportunities&quot; /&gt;Opportunities&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We recently &lt;a href=&quot;https://twitter.com/debezium/status/1184514850627739649&quot;&gt;tweeted&lt;/a&gt; about an Internship opening on the Debezium project.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Are you in the Czech Republic area?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Are you passionate about Open Source?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Do you think change data capture is interesting?&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you answered yes to any or all of these, then you should definitely check out &lt;a href=&quot;https://global-redhat.icims.com/jobs/73814/software-developer-internship---debezium-project/job?hub=7&amp;amp;mobile=false&amp;amp;width=1470&amp;amp;height=500&amp;amp;bga=true&amp;amp;needsRedirect=false&amp;amp;jan1offset=60&amp;amp;jun1offset=120&quot;&gt;the details&lt;/a&gt; and apply!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We intend to publish new additions of this newsletter periodically.
Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.
You can reach out to us via any of our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/</id>
<title>Debezium 1.0.0.Beta1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-17T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/17/debezium-1-0-0-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



History is in the making as Debezium begins to sprint to its 1.0 milestone.
It&#8217;s my pleasure to announce the release of Debezium 1.0.0.Beta1!


This new Debezium release includes several notable new features, enhancements, and fixes:




ExtractNewDocumentState and EventRouter SMTs propagate heartbeat &amp; schema change messages (DBZ-1513)


Provides alternative mapping for INTERVAL columns via interval.handling.mode (DBZ-1498)


Ensure message keys have the right column order (DBZ-1507)


Warn of table locking problems in connector logs (DBZ-1280)




Additionally, several PostgreSQL issues were fixed to improve snapshotting large databases environments (DBZ-685) and specific circumstances where write ahead logs (WAL) would continue to consume diskspace (DBZ-892).


In total, this release contains 18 fixes.


Thanks...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;History is in the making as Debezium begins to sprint to its 1.0 milestone.
It’s my pleasure to announce the release of Debezium &lt;strong&gt;1.0.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release includes several notable new features, enhancements, and fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ExtractNewDocumentState and EventRouter SMTs propagate heartbeat &amp;amp; schema change messages (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1513&quot;&gt;DBZ-1513&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Provides alternative mapping for &lt;code&gt;INTERVAL&lt;/code&gt; columns via &lt;code&gt;interval.handling.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1498&quot;&gt;DBZ-1498&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ensure message keys have the right column order (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1507&quot;&gt;DBZ-1507&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Warn of table locking problems in connector logs (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1280&quot;&gt;DBZ-1280&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additionally, several PostgreSQL issues were fixed to improve snapshotting large databases environments (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-685&quot;&gt;DBZ-685&lt;/a&gt;) and specific circumstances where write ahead logs (WAL) would continue to consume diskspace (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-892&quot;&gt;DBZ-892&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In total, this release contains &lt;a href=&quot;https://debezium.io/releases/1.0/release-notes/#release-1.0.0-beta1&quot;&gt;18 fixes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to all the community members who helped make this happen:
&lt;a href=&quot;https://github.com/pushpavanthar&quot;&gt;Purushotham Pushpavanthar&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jfinsel&quot;&gt;Jeremy Finzel&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grantcooksey&quot;&gt;Grant Cooksey&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/</id>
<title>Strategies for Handling Unchanged Postgres TOAST Values</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-08T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/08/handling-unchanged-postgres-toast-values/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="postgres"></category>
<category term="kafka-streams"></category>
<summary>



Let&#8217;s talk about TOAST.
Toast?
No, TOAST!


So what&#8217;s that?
TOAST (The Oversized-Attribute Storage Technique) is a mechanism in Postgres which stores large column values in multiple physical rows, circumventing the page size limit of 8 KB.


    


Typically, TOAST storage is transparent to the user, so you don&#8217;t really have to care about it.
There&#8217;s an exception, though:
if a table row has changed, any unchanged values that were stored using the TOAST mechanism are not included in the message that Debezium receives from the database,
unless they are part of the table’s replica identity.
Consequently, such unchanged TOAST column value will not be contained...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s talk about TOAST.
Toast?
No, TOAST!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what’s that?
&lt;a href=&quot;https://www.postgresql.org/docs/current/storage-toast.html&quot;&gt;TOAST&lt;/a&gt; (The Oversized-Attribute Storage Technique) is a mechanism in Postgres which stores large column values in multiple physical rows, circumventing the page size limit of 8 KB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/postgres_toast.jpg&quot; style=&quot;max-width:100%; width:40%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;TOAST!&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Typically, TOAST storage is transparent to the user, so you don’t really have to care about it.
There’s an exception, though:
if a table row has changed, any &lt;em&gt;unchanged&lt;/em&gt; values that were stored using the TOAST mechanism are not included in the message that Debezium receives from the database,
unless they are part of the table’s &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#replica-identity&quot;&gt;replica identity&lt;/a&gt;.
Consequently, such unchanged TOAST column value will not be contained in Debezium data change events sent to Apache Kafka.
In this post we’re going to discuss different strategies for dealing with this situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When encountering an unchanged TOAST column value in the logical replication message received from the database,
the Debezium Postgres connector will represent that value with a configurable placeholder.
By default, that’s the literal &lt;code&gt;__debezium_unavailable_value&lt;/code&gt;,
but that value can be overridden using the &lt;code&gt;toasted.value.placeholder&lt;/code&gt; connector property.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s consider the following Postgres table definition as an example:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE TABLE customers (
  id SERIAL NOT NULL PRIMARY KEY,
  first_name VARCHAR(255) NOT NULL,
  last_name VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL UNIQUE,
  biography TEXT
);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here, the &lt;code&gt;biography TEXT&lt;/code&gt; column is a TOAST-able column, as its value may exceed the page size limit.
So when issuing an update such as &lt;code&gt;update inventory.customers set first_name = &#39;Dana&#39; where id = 1004;&lt;/code&gt;,
you might receive a data change event in Apache Kafka which looks like this
(assuming the table has the default replica identity):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;before&quot;: null,
  &quot;after&quot;: {
    &quot;id&quot;: 1004,
    &quot;first_name&quot;: &quot;Dana&quot;,
    &quot;last_name&quot;: &quot;Kretchmar&quot;,
    &quot;email&quot;: &quot;annek@noanswer.org&quot;,
    &quot;biography&quot;: &quot;__debezium_unavailable_value&quot;
  },
  &quot;source&quot;: {
    &quot;version&quot;: &quot;0.10.0.Final&quot;,
    &quot;connector&quot;: &quot;postgresql&quot;,
    &quot;name&quot;: &quot;dbserver1&quot;,
    &quot;ts_ms&quot;: 1570448151151,
    &quot;snapshot&quot;: &quot;false&quot;,
    &quot;db&quot;: &quot;sourcedb&quot;,
    &quot;schema&quot;: &quot;inventory&quot;,
    &quot;table&quot;: &quot;customers&quot;,
    &quot;txId&quot;: 627,
    &quot;lsn&quot;: 34650016,
    &quot;xmin&quot;: null
  },
  &quot;op&quot;: &quot;u&quot;,
  &quot;ts_ms&quot;: 1570448151611
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how the &lt;code&gt;biography&lt;/code&gt; field (whose value hasn’t changed with the &lt;code&gt;UPDATE&lt;/code&gt;) has the special &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; marker value.
Now, if change event consumers receive that placeholder value,
the question arises how they should react to this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One way, and &lt;strong&gt;certainly the easiest&lt;/strong&gt; from a consumer’s perspective, is to avoid the situation in the first place.
This can be achieved by using a &quot;replica identity&quot; of &lt;code&gt;FULL&lt;/code&gt; for the Postgres table in question.
Alternatively, the replica identity can be based on an index which comprises the TOAST-able column.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;excluding_unchanged_values&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#excluding_unchanged_values&quot; /&gt;Excluding Unchanged Values&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If changing the source table’s replica identity is not an option,
one approach for consumers that update a sink datastore (e.g. a database, cache or search index) is to ignore any field of a change event which has the placeholder value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This means that any column with the placeholder value must be omitted from the update statement executed on the sink datastore.
E.g. in terms of a SQL database, an specific &lt;code&gt;UPDATE&lt;/code&gt; statement must be built and executed which doesn’t contain the column(s) with the placeholder value.
Users of Hibernate ORM may feel reminded of the &quot;dynamic updates&quot; feature which works similar.
Some datastores and connectors might only support full updates, though, in which case this strategy isn’t viable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;triggers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#triggers&quot; /&gt;Triggers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One interesting variation of the &quot;ignore&quot; approach is the usage of triggers in the sink database:
registered for the column that may receive the marker value, they can &quot;veto&quot; such change and just keep the previously stored value instead.
The following shows an example of such a trigger in Postgres:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;CREATE OR REPLACE FUNCTION ignore_unchanged_biography()
  RETURNS TRIGGER AS
$BODY$
BEGIN
  IF NEW.&quot;biography&quot; = &#39;__debezium_unavailable_value&#39;
  THEN
    NEW.&quot;biography&quot; = OLD.&quot;biography&quot;;
  END IF;

  RETURN NEW;
END;
$BODY$ LANGUAGE PLPGSQL;

CREATE TRIGGER customer_biography_trigger
BEFORE UPDATE OF &quot;biography&quot;
  ON customers
FOR EACH ROW
EXECUTE PROCEDURE ignore_unchanged_biography();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will keep the old value for the &lt;code&gt;biography&lt;/code&gt; column if it were to be set to the &lt;code&gt;__debezium_unavailable_value&lt;/code&gt; marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;stateful_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateful_stream_processing&quot; /&gt;Stateful Stream Processing&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An alternative approach to dealing with unchanged TOAST column values is a stateful stream processing application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This application can persist the latest value of a TOAST column
(as obtained from a snapshot, an insert event or an update including the TOAST-able column) in a state store and
put the value back into change events with the marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium makes sure that all change events for one particular record always go into the same partition,
so they they will be processed in the exact same order as they were created.
This ensures that the latest value is available in the statestore when receiving a change event with the marker value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; with its state store API comes in very handy for building such a service.
Based on &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; and its extension for building &lt;a href=&quot;https://quarkus.io/guides/kafka-streams-guide&quot;&gt;Kafka Streams applications&lt;/a&gt; running either on the JVM or natively via GraalVM,
a solution could look like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class TopologyProducer {

    private static final Logger LOG = LoggerFactory.getLogger(TopologyProducer.class);

    static final String BIOGRAPHY_STORE = &quot;biography-store&quot;;

    @ConfigProperty(name = &quot;pgtoast.customers.topic&quot;)
    String customersTopic;

    @ConfigProperty(name = &quot;pgtoast.customers.enriched.topic&quot;)
    String customersEnrichedTopic;

    @Produces
    public Topology buildTopology() {
        StreamsBuilder builder = new StreamsBuilder();

        StoreBuilder&amp;lt;KeyValueStore&amp;lt;JsonObject, String&amp;gt;&amp;gt; biographyStore = &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
                Stores.keyValueStoreBuilder(
                    Stores.persistentKeyValueStore(BIOGRAPHY_STORE),
                    new JsonObjectSerde(),
                    new Serdes.StringSerde()
                );
        builder.addStateStore(biographyStore);

        builder.&amp;lt;JsonObject, JsonObject&amp;gt;stream(customersTopic) &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
                .transformValues(ToastColumnValueProvider::new, BIOGRAPHY_STORE)
                .to(customersEnrichedTopic);

        return builder.build();
    }

    class ToastColumnValueProvider implements
            ValueTransformerWithKey&amp;lt;JsonObject, JsonObject, JsonObject&amp;gt; {

        private KeyValueStore&amp;lt;JsonObject, String&amp;gt; biographyStore;

        @Override
        @SuppressWarnings(&quot;unchecked&quot;)
        public void init(ProcessorContext context) {
            biographyStore = (KeyValueStore&amp;lt;JsonObject, String&amp;gt;) context.getStateStore(
                TopologyProducer.BIOGRAPHY_STORE);
        }

        @Override
        public JsonObject transform(JsonObject key, JsonObject value) {
            JsonObject payload = value.getJsonObject(&quot;payload&quot;);
            JsonObject newRowState = payload.getJsonObject(&quot;after&quot;);
            String biography = newRowState.getString(&quot;biography&quot;);

            if (isUnavailableValueMarker(biography)) { &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
                String currentValue = biographyStore.get(key); &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;

                if (currentValue == null) {
                    LOG.warn(&quot;No biography value found for key &#39;{}&#39;&quot;, key);
                }
                else {
                    value = Json.createObjectBuilder(value) &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;
                        .add(
                            &quot;payload&quot;,
                            Json.createObjectBuilder(payload)
                                .add(
                                    &quot;after&quot;,
                                    Json.createObjectBuilder(newRowState).add(
                                        &quot;biography&quot;,
                                        currentValue
                                    )
                                )
                        )
                        .build();
                }
            }
            else { &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
                biographyStore.put(key, biography);
            }

            return value;
        }

        private boolean isUnavailableValueMarker(String value) {
            return &quot;__debezium_unavailable_value&quot;.contentEquals(value);
        }

        @Override
        public void close() {
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Set up a state store for storing the latest &lt;code&gt;biography&lt;/code&gt; value per customer id&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The actual streaming pipeline: for each message on the customers topic, apply the logic for replacing the TOAST column marker value and write the transformed message to an output topic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Check whether the &lt;code&gt;biography&lt;/code&gt; value from the incoming message is the marker&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If so, get the current &lt;code&gt;biography&lt;/code&gt; value for the customer from the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Replace the marker value with the actual value obtained from the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;6&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If the incoming message has an actual &lt;code&gt;biography&lt;/code&gt; value, put this to the state store&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, if a consumer subscribes to the &quot;enriched&quot; topic,
it will see any customer change events with the actual value of any unchanged TOAST columns,
as materialized from the state store.
The fact that the Debezium connector originally emitted the special marker value,
is fully transparent at that point.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Primary Key Changes&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a record’s primary key gets updated,
Debezium will create two change events: one &quot;delete&quot; event using the old key and one &quot;insert&quot; event with the new key.
When processing the second event, the stream processing application will not be able to look up the &lt;code&gt;biography&lt;/code&gt; value stored earlier on, as it has been under the old key.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One way to address this would be to expose the original key value e.g. as a message header of the insert event.
This requirement is tracked as &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1531&quot;&gt;DBZ-1531&lt;/a&gt;;
let us know if you’d like to contribute and implement this feature.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;when_to_use_what&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#when_to_use_what&quot; /&gt;When to Use What?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve discussed different options for dealing with unchanged TOAST column values in Debezium’s data change events.
Which one should be used in which case then?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Changing the replica identity to &lt;code&gt;FULL&lt;/code&gt; is the easiest approach by far:
a single configuration to the source table avoids the problem to begin with.
It’s not the most efficient solution, though, and some DBAs might be reluctant to apply this setting.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using the change events to update some kind of sink data store,
it may sound attractive at first to simply omit any field with the special marker value when issuing an update.
But this technique has some downsides: not all data stores and the corresponding connectors might support partial updates.
Instead there might only be the option to do full updates to a record in the sink data store based on the incoming data.
Even when that option exists, it might be sub-optimal.
E.g. for a SQL database, a statement just with the available values may be executed.
This is at odds with efficient usage of prepared statements and batching, though:
as the &quot;shape&quot; of the data may change between two updates to the same table,
the same prepared statement cannot be re-used and performance may suffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The trigger-based approach isn’t prone to these problems:
any updates to a table will have the same number of columns, so the consumer (e.g. a sink connector) may re-use the same prepared statement and batch multiple records into a single execution.
One thing to be aware of is the organizational cost associated with this approach:
triggers must be installed for each affected column and be kept in sync when table structures change.
This must be done individually in each sink datastore, and not all stores have may have support for triggers to begin with.
But where possible, triggers can be a great solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, stream processing makes the usage of TOAST-able columns and the absence of their values in update events fully transparent to consumers.
The enrichment logic is implemented in a single place, from which all the consumers of the change event stream benefit,
without the need for individual solutions in each one of them.
Also, it’s the only viable solution if consumers themselves are stateless and don’t have any way to materialize the last value of such column, e.g. when streaming change events to a browser via web sockets or GraphQL subscriptions.
The price to pay is the overhead of maintaining and operating a separate service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On a side note, such stream processing application might also be provided as a configurable, ready-to-use component coming as a part of the Debezium platform.
This might be useful not only for Postgres, but also when thinking about other Debezium connectors.
For instance, in case of Cassandra, change events will only ever contain the updated fields;
a similar mode could be envisioned for MySQL by supporting its &quot;non full&quot; binlog mode.
In both cases, a stateful stream processing service could be used to hydrate full data change events based on earlier row state retrieved from a local state store and an incoming &quot;patch&quot; style change event.
If you think that’d be a useful addition to Debezium, please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, there are no silver bullets:
you should choose a solution based on your specific situation and requirements.
As a starting point you can find a basic implementation of the trigger and Kafka Streams approaches in the Debezium &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/postgres-toast&quot;&gt;examples repository&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Which approach would you prefer?
Or perhaps you have even further alternatives in mind?
Tell us about it in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href=&quot;https://twitter.com/dave_cramer/&quot;&gt;Dave Cramer&lt;/a&gt; and Jiri Pechanec for their feedback while working on this post and the accompanying example code!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/</id>
<title>Debezium 0.10 Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-02T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/02/debezium-0-10-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="cassandra"></category>
<summary>



On behalf of the Debezium community it&#8217;s my great pleasure to announce the release of Debezium 0.10.0.Final!


As you&#8217;d expect it, there were not many changes since last week&#8217;s CR2,
one exception being a performance fix for the pgoutput plug-in of the Postgres connector,
which may have suffered from slow processing when dealing with many small transactions in a short period of time
(DBZ-1515).


This release finalizes the work of overall eight preview releases.
We have discussed the new features and changes in depth in earlier announcements,
but here are some highlights of Debezium 0.10:







Incubating CDC support for Apache Cassandra via our first community-led connector


Support for the...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;On behalf of the Debezium community it’s my great pleasure to announce the release of Debezium &lt;strong&gt;0.10.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you’d expect it, there were not many changes since last week’s CR2,
one exception being a performance fix for the &lt;code&gt;pgoutput&lt;/code&gt; plug-in of the Postgres connector,
which may have suffered from slow processing when dealing with many small transactions in a short period of time
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1515&quot;&gt;DBZ-1515&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release finalizes the work of overall &lt;a href=&quot;https://debezium.io/releases/0.10/&quot;&gt;eight preview releases&lt;/a&gt;.
We have discussed the new features and changes in depth in earlier announcements,
but here are some highlights of Debezium 0.10:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Incubating &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/cassandra.html&quot;&gt;CDC support for Apache Cassandra&lt;/a&gt; via our first community-led connector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for the &lt;code&gt;pgoutput&lt;/code&gt; logical decoding plug-in of Postgres 10 and later; support for &quot;exported&quot; snapshots with &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html&quot;&gt;Postgres&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Extended and more unified metrics across the different connectors&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More consistent and improved &lt;code&gt;source&lt;/code&gt; structure in change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Significantly less memory usage in the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/sqlserver.html&quot;&gt;SQL Server connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Many improvements and bugfixes to the DDL parser of the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/mysql.html&quot;&gt;MySQL connector&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lots of improvements to the SMTs for &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/outbox-event-router.html&quot;&gt;routing outbox events&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/event-flattening.html&quot;&gt;extracting the &quot;after&quot; state&lt;/a&gt; from change events&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Customizable message keys&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reworked &lt;a href=&quot;https://debezium.io/blog/2019/09/05/website-documentation-overhaul/&quot;&gt;website and restructured documentation&lt;/a&gt;, organized by releases&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, we also removed some previously deprecated options and did some clean-up of the message structures produced by the Debezium connectors, e.g. in regards to certain type mappings.
When upgrading from earlier releases,
please make sure to carefully study the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes/&quot;&gt;release notes&lt;/a&gt;, which discuss in detail any changed or removed functionality, as well as options which for instance allow to keep the original &lt;code&gt;source&lt;/code&gt; structure for some time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Pleas refer to the orignal announcements for more details (&lt;a href=&quot;https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/&quot;&gt;Alpha2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/&quot;&gt;Beta3&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/&quot;&gt;Beta4&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/&quot;&gt;CR1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/&quot;&gt;CR2&lt;/a&gt;).
Altogether, a whopping &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(0.10.0.Alpha1%2C%200.10.0.Alpha2%2C%200.10.0.Beta1%2C%200.10.0.Beta2%2C%200.10.0.Beta3%2C%200.10.0.Beta4%2C%200.10.0.CR1%2C%200.10.0.CR2%2C%200.10.0.Final)&quot;&gt;171 issues&lt;/a&gt; were resolved in Debezium 0.10.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Such effort would not nearly be possible without all the fantastic people in the Debezium community.
Until today, almost &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/COPYRIGHT.txt&quot;&gt;130 people&lt;/a&gt; have contributed to the main Debezium code repository,
plus some more to the incubator and container image repositories.
But submitting pull requests with code changes is not the only way to help,
we’re equally thankful for each bug report, feature request, suggestions in the chat rooms etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another great thing to observe is the growing number of blog posts, conference presentations and other material covering Debezium in one way or another.
We maintain a list of &lt;a href=&quot;https://debezium.io/documentation/online-resources/&quot;&gt;Debezium-related resources&lt;/a&gt; on the website;
if you know of other contents which should be linked there, please file a PR for adding it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I can’t begin to express how lucky we feel about all these amazing contributions,
no matter whether small or large!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;coming_next_debezium_1_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#coming_next_debezium_1_0&quot; /&gt;Coming Next: Debezium 1.0!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With Debezium 0.10.0.Final being done, the question is: what’s next?
If you thought 0.11, then we got to disappoint you — we’re finally setting course towards Debezium 1.0!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With all the community feedback we got (in parts from huge deployments with hundreds of Debezium connectors), and with the clean-up changes done for 0.10, we feel that it’s finally about time for the 1.0 release and the even increased expectations towards it in regards to compatibility and stability.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We don’t expect many new functionality for 1.0 in comparison over 0.10
(with exception of the incubating connectors),
the focus will primarily be on further bug fixing, stability and usability improvements.
In the good old tradition of open source, we don’t specify any timeline other than &quot;it’s done, when it’s done&quot;.
But it should be safe to say that it will be done quicker than 0.10:
going forward, we’d like to increase the release cadence and publish new minor releases more often, for sure doing less than eight preview releases as in 0.10.
Any contributions, input on the roadmap and other feedback will be very welcomed of course.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Upwards and onwards!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/</id>
<title>Building Audit Logs with Change Data Capture and Stream Processing</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-10-01T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/10/01/audit-logs-with-change-data-capture-and-stream-processing/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="apache-kafka"></category>
<category term="kafka-streams"></category>
<category term="featured"></category>
<summary>





It is a common requirement for business applications to maintain some form of audit log,
i.e. a persistent trail of all the changes to the application&#8217;s data.
If you squint a bit, a Kafka topic with Debezium data change events is quite similar to that:
sourced from database transaction logs, it describes all the changes to the records of an application.
What&#8217;s missing though is some metadata: why, when and by whom was the data changed?
In this post we&#8217;re going to explore how that metadata can be provided and exposed via change data capture (CDC), and how stream processing can be used to enrich...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is a common requirement for business applications to maintain some form of audit log,
i.e. a persistent trail of all the changes to the application’s data.
If you squint a bit, a Kafka topic with Debezium data change events is quite similar to that:
sourced from database transaction logs, it describes all the changes to the records of an application.
What’s missing though is some metadata: why, when and by whom was the data changed?
In this post we’re going to explore how that metadata can be provided and exposed via change data capture (CDC), and how stream processing can be used to enrich the actual data change events with such metadata.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Reasons for maintaining data audit trails are manyfold:
e.g. regulatory requirements may mandate businesses to keep complete historic information of their customer, purchase order, invoice or other data.
Also for an enterprise’s own purposes it can be very useful to have insight into why and how certain data has changed, e.g. allowing to improve business processes or analyze errors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One common approach for creating audit trails are application-side libraries.
Hooked into the chosen persistence library,
they’d maintain specific column(s) in the data tables (&quot;createdBy&quot;, &quot;lastUpdated&quot; etc.),
and/or copy earlier record versions into some form of history tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are some disadvantages to this, though:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;writing records in history tables as part of OLTP transactions increases the number of executed statements within the transaction (for each update or delete, also an insert must be written into the corresponding history table) and thus may cause longer response times of the application&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;oftentimes no audit events can be provided in case of bulk updates and deletes (e.g. &lt;code&gt;DELETE from purchaseorders where status = &#39;SHIPPED&#39;&lt;/code&gt;),
as the listeners used to hook the library into the persistence framework are not aware of all the affected records&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;changes done directly in the database cannot be tracked, e.g. when running a data load, doing batch processing in a stored procedure or when bypassing the application during an emergency data patch&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another technique are database triggers.
They won’t miss any operations, no matter whether issued from the application or the database itself.
They’ll also be able to process each record affected by a bulk statement.
On the downside, there’s still is the problem of increased latency when executing triggers as part of OLTP transactions.
Also, a process must be in place for installing and updating the triggers for each table.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;audit_logs_based_on_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#audit_logs_based_on_change_data_capture&quot; /&gt;Audit Logs Based on Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The aforementioned problems don’t exist when leveraging the transaction log as the source for an audit trail and using change data capture for retrieving the change information and sending it into a message broker or log such as Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Running asynchronously, the CDC process can extract the change data without impacting OLTP transactions.
The transaction logs contain one entry whenever there’s a data change,
be it issued from the application or directly executed in the database.
There’ll be a log entry for each record updated or deleted in a bulk operation,
so a change event for each of them can be produced.
Also there is no impact on the data model, i.e. no special columns or history tables must be created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But how can CDC access the metadata we’d discussed initially?
This could for instance be data such as the application user that performed a data change,
their IP address and device configuration, a tracing span id, or an identifier for the application use case.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As that metadata typically isn’t (nor shouldn’t) be stored in the actual business tables of an application, it must be provided separately.
One approach is to have a separate table where this metadata is stored.
For each executed transaction, the business application produces one record in that table, containing all the required metadata and using the transaction id as a primary key.
When running manual data changes, it is easy to also provide the metadata record with an additional insert.
As Debezium’s data change events contain the id of the transaction causing the specific change,
the data change events and the metadata records can be correlated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the remainder of this post we’re going to take a closer look at how a business application can provide the transaction-scoped metadata and how data change events can be enriched with the corresponding metadata using the Kafka Streams API.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;solution_overview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#solution_overview&quot; /&gt;Solution Overview&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following image shows the overall solution design, based on the example of a microservice for managing vegetable data:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_overview.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Auditing With Change Data Capture and Stream Processing&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are two services involved:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;vegetables-service&lt;/em&gt;: a simple REST service for inserting and updating vegetable data into a Postgres database;
as part of its processing, it will not only update its actual &quot;business table&quot; &lt;code&gt;vegetable&lt;/code&gt;,
but also insert some auditing metadata into a dedicated metadata table &lt;code&gt;transaction_context_data&lt;/code&gt;;
Debezium is used to stream change events from the two tables into corresponding topics in Apache Kafka&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;log-enricher&lt;/em&gt;: a stream processing application built with Kafka Streams and Quarkus,
which enriches the messages from the CDC topic containing the vegetable change events (&lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt;) with the corresponding metadata in the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; topic and writes the enriched vegetable change event back to Kafka into the &lt;code&gt;dbserver1.inventory.vegetable.enriched&lt;/code&gt; topic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find a &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;complete example&lt;/a&gt; with all the components and instructions for running them on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;providing_auditing_metadata&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#providing_auditing_metadata&quot; /&gt;Providing Auditing Metadata&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s first discuss how an application such as the vegetable service can provide the required auditing metadata.
As an example, the following metadata should be made available for auditing purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The application user that did a data change, as represented by the &lt;code&gt;sub&lt;/code&gt; claim of a JWT token (&lt;a href=&quot;https://tools.ietf.org/html/rfc7519&quot;&gt;JSON Web Token&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The request timestamp, as represented by the &lt;code&gt;Date&lt;/code&gt; HTTP header&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A use case identifier, as provided via a custom Java annotation on the invoked REST resource method&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here is a basic implementation of a REST resource for persisting a new vegetable using the &lt;a href=&quot;https://jcp.org/en/jsr/detail?id=370&quot;&gt;JAX-RS API&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Path(&quot;/vegetables&quot;)
@RequestScoped
@Produces(MediaType.APPLICATION_JSON)
@Consumes(MediaType.APPLICATION_JSON)
public class VegetableResource {

    @Inject
    VegetableService vegetableService;

    @POST
    @RolesAllowed({&quot;farmers&quot;})
    @Transactional
    @Audited(useCase=&quot;CREATE VEGETABLE&quot;)
    public Response createVegetable(Vegetable vegetable) {
        if (vegetable.getId() != null) {
            return Response.status(Status.BAD_REQUEST.getStatusCode()).build();
        }

        vegetable = vegetableService.createVegetable(vegetable);

        return Response.ok(vegetable).status(Status.CREATED).build();
    }

    // update, delete ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’ve ever built REST services with JAX-RS before, the implementation will look familiar to you:
a resource method annotated with &lt;code&gt;@POST&lt;/code&gt; takes the incoming request payload and passes it to a service bean which is injected via CDI.
The &lt;code&gt;@Audited&lt;/code&gt; annotation is special, though.
It is a custom annotation type which serves two purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Specifying the use case that should be referenced in the audit log (&quot;CREATE VEGETABLE&quot;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Binding an &lt;a href=&quot;https://jcp.org/en/jsr/detail?id=318&quot;&gt;interceptor&lt;/a&gt; which will be triggered for each invocation of a method annotated with &lt;code&gt;@Audited&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;That interceptor kicks in whenever a method annotated with &lt;code&gt;@Audited&lt;/code&gt; is invoked and implements the logic for writing the transaction-scoped audit metadata.
It looks like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Interceptor &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
@Audited(useCase = &quot;&quot;)
@Priority(value = Interceptor.Priority.APPLICATION + 100) &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
public class TransactionInterceptor {

    @Inject
    JsonWebToken jwt; &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

    @Inject
    EntityManager entityManager;

    @Inject
    HttpServletRequest request;

    @AroundInvoke
    public Object manageTransaction(InvocationContext ctx) throws Exception {
        BigInteger txtId = (BigInteger) entityManager &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
            .createNativeQuery(&quot;SELECT txid_current()&quot;)
            .getSingleResult();
        String useCase = ctx.getMethod().getAnnotation(Audited.class).useCase();

        TransactionContextData context = new TransactionContextData(); &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;

        context.transactionId = txtId.longValueExact();
        context.userName = jwt.&amp;lt;String&amp;gt;claim(&quot;sub&quot;).orElse(&quot;anonymous&quot;);
        context.clientDate = getRequestDate();
        context.useCase = useCase;

        entityManager.persist(context);

        return ctx.proceed(); &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;b&gt;(6)&lt;/b&gt;
    }

    private ZonedDateTime getRequestDate() {
        String requestDate = request.getHeader(HttpHeaders.DATE);
        return requestDate != null ?
            ZonedDateTime.parse(requestDate, DateTimeFormatter.RFC_1123_DATE_TIME) :
            null;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;@Interceptor&lt;/code&gt; and &lt;code&gt;@Audited&lt;/code&gt; mark this as an interceptor bound to our custom &lt;code&gt;@Audited&lt;/code&gt; annotion.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The &lt;code&gt;@Priority&lt;/code&gt; annotation controls at which point in the interceptor stack the auditing interceptor should be invoked.
Any application-provided interceptors should have a priority larger than &lt;code&gt;Priority.APPLICATION&lt;/code&gt; (2000);
in particular, this ensures that a transaction will have been started before by means of the &lt;code&gt;@Transactional&lt;/code&gt; annotation and its accompanying interceptor which run in the &lt;code&gt;Priority.PLATFORM_BEFORE&lt;/code&gt; range (&amp;lt; 1000).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;The caller’s JWT token injected via the &lt;a href=&quot;https://microprofile.io/project/eclipse/microprofile-jwt-auth&quot;&gt;MicroProfile JWT RBAC&lt;/a&gt; API&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For each audited method the interceptor fires and will&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;obtain the current transaction id (the exact way for doing so is database-specific, in the example the &lt;code&gt;txid_current()&lt;/code&gt; function from Postgres is called) &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;persist a &lt;code&gt;TransactionContextData&lt;/code&gt; entity via JPA; its primary key value is the transaction id selected before, and it has attributes for the user name (obtained from the JWT token),
the request date (obtained from the &lt;code&gt;DATE&lt;/code&gt; HTTP request header) and the use case identifier (obtained from the &lt;code&gt;@Audited&lt;/code&gt; annotation of the invoked method) &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;continue the call flow of the invoked method &lt;i class=&quot;conum&quot; data-value=&quot;6&quot; /&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When invoking the REST service to create and update a few vegetables,
the following records should be created in the database
(refer to the README in the provided example for instructions on building the example code and &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog#inserting-some-data-and-observing-the-audit-log&quot;&gt;invoking the vegetable service&lt;/a&gt; with a suitable JWT token):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;vegetablesdb&amp;gt; select * from inventory.vegetable;
+------+---------------+---------+
| id   | description   | name    |
|------+---------------+---------|
| 1    | Spicy!        | Potato  |
| 11   | Delicious!    | Pumpkin |
| 10   | Tasty!        | Tomato  |
+------+---------------+---------+&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;vegetablesdb&amp;gt; select * from inventory.transaction_context_data;
+------------------+---------------------+------------------+----------------+
| transaction_id   | client_date         | usecase          | user_name      |
|------------------+---------------------+------------------+----------------|
| 608              | 2019-08-22 08:12:31 | CREATE VEGETABLE | farmerbob      |
| 609              | 2019-08-22 08:12:31 | CREATE VEGETABLE | farmerbob      |
| 610              | 2019-08-22 08:12:31 | UPDATE VEGETABLE | farmermargaret |
+------------------+---------------------+------------------+----------------+&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;enriching_change_events_with_auditing_metadata&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#enriching_change_events_with_auditing_metadata&quot; /&gt;Enriching Change Events with Auditing Metadata&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the business data (vegetables) and the transaction-scoped metadata being stored in the database,
it’s time to set up the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html&quot;&gt;Debezium Postgres connector&lt;/a&gt; and stream the data changes from the &lt;code&gt;vegetable&lt;/code&gt; and &lt;code&gt;transaction_context_data&lt;/code&gt; tables into corresponding Kafka topics.
Again refer to the example README file for the details of &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog#deploy-the-debezium-postgres-connector&quot;&gt;deploying the connector&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;dbserver1.inventory.vegetable&lt;/code&gt; topic should contain change events for created, updated and deleted vegetable records, whereas the &lt;code&gt;dbserver1.inventory.transaction_context_data&lt;/code&gt; topic should only contain create messages for each inserted metadata record.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Topic Retention&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to manage the growth of involved topics, the retention policy for each topic should be well-defined.
For instance for the actual audit log topic with the enriched change events, a time based retention policy might be suitable, keeping each log event for as long as needed as per your requirements.
The transaction metadata topic on the other hand can be fairly short-lived, as its entries are not needed any longer, once all corresponding data change events have been processed.
It may be a good idea to set up some monitoring of the end-to-end lag in order to make sure the log enricher stream application keeps up with the incoming messages and doesn’t fall behind that far so it is at risk of transaction messages being discarded before processing the corresponding change events.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, if we look at messages from the two topics, we can see that they can be correlated based on the transaction id.
It is part of the &lt;code&gt;source&lt;/code&gt; structure of vegetable change events,
and it is the message key of transaction metadata events:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_input_messages.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Vegetable and Transaction Metadata Messages&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once we’ve found the corresponding transaction event for a given vegetable change event,
the &lt;code&gt;client_date&lt;/code&gt;, &lt;code&gt;usecase&lt;/code&gt; and &lt;code&gt;user_name&lt;/code&gt; attributes from the former can be added to the latter:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/auditing_output_message.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Enriched Vegetable Message&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This kind of message transformation is a perfect use case for &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt;,
a Java API for implementing stream processing applications on top of Kafka topics,
providing operators that let you filter, transform, aggregate and join Kafka messages.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As runtime environment for our stream processing application we’re going to use &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt;,
which is &quot;a Kubernetes Native Java stack tailored for GraalVM &amp;amp; OpenJDK HotSpot, crafted from the best of breed Java libraries and standards&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Building Kafka Streams Applications with Quarkus&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Amongst many others, Quarkus comes with an &lt;a href=&quot;https://quarkus.io/guides/kafka-streams-guide&quot;&gt;extension for Kafka Streams&lt;/a&gt;,
which allows to build stream processing applications running on the JVM and as native code compiled ahead-of-time.
It takes care of the lifecycle of the streaming topology,
so you don’t have to deal with details like registering JVM shutdown hooks,
awaiting the creation of all input topics and more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The extension also comes with &quot;live development&quot; support,
which automatically reloads the stream processing application while you’re working on it,
allowing for very fast turnaround cycles during development.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_joining_logic&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_joining_logic&quot; /&gt;The Joining Logic&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When thinking about the actual implementation of the enrichment logic,
a &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#kstream-kstream-join&quot;&gt;stream-to-stream&lt;/a&gt; join might appear as a suitable solution.
By creating &lt;code&gt;KStream&lt;/code&gt;s for the two topics, we may try and implement the joining functionality.
One challenge though is how to define a suitable &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#windowing-sliding&quot;&gt;joining window&lt;/a&gt;,
as there is no timing guarantees between messages on the two topics,
and we must not miss any event.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another problem arises in regards to ordering guarantees of the change events.
By default, Debezium will use a table’s primary key as the message key for the corresponding Kafka messages.
This means that all messages for the same vegetable record will have the same key and thus will go into the same partition of the vegetables Kafka topic.
This in turn guarantees that a consumer of these events sees all the messages pertaining to the same vegetable record in the exact same order as they were created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, in order to join the two streams, the message key must be the same on both sides.
This means the vegetables topic must be re-keyed by transaction id
(we cannot re-key the transaction metadata topic, as there’s no information about concerned vegetables contained in the metadata events; and even if that were the case, one transaction might impact multiple vegetable records).
By doing so, we’d loose the original ordering guarantees, though.
One vegetable record might be modified in two subsequent transactions,
and its change events may end up in different partitions of the re-keyed topic,
which may cause a consumer to receive the second change event before the first one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If a &lt;code&gt;KStream&lt;/code&gt;-&lt;code&gt;KStream&lt;/code&gt; join isn’t feasible, what else could be done?
&lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins-kstream-globalktable&quot;&gt;A join&lt;/a&gt; between a &lt;code&gt;KStream&lt;/code&gt; and &lt;code&gt;GlobalKTable&lt;/code&gt; looks promising, too.
It doesn’t have the &lt;a href=&quot;https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html#streams-developer-guide-dsl-joins-co-partitioning&quot;&gt;co-partitioning requirements&lt;/a&gt; of stream-to-stream joins,
as all partitions of the &lt;code&gt;GlobalKTable&lt;/code&gt; are present on all nodes of a distributed Kafka Streams application.
This seems like an acceptable trade-off, because the messages from the transaction metadata topic can be discarded rather quickly and the size of the corresponding table should be within reasonable bounds.
So we could have a &lt;code&gt;KStream&lt;/code&gt; sourced from the vegetables topic and a &lt;code&gt;GlobalKTable&lt;/code&gt; based on the transaction metadata topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But unfortunately, there is a timing issue:
as the messages are consumed from multiple topics, it may happen that at the point in time when an element from the vegetables stream is processed, the corresponding transaction metadata message isn’t available yet.
So depending on whether we’d be using an inner join or a left join,
we’d in this case either skip change events or propagate them without having enriched them with the transaction metadata.
Both outcomes are not desirable.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;customized_joins_with_buffering&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customized_joins_with_buffering&quot; /&gt;Customized Joins With Buffering&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The combination of &lt;code&gt;KStream&lt;/code&gt; and &lt;code&gt;GlobalKTable&lt;/code&gt; still hints into the right direction.
Only that instead of relying on the built-in join operators we’ll have to implement a custom joining logic.
The basic idea is to buffer messages arriving on the vegetable &lt;code&gt;KStream&lt;/code&gt; until the corresponding transaction metadata message is available from the &lt;code&gt;GlobalKTable&lt;/code&gt;s state store.
This can be achieved by creating a custom &lt;a href=&quot;https://kafka.apache.org/23/javadoc/org/apache/kafka/streams/kstream/KStream.html#transform-org.apache.kafka.streams.kstream.TransformerSupplier-java.lang.String…​-&quot;&gt;transformer&lt;/a&gt; which implements the required buffering logic and is applied to the vegetable &lt;code&gt;KStream&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s begin with the streaming topology itself.
Thanks to the Quarkus Kafka Streams extension,
a CDI producer method returning the &lt;code&gt;Topology&lt;/code&gt; object is all that’s needed for that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class TopologyProducer {

    static final String STREAM_BUFFER_NAME = &quot;stream-buffer-state-store&quot;;
    static final String STORE_NAME = &quot;transaction-meta-data&quot;;

    @ConfigProperty(name = &quot;audit.context.data.topic&quot;)
    String txContextDataTopic;

    @ConfigProperty(name = &quot;audit.vegetables.topic&quot;)
    String vegetablesTopic;

    @ConfigProperty(name = &quot;audit.vegetables.enriched.topic&quot;)
    String vegetablesEnrichedTopic;

    @Produces
    public Topology buildTopology() {
        StreamsBuilder builder = new StreamsBuilder();

        StoreBuilder&amp;lt;KeyValueStore&amp;lt;Long, JsonObject&amp;gt;&amp;gt; streamBufferStateStore =
                Stores
                    .keyValueStoreBuilder(
                        Stores.persistentKeyValueStore(STREAM_BUFFER_NAME),
                        new Serdes.LongSerde(),
                        new JsonObjectSerde()
                    )
                    .withCachingDisabled();
            builder.addStateStore(streamBufferStateStore); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;

        builder.globalTable(txContextDataTopic, Materialized.as(STORE_NAME)); &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;

        builder.&amp;lt;JsonObject, JsonObject&amp;gt;stream(vegetablesTopic) &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
                .filter((id, changeEvent) -&amp;gt; changeEvent != null)
                .filter((id, changeEvent) -&amp;gt; !changeEvent.getString(&quot;op&quot;).equals(&quot;r&quot;))
                .transform(() -&amp;gt; new ChangeEventEnricher(), STREAM_BUFFER_NAME)
                .to(vegetablesEnrichedTopic);

        return builder.build();
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;State store which will serve as the buffer for change events that cannot be processed yet&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;GlobalKTable&lt;/code&gt; based on the transaction metadata topic&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;&lt;code&gt;KStream&lt;/code&gt; based on the vegetables topic; on this stream, any incoming tombstone markers are filtered, the reasoning being that the retention policy for an audit trail topic typically should be time-based than based on log compaction;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;similarly, snapshot events are filtered, assuming they are not relevant for an audit trail and there wouldn’t be any corresponding metadata provided by the application for the snapshot transaction initiated by the Debezium connector&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Any other messages are enriched with the corresponding transaction metadata via a custom &lt;code&gt;Transformer&lt;/code&gt; (see below) and finally are written to an output topic&lt;/p&gt;
&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The topic names are injected using the &lt;a href=&quot;https://microprofile.io/project/eclipse/microprofile-config&quot;&gt;MicroProfile Config API&lt;/a&gt;, with the values being provided in Quarkus &lt;em&gt;application.properties&lt;/em&gt; configuration file.
Besides the topic names, this file also has the information about the Kafka bootstrap server, default serdes any more:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;audit.context.data.topic=dbserver1.inventory.transaction_context_data
audit.vegetables.topic=dbserver1.inventory.vegetable
audit.vegetables.enriched.topic=dbserver1.inventory.vegetable.enriched

# may be overridden with env vars
quarkus.kafka-streams.bootstrap-servers=localhost:9092
quarkus.kafka-streams.application-id=auditlog-enricher
quarkus.kafka-streams.topics=${audit.context.data.topic},${audit.vegetables.topic}

# pass-through
kafka-streams.cache.max.bytes.buffering=10240
kafka-streams.commit.interval.ms=1000
kafka-streams.metadata.max.age.ms=500
kafka-streams.auto.offset.reset=earliest
kafka-streams.metrics.recording.level=DEBUG
kafka-streams.default.key.serde=io.debezium.demos.auditing.enricher.JsonObjectSerde
kafka-streams.default.value.serde=io.debezium.demos.auditing.enricher.JsonObjectSerde
kafka-streams.processing.guarantee=exactly_once&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the next step let’s take a look at the &lt;code&gt;ChangeEventEnricher&lt;/code&gt; class, our custom transformer.
The implemention is based on the assumption that change events are serialized as JSON,
but of course it could be done equally well using other formats such as Avro or Protocol Buffers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a bit of code, but hopefully its decomposition into multiple smaller methods makes it comprehensible:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;class ChangeEventEnricher implements Transformer
        &amp;lt;JsonObject, JsonObject, KeyValue&amp;lt;JsonObject, JsonObject&amp;gt;&amp;gt; {

    private static final Long BUFFER_OFFSETS_KEY = -1L;

    private static final Logger LOG = LoggerFactory.getLogger(ChangeEventEnricher.class);

    private ProcessorContext context;
    private KeyValueStore&amp;lt;JsonObject, JsonObject&amp;gt; txMetaDataStore;
    private KeyValueStore&amp;lt;Long, JsonObject&amp;gt; streamBuffer; &lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;(5)&lt;/b&gt;

    @Override
    @SuppressWarnings(&quot;unchecked&quot;)
    public void init(ProcessorContext context) {
        this.context = context;
        streamBuffer = (KeyValueStore&amp;lt;Long, JsonObject&amp;gt;) context.getStateStore(
            TopologyProducer.STREAM_BUFFER_NAME
        );
        txMetaDataStore = (KeyValueStore&amp;lt;JsonObject, JsonObject&amp;gt;) context.getStateStore(
            TopologyProducer.STORE_NAME
        );

        context.schedule(
            Duration.ofSeconds(1),
            PunctuationType.WALL_CLOCK_TIME, ts -&amp;gt; enrichAndEmitBufferedEvents()
        ); &lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;(4)&lt;/b&gt;
    }

    @Override
    public KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; transform(JsonObject key, JsonObject value) {
        boolean enrichedAllBufferedEvents = enrichAndEmitBufferedEvents(); &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;

        if (!enrichedAllBufferedEvents) {
            bufferChangeEvent(key, value);
            return null;
        }

        KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enriched = enrichWithTxMetaData(key, value); &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
        if (enriched == null) { &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
            bufferChangeEvent(key, value);
        }

        return enriched;
    }

    /**
     * Enriches the buffered change event(s) with the metadata from the associated
     * transactions and forwards them.
     *
     * @return {@code true}, if all buffered events were enriched and forwarded,
     *         {@code false} otherwise.
     */
    private boolean enrichAndEmitBufferedEvents() { &lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;(3)&lt;/b&gt;
        Optional&amp;lt;BufferOffsets&amp;gt; seq = bufferOffsets();

        if (!seq.isPresent()) {
            return true;
        }

        BufferOffsets sequence = seq.get();

        boolean enrichedAllBuffered = true;

        for(long i = sequence.getFirstValue(); i &amp;lt; sequence.getNextValue(); i++) {
            JsonObject buffered = streamBuffer.get(i);

            LOG.info(&quot;Processing buffered change event for key {}&quot;,
                    buffered.getJsonObject(&quot;key&quot;));

            KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enriched = enrichWithTxMetaData(
                    buffered.getJsonObject(&quot;key&quot;), buffered.getJsonObject(&quot;changeEvent&quot;));
            if (enriched == null) {
                enrichedAllBuffered = false;
                break;
            }

            context.forward(enriched.key, enriched.value);
            streamBuffer.delete(i);
            sequence.incrementFirstValue();
        }

        if (sequence.isModified()) {
            streamBuffer.put(BUFFER_OFFSETS_KEY, sequence.toJson());
        }

        return enrichedAllBuffered;
    }

    /**
     * Adds the given change event to the stream-side buffer.
     */
    private void bufferChangeEvent(JsonObject key, JsonObject changeEvent) { &lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;(2)&lt;/b&gt;
        LOG.info(&quot;Buffering change event for key {}&quot;, key);

        BufferOffsets sequence = bufferOffsets().orElseGet(BufferOffsets::initial);

        JsonObject wrapper = Json.createObjectBuilder()
                .add(&quot;key&quot;, key)
                .add(&quot;changeEvent&quot;, changeEvent)
                .build();

        streamBuffer.putAll(Arrays.asList(
                KeyValue.pair(sequence.getNextValueAndIncrement(), wrapper),
                KeyValue.pair(BUFFER_OFFSETS_KEY, sequence.toJson())
        ));
    }

    /**
     * Enriches the given change event with the metadata from the associated
     * transaction.
     *
     * @return The enriched change event or {@code null} if no metadata for the
     *         associated transaction was found.
     */
    private KeyValue&amp;lt;JsonObject, JsonObject&amp;gt; enrichWithTxMetaData(JsonObject key,
            JsonObject changeEvent) { &lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;(1)&lt;/b&gt;
        JsonObject txId = Json.createObjectBuilder()
                .add(&quot;transaction_id&quot;, changeEvent.get(&quot;source&quot;).asJsonObject()
                        .getJsonNumber(&quot;txId&quot;).longValue())
                .build();

        JsonObject metaData = txMetaDataStore.get(txId);

        if (metaData != null) {
            LOG.info(&quot;Enriched change event for key {}&quot;, key);

            metaData = Json.createObjectBuilder(metaData.get(&quot;after&quot;).asJsonObject())
                    .remove(&quot;transaction_id&quot;)
                    .build();

            return KeyValue.pair(
                    key,
                    Json.createObjectBuilder(changeEvent)
                        .add(&quot;audit&quot;, metaData)
                        .build()
            );
        }

        LOG.warn(&quot;No metadata found for transaction {}&quot;, txId);
        return null;
    }

    private Optional&amp;lt;BufferOffsets&amp;gt; bufferOffsets() {
        JsonObject bufferOffsets = streamBuffer.get(BUFFER_OFFSETS_KEY);
        if (bufferOffsets == null) {
            return Optional.empty();
        }
        else {
            return Optional.of(BufferOffsets.fromJson(bufferOffsets));
        }
    }

    @Override
    public void close() {
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;colist arabic&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;1&quot; /&gt;&lt;b&gt;1&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;When a vegetables change event arrives, look up the corresponding metadata in the state store of the
transaction topic’s &lt;code&gt;GlobalKTable&lt;/code&gt;, using the transaction id from the &lt;code&gt;source&lt;/code&gt; block of the change event as the key;
if the metadata could be found, add the metadata to change event (under the &lt;code&gt;audit&lt;/code&gt; field) and return that enriched event&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;2&quot; /&gt;&lt;b&gt;2&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;If the metadata could not be found, add the incoming event into the buffer of change events and return&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;3&quot; /&gt;&lt;b&gt;3&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;Before actually getting to the incoming event, all buffered events are processed;
this is required to make sure that the original change events is retained;
only if all could be enriched, the incoming event will be processed, too&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;4&quot; /&gt;&lt;b&gt;4&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;In order to emit buffered events also if no new change event is coming in,
a punctuation is scheduled that periodically processes the buffer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;i class=&quot;conum&quot; data-value=&quot;5&quot; /&gt;&lt;b&gt;5&lt;/b&gt;&lt;/td&gt;
&lt;td&gt;A buffer for vegetable events whose corresponding metadata hasn’t arrived yet&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key piece is the buffer for unprocessable change events.
To maintain the order of events, the buffer must be processed in order of insertion,
beginning with the event inserted first
(think of a FIFO queue).
As there’s no guaranteed traversing order when getting all the entries from a &lt;code&gt;KeyValueStore&lt;/code&gt;,
this is implemented by using the values of a strictly increasing sequence as the keys.
A &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/auditlog/log-enricher/src/main/java/io/debezium/demos/auditing/enricher/BufferOffsets.java&quot;&gt;special entry&lt;/a&gt; in the key value store is used to store the information about the current &quot;oldest&quot; index in the buffer and the next sequence value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One could also think of alternative implementations for such buffer, e.g. based on a Kafka topic or a custom &lt;code&gt;KeyValueStore&lt;/code&gt; implementation that ensures iteration order from oldest to newest entry.
Ultimately, it could also be useful if Kafka Streams came with built-in means of retrying a stream element that cannot be joined yet; this would avoid any custom buffering implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;If Things Go Wrong&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For a reliable and consistent processing logic it’s vital to think about the behavior in case of failures,
e.g. if the stream application crashes after adding an element to the buffer but before updating the sequence value.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key to this is the &lt;code&gt;exactly_once&lt;/code&gt; value of the &lt;code&gt;processing.guarantee&lt;/code&gt; property given in &lt;em&gt;application.properties&lt;/em&gt;.
This ensures a transactionally consistent processing; e.g. in the aforementioned scenario,
after a restart the original change event would be handled again, and the buffer state would look exactly like it did before the event was processed for the first time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Consumers of the enriched vegetable events should apply an isolation level of &lt;code&gt;read_committed&lt;/code&gt;;
otherwise they may see uncommitted and thus duplicate messages in case of an application crash after a buffered event was forwarded but before it was removed from the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the custom transformer logic in place, we can build the Quarkus project and run the stream processing application.
You should see messages like this in the &lt;code&gt;dbserver1.inventory.vegetable.enriched&lt;/code&gt; topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;{&quot;id&quot;:10}
{
    &quot;before&quot;: {
        &quot;id&quot;: 10,
        &quot;description&quot;: &quot;Yummy!&quot;,
        &quot;name&quot;: &quot;Tomato&quot;
    },
    &quot;after&quot;: {
        &quot;id&quot;: 10,
        &quot;description&quot;: &quot;Tasty!&quot;,
        &quot;name&quot;: &quot;Tomato&quot;
    },
    &quot;source&quot;: {
        &quot;version&quot;: &quot;0.10.0-SNAPSHOT&quot;,
        &quot;connector&quot;: &quot;postgresql&quot;,
        &quot;name&quot;: &quot;dbserver1&quot;,
        &quot;ts_ms&quot;: 1569700445392,
        &quot;snapshot&quot;: &quot;false&quot;,
        &quot;db&quot;: &quot;vegetablesdb&quot;,
        &quot;schema&quot;: &quot;inventory&quot;,
        &quot;table&quot;: &quot;vegetable&quot;,
        &quot;txId&quot;: 610,
        &quot;lsn&quot;: 34204240,
        &quot;xmin&quot;: null
    },
    &quot;op&quot;: &quot;u&quot;,
    &quot;ts_ms&quot;: 1569700445537,
    &quot;audit&quot;: {
        &quot;client_date&quot;: 1566461551000000,
        &quot;usecase&quot;: &quot;UPDATE VEGETABLE&quot;,
        &quot;user_name&quot;: &quot;farmermargaret&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Of course, the buffer processing logic may be adjusted as per your specific requirements;
for instance instead of indefinitely waiting for corresponding transaction metadata,
we may also decide that it makes more sense to propagate change events unenriched after some waiting time or to raise an exception indicating the missing metadata.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to see whether the buffering works as expected, you could do a small experiment:
modify a vegetable record using SQL directly in the database.
Debezium will capture the event, but as there’s no corresponding transaction metadata provided,
the event will not be forwarded to the enriched vegetables topic.
If you add another vegetable using the REST API,
this one also will not be propagated:
although there is a metadata record for it, it’s blocked by the other change event.
Only once you have inserted a metadata record for the first change’s transaction into the &lt;code&gt;transaction_context_data&lt;/code&gt; table,
both change events will be processed and sent to the output topic.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this blog post we’ve discussed how change data capture in combination with stream processing can be used to build audit logs in an efficient, low-overhead way.
In contrast to library and trigger-based approaches, the events that form the audit trail are retrieved via CDC from the database’s transaction logs,
and apart from the insertion of a single metadata record per transaction
(which in similar form would be required for any kind of audit log), no overhead to OLTP transactions is incurred.
Also audit log entries can be obtained when data records are subject to bulk updates or deletes,
something typically not possible with library-based auditing solutions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Additional metadata that typically should be part of an audit log,
can be provided by the application via a separate table,
which also is captured via Debezium.
With the help of Kafka Streams the actual data change events can be enriched with the data from that metadata table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One aspect we haven’t discussed yet is querying the audit trail entries,
e.g. to examine specific earlier versions of the data.
To do so, the enriched change data events typically would be stored in a queryable database.
Unlike a basic data replication pipeline, not only the latest version of each record would be stored in the database in that case, but all the versions, i.e. the primary keys typically would be amended with the transaction id of each change.
This would allow to select single data records or even joins of multiple tables to get the data valid as per a given transaction id.
How this could be implemented in detail may be discussed in a future post.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Your feedback on this approach for building audit logs is very welcomed,
just post a comment below.
To get started with your own implementation,
you can check out &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/auditlog&quot;&gt;the code&lt;/a&gt; in the Debezium examples repository on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to &lt;a href=&quot;https://twitter.com/crancran77&quot;&gt;Chris Cranford&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/hpgrahsl&quot;&gt;Hans-Peter Grahsl&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/hashhar&quot;&gt;Ashhar Hasan&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/jbfletch_&quot;&gt;Anna McDonald&lt;/a&gt; and Jiri Pechanec for their feedback while working on this post and the accompanying example code!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/</id>
<title>Debezium 0.10.0.CR2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-09-26T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/26/debezium-0-10-0-cr2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="cassandra"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 0.10.0.CR2!


After the CR1 release we decided to do another candidate release, as there was not only a good number of bug fixes coming in, but also a few very useful feature implementations were provided by the community, which we didn&#8217;t want to delay.
So we adjusted the original plan a bit and now aim for Debezium 0.10 Final in the course of next week,
barring any unforeseen regressions.


As usual, let&#8217;s take a closer look at some of the new features and resolved bugs.




Customizable Message Keys


Being able to configure which columns of a table...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.10.0.CR2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the CR1 release we decided to do another candidate release, as there was not only a good number of bug fixes coming in, but also a few very useful feature implementations were provided by the community, which we didn’t want to delay.
So we adjusted the original plan a bit and now aim for Debezium 0.10 Final in the course of next week,
barring any unforeseen regressions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As usual, let’s take a closer look at some of the new features and resolved bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;customizable_message_keys&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#customizable_message_keys&quot; /&gt;Customizable Message Keys&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Being able to configure which columns of a table should go into the key of corresponding Kafka messages has been a long-standing feature request (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1015&quot;&gt;DBZ-1015&lt;/a&gt;).
To recap, by default the message key of Debezium’s data change events will contain the primary key column(s) of the represented table.
That’s a sensible default, but sometimes more flexibility is desirable:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Some tables don’t have a primary key, esp. in legacy data models&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Choosing a different message key than the primary key may facilitate stream processing applications that operate on multiple change data topics&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The second case is especially of interest when building &lt;a href=&quot;https://kafka.apache.org/documentation/streams/&quot;&gt;Kafka Streams&lt;/a&gt; applications that join multiple CDC topics:
in general, topic joins can only be done if the message key is the same on both sides of the join.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For instance let’s assume we have two tables, &lt;code&gt;Customer&lt;/code&gt; and &lt;code&gt;CustomerDetails&lt;/code&gt; with different primary keys and a foreign key relationship from &lt;code&gt;CustomerDetails&lt;/code&gt; to &lt;code&gt;Customer&lt;/code&gt;.
By choosing that foreign key column as the message key for customer detail change events,
the two table streams could be joined without the need for re-keying the customer details topic.
To do so, the new &lt;code&gt;message.key.columns&lt;/code&gt; option can be used like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;message.key.columns=dbserver1.inventory.customerdetails:CustomerId&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Customizable message keys are supported for all the relational Debezium connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;pluggable_serializers_for_cassandra&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pluggable_serializers_for_cassandra&quot; /&gt;Pluggable Serializers for Cassandra&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;From previous announcements you might remember that Debezium’s Cassandra connector is a bit different from the other ones,
as it’s not based on the Kafka Connect framework.
As such, until now it didn’t support the notion of configurable message serializers;
Avro was the only supported message format.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As part of the ongoing efforts to align the Cassandra connector more closely with the other ones,
it now allows to configure different serializers, so you also could use JSON, ProtoBuf and other formats (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1405&quot;&gt;DBZ-1405&lt;/a&gt;).
The serializer framework from Kafka Connect is reused for that, so you can leverage all the existing serializers and configure them exactly the same way as done for any other connector.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;improved_handling_of_postgres_toast_columns&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#improved_handling_of_postgres_toast_columns&quot; /&gt;Improved handling of Postgres TOAST Columns&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/storage-toast.html&quot;&gt;TOAST columns&lt;/a&gt; are a mechanism in Postgres for dealing with column values that exceed the page size limit (typically 8 KB).
While the usage of TOAST is transparent when interacting with the database itself,
this is not the case when obtaining change events via logical decoding.
As TOASTed values are not stored within the physical data row itself,
logical decoding does not expose the value of &lt;strong&gt;unchanged&lt;/strong&gt; TOAST columns
(unless the column is part of the table’s replica identity).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This situation used to be handled in different ways by the the logical decoding plug-ins supported by Debezium (&lt;code&gt;pgoutput&lt;/code&gt;, &lt;code&gt;decoderbufs&lt;/code&gt; and &lt;code&gt;wal2json&lt;/code&gt;),
one approach being the retrieval of such column &quot;out of bands&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unfortunately, there’s no way for savely doing this when considering concurrent writes to such record.
So we reworked how TOAST columns are handled:
if a TOAST column’s value hasn’t changed and that column isn’t part of the table’s replica identity, its value will not be contained in &lt;code&gt;UPDATE&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt; events.
Instead, a configurable marker value will be exported in this case
(defaulting to &lt;code&gt;__debezium_unavailable_value&lt;/code&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This avoids the race conditions that were possible before, but of course raises the question how consumers should deal with this marker value.
There are multiple possible answers to that:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The value could simply be ignored; for instance a consumer that writes change events to a database, may omit that column from the &lt;code&gt;UPDATE&lt;/code&gt; statement it issues&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When not working with dynamic updates, a trigger may be installed in a sink database,
that ignores any updates that would set a column value to the marker, keeping the previous value&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When actually requiring complete change events including any TOAST column within the Kafka change data topic itself, a stateful Kafka Streams application could be built which hydrates incoming change events with the marker value based on the previous column value persisted in a state store&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thinking about it, the last approach might be an interesting topic for a future blog post :-)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes_and_other_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes_and_other_changes&quot; /&gt;Bugfixes and Other Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these feature implementations, this release contains a number of bugfixes, too:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When using the &lt;code&gt;pgoutput&lt;/code&gt; logical decoding plug-in for Postgres, custom publication names are supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1436&quot;&gt;DBZ-1436&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The Postgres connector will retry for a configurable period of time to obtain a replication slot, which can be helpful when rebalancing existing connectors in a cluster (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1426&quot;&gt;DBZ-1426&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reserved characters in column names can be replaced when using Avro as message format (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1044&quot;&gt;DBZ-1044&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Default values without the time part for MySQL &lt;code&gt;DATETIME&lt;/code&gt; columns are supported now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1501&quot;&gt;DBZ-1501&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MySQL &lt;code&gt;CREATE DATABASE&lt;/code&gt; and &lt;code&gt;CREATE TABLE&lt;/code&gt; statements with default character sets are supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1470&quot;&gt;DBZ-1470&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Testing for MongoDB has been expanded to also cover version 4.2 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1389&quot;&gt;DBZ-1389&lt;/a&gt;), and the Postgres driver has been updated to the latest and greatest version 42.2.7 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1462&quot;&gt;DBZ-1462&lt;/a&gt;).
We’re also happy to report that going forward, the Debezium container images are also available in the &lt;a href=&quot;https://quay.io/&quot;&gt;quay.io&lt;/a&gt; container registry (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1178&quot;&gt;DBZ-1178&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, not less than &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.CR2&quot;&gt;30 issues&lt;/a&gt; were fixed in the 0.10 CR2 release.
Please refer to the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes#release-0-10-0-cr2&quot;&gt;release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release wouldn’t be possible without all the fantastic people from the Debezium community who contributed:
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ChingTsai&quot;&gt;Ching Tsai&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/javierholguera&quot;&gt;Javier Holguera&lt;/a&gt;
&lt;a href=&quot;https://github.com/jerrinot&quot;&gt;Jaromir Hamala&lt;/a&gt;,
&lt;a href=&quot;https://github.com/josharenberg&quot;&gt;Josh Arenberg&lt;/a&gt; and
&lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to all of you!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/</id>
<title>Debezium 0.10.0.CR1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-09-10T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/10/debezium-0-10-0-cr1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="cassandra"></category>
<summary>



The Debezium community is on the homestretch towards the 0.10 release and we&#8217;re happy to announce the availability of Debezium 0.10.0.CR1!


Besides a number of bugfixes to the different connectors, this release also brings a substantial improvement to the way initial snapshots can be done with Postgres.
Unless any major regressions show up, the final 0.10 release should follow very soon.




Exported Snapshots for Postgres


One capability of Postgres' logical decoding facility that has not been leveraged by Debezium so far is the notion of exported snapshots:
when a replication slot is created, a transaction with SNAPSHOT isolation mode can be started, which allows to...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium community is on the homestretch towards the 0.10 release and we’re happy to announce the availability of Debezium &lt;strong&gt;0.10.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides a number of bugfixes to the different connectors, this release also brings a substantial improvement to the way initial snapshots can be done with Postgres.
Unless any major regressions show up, the final 0.10 release should follow very soon.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;exported_snapshots_for_postgres&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#exported_snapshots_for_postgres&quot; /&gt;Exported Snapshots for Postgres&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One capability of Postgres&#39; logical decoding facility that has not been leveraged by Debezium so far is the notion of &lt;a href=&quot;https://www.postgresql.org/docs/10/logicaldecoding-explanation.html#id-1.8.14.8.5&quot;&gt;exported snapshots&lt;/a&gt;:
when a replication slot is created, a transaction with &lt;code&gt;SNAPSHOT&lt;/code&gt; isolation mode can be started, which allows to export tables from the database without taking any locks, exactly at the moment when the slot is created.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Based on earlier work which allows for much more flexibility in regards to how snapshot are handled by the Debezium Postgres connector,
the exported snapshot functionality can now be used via the new &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/connectors/postgresql.html#snapshots&quot;&gt;snapshot mode&lt;/a&gt; &lt;code&gt;exported&lt;/code&gt;
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1035&quot;&gt;DBZ-1035&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1440&quot;&gt;DBZ-1440&lt;/a&gt;).
We encourage you to give this a test ride and report back any issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Eventually, this mode should become the default snapshotting behavior, as it doesn’t require the connector to obtain any locks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes&quot; /&gt;Bugfixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Due to a value overflow multiple users had reported issues when capturing the values of temporal columns with values far out in the future, e.g. years after 3000
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-949&quot;&gt;DBZ-949&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1205&quot;&gt;DBZ-1205&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1255&quot;&gt;DBZ-1255&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While such values are perhaps not too common in typical enterprise applications
(after all, who wants to have their purchase order delivery scheduled in a thousand years from now),
there’s still several use cases working with such values, e.g. think of potentially large half-life times when modelling nuclear decay processes.
So we did a larger refactoring of the code dealing with temporal values, and we are happy to report that these issues have been fixed now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Other bugfixes were done in regards to dealing with &quot;no-op&quot; events in MongoDB
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1464&quot;&gt;DBZ-1464&lt;/a&gt;) and the recently added propagation of &lt;code&gt;source&lt;/code&gt; fields to outgoing message when using the &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/configuration/event-flattening.html&quot;&gt;new record state extraction SMT&lt;/a&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1448&quot;&gt;DBZ-1448&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cassandra_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cassandra_connector&quot; /&gt;Cassandra Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After the &lt;a href=&quot;https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/&quot;&gt;initial release&lt;/a&gt; of the Debezium connector for Cassandra,
work has begun to further align it with the other Debezium connectors
(unlike the relational connectors and the one for MongoDB, the Cassandra connector currently is not based on Apache Kafka Connect,
but runs as a stand-alone process).
The first outcome of this is that its configured via a properties file now
(similar to using Kafka Connect in standalone mode) and not via a YAML file.
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1406&quot;&gt;DBZ-1406&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next step will be to make the aspect of message serialization configurable:
while currently only Avro is supported by the connector,
it will eventually support the notion of pluggable converters,
allowing you to use the JSON, Avro and any other converters you may already know from Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;reworked_website_and_documentation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#reworked_website_and_documentation&quot; /&gt;Reworked Website and Documentation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When reading this blog post, it’s hard to miss:
the Debezium website has &lt;a href=&quot;https://debezium.io/blog/2019/09/05/website-documentation-overhaul/&quot;&gt;received a facelift&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Information for the current stable and development releases (0.9 and 0.10 at this time) is much easier to find now.
Also the documentation has been re-organized and is published in version-specific approach now,
i.e. you can now obtain the specific documentation applying for a particular release.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The blog section of the website has been reworked, too:
the main page shows an introductory snippet for the most recent posts,
whereas a number of &quot;featured&quot; blog posts is listed on the left.
These are typically earlier blog posts which explore advanced topics such as the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox pattern&lt;/a&gt; in depth and which we wanted to make easier to discover and consume.
We hope you like the new website and documentation structure and it helps you to find all the information you’re looking for better than before.
If you run into any issues (formatting glitches, broken links etc.), please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Refer to the &lt;a href=&quot;https://debezium.io/releases/0.10/release-notes#release-0-10-0-cr1&quot;&gt;0.10.0.CR1 release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, many thanks to all the awesome people from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/garrett528&quot;&gt;Andrew Garrett&lt;/a&gt;,
&lt;a href=&quot;https://github.com/bingqinzhou&quot;&gt;Bingqin Zhou&lt;/a&gt;
&lt;a href=&quot;https://github.com/cscetbon&quot;&gt;Cyril Scetbon&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivanobulo&quot;&gt;Ivan Luzyanin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/levzem&quot;&gt;Lev Zemlyanov&lt;/a&gt; and
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/09/05/website-documentation-overhaul/</id>
<title>Site and Documentation Overhaul</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-09-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/09/05/website-documentation-overhaul/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="website"></category>
<summary>



This past summer has been a super exciting time for the team.
Not only have we been working hard on Debezium 0.10 but we have unveiled some recent changes to debezium.io.




New Releases Page


It is important that the Debezium community be able to find information easily about a given release series.
We have introduced a new Releases area on the site that describes details about each release series (e.g. 0.9, the current stable release and 0.10, the current development release) such as:




What database or Apache Kafka (Connect) platforms were tested


What Java version is supported


How to migrate to a specific release series


Where to download...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This past summer has been a super exciting time for the team.
Not only have we been working hard on Debezium 0.10 but we have unveiled some recent changes to &lt;a href=&quot;https://debezium.io/&quot;&gt;debezium.io&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_releases_page&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_releases_page&quot; /&gt;New Releases Page&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is important that the Debezium community be able to find information easily about a given release series.
We have introduced a new &lt;a href=&quot;https://debezium.io/releases&quot;&gt;Releases&lt;/a&gt; area on the site that describes details about each release series (e.g. &lt;a href=&quot;https://debezium.io/releases/0.9&quot;&gt;0.9&lt;/a&gt;, the current &lt;em&gt;stable release&lt;/em&gt; and &lt;a href=&quot;https://debezium.io/releases/0.10&quot;&gt;0.10&lt;/a&gt;, the current &lt;em&gt;development&lt;/em&gt; release) such as:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What database or Apache Kafka (Connect) platforms were tested&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What Java version is supported&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to migrate to a specific release series&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Where to download the connectors or other series artifacts&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;What changes were introduced in that series&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And much more…​&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The goal is to make it simple and easy to find all information about a specific release series in a single place.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_documentation&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_documentation&quot; /&gt;New Documentation&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Probably one of the most limiting factors with how our documentation was published previously is that it focused solely on the idea of the latest stable version.
Also the documentation sources were separated from the actual code sources in the main code repository.
This presented several drawbacks:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Confusing to users of older releases&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prevented publishing documentation for development versions&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Caused friction for contributors when implementing new features that need documentation updates&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What we felt we needed to provide the community was documentation published by version.
This would allow documentation to be tailored specific to that version, allowing fluid changes for future versions without impact to prior versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Such a solution also has the benefit that it enables the Debezium team to publish development version documentation easily, which is a critical step in helping users who test unstable releases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With &lt;a href=&quot;https://antora.org/&quot;&gt;Antora&lt;/a&gt; we found a toolchain which addresses these needs.
It allows us to maintain different versions of the documentation right next to the actual code and aggregate them on the website.
Going forward, Debezium documentation can be found at &lt;a href=&quot;https://debezium.io/documentation&quot;&gt;Reference documentation&lt;/a&gt;.
This page allows visitors to quickly navigate to documentation for a specific version.
Once in the documentation, you can quickly navigate between various Debezium versions easily.
Built by the friendly folks behind AsciiDoctor, Antora comes with lots of well thought out details;
e.g. there’s an &quot;Edit this Page&quot; link on each page, which makes it very easy to create GitHub pull requests with documentation fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, we also took the time to fill the long-standing place holder pages describing the Debezium &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/architecture.html&quot;&gt;Architecture&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/documentation/reference/0.10/features.html&quot;&gt;Features&lt;/a&gt; with some actual contents.
Woohoo!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We certainly hope these recent changes make it much easier for the community.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If something is unclear, could be improved, or worse a link that isn’t working, we welcome feedback.
You can report such concerns to us by &lt;a href=&quot;https://issues.redhat.com/browse/DBZ&quot;&gt;opening an issue&lt;/a&gt; for our website.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/</id>
<title>Debezium 0.10.0.Beta4 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-08-20T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/08/20/debezium-0-10-0-beta4-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



The temperatures are slowly cooling off after the biggest summer heat,
an the Debezium community is happy to announce the release of Debezium 0.10.0.Beta4.
In this release we&#8217;re happy to share some news we don&#8217;t get to share too often:
with Apache Cassandra,
another database gets added to the list of databases supported by Debezium!


In addition, we finished our efforts for rebasing the existing Postgres connector to Debezium framework structure established for the SQL Server and Oracle connectors.
This means more shared coded between these connectors, and in turn reduced maintenance efforts for the development team going forward;
but there&#8217;s one immediately tangible advantage for you...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The temperatures are slowly cooling off after the biggest summer heat,
an the Debezium community is happy to announce the release of Debezium &lt;strong&gt;0.10.0.Beta4&lt;/strong&gt;.
In this release we’re happy to share some news we don’t get to share too often:
with &lt;a href=&quot;http://cassandra.apache.org/&quot;&gt;Apache Cassandra&lt;/a&gt;,
another database gets added to the list of databases supported by Debezium!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, we finished our efforts for rebasing the existing Postgres connector to Debezium framework structure established for the SQL Server and Oracle connectors.
This means more shared coded between these connectors, and in turn reduced maintenance efforts for the development team going forward;
but there’s one immediately tangible advantage for you coming with this, too:
the Postgres connector now exposes the same metrics you already know from the other connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, the new release contains a range of bugfixes and other useful improvements.
Let’s explore some details below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;incubating_cassandra_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#incubating_cassandra_connector&quot; /&gt;Incubating Cassandra Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have been following this blog lately, you’ll have read about the latest addition to the Debezium family
in Joy Gao’s excellent posts about the new connector
(&lt;a href=&quot;https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/&quot;&gt;part 1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/&quot;&gt;part 2&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In case you haven’t read those yet, we’d highly recommend to do so in order to learn more about the challenges encountered when implementing a CDC connector for a distributed datastore such as Cassandra as well as the design decisions made in order to come up with a first &quot;minimal viable product&quot;.
Joy also did a &lt;a href=&quot;https://www.infoq.com/presentations/wepay-database-streaming/&quot;&gt;great talk at QCon&lt;/a&gt; last year, which touches on the topic of CDC for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having been originally developed internally at &lt;a href=&quot;https://debezium.io/blog/2017/02/22/Debezium-at-WePay/&quot;&gt;long-term Debezium user&lt;/a&gt; WePay,
the &lt;a href=&quot;https://wecode.wepay.com/&quot;&gt;WePay team&lt;/a&gt; decided to open-source their work, put it under the Debezium umbrella and continue to evolve it there.
That’s really great news for the Debezium community!
We couldn’t be happier about this contribution and look forward to evolving this new connector together in the open.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point the Cassandra connector is in &quot;incubating&quot; state,
i.e. its design and implementation are still pretty much in flux, the event structure which it creates may change in future releases etc.
Note that, unlike the other Debezium connectors, this one currently is not based on Kafka Connect.
Instead, it is implemented as a standalone process running on Cassandra node(s) themselves.
Refer to the blog posts linked above for the reasoning behind this design and possible future developments around this.
Needless to say, any ideas and contributions in this area will be highly welcomed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Together with the connector we’ve also provided an initial draft of the &lt;a href=&quot;https://debezium.io/docs/connectors/cassandra&quot;&gt;connector documentation&lt;/a&gt;;
this is still work-in-progress and will be amended in the next few days.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;further_new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#further_new_features&quot; /&gt;Further New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector supports the metrics known from SQL Server and Oracle now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-777&quot;&gt;DBZ-777&lt;/a&gt;).
When using the SQL Server connector, it is now ensured that tables are snapshotted in a deterministic order,
as defined by the given table whitelist configuration (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1254&quot;&gt;DBZ-1254&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There have also been two improvements to our SMTs (single message transformations):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SMT for &lt;a href=&quot;https://debezium.io/docs/configuration/event-flattening/&quot;&gt;new record state extraction&lt;/a&gt; allows to add additional columns for propagating metadata fields from the &lt;code&gt;source&lt;/code&gt; block
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1395&quot;&gt;DBZ-1395&lt;/a&gt;, e.g. useful to propagate the transaction into sink tables).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The default structure produced by the &lt;a href=&quot;https://debezium.io/docs/configuration/outbox-event-router/&quot;&gt;outbox routing SMT&lt;/a&gt; has been further streamlined (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1385&quot;&gt;DBZ-1385&lt;/a&gt;);
the message value will now only contain the contents of the configured outbox table payload column.
In case you want to re-add the &lt;code&gt;eventType&lt;/code&gt; value, you can configure it as an &quot;additional field&quot;,
which either goes into the message as a header (recommended) or into the message value,
which as before will be a nested structure then.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bugfixes_and_other_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bugfixes_and_other_improvements&quot; /&gt;Bugfixes and Other Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, here’s an overview of asorted bugfixes in the 0.10 Beta4 release:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The MySQL connector handles &lt;code&gt;GRANT DELETE ON &amp;lt;table&amp;gt;&lt;/code&gt; statements correctly (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1411&quot;&gt;DBZ-1411&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Superfluous tables scans are avoided when using the &lt;code&gt;initial_schema_only&lt;/code&gt; snapshot strategy with SQL Server (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1417&quot;&gt;DBZ-1417&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The superfluous creation of connections is avoided when obtaining the xmin position of Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1381&quot;&gt;DBZ-1381&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The new record state extraction SMT handles heartbeat events correctly (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1430&quot;&gt;DBZ-1430&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta4 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta4&quot;&gt;release notes&lt;/a&gt; for the complete list of addressed issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big thank you goes out to all the contributors from the Debezium community who worked on this release:
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; and
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/</id>
<title>Debezium 0.10.0.Beta3 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-07-25T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/25/debezium-0-10-0-beta3-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



The summer is at its peak but Debezium community is not relenting in its effort so the Debezium 0.10.0.Beta3 is released.


This version not only continues in incremental improvements of Debezium but also brings new shiny features.


All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
This is no longer necessary. Debezium now supports (DBZ-766) pgoutput replication protocol that is available out-of-the-box since PostgreSQL 10.


There is a set of further minor improvements.
The tombstones for deletes are configurable for...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The summer is at its peak but Debezium community is not relenting in its effort so the Debezium &lt;strong&gt;0.10.0.Beta3&lt;/strong&gt; is released.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This version not only continues in incremental improvements of Debezium but also brings new shiny features.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All of you who are using PostgreSQL 10 and higher as a service offered by different cloud providers definitely felt the complications when you needed to deploy logical decoding plugin necessary to enable streaming.
This is no longer necessary. Debezium now supports (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-766&quot;&gt;DBZ-766&lt;/a&gt;) &lt;a href=&quot;https://www.postgresql.org/docs/10/protocol-logical-replication.html&quot;&gt;pgoutput&lt;/a&gt; replication protocol that is available out-of-the-box since PostgreSQL 10.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There is a set of further minor improvements.
The tombstones for deletes are configurable for all connectors now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1365&quot;&gt;DBZ-1365&lt;/a&gt;).
Also tables without primary keys are now supported for all connectors (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-916&quot;&gt;DBZ-916&lt;/a&gt;).
This further reduces the gap between old and new connectors capabilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are improvements for heartbeat system.
Heartbeat messages now contain the timestamp (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1363&quot;&gt;DBZ-1363&lt;/a&gt;) of when they were created in their body.
The new messages are properly skipped by the Outbox router (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1388&quot;&gt;DBZ-1388&lt;/a&gt;).
MySQL connector additionally uses heartbeats for &lt;code&gt;BinlogReader&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1338&quot;&gt;DBZ-1338&lt;/a&gt;).
MongoDB connector now utilizes heartbeats too (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1198&quot;&gt;DBZ-1198&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we now that metrics are very important for keeping Debezium happy in production we have extended the set of supported metrics.
A new metric count of events in error (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1222&quot;&gt;DBZ-1222&lt;/a&gt;) is added so it is easy to monitor any non-standards in processing.
Database history recovery can take a long time during startup so it is now possible to monitor the progress of it (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1356&quot;&gt;DBZ-1356&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other changes include updating of Docker images to use Kafka 2.3.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1358&quot;&gt;DBZ-1358&lt;/a&gt;).
PostgreSQL supports lockless snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1238&quot;&gt;DBZ-1238&lt;/a&gt;) and Outbox router now  process delete messages (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1320&quot;&gt;DBZ-1320&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We continue with stabilization of the 0.10 release line, with lots of bug fixes to the different connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Multiple defects in MySQL parser have been fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1398&quot;&gt;DBZ-1398&lt;/a&gt;, (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1397&quot;&gt;DBZ-1397&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1376&quot;&gt;DBZ-1376&lt;/a&gt;) and &lt;code&gt;SAVEPOINT&lt;/code&gt; statements are no longer recorded in database history (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-794&quot;&gt;DBZ-794&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Under certain circumstances, it was possible that PostgreSQL connector lost the first event while switching to streaming from the snapshot (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1400&quot;&gt;DBZ-1400&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta3 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta3&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/BinLi1988&quot;&gt;Bin Li&lt;/a&gt;,
&lt;a href=&quot;https://github.com/brbrown25&quot;&gt;Brandon Brown&lt;/a&gt; and
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/</id>
<title>Streaming Cassandra at WePay - Part 2</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-07-15T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/15/streaming-cassandra-at-wepay-part-2/" rel="alternate" type="text/html" />
<author>
<name>Joy Gao</name>
</author>
<category term="cassandra"></category>
<summary>



This post originally appeared on the WePay Engineering blog.


In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:




Cassandra to Kafka with CDC agent


Kafka with BigQuery with KCBQ


Transformation with BigQuery view






Cassandra to Kafka with CDC Agent


The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-2&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the first half of this blog post series, we explained our decision-making process of designing a streaming data pipeline for Cassandra at WePay. In this post, we will break down the pipeline into three sections and discuss each of them in more detail:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Cassandra to Kafka with CDC agent&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka with BigQuery with KCBQ&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformation with BigQuery view&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;cassandra_to_kafka_with_cdc_agent&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#cassandra_to_kafka_with_cdc_agent&quot; /&gt;Cassandra to Kafka with CDC Agent&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Cassandra CDC agent is a JVM process that is intended to be deployed on each node in a Cassandra cluster. The agent is comprised of several interdependent processors, running concurrently and working together to publish change events to Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;snapshot_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#snapshot_processor&quot; /&gt;Snapshot Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for bootstrapping new tables. It looks up the CDC configuration to determine the snapshot mode, and performs snapshot on CDC-enabled tables if needed. To snapshot a table, the agent performs a full table scan and converts each row in the result set into an individual create event, and then sequentially enqueues them to an in-memory &lt;a href=&quot;https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/BlockingQueue.html&quot;&gt;BlockingQueue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;commit_log_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_processor&quot; /&gt;Commit Log Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for watching the CDC directory for new commit logs, parsing the commit log files via Cassandra’s &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/commitlog/CommitLogReader.java&quot;&gt;&lt;code&gt;CommitLogReader&lt;/code&gt;&lt;/a&gt;, transforming deserialized mutations into standardized change events, and finally enqueuing them to the same queue as the snapshot processor.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, some readers may have concerns in regard to running Snapshot Processor and Commit Log Processors concurrently rather than serially. The reason is that Cassandra uses a &lt;a href=&quot;https://datastax.github.io/cpp-driver/topics/basics/client_side_timestamps/&quot;&gt;client-side timestamp&lt;/a&gt; to determine event order, and resolves conflicts with last write wins. This client-side timestamp is deliberately stored in each change event. This is why snapshotting doesn’t have to proceed commit log processing – the ordering is determined later on when the data is queried in the data warehouse.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;queue_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#queue_processor&quot; /&gt;Queue Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for dequeuing change events, transforming them into &lt;a href=&quot;https://avro.apache.org/docs/1.8.1/spec.html&quot;&gt;Avro&lt;/a&gt; records, and sending them to Kafka via a Kafka producer. It also tracks the position of the most recently sent event, so that on restart it is able to pick up from where it left off.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Implementing an in-memory queue in the CDC agent seems like overkill at first. Given there is only a single thread doing the enqueue and another thread doing the dequeue, the performance boost is negligible. The motivation here is to decouple the work of parsing commit logs, which should be done serially in the right order, from the work of serializing and publishing Kafka events, which can be parallelized by multiple threads for different tables. Although such parallelization is not implemented at the moment, we want the flexibility of adding this feature in the near future.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some may also wonder why &lt;a href=&quot;https://docs.confluent.io/current/connect/index.html&quot;&gt;Kafka Connect&lt;/a&gt; is not used here as it seems like a natural fit for streaming. It is a great option if we wanted distributed parallel processing with fault tolerance. However, it is more complicated to deploy, monitor, and debug than a Kafka producer. For the purpose of building a minimum viable infrastructure, we chose Kafka producer at the time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;schema_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#schema_processor&quot; /&gt;Schema Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to support automatic schema evolution, this processor periodically polls the database for the latest table schema, and updates the in-memory schema cache if a change is detected. Snapshot Processor and Commit Log Processor both look up table schema from this cache and attach it as part of the change event prior to enqueue. Then upon dequeue, the Queue Processor transforms the attached table schema into an Avro schema for record serialization.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;commit_log_post_processor&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_post_processor&quot; /&gt;Commit Log Post Processor&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This processor is responsible for cleaning up commit logs after they have been processed. The default Commit Log Post Processor implementation will simply perform deletion. A custom Commit Log Post Processor can be configured for use case such as archiving commit log files to &lt;a href=&quot;https://aws.amazon.com/s3/&quot;&gt;S3&lt;/a&gt; or &lt;a href=&quot;https://cloud.google.com/storage/&quot;&gt;GCS&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;kafka_to_bigquery_with_kcbq&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_to_bigquery_with_kcbq&quot; /&gt;Kafka to BigQuery with KCBQ&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the events arrive in Kafka, we use KCBQ to send the events data to BigQuery without performing special transformations, just like in our &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;MySQL streaming data pipeline&lt;/a&gt;. We have written a previous &lt;a href=&quot;https://wecode.wepay.com/posts/kafka-bigquery-connector&quot;&gt;blog post&lt;/a&gt; explaining this connector in more details.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;transformation_with_bigquery_view&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#transformation_with_bigquery_view&quot; /&gt;Transformation with BigQuery View&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once the events are in BigQuery, this is where the heavy-lifting is being done. We create &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;virtual views&lt;/a&gt; on top of the raw tables to merge the data in a way that mirrors the source table in Cassandra. Note that each row in the raw tables contains limited data – only columns that have been modified have states. This means selecting the latest row for each primary key will not provide us with data that is consistent with source. Instead, the query must identify the latest cell in each column for each primary key. This can be achieved with self-joins on the primary key for each column in the table. Although joins are slow in MySQL, BigQuery’s parallel execution engine and columnar storage makes this possible. A view on top of a 1TB Cassandra table in BigQuery takes about 100 seconds to query.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;compaction&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#compaction&quot; /&gt;Compaction&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The fact that the BigQuery view is virtual implies each time the view is queried essentially triggers a full compaction of the raw data. This means the cost will go up with the number of queries, not to mention the duplicated events amplifies the amount of data that needs to be processed by a factor of N, where N is the replication factor. To save cost and improve performance, periodic compaction by materializing the view is necessary.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;future_development_work&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#future_development_work&quot; /&gt;Future Development Work&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;support_for_cassandra_4_0&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_cassandra_4_0&quot; /&gt;Support for Cassandra 4.0&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Cassandra 4.0, the improved CDC feature allows the connector to be able to parse events in real-time as they are written rather than in micro-batches on each commit log flush. This reduces latency substantially.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;performance_optimization&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_optimization&quot; /&gt;Performance Optimization&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As mentioned earlier, there is a single thread responsible for dequeuing, serializing, and publishing Kafka records. However, as the write throughput increases, if the performance of the agent does not keep up, it would result in a backlog of unprocessed commit logs which could potentially impact the health of our production database. The next step is to leverage parallel processing of events to optimize performance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;streamline_with_debezium_and_kafka_connect&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streamline_with_debezium_and_kafka_connect&quot; /&gt;Streamline with Debezium and Kafka Connect&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We initially built the Cassandra CDC agent as a standalone project. Now that it is open-sourced as a &lt;a href=&quot;https://debezium.io/&quot;&gt;Debezium&lt;/a&gt; connector, we can replace some of our custom classes with existing ones in Debezium. Another improvement is to support common features that all Debezium connectors have, such as support for multiple serialization formats. Finally, the CDC agent is not fault tolerant; robust alert and monitoring are required as part of deployment. One area to explore in the future is to build the CDC agent on top of Kafka Connect as a source connector, this further streamlines the Cassandra connector with other Debezium connectors, and provides scalability and fault tolerance for free.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;closing_remarks&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#closing_remarks&quot; /&gt;Closing Remarks&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra being a peer-to-peer distributed database poses some really interesting challenges for CDC that do not exist in relational databases like MySQL and Postgres, or even a single-master NoSQL database like MongoDB. Note that it is worth evaluating the limitations before rolling out your own real-time data pipeline for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides understanding Cassandra internals, we learned a few lessons on engineering productivity along the way:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;minimum_viable_product_philosophy&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_product_philosophy&quot; /&gt;Minimum Viable Product Philosophy&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By stripping away all features except for the essentials, we were able to build, test, and deploy a working solution in a reasonable time with limited resources. Had we aimed to design a pipeline that encompasses all features upfront, it would have taken a lot longer and required much more resources.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;community_involvement&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#community_involvement&quot; /&gt;Community Involvement&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra is an open-source project. Rather than tackling the problem solo, we were engaged with the Cassandra community from the very start (i.e. sharing experiences with committers and users via &lt;a href=&quot;https://www.meetup.com/Apache-Cassandra-Bay-Area/&quot;&gt;meetups&lt;/a&gt;, &lt;a href=&quot;https://user.cassandra.apache.narkive.com/njOxVaxP/using-cdc-feature-to-stream-c-to-kafka-design-proposal&quot;&gt;discussing proposals in mailing list&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=0K0fYHsFBZg&quot;&gt;presenting proof-of-concept in conferences&lt;/a&gt;, etc.); all of which provided us with valuable feedback throughout the design and implementation stages.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/</id>
<title>Streaming Cassandra at WePay - Part 1</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-07-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/12/streaming-cassandra-at-wepay-part-1/" rel="alternate" type="text/html" />
<author>
<name>Joy Gao</name>
</author>
<category term="cassandra"></category>
<category term="featured"></category>
<summary>



This post originally appeared on the WePay Engineering blog.


Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. Vitess) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.




Batch ETL Options


After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This post originally appeared on the &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-cassandra-at-wepay-part-1&quot;&gt;WePay Engineering blog&lt;/a&gt;.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Historically, MySQL had been the de-facto database of choice for microservices at WePay. As WePay scales, the sheer volume of data written into some of our microservice databases demanded us to make a scaling decision between sharded MySQL (i.e. &lt;a href=&quot;https://vitess.io&quot;&gt;Vitess&lt;/a&gt;) and switching to a natively sharded NoSQL database. After a series of evaluations, we picked Cassandra, a NoSQL database, primarily because of its high availability, horizontal scalability, and ability to handle high write throughput.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;batch_etl_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#batch_etl_options&quot; /&gt;Batch ETL Options&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After introducing Cassandra to our infrastructure, our next challenge was to figure out a way to expose data in Cassandra to &lt;a href=&quot;https://cloud.google.com/bigquery/&quot;&gt;BigQuery&lt;/a&gt;, our data warehouse, for analytics and reporting. We quickly built an Airflow &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/hooks/cassandra_hook.py&quot;&gt;hook&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/airflow/blob/master/airflow/contrib/operators/cassandra_to_gcs.py&quot;&gt;operator&lt;/a&gt; to execute full loads. This obviously doesn’t scale, as it rewrites the entire database on each load. To scale the pipeline, we evaluated two incremental load approaches, but both have their shortcomings:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Range query. This is a common ETL approach where data is extracted via a range query at regular intervals, such as hourly or daily. Anyone familiar with &lt;a href=&quot;https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key&quot;&gt;Cassandra data modelling&lt;/a&gt; would quickly realize how unrealistic this approach is. Cassandra tables need to be modeled to optimize query patterns used in production. Adding this query pattern for analytics in most cases means cloning the table with different clustering keys. RDBMS folks might suggest secondary index to support this query pattern, but &lt;a href=&quot;https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes&quot;&gt;secondary index in Cassandra are local&lt;/a&gt;, therefore this approach would pose performance and scaling issues of its own.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Process unmerged SSTables. SSTables are Cassandra’s immutable storage files. Cassandra offers a &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/tools/ToolsSSTabledump.html&quot;&gt;sstabledump&lt;/a&gt; CLI command that converts SSTable content into human-readable JSON. However, Cassandra is built on top of the concept of &lt;a href=&quot;https://en.wikipedia.org/wiki/Log-structured_merge-tree&quot;&gt;Log-Structured Merge (LSM) Tree&lt;/a&gt;, meaning SSTables merge periodically into new compacted files. Depending on the compaction strategy, detecting unmerged SSTable files out-of-band may be challenging (we later learned about the &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/operations/opsBackupIncremental.html&quot;&gt;incremental backup&lt;/a&gt; feature in Cassandra which only backs up uncompacted SSTables; so this approach would have worked as well.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given these challenges, and having built and operated a &lt;a href=&quot;https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka&quot;&gt;streaming data pipeline for MySQL&lt;/a&gt;, we began to explore streaming options for Cassandra.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;streaming_options&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#streaming_options&quot; /&gt;Streaming Options&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;double_writing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#double_writing&quot; /&gt;Double-Writing&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/double-write.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writer send two distinct writes&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is to publish to Kafka every time a write is performed on Cassandra. This double-writing could be performed via the built-in trigger or a custom wrapper around the client. There are performance problems with this approach. First, due to the fact that we now need to write to two systems instead of one, write latency is increased. More importantly, when a write to one system fails due to a timeout, whether the write is successful or not is indeterministic. To guarantee data consistency on both systems, we would have to implement &lt;a href=&quot;https://en.wikipedia.org/wiki/Distributed_transaction&quot;&gt;distributed transactions&lt;/a&gt;, but multiple roundtrips for consensus will increase latency and reduce throughput further. This defeats the purpose of a high write-throughput database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;kafka_as_event_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_as_event_source&quot; /&gt;Kafka as Event Source&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/event-source.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing writes sent to Kafka and then downstream DB&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is to write to Kafka rather than directly writing to Cassandra; and then apply the writes to Cassandra by consuming events from Kafka. Event sourcing is a pretty popular approach these days. However, if you already have existing services directly writing to Cassandra, it would require a change in application code and a nontrivial migration. This approach also violates &lt;a href=&quot;https://docs.oracle.com/cd/E17076_05/html/gsg_db_rep/C/rywc.html&quot;&gt;read-your-writes consistency&lt;/a&gt;: the requirement that if a process performs a write, then the same process performing a subsequent read must observe the write’s effects. Since writes are routed through Kafka, there will be a lag between when the write is issued and when it is applied; during this time, reads to Cassandra will result in stale data. This may cause unforeseeable production issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;parsing_commit_logs&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#parsing_commit_logs&quot; /&gt;Parsing Commit Logs&lt;/h3&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cassandra/commit-log.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Image showing commit logs sent to Kafka&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra introduced a &lt;a href=&quot;http://cassandra.apache.org/doc/3.11.3/operating/cdc.html&quot;&gt;change data capture (CDC) feature&lt;/a&gt; in 3.0 to expose its commit logs. Commit logs are write-ahead logs in Cassandra designed to provide durability in case of machine crashes. They are typically discarded upon flush. With CDC enabled, they are instead transferred to a local CDC directory upon flush, which is then readable by other processes on the Cassandra node. This allows us to use the same CDC mechanism as in our MySQL streaming pipeline. It decouples production operations from analytics, and thus does not require additional work from application engineers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Ultimately, after considering throughput, consistency, and separation of concerns, the final option – parsing commit logs – became the top contender.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;commit_log_deep_dive&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#commit_log_deep_dive&quot; /&gt;Commit Log Deep Dive&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Aside from exposing commit logs, Cassandra also provides &lt;code&gt;CommitLogReader&lt;/code&gt; and &lt;code&gt;CommitLogReadHandler&lt;/code&gt; classes to help with the deserialization of logs. It seems like the hard work has been done, and what’s left is applying transformations – converting deserialized representations into Avro records and publish them to Kafka. However, as we dug further into the implementation of the CDC feature and of Cassandra itself, we realized that there are many new challenges.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;delayed_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#delayed_processing&quot; /&gt;Delayed Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Commit logs only arrive in the CDC directory when it is full, in which case it would be flushed/discarded. This implies there is a delay between when the event is logged and when the event is captured. If little to no writes are executed, then the delay in event capturing could be arbitrarily long.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;space_management&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#space_management&quot; /&gt;Space Management&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In MySQL you can set binlog retention such that the logs will be automatically deleted after the configured retention period. However in Cassandra there is no such option. Once the commit logs are transferred to CDC directory, consumption must be in place to clean up commit logs after processing. If the available disk space for CDC directory exceeds a given threshold, further writes to the database will be rejected.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;duplicated_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicated_events&quot; /&gt;Duplicated Events&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Commit logs on an individual Cassandra node do not reflect all writes to the cluster; they only reflect writes to the node. This makes it necessary to process commit logs on all nodes. But with a replication factor of N, N copies of each event are sent downstream.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;out_of_order_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_order_events&quot; /&gt;Out-of-Order Events&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Writes to an individual Cassandra node are logged serially as they arrive. However, these events may arrive out-of-order from when they are issued. Downstream consumers of these events must understand the event time and implement last write wins logic similar to &lt;a href=&quot;https://docs.datastax.com/en/archived/cassandra/3.0/cassandra/dml/dmlAboutReads.html&quot;&gt;Cassandra’s read path&lt;/a&gt; to get the correct result.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;out_of_band_schema_change&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#out_of_band_schema_change&quot; /&gt;Out-of-Band Schema Change&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Schema changes of tables are communicated via a &lt;a href=&quot;https://en.wikipedia.org/wiki/Gossip_protocol&quot;&gt;gossip protocol&lt;/a&gt; and are not recorded in commit logs. Therefore changes in schema could only be detected on a best-effort basis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;incomplete_row_data&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#incomplete_row_data&quot; /&gt;Incomplete Row Data&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Cassandra does not perform read before write, as a result change events do not capture the state of every column, they only capture the state of modified columns. This makes the change event less useful than if the full row is available.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once we acquired a deep understanding of Cassandra commit logs, we re-assessed our requirements against the given constraints in order to design a &lt;a href=&quot;https://riccomini.name/minimum-viable-infrastructure&quot;&gt;minimum viable infrastructure&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;minimum_viable_infrastructure&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#minimum_viable_infrastructure&quot; /&gt;Minimum Viable Infrastructure&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Borrowing from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Minimum_viable_product&quot;&gt;minimum viable product&lt;/a&gt; philosophy, we want to design a data pipeline with a minimum set of features and requirements to satisfy our immediate customers. For Cassandra CDC, this means:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Production database’s health and performance should not be negatively impacted by introducing CDC; slowed operations and system downtimes are much costlier than a delay in the analytics pipeline&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Querying Cassandra tables in our data warehouse should match the results of querying the production database (barring delays); having duplicate and/or incomplete rows amplifies post-processing workload for every end user
With these criteria in front of us, we began to brainstorm for solutions, and ultimately came up with three approaches:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stateless_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateless_stream_processing&quot; /&gt;Stateless Stream Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This solution is inspired by Datastax’s &lt;a href=&quot;https://www.datastax.com/dev/blog/advanced-replication-in-dse-5-1&quot;&gt;advanced replication blog post&lt;/a&gt;. The idea is to deploy an agent on each Cassandra node to process local commit logs. Each agent is considered as “primary” for a subset of writes based on partition keys, such that every event has exactly one primary agent. Then during CDC, in order to avoid duplicate events, each agent only sends an event to Kafka if it is the primary agent for the event. To handle eventual consistency, each agent would sort events into per-table time-sliced windows as they arrive (but doesn’t publish them right away); when a window expires, events in that window are hashed, and the hash is compared against other nodes. If they don’t match, data is fetched from the inconsistent node so the correct value could be resolved by last write wins. Finally the corrected events in that window will be sent to Kafka. Any out-of-order event beyond the time-sliced windows would have to be logged into an out-of-sequence file and handled separately. Since deduplication and ordering are done in-memory, concerns with agent failover causing data loss, OOM issues impacting production database, and the overall complexity of this implementation stopped us from exploring it further.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;stateful_stream_processing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#stateful_stream_processing&quot; /&gt;Stateful Stream Processing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This solution is the most feature rich. The idea is that the agent on each Cassandra node will process commit logs and publish events to Kafka without deduplication and ordering. Then a stream processing engine will consume these raw events and do the heavy lifting (such as filtering out duplicate events with a cache, managing event orders with event-time windowing, and capturing state of unmodified columns by performing read before write on a state store), and then publish these derived events to a separate Kafka topic. Finally, &lt;a href=&quot;https://github.com/wepay/kafka-connect-bigquery&quot;&gt;KCBQ&lt;/a&gt; will be used to consume events from this topic and upload them to BigQuery. This approach is appealing because it solves the problem generically – anyone can subscribe to the latter Kafka topic without needing to handle deduplication and ordering on their own. However, this approach introduces a nontrivial amount of operational overhead; we would have to maintain a stream processing engine, a database, and a cache.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;processing_on_read&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#processing_on_read&quot; /&gt;Processing-On-Read&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Similar to the previous approach, the idea is to process commit logs on each Cassandra node and send events to Kafka without deduplication and ordering. Unlike the previous approach, the stream processing portion is completely eliminated. Instead the raw events will be directly uploaded to BigQuery via KCBQ. &lt;a href=&quot;https://cloud.google.com/bigquery/docs/views-intro&quot;&gt;Views&lt;/a&gt; are created on top of the raw tables to handle deduplication, ordering, and merging of columns to form complete rows. Because BigQuery views are virtual tables, the processing is done lazily each time the view is queried. To prevent the view query from getting too expensive, the views would be materialized periodically. This approach removes both operational complexity and code complexity by leveraging BigQuery’s &lt;a href=&quot;https://cloud.google.com/blog/products/gcp/bigquery-under-the-hood&quot;&gt;massively parallel query engine&lt;/a&gt;. However, the drawback is that non-KCBQ downstream consumers must do all the work on their own.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Given that our main purpose of streaming Cassandra is data warehousing, we ultimately decided to implement &lt;em&gt;processing-on-read&lt;/em&gt;. It provides the essential features for our existing use case, and offers the flexibility to expand into the other two more generic solutions mentioned above in the future.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;open_source&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#open_source&quot; /&gt;Open Source&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;During this process of building a real-time data pipeline for Cassandra, we have received a substantial amount of interest on this project. As a result, we have decided to open-source the Cassandra CDC agent under the &lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; umbrella as an &lt;a href=&quot;https://github.com/debezium/debezium-incubator&quot;&gt;incubating connector&lt;/a&gt;. If you would like to learn more or contribute, check out the work-in-progress pull request for &lt;a href=&quot;https://github.com/debezium/debezium-incubator/pull/98&quot;&gt;source code&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium.github.io/pull/325&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the second half of this blog post series, we will elaborate on the CDC implementation itself in more details. Stay tuned!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/</id>
<title>Tutorial for Adding Sentry into Debezium Container Images</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-07-08T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/07/08/tutorial-sentry-debezium-container-images/" rel="alternate" type="text/html" />
<author>
<name>Renato Mefi</name>
</author>
<category term="sentry"></category>
<category term="docker"></category>
<summary>



Debezium has received a huge improvement to the structure of its container images recently,
making it extremely simple to extend its behaviour.


This is a small tutorial showing how you can for instance add Sentry,
"an open-source error tracking [software] that helps developers monitor and fix crashes in real time".
Here we&#8217;ll use it to collect and report any exceptions from Kafka Connect and its connectors.
Note that this is only applicable for Debezium 0.9+.


We need a few things to have Sentry working, and we&#8217;ll add all of them and later have a Dockerfile which gets it all glued correctly:




Configure Log4j


SSL certificate for sentry.io, since...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium has received a huge improvement to the structure of its container images &lt;a href=&quot;https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/&quot;&gt;recently&lt;/a&gt;,
making it extremely simple to extend its behaviour.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a small tutorial showing how you can for instance add &lt;a href=&quot;https://sentry.io/welcome/&quot;&gt;Sentry&lt;/a&gt;,
&quot;an open-source error tracking [software] that helps developers monitor and fix crashes in real time&quot;.
Here we’ll use it to collect and report any exceptions from Kafka Connect and its connectors.
Note that this is only applicable for Debezium 0.9+.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We need a few things to have Sentry working, and we’ll add all of them and later have a Dockerfile which gets it all glued correctly:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Configure Log4j&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SSL certificate for &lt;a href=&quot;https://sentry.io&quot;&gt;sentry.io&lt;/a&gt;, since it’s not by default in the JVM trusted chain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;sentry&lt;/code&gt; and &lt;code&gt;sentry-log4j&lt;/code&gt; libraries&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;log4j_configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#log4j_configuration&quot; /&gt;Log4j Configuration&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s create a file &lt;em&gt;config/log4j.properties&lt;/em&gt; in our local project which is a copy of the one shipped with Debezium images and add Sentry to it.
Note we added &lt;code&gt;Sentry&lt;/code&gt; to &lt;code&gt;log4j.rootLogger&lt;/code&gt; and created the section &lt;code&gt;log4j.appender.Sentry&lt;/code&gt;, the rest remains as the original configuration:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-config&quot; data-lang=&quot;config&quot;&gt;kafka.logs.dir=logs

log4j.rootLogger=INFO, stdout, appender, Sentry

# Disable excessive reflection warnings - KAFKA-5229
log4j.logger.org.reflections=ERROR

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.threshold=INFO
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n

log4j.appender.appender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.appender.DatePattern=&#39;.&#39;yyyy-MM-dd-HH
log4j.appender.appender.File=${kafka.logs.dir}/connect-service.log
log4j.appender.appender.layout=org.apache.log4j.PatternLayout
log4j.appender.appender.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n

log4j.appender.Sentry=io.sentry.log4j.SentryAppender
log4j.appender.Sentry.threshold=WARN&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sentry_io_ssl_certificate&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sentry_io_ssl_certificate&quot; /&gt;Sentry.io SSL certificate&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Download the &lt;em&gt;getsentry.pem&lt;/em&gt; file from &lt;a href=&quot;https://docs.sentry.io/ssl/&quot;&gt;sentry.io&lt;/a&gt; and put it in your project’s directory under &lt;em&gt;ssl/&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_dockerfile&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_dockerfile&quot; /&gt;The Dockerfile&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we can glue everything together in our Debezium image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Let’s first create a JKS file with our Sentry certificate; this uses a Docker multi-stage building process, where we are generating a &lt;code&gt;certificates.jks&lt;/code&gt; which we’ll later copy into our Kafka Connect with Debezium stage&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy &lt;code&gt;log4j.properties&lt;/code&gt; into &lt;code&gt;$KAFKA_HOME/config/log4j.properties&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copy the JKS file from the multi-stage build&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Set ENV with the Sentry version and m5sums&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Download Sentry dependencies, the script you see called &lt;code&gt;docker-maven-download&lt;/code&gt; is a helper which we ship by default in our images.
In this case we’re using it to download a JAR file from Maven Central and put it in the Kafka libs directory.
We do that by setting the ENV var &lt;code&gt;MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs&lt;/code&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-dockerfile&quot; data-lang=&quot;dockerfile&quot;&gt;FROM fabric8/java-centos-openjdk8-jdk:1.6 as ssl-jks

ARG JKS_STOREPASS=&quot;any random password, you can also set it outside via the arguments from docker build&quot;

USER root:root

COPY /ssl /ssl

RUN chown -R jboss:jboss /ssl

USER jboss:jboss

WORKDIR /ssl

RUN keytool -import -noprompt -alias getsentry \
    -storepass &quot;${JKS_STOREPASS}&quot; \
    -keystore certificates.jks \
    -trustcacerts -file &quot;/ssl/getsentry.pem&quot;

FROM debezium/connect:0.10 AS kafka-connect

EXPOSE 8083

COPY config/log4j.properties &quot;$KAFKA_HOME/config/log4j.properties&quot;

COPY --from=ssl-jks --chown=kafka:kafka /ssl/certificates.jks /ssl/

ENV SENTRY_VERSION=1.7.23 \
    MAVEN_DEP_DESTINATION=$KAFKA_HOME/libs

RUN docker-maven-download \
        central io/sentry sentry &quot;$SENTRY_VERSION&quot; 4bf1d6538c9c0ebc22526e2094b9bbde &amp;amp;&amp;amp; \
    docker-maven-download \
        central io/sentry sentry-log4j &quot;$SENTRY_VERSION&quot; 74af872827bd7e1470fd966449637a77&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;build_and_run&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_and_run&quot; /&gt;Build and Run&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now we can simply build the image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ docker build -t debezium/connect-sentry:1 --build-arg=JKS_STOREPASS=&quot;123456789&quot; .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When running the image we have now to configure our Kafka Connect application to load the JKS file by setting &lt;code&gt;KAFKA_OPTS: -Djavax.net.ssl.trustStore=/ssl/certificates.jks -Djavax.net.ssl.trustStorePassword=&amp;lt;YOUR TRUSTSTORE PASSWORD&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Sentry can be &lt;a href=&quot;https://docs.sentry.io/clients/java/config/#id2&quot;&gt;configured in many ways&lt;/a&gt;, I like to do it via environment variables, the minimum we can set is the Sentry DSN (which is necessary to point to your project) and the actual running environment name (i.e.: production, staging).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this case we can configure the variables: &lt;code&gt;SENTRY_DSN=&amp;lt;GET THE DNS IN SENTRY’S DASHBOARD&amp;gt;&lt;/code&gt;, &lt;code&gt;SENTRY_ENVIRONMENT=dev&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In case you’d like to learn more about using the Debezium container images, please &lt;a href=&quot;https://debezium.io/docs/tutorial/#starting_docker&quot;&gt;check our tutorial&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And that’s it, a basic  a recipe for extending our Docker setup using Sentry as an example;
other modifications should also be as simple as this one.
As an example how a &lt;code&gt;RecordTooLarge&lt;/code&gt; exception from the Kafka producer would look like in this setup, see the picture below:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/sentry/example-record-too-large-exception.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Sentry Exception example&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to the recent refactor of the Debezium container images, it got very easy to amend them with your custom extensions.
Downloading external dependencies and adding them to the images became a trivial task and we’d love to hear your feedback about it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you are curious about the refactoring itself, you can find the details in pull request &lt;a href=&quot;https://github.com/debezium/docker-images/pull/131&quot;&gt;debezium/docker-images#131&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/</id>
<title>Debezium 0.10.0.Beta2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-06-28T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/28/debezium-0-10-0-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.10.0.Beta2!


This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
23 issues were fixed for this release;
a couple of those relate to the DDL parser of the MySQL connector,
e.g. around RENAME INDEX (DBZ-1329),
SET NEW in triggers (DBZ-1331)
and function definitions with the COLLATE keyword (DBZ-1332).


For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
(DBZ-1347).
Also the "include.unknown.datatypes" option works as expected now during snapshotting
(DBZ-1335)
and the connector won&#8217;t stumple upon materialized views during snapshotting any longer
(DBZ-1345).


The SQL Server connector will use much less...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.10.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This further stabilizes the 0.10 release line, with lots of bug fixes to the different connectors.
&lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Beta2&quot;&gt;23 issues&lt;/a&gt; were fixed for this release;
a couple of those relate to the DDL parser of the MySQL connector,
e.g. around &lt;code&gt;RENAME INDEX&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1329&quot;&gt;DBZ-1329&lt;/a&gt;),
&lt;code&gt;SET NEW&lt;/code&gt; in triggers (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1331&quot;&gt;DBZ-1331&lt;/a&gt;)
and function definitions with the &lt;code&gt;COLLATE&lt;/code&gt; keyword (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1332&quot;&gt;DBZ-1332&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For the Postgres connector we fixed a potential inconsistency when flushing processed LSNs to the database
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1347&quot;&gt;DBZ-1347&lt;/a&gt;).
Also the &quot;include.unknown.datatypes&quot; option works as expected now during snapshotting
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1335&quot;&gt;DBZ-1335&lt;/a&gt;)
and the connector won’t stumple upon materialized views during snapshotting any longer
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1345&quot;&gt;DBZ-1345&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The SQL Server connector will use much less memory in many situations
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;)
and it’s configurable now whether it should emit tombstone events for deletions or not
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-835&quot;&gt;DBZ-835&lt;/a&gt;).
This also was added for the Oracle connector, bringing consistency for this option across all the connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that this release can be used with Apache Kafka 2.x, but not with 1.x.
This was an unintentional change and compatibility with 1.x will be restored for the Beta3 release
(the issue to track is &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1361&quot;&gt;DBZ-1361&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the 0.10.0.Beta2 &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta2&quot;&gt;release notes&lt;/a&gt; to learn more about all resolved issues and the upgrading procedure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to everybody from the Debezium community who contributed to this release:
&lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/willome&quot;&gt;Guillaume Rosauro&lt;/a&gt;,
&lt;a href=&quot;https://github.com/szczeles&quot;&gt;Mariusz Strzelecki&lt;/a&gt; and
&lt;a href=&quot;https://github.com/ssouris&quot;&gt;Stathis Souris&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/19/debezium-wears-fedora/</id>
<title>Debezium Wears Fedora</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-06-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/19/debezium-wears-fedora/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="postgres"></category>
<category term="fedora"></category>
<category term="vagrant"></category>
<summary>



The Debezium project strives to provide an easy deployment of connectors,
so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.


This is true for all connectors but for the Debezium PostgreSQL connector.
This connector is specific in the regard that it requires a logical decoding plug-in to be installed inside the PostgreSQL source database(s) themselves.
Currently, there are two supported logical plug-ins:




postgres-decoderbufs, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community


JSON-based, which is based on JSON and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium project strives to provide an easy deployment of connectors,
so users can try and run connectors of their choice mostly by getting the right connector archive and unpacking it into the plug-in path of Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is true for all connectors but for the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium PostgreSQL connector&lt;/a&gt;.
This connector is specific in the regard that it requires a &lt;a href=&quot;https://www.postgresql.org/docs/current/logicaldecoding-explanation.html&quot;&gt;logical decoding plug-in&lt;/a&gt; to be installed inside the PostgreSQL source database(s) themselves.
Currently, there are two supported logical plug-ins:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/debezium/&quot;&gt;postgres-decoderbufs&lt;/a&gt;, which uses Protocol Buffers as a very compact transport format and which is maintained by the Debezium community&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/eulerto/wal2json&quot;&gt;JSON-based&lt;/a&gt;, which is based on JSON and which is maintained by its own upstream community&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;These plug-ins can be consumed and deployed in two ways;
the easiest one is to use one of our pre-made &lt;a href=&quot;https://hub.docker.com/r/debezium/postgres&quot;&gt;Postgres container images&lt;/a&gt;,
which contain both plug-ins and are already configured as required.
If you are using containers in your datacenter, and/or if you start a fresh database from scratch,
then this can be a great option.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other approach is building from source.
Even if this is usually an easy task, it still brings a barrier to an easy start and requires a non-trivial knowledge of the Linux operating system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To bridge the gap between those two extremes we’ve created and published an &lt;a href=&quot;https://src.fedoraproject.org/rpms/postgres-decoderbufs&quot;&gt;RPM package&lt;/a&gt;,
available for Fedora 30 and later.
By installing this package you will have the necessary binaries deployed, and the only task remaining is to configure PostgreSQL to enable the plug-in.
The RPM is based on the latest stable Debezium release,
&lt;a href=&quot;https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5.Final&lt;/a&gt; at this point.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#example&quot; /&gt;Example&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s show how the package works.
We will use the &lt;a href=&quot;https://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt; tool as an easy way for firing up a pre-provisioned virtual machine with Fedora.
Of course, that’s not a requirement and the same steps apply for any other way of running Fedora.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Create and start virtual machine with Fedora 30:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant init fedora/30-cloud-base

A `Vagrantfile` has been placed in this directory. You are now
ready to `vagrant up` your first virtual environment! Please read
the comments in the Vagrantfile as well as documentation on
`vagrantup.com` for more information on using Vagrant.

$ vagrant up

Bringing machine &#39;default&#39; up with &#39;virtualbox&#39; provider...
.
.
.
==&amp;gt; default: Machine booted and ready!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Log into the virtual machine:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ vagrant ssh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Install the PostgreSQL server and Protocol Buffers logical decoding plug-in:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo dnf -y install postgresql postgres-decoderbufs
.
.
.
Installed:
  postgres-decoderbufs-0.9.5-1.fc30.x86_64              postgresql-11.3-1.fc30.x86_64
  postgis-2.5.1-1.fc30.x86_64                           armadillo-9.400.4-1.fc30.x86_64
  blas-3.8.0-12.fc30.x86_64                             cairo-1.16.0-5.fc30.x86_64
  cups-libs-1:2.2.11-2.fc30.x86_64                      fontconfig-2.13.1-8.fc30.x86_64
  lapack-3.8.0-12.fc30.x86_64                           libgfortran-9.1.1-1.fc30.x86_64
  libpq-11.3-2.fc30.x86_64                              libquadmath-9.1.1-1.fc30.x86_64
  mariadb-connector-c-3.0.10-1.fc30.x86_64              mariadb-connector-c-config-3.0.10-1.fc30.noarch
  nss-3.44.0-2.fc30.x86_64                              nss-softokn-3.44.0-2.fc30.x86_64
  nss-softokn-freebl-3.44.0-2.fc30.x86_64               nss-sysinit-3.44.0-2.fc30.x86_64
  nss-util-3.44.0-2.fc30.x86_64                         poppler-0.73.0-9.fc30.x86_64
  postgresql-server-11.3-1.fc30.x86_64                  proj-5.2.0-2.fc30.x86_64
  proj-datumgrid-1.8-2.fc30.noarch                      uriparser-0.9.3-1.fc30.x86_64
  SuperLU-5.2.1-6.fc30.x86_64                           arpack-3.5.0-6.fc28.x86_64
  atk-2.32.0-1.fc30.x86_64                              avahi-libs-0.7-18.fc30.x86_64
  cfitsio-3.450-3.fc30.x86_64                           dejavu-fonts-common-2.37-1.fc30.noarch
  dejavu-sans-fonts-2.37-1.fc30.noarch                  fontpackages-filesystem-1.44-24.fc30.noarch
  freexl-1.0.5-3.fc30.x86_64                            fribidi-1.0.5-2.fc30.x86_64
  gdal-libs-2.3.2-7.fc30.x86_64                         gdk-pixbuf2-2.38.1-1.fc30.x86_64
  gdk-pixbuf2-modules-2.38.1-1.fc30.x86_64              geos-3.7.1-1.fc30.x86_64
  giflib-5.1.9-1.fc30.x86_64                            graphite2-1.3.10-7.fc30.x86_64
  gtk-update-icon-cache-3.24.8-1.fc30.x86_64            gtk2-2.24.32-4.fc30.x86_64
  harfbuzz-2.3.1-1.fc30.x86_64                          hdf5-1.8.20-6.fc30.x86_64
  hicolor-icon-theme-0.17-5.fc30.noarch                 jasper-libs-2.0.14-8.fc30.x86_64
  jbigkit-libs-2.1-16.fc30.x86_64                       lcms2-2.9-5.fc30.x86_64
  libXcomposite-0.4.4-16.fc30.x86_64                    libXcursor-1.1.15-5.fc30.x86_64
  libXdamage-1.1.4-16.fc30.x86_64                       libXfixes-5.0.3-9.fc30.x86_64
  libXft-2.3.2-12.fc30.x86_64                           libXi-1.7.9-9.fc30.x86_64
  libXinerama-1.1.4-3.fc30.x86_64                       libaec-1.0.4-1.fc30.x86_64
  libdap-3.20.3-1.fc30.x86_64                           libgeotiff-1.4.3-3.fc30.x86_64
  libgta-1.0.9-2.fc30.x86_64                            libjpeg-turbo-2.0.2-1.fc30.x86_64
  libkml-1.3.0-19.fc30.x86_64                           libspatialite-4.3.0a-11.fc30.x86_64
  libtiff-4.0.10-4.fc30.x86_64                          libwebp-1.0.2-2.fc30.x86_64
  netcdf-4.4.1.1-12.fc30.x86_64                         nspr-4.21.0-1.fc30.x86_64
  ogdi-3.2.1-4.fc30.x86_64                              openblas-0.3.5-5.fc30.x86_64
  openblas-openmp-0.3.5-5.fc30.x86_64                   openblas-serial-0.3.5-5.fc30.x86_64
  openblas-threads-0.3.5-5.fc30.x86_64                  openblas-threads64_-0.3.5-5.fc30.x86_64
  openjpeg2-2.3.1-1.fc30.x86_64                         pango-1.43.0-3.fc30.x86_64
  pixman-0.38.0-1.fc30.x86_64                           poppler-data-0.4.9-3.fc30.noarch
  protobuf-c-1.3.1-2.fc30.x86_64                        unixODBC-2.3.7-4.fc30.x86_64
  xerces-c-3.2.2-2.fc30.x86_64

Complete!&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, initialize the database:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo /usr/bin/postgresql-setup --initdb&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now enable the plug-in in the database server configuration file &lt;code&gt;/var/lib/pgsql/data/postgresql.conf&lt;/code&gt; by adding the following parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# MODULES
shared_preload_libraries = &#39;decoderbufs&#39;

# REPLICATION
wal_level = logical             # minimal, archive, hot_standby, or logical (change requires restart)
max_wal_senders = 8             # max number of walsender processes (change requires restart)
wal_keep_segments = 4           # in logfile segments, 16MB each; 0 disables
#wal_sender_timeout = 60s       # in milliseconds; 0 disables
max_replication_slots = 4       # max number of replication slots (change requires restart)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Configure the security file &lt;code&gt;/var/lib/pgsql/data/pg_hba.conf&lt;/code&gt; for the database user that will be used by Debezium (e.g. &lt;code&gt;debezium&lt;/code&gt;) by adding these parameters:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;local   replication     debezium                          trust
host    replication     debezium  127.0.0.1/32            trust
host    replication     debezium  ::1/128                 trust&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, restart PostgreSQL:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo systemctl restart postgresql&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And that’s it:
Now we have a PostgreSQL database, that is ready to stream changes to the Debezium PostgreSQL connector.
Of course, the plug-in can also be installed to an already existing database (Postgres versions 9 and later),
just by installing the RPM package and setting up the config and security files in the described way.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;outlook_pgoutput&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook_pgoutput&quot; /&gt;Outlook: pgoutput&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the decoderbufs plug-in is our recommended choice for a logical decoding plug-in,
there are cases where you may not be able to use it.
Most specifically, you typically don’t have the flexibility to install custom plug-ins in cloud-based environments such as Amazon RDS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is why we’re exploring a &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/DBZ-766&quot;&gt;third alternative&lt;/a&gt; to decoderbufs and wal2sjon right now,
which is to leverage Postgres logical replication mechanism.
There’s a built-in plug-in, &lt;em&gt;pgoutput&lt;/em&gt; based on this, which exists in every Postgres database since version 10.
We’re still in the process of exploring the implications (and possible limitations) of using &lt;em&gt;pgoutput&lt;/em&gt;,
but so far things look promising and it may eventually be a valuable tool to have in the box.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Stay tuned for more details coming soon!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/</id>
<title>Debezium 0.10.0.Beta1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-06-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/12/debezium-0-10-0-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Another week, another Debezium release&#8201;&#8212;&#8201;I&#8217;m happy to announce the release of Debezium 0.10.0.Beta1!


Besides the upgrade to Apache Kafka 2.2.1 (DBZ-1316),
this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
(DBZ-1317).


A very welcomed usability improvement is that the connectors will log a warning now
if not at least one table is actually captured as per the whitelist/blacklist configuration
(DBZ-1242).
This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
in which case the connectors "work as intended", but no events are propagated to the message broker.


Please see the release notes for the complete...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another week, another Debezium release — I’m happy to announce the release of Debezium &lt;strong&gt;0.10.0.Beta1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides the upgrade to Apache Kafka 2.2.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1316&quot;&gt;DBZ-1316&lt;/a&gt;),
this mostly fixes some bugs, including a regression to the MongoDB connector introduced in the Alpha2 release
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1317&quot;&gt;DBZ-1317&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A very welcomed usability improvement is that the connectors will log a warning now
if not at least one table is actually captured as per the whitelist/blacklist configuration
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1242&quot;&gt;DBZ-1242&lt;/a&gt;).
This helps to prevent the accidental exclusion all tables by means of an incorrect filter expression,
in which case the connectors &quot;work as intended&quot;, but no events are propagated to the message broker.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-beta1&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in this release.
Also make sure to examine the upgrade guidelines for 0.10.0.Alpha1 and Alpha2 when upgrading from earlier versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to community members &lt;a href=&quot;https://github.com/pan3793&quot;&gt;Cheng Pan&lt;/a&gt; and &lt;a href=&quot;https://github.com/ChingTsai&quot;&gt;Ching Tsai&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/</id>
<title>Debezium&#8217;s Newsletter 01/2019</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-06-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/05/debezium-newsletter-01-2019/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<category term="newsletter"></category>
<summary>



Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.




Upcoming Events




Paris Data Engineers Meet-up


Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium






Articles


Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
for Microservices With Debezium.  You can watch the full presentation here.


Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  This article
by Sincy Sebastian shows just how simple it is to replicate change...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Welcome to the first edition of the Debezium community newsletter in which we share blog posts, group discussions, as well as StackOverflow
questions that are relevant to our user community.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;upcoming_events&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#upcoming_events&quot; /&gt;Upcoming Events&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://www.meetup.com/fr-FR/Paris-Data-Engineers/events/260694777/&quot;&gt;Paris Data Engineers Meet-up&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://2019.javazone.no&quot;&gt;Oslo JavaZone 2019 - Change Data Streaming For Microservices With Apache Kafka and Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;articles&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#articles&quot; /&gt;Articles&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Gunnar Morling recently attended Kafka Summit in London where he gave a talk on Change Data Streaming Patterns
for Microservices With Debezium.  You can watch the full presentation &lt;a href=&quot;https://www.confluent.io/kafka-summit-lon19/change-data-streaming-patterns-microservices-debezium&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Strimzi provides an easy way to run Apache Kafka on Kubernetes or Openshift.  &lt;a href=&quot;https://medium.com/@sincysebastian/setup-kafka-with-debezium-using-strimzi-in-kubernetes-efd494642585&quot;&gt;This article&lt;/a&gt;
by Sincy Sebastian shows just how simple it is to replicate change events from MySQL to Elastic Search using Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium allows replicating data between heterogeneous data stores with ease.  &lt;a href=&quot;https://blog.couchbase.com/kafka-connect-mysql-couchbase-debezium/&quot;&gt;This article&lt;/a&gt; by Matthew Groves
explains how you can replicate data from MySQL to CouchBase.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As the size of data that systems maintain continues to grow, this begins to impact how we capture, compute, and report
real-time analytics. &lt;a href=&quot;https://medium.com/high-alpha/data-stream-processing-for-newbies-with-kafka-ksql-and-postgres-c30309cfaaf8&quot;&gt;This article&lt;/a&gt; by Maria Patterson
explains how you can use Debezium to stream data from Postgres, perform analytical calculations using KSQL, and then
stream those results back to Postgres for consumption.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In a &lt;a href=&quot;https://medium.com/@singaretti/streaming-de-dados-do-postgresql-utilizando-kafka-debezium-v2-d49f46d70b37&quot;&gt;recent article&lt;/a&gt; published in Portuguese,
Paulo Singaretti illustrates how they use Debezium and Kafka to stream changes from their relational database and then store
the change stream results in Google Cloud Services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;a href=&quot;https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/&quot;&gt;recent blog&lt;/a&gt; by Jia Zhai provides
a complete tutorial showing how to use Debezium connectors with Apache Pulsar.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;time_to_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#time_to_upgrade&quot; /&gt;Time to upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium version &lt;a href=&quot;https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/&quot;&gt;0.9.5&lt;/a&gt; was just released.
If you are using the 0.9 branch you should definitely check out 0.9.5.  For details on the bug fixes as well as
the enhancements this version includes, check out the
&lt;a href=&quot;https://issues.redhat.com/secure/ReleaseNote.jspa?projectId=12317320&amp;amp;version=12341657&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team has also begun active development on the next major version, 0.10.  We recently published
a &lt;a href=&quot;https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/&quot;&gt;blog&lt;/a&gt; that provides an overview
behind what 0.10 is meant to deliver.  If you want details on the bug fixes and enhancements we’ve packed
into this release, you can view the &lt;a href=&quot;https://issues.redhat.com/issues/?jql=fixVersion%20IN%20(0.10.0.Alpha1%2C%200.10.0.Alpha2)%20ORDER%20BY%20KEY&quot;&gt;issue list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;questions_and_answers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#questions_and_answers&quot; /&gt;Questions and answers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55569090/how-to-let-debezium-start-reading-binlog-from-the-last-row&quot;&gt;How to have Debezium start reading binlog from last row&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55633590/is-it-possible-to-apply-smt-single-message-transforms-to-messages-from-specifi&quot;&gt;Is it possible to apply Single Message Transforms to messages from specified topics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/55648457/kafkaconnect-produces-cdc-event-with-null-value-when-reading-from-mongodb-with-d&quot;&gt;Kafka Connect produces CDC events with null values when reading from MongoDB with Debezium&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/1bae4e45-c6c4-4190-9955-44f901b8ca04%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Renaming Topics&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/cfc333f1-b5f6-462b-a1c8-0f65bc91b725%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Stream changes with differing column names between source and destination&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://groups.google.com/d/msgid/debezium/18c1239f-af69-4161-8adc-329a91aa4c7e%40googlegroups.com?utm_medium=email&amp;amp;utm_source=footer&quot;&gt;Can’t connect to debezium kafka from Docker host&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;feedback&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#feedback&quot; /&gt;Feedback&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We intend to publish new additions of this newsletter periodically.  Should anyone have any suggestions on changes or what could be highlighted here, we welcome that feedback.  You can reach out to us via any of our community channels found &lt;a href=&quot;https://debezium.io/community&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/</id>
<title>Debezium 0.10.0.Alpha2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-06-03T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/06/03/debezium-0-10-0-alpha2-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Release early, release often&#8201;&#8212;&#8201;Less than a week since the Alpha1 we are announcing the release of Debezium 0.10.0.Alpha2!


This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.


The change in the logic of the snapshot field has been delivered (DBZ-1295) as outlined in the last announcement.
All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.


Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (CVE-2019-0201).


Our...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Release early, release often — Less than a week since the Alpha1 we are announcing the release of Debezium &lt;strong&gt;0.10.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is an incremental release that completes some of the tasks started in the Alpha1 release and provides a few bugfixes and also quality improvements in our Docker images.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The change in the logic of the &lt;code&gt;snapshot&lt;/code&gt; field has been delivered (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1295&quot;&gt;DBZ-1295&lt;/a&gt;) as outlined in &lt;a href=&quot;2019/05/29/debezium-0-10-0-alpha1-released/#outlook&quot;&gt;the last announcement&lt;/a&gt;.
All connectors now provide information which of the records is the last one in the snapshot phase so that downstream consumers can react to this.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Apache ZooKeeper was upgraded to version 3.4.14 to fix a security vulnerability (&lt;a href=&quot;https://nvd.nist.gov/vuln/detail/CVE-2019-0201&quot;&gt;CVE-2019-0201&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Our regular contributor &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato&lt;/a&gt; dived deeply into our image build scripts and enriched (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1279&quot;&gt;DBZ-1279&lt;/a&gt;) them with a Dockerfile linter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Schema change events include the table name(s) in the metadata describing which tables are affected by the change (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-871&quot;&gt;DBZ-871&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/barrti&quot;&gt;Bartosz Miedlar&lt;/a&gt; has fixed a bug in MySQL ANTLR grammar causing issues with identifiers in backquotes (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1300&quot;&gt;DBZ-1300&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope we will be able to keep the recent release cadence and get lout the first beta version of 0.10 in two weeks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Stay tuned for more!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/</id>
<title>Debezium 0.10.0.Alpha1 &quot;Spring Clean-Up&quot; Edition Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-05-29T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/29/debezium-0-10-0-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 0.10.0.Alpha1!


The major theme for Debezium 0.10 will be to do some clean-up
(that&#8217;s what you do at this time of the year, right?);
we&#8217;ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.


This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
depending on your usage of features and options already marked as deprecated in 0.9 and before.
But no worries, we&#8217;re describing all changes in great detail...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.10.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The major theme for Debezium 0.10 will be to do some clean-up
(that’s what you do at this time of the year, right?);
we’ve planned to remove a few deprecated features and to streamline some details in the structure the CDC events produced by the different Debezium connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This means that upgrading to Debezium 0.10 from earlier versions might take a bit more planning and consideration compared to earlier upgrades,
depending on your usage of features and options already marked as deprecated in 0.9 and before.
But no worries, we’re describing all changes in great detail in this blog post and the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-10-0-alpha1&quot;&gt;release notes&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;why&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#why&quot; /&gt;Why?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;First of all, let’s discuss a bit why we’re doing these changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Over the last three years, Debezium has grown from supporting just a single database into an entire family of &lt;a href=&quot;https://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for a range of different relational databases and MongoDB,
as well as accompanying components such as message transformations for &lt;a href=&quot;https://debezium.io/docs/configuration/topic-routing/&quot;&gt;topic routing&lt;/a&gt; or &lt;a href=&quot;https://debezium.io/docs/configuration/outbox-event-router/&quot;&gt;implementing the outbox pattern&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As in any mature project, over time we figured that a few things should be done differently in the code base than we had thought at first.
For instance we moved from a hand-written parser for processing MySQL DDL statements to a much more robust implementation based on Antlr.
Also we realized the way certain temporal column types were exported was at risk of value overflow in certain conditions,
so we added a new mode not prone to these issues.
As a last example, we made options like the batch size used during snapshotting consistent across the different connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Luckily, Debezium quickly gained traction and despite the 0.x version number, it is used heavily in production at a large number of organizations, and users rely on its stability.
So whenever we did such changes, we aimed at making the upgrade experience as smooth as possible;
usually that means that the previous behavior is still available but is marked as deprecated in the documentation,
while a new improved option, implementation etc. is added and made the default behavior.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time we realized that there are a couple of differences between the connectors which shouldn’t really be there.
Specifically, the &lt;code&gt;source&lt;/code&gt; block of change events has some differences which make a uniform handling by consumers more complex than it should be;
for instance the timestamp field is named &quot;ts_sec&quot; in MySQL events but &quot;ts_usec&quot; for Postgres.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With all this in mind, we decided that it is about time to clean up these issues.
This done for a couple of purposes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Keeping the code base maintainable and open for future development by removing legacy code such as deprecated options and their handling as well as the legacy MySQL DDL parser&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Making CDC events from different connectors easier to consume by unifying the &lt;code&gt;source&lt;/code&gt; block created by the different connectors as far as possible&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Preparing the project to go to version 1.0 with an even stronger promise of retaining backwards compatibility than already practiced today&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;what&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what&quot; /&gt;What?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now as we have discussed why we feel it’s time for some &quot;clean-up&quot;, let’s take a closer look at the most relevant changes.
Please also refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#breaking_changes&quot;&gt;&quot;breaking changes&quot;&lt;/a&gt; section of the migration notes for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The legacy DDL parser for MySQL has been removed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-736&quot;&gt;DBZ-736&lt;/a&gt;);
if you are not using the Antlr-based one yet (it was introduced in 0.8 and became the default in 0.9),
it’s highly recommended that you test it with your databases.
Should you run into any parsing errors, please report them so we can fix them for the 0.10 Final release.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The SMTs for retrieving the new record/document state from change events have been renamed from
&lt;code&gt;io.debezium.transforms.UnwrapFromEnvelope&lt;/code&gt; and &lt;code&gt;io.debezium.connector.mongodb.transforms.UnwrapFromMongoDbEnvelope&lt;/code&gt;
into &lt;code&gt;ExtractNewRecordState&lt;/code&gt; and &lt;code&gt;ExtractNewDocumentState&lt;/code&gt;, respectively
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-677&quot;&gt;DBZ-677&lt;/a&gt;).
The old names can still be used as of 0.10, but doing so will raise a warning.
They are planned for removal in Debezium 0.11.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several connector options that were deprecated in earlier Debezium versions have been removed
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1234&quot;&gt;DBZ-1234&lt;/a&gt;):
the &lt;code&gt;drop.deletes&lt;/code&gt; option of new record/document state extraction SMTs (superseded by &lt;code&gt;delete.handling.mode&lt;/code&gt; option),
the &lt;code&gt;rows.fetch.size&lt;/code&gt; option (superseded by &lt;code&gt;snapshot.fetch.size&lt;/code&gt;),
the &lt;code&gt;adaptive&lt;/code&gt; value of &lt;code&gt;time.precision.mode&lt;/code&gt; option for MySQL (prone to value loss, use &lt;code&gt;adaptive_microseconds&lt;/code&gt; instead) and
the &lt;code&gt;snapshot.minimal.locks&lt;/code&gt; for the MySQL connector (superseded by &lt;code&gt;snapshot.locking.mode&lt;/code&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several option names of the (incubating) SMT for the outbox pattern
have been renamed for the sake of consistency (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1289&quot;&gt;DBZ-1289&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Several fields within the &lt;code&gt;source&lt;/code&gt; block of CDC events have been renamed for the sake of consistency
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-596&quot;&gt;DBZ-596&lt;/a&gt;);
as this is technically a backwards-incompatible change when using Avro and the schema registry,
we’ve added a connector option &lt;code&gt;source.struct.version&lt;/code&gt; which, when set to the value &lt;code&gt;v1&lt;/code&gt;, will have connectors produce the previous &lt;code&gt;source&lt;/code&gt; structure.
&lt;code&gt;v2&lt;/code&gt; is the default and any consumers should be adjusted to work with the new &lt;code&gt;source&lt;/code&gt; structure as soon as possible.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;new_features_and_bugfixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features_and_bugfixes&quot; /&gt;New Features and Bugfixes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides these changes, the 0.10.0.Alpha1 release also contains some feature additions and bug fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The SQL Server connector supports custom SELECT statements for snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1224&quot;&gt;DBZ-1224&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;database, schema and table/collection names have been added consistently to the &lt;code&gt;source&lt;/code&gt; block for CDC events from all connectors
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-875&quot;&gt;DBZ-875&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Client authentication works for the MySQL connector(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1228&quot;&gt;DBZ-1228&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The embedded engine doesn’t duplicate events after restarts any longer (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1276&quot;&gt;DBZ-1276&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A parser bug related to &lt;code&gt;CREATE INDEX&lt;/code&gt; statements was fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1264&quot;&gt;DBZ-1264&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.10.0.Alpha1&quot;&gt;30 issues&lt;/a&gt; were addressed in this release.
Many thanks to &lt;a href=&quot;https://github.com/Arkoprabho&quot;&gt;Arkoprabho Chakraborti&lt;/a&gt;, &lt;a href=&quot;https://github.com/rsatishm&quot;&gt;Ram Satish&lt;/a&gt; and &lt;a href=&quot;https://github.com/Wang-Yu-Chao&quot;&gt;Yuchao Wang&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Speaking of contributors, we did some housekeeping to &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/COPYRIGHT.txt&quot;&gt;the list&lt;/a&gt; of everyone ever contributing to Debezium, too.
Not less than exactly &lt;strong&gt;111 individuals have contributed&lt;/strong&gt; code up to this point,
which is just phenomenal! Thank you so much everyone, you folks rock!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot; /&gt;Outlook&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Going forward, there are some more details we’d like to unify across the different connectors before going to Debezium 0.10 Final.
For instance the &lt;code&gt;source&lt;/code&gt; attribute &lt;code&gt;snapshot&lt;/code&gt; will be changed so it can take one of three states: &lt;code&gt;true&lt;/code&gt;, &lt;code&gt;false&lt;/code&gt; or &lt;code&gt;last&lt;/code&gt;
(indicating that this event is the last one created during initial snapshotting).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ll also continue our efforts to to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
Another thing we’re actively exploring is how the Postgres could take advantage of the &quot;logical replication&quot; feature added in Postgres 10.
This may provide us with a way to ingest change events without requiring a custom server-side logical decoding plug-in,
which proves challenging in cloud environments where there’s typically just a limited set of logical decoding options available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/</id>
<title>Tutorial for Using Debezium Connectors With Apache Pulsar</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-05-23T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/23/tutorial-using-debezium-connectors-with-apache-pulsar/" rel="alternate" type="text/html" />
<author>
<name>Jia Zhai, StreamNative</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<summary>



This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.


Debezium is an open source project for change data capture (CDC). It is built on Apache Kafka Connect and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
Apache Pulsar includes a set of built-in connectors based on Pulsar IO framework, which is counter part to Apache Kafka Connect.


As of version 2.3.0, Pulsar IO comes with support for the Debezium source connectors out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
This tutorial walks you through setting...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;This is a guest post by Apache Pulsar PMC Member and Committer Jia Zhai.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io&quot;&gt;Debezium&lt;/a&gt; is an open source project for change data capture (CDC). It is built on &lt;a href=&quot;https://kafka.apache.org/documentation/#connectapi&quot;&gt;Apache Kafka Connect&lt;/a&gt; and supports multiple databases, such as MySQL, MongoDB, PostgreSQL, Oracle, and SQL Server.
&lt;a href=&quot;http://pulsar.apache.org&quot;&gt;Apache Pulsar&lt;/a&gt; includes a set of &lt;a href=&quot;https://pulsar.apache.org/docs/en/io-connectors&quot;&gt;built-in connectors&lt;/a&gt; based on Pulsar IO framework, which is counter part to Apache Kafka Connect.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As of version 2.3.0, Pulsar IO comes with support for the &lt;a href=&quot;http://pulsar.apache.org/docs/en/2.3.0/io-cdc-debezium&quot;&gt;Debezium source connectors&lt;/a&gt; out of the box, so you can leverage Debezium to stream changes from your databases into Apache Pulsar.
This tutorial walks you through setting up the Debezium connector for MySQL with Pulsar IO.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;tutorial_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#tutorial_steps&quot; /&gt;Tutorial Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This tutorial is similar to the &lt;a href=&quot;https://debezium.io/docs/tutorial&quot;&gt;Debezium tutorial&lt;/a&gt;, except that storage of event streams is changed from Kafka to Pulsar.
It mainly includes six steps:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;olist arabic&quot;&gt;
&lt;ol class=&quot;arabic&quot;&gt;
&lt;li&gt;
&lt;p&gt;Start a MySQL server;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start standalone Pulsar service;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Start the Debezium connector in Pulsar IO. Pulsar IO reads database changes existing in MySQL server;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Subscribe Pulsar topics to monitor MySQL changes;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clean up.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_1_start_a_mysql_server&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_1_start_a_mysql_server&quot; /&gt;Step 1: Start a MySQL server&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start a MySQL server that contains a database example, from which Debezium captures changes. Open a new terminal to start a new container that runs a MySQL database server pre-configured with a database named inventory:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker run --rm --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=debezium -e MYSQL_USER=mysqluser -e MYSQL_PASSWORD=mysqlpw debezium/example-mysql:0.9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following information is displayed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;2019-03-25T14:12:41.178325Z 0 [Note] Event Scheduler: Loaded 0 events
2019-03-25T14:12:41.178670Z 0 [Note] mysqld: ready for connections.
Version: &#39;5.7.25-log&#39;  socket: &#39;/var/run/mysqld/mysqld.sock&#39;  port: 3306  MySQL Community Server (GPL)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_2_start_standalone_pulsar_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_2_start_standalone_pulsar_service&quot; /&gt;Step 2: Start standalone Pulsar service&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start Pulsar service locally in standalone mode.
Support for running Debezium connectors in Pulsar IO is introduced in Pulsar 2.3.0.
Download &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz&quot;&gt;Pulsar binary of 2.3.0 release&lt;/a&gt; and &lt;a href=&quot;https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;&gt;pulsar-io-kafka-connect-adaptor-2.3.0.nar of 2.3.0 release&lt;/a&gt;.
In Pulsar, all Pulsar IO connectors are packaged as separate &lt;a href=&quot;https://medium.com/hashmapinc/nifi-nar-files-explained-14113f7796fd&quot;&gt;NAR&lt;/a&gt; files.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/apache-pulsar-2.3.0-bin.tar.gz
$ wget https://archive.apache.org/dist/pulsar/pulsar-2.3.0/connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar
$ tar zxf apache-pulsar-2.3.0-bin.tar.gz
$ cd apache-pulsar-2.3.0
$ mkdir connectors
$ cp ../pulsar-io-kafka-connect-adaptor-2.3.0.nar connectors
$ bin/pulsar standalone&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-1.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;start pulsar standalone]&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_3_start_the_debezium_mysql_connector_in_pulsar_io&quot; /&gt;Step 3: Start the Debezium MySQL connector in Pulsar IO&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start the Debezium MySQL connector in Pulsar IO, with local run mode, in another terminal tab.
The “debezium-mysql-source-config.yaml” file contains all the configuration, and main parameters are listed under the “configs” node. The .yaml file contains the &quot;task.class&quot; parameter. The configuration file also
includes MySQL related parameters (like server, port, user, password) and two names of Pulsar topics for &quot;history&quot; and &quot;offset&quot; storage.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin source localrun  --sourceConfigFile debezium-mysql-source-config.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The content in the “debezium-mysql-source-config.yaml” file is as follows.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;tenant: &quot;test&quot;
namespace: &quot;test-namespace&quot;
name: &quot;debezium-kafka-source&quot;
topicName: &quot;kafka-connect-topic&quot;
archive: &quot;connectors/pulsar-io-kafka-connect-adaptor-2.3.0.nar&quot;

parallelism: 1

configs:
  ## sourceTask
  task.class: &quot;io.debezium.connector.mysql.MySqlConnectorTask&quot;

  ## config for mysql, docker image: debezium/example-mysql:0.8
  database.hostname: &quot;localhost&quot;
  database.port: &quot;3306&quot;
  database.user: &quot;debezium&quot;
  database.password: &quot;dbz&quot;
  database.server.id: &quot;184054&quot;
  database.server.name: &quot;dbserver1&quot;
  database.whitelist: &quot;inventory&quot;

  database.history: &quot;org.apache.pulsar.io.debezium.PulsarDatabaseHistory&quot;
  database.history.pulsar.topic: &quot;history-topic&quot;
  database.history.pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
  ## KEY_CONVERTER_CLASS_CONFIG, VALUE_CONVERTER_CLASS_CONFIG
  key.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
  value.converter: &quot;org.apache.kafka.connect.json.JsonConverter&quot;
  ## PULSAR_SERVICE_URL_CONFIG
  pulsar.service.url: &quot;pulsar://127.0.0.1:6650&quot;
  ## OFFSET_STORAGE_TOPIC_CONFIG
  offset.storage.topic: &quot;offset-topic&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Tables are created automatically in the aforementioned MySQL server. So Debezium connector reads history records from MySQL binlog file from the beginning. In the output you will find the connector has already been triggered and processed in 47 records.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-2.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;connector start process records&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For more information on how to manage connectors, see the &lt;a href=&quot;http://pulsar.apache.org/docs/en/io-managing/&quot;&gt;Pulsar IO documentation&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Records that have been captured and read by Debezium are automatically published to Pulsar topics. When you start a new terminal, you will find the current topics in Pulsar with the following command:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-admin topics list public/default&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-3.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;list Pulsar topics&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For each table, which has been changed, the change data is stored in a separate Pulsar topic.  Except database table related topics, another two topics named “history-topic” and “offset-topic” are used to store history and offset related data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;persistent://public/default/history-topic
persistent://public/default/offset-topic&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_4_subscribe_pulsar_topics_to_monitor_mysql_changes&quot; /&gt;Step 4: Subscribe Pulsar topics to monitor MySQL changes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Take the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; topic as an example.
Use the CLI command to consume this topic and monitor changes while the “products” table changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt; $ bin/pulsar-client consume -s &quot;sub-products&quot; public/default/dbserver1.inventory.products -n 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The output is as follows:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;…
22:17:41.201 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribing to topic on cnx [id: 0xfe0b4feb, L:/127.0.0.1:55585 - R:localhost/127.0.0.1:6650]
22:17:41.223 [pulsar-client-io-1-1] INFO  org.apache.pulsar.client.impl.ConsumerImpl - [public/default/dbserver1.inventory.products][sub-products] Subscribed to topic on localhost/127.0.0.1:6650 -- consumer: 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also consume the offset topic to monitor the offset changes while the table changes are stored in the &lt;code&gt;persistent://public/default/dbserver1.inventory.products&lt;/code&gt; Pulsar topic.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ bin/pulsar-client consume -s &quot;sub-offset&quot; offset-topic -n 0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_5_make_changes_in_mysql_server_and_verify_that_changes_are_recorded_in_pulsar_topics_immediately&quot; /&gt;Step 5: Make changes in MySQL server, and verify that changes are recorded in Pulsar topics immediately&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Start a MySQL CLI docker connector,  and you can make changes to the “products” table in MySQL server.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 sh -c &#39;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After running the command, MySQL CLI is displayed, and you can change the names of the two items in the “products” table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; use inventory;
mysql&amp;gt; show tables;
mysql&amp;gt; SELECT * FROM  products ;
mysql&amp;gt; UPDATE products SET name=&#39;1111111111&#39; WHERE id=101;
mysql&amp;gt; UPDATE products SET name=&#39;1111111111&#39; WHERE id=107;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-4.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;mysql updates&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you consume products topic, you find that two changes have been added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-5.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;table topic stores mysql updates&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you consume the offset topic, you find that two offsets have been added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-6.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;offset topic get updated&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the terminal where you local-run the connector, you find two more records have been processed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/pulsar_tutorial/pulsar-mysql-7.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;table topic get more records&quot;&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;step_6_clean_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#step_6_clean_up&quot; /&gt;Step 6: Clean up.&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Use “Ctrl + C” to close terminals. Use “docker ps” and “docker kill” to stop MySQL related containers.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;mysql&amp;gt; quit

$ docker ps
CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                               NAMES
84d66c2f591d        debezium/example-mysql:0.8   &quot;docker-entrypoint.s…&quot;   About an hour ago   Up About an hour    0.0.0.0:3306-&amp;gt;3306/tcp, 33060/tcp   mysql

$ docker kill 84d66c2f591d&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To delete Pulsar data, delete data directory in the Pulsar binary directory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;$ pwd
/Users/jia/ws/releases/apache-pulsar-2.3.0

$ rm -rf data&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#conclusion&quot; /&gt;Conclusion&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Pulsar IO framework allows to run the Debezium connectors for change data capture, streaming data changes from different databases into Apache Pulsar. In this tutorial you’ve learned how to capture data changes in a MySQL database and propagate them to Pulsar. We are improving support for running the Debezium connectors with Apache Pulsar continuously, it will be much easier to use after Pulsar 2.4.0 release.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/</id>
<title>Debezium 0.9.5.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-05-06T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/05/06/debezium-0-9-5-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.5.Final!


This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
The release contains 18 resolved issues overall.




Apache Kafka Update and New Features

This release has been built against and tested with Apache Kafka 2.2.0 (DBZ-1227).
Earlier versions are continued to be supported as well.


For all the connectors it is possible now to specify the batch size when taking snapshots (DBZ-1247).
The new connector option snapshot.fetch.size has been introduced for that.
This option replaces the earlier option rows.fetch.size which existed in some of the connectors and...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.5.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a recommended update for all users of earlier versions; besides bug fixes also a few new features are provide.
The release contains &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project+%3D+DBZ+AND+fixVersion+%3D+0.9.5.Final&quot;&gt;18 resolved issues&lt;/a&gt; overall.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;apache_kafka_update_and_new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#apache_kafka_update_and_new_features&quot; /&gt;Apache Kafka Update and New Features&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release has been built against and tested with Apache Kafka 2.2.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1227&quot;&gt;DBZ-1227&lt;/a&gt;).
Earlier versions are continued to be supported as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For all the connectors it is possible now to specify the batch size when taking snapshots (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1247&quot;&gt;DBZ-1247&lt;/a&gt;).
The new connector option &lt;code&gt;snapshot.fetch.size&lt;/code&gt; has been introduced for that.
This option replaces the earlier option &lt;code&gt;rows.fetch.size&lt;/code&gt; which existed in some of the connectors and which will be removed in Debezium 0.10.
Existing connector instances should therefore be re-configured to use the new option.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Continuing the work from Debezium 0.9.4, the Postgres connector supports some more column types:
&lt;code&gt;MACADDR&lt;/code&gt; and &lt;code&gt;MACADDR8&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1193&quot;&gt;DBZ-1193&lt;/a&gt;) as well as &lt;code&gt;INT4RANGE&lt;/code&gt;, &lt;code&gt;INT8RANGE&lt;/code&gt; and &lt;code&gt;NUMRANGE&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1076&quot;&gt;DBZ-1076&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#fixes&quot; /&gt;Fixes&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Amongst others, this release includes the following fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Failing to specify value for &lt;code&gt;database.server.name&lt;/code&gt; results in invalid Kafka topic name (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-212&quot;&gt;DBZ-212&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Postgres Connector times out in schema discovery for DBs with many tables (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1214&quot;&gt;DBZ-1214&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Oracle connector: JDBC transaction can only capture single DML record (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1223&quot;&gt;DBZ-1223&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lost precision for timestamp with timezone (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1236&quot;&gt;DBZ-1236&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NullpointerException due to optional value for commitTime (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1241&quot;&gt;DBZ-1241&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Default value for datetime(0) is incorrectly handled (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1243&quot;&gt;DBZ-1243&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Microsecond precision is lost when reading timetz data from Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1260&quot;&gt;DBZ-1260&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-5-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.5.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re very thankful to the following community members who contributed to this release:
&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jorkzijlstra&quot;&gt;Jork Zijlstra&lt;/a&gt;,
&lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mrozieres&quot;&gt;Mathieu Rozieres&lt;/a&gt; and
&lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot; /&gt;Outlook&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release is planned to be the last in the 0.9 line.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re now going to focus on Debezium 0.10, whose main topic will be to clean up a few things:
we’d like to remove a few deprecated options and features (e.g. the legacy DDL parser in the MySQL connector).
We’re also planning to do a thorough review of the event structure of the different connectors;
for instance in the &lt;code&gt;source&lt;/code&gt; block of CDC messages there are a some field names that should be unified.
We believe users will benefit from a more consistent experience across the connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another focus area will be to migrate the existing Postgres connector to the framework classes established for the SQL Server and Oracle connectors.
This will allow to expose some new features for the Postgres connector, e.g. the monitoring capabilities already rolled out for the other two connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/04/18/hello-debezium/</id>
<title>Debezium&#8217;s Team Grows</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-04-18T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/04/18/hello-debezium/" rel="alternate" type="text/html" />
<author>
<name>Chris Cranford</name>
</author>
<category term="community"></category>
<category term="news"></category>
<summary>

Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.


My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
initial software endeavors were based on open source software, several which are still heavily used today
such as Hibernate ORM.





When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
of the project since 2.0, so it was...
</summary>
<content type="html">
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hello everyone, my name is Chris Cranford and I recently joined the Debezium team.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;My journey at Red Hat began just over three years ago; however I have been in this line of work for nearly
twenty years.  All throughout my career, I have advocated and supported open source software.  Many of my
initial software endeavors were based on open source software, several which are still heavily used today
such as &lt;a href=&quot;http://www.hibernate.org&quot;&gt;Hibernate ORM&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;!-- more --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When I first joined Red Hat, I had the pleasure to work on the Hibernate ORM team. I had been an end user
of the project since 2.0, so it was an excellent fit to be able to contribute full time to a project that
had served me well in the corporate world n-times over.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It wasn’t long ago when &lt;a href=&quot;http://twitter.com/gunnarmorling&quot;&gt;@gunnarmorling&lt;/a&gt; and I had a brief exchange about
Debezium.  I had not heard of the project and I was super stoked because I immediately saw parallel in
its goals and &lt;a href=&quot;http://www.hibernate.org/orm/envers&quot;&gt;Hibernate Envers&lt;/a&gt;, a change data capture solution that
is based on Hibernate’s event framework that I was currently maintaining.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I believe one of my first &quot;wow&quot; moments was when I realized how well Debezium fits into the micro-services
world.  The idea of being able to share data between micro-services in a very decoupled way is a massive
win for building reusable components and minimizes technical debt.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium just felt like the next logical step.  There are so many new and exciting things to come and
the team and myself cannot wait to share them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So lets get started!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;--Chris&lt;/p&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/</id>
<title>Debezium 0.9.4.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-04-11T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/04/11/debezium-0-9-4-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.4.Final!


This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
Overall, 17 issues were resolved.




MySQL Connector Improvements


The Debezium connector for MySQL comes with two new metrics:




Whether GTID is enabled for offset tracking or not (DBZ-1221)


Number of filtered events (DBZ-1206)




It also supports database connections using TLS 1.2 (DBZ-1208) now.




New Postgres Datatypes


The Postgres connector now allows to capture changes to columns of the CIDR and INET types (DBZ-1189).




Bug Fixes


The fixed bugs include the following:




Closing connection after snapshotting (DBZ-1218)


Can parse ALTER statement affecting enum column...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.4.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a drop-in replacement for earlier Debezium 0.9.x versions, containing mostly bug fixes and some improvements related to metrics.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.4.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;mysql_connector_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#mysql_connector_improvements&quot; /&gt;MySQL Connector Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium connector for MySQL comes with two new metrics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Whether GTID is enabled for offset tracking or not (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1221&quot;&gt;DBZ-1221&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Number of filtered events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1206&quot;&gt;DBZ-1206&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It also supports database connections using TLS 1.2 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1208&quot;&gt;DBZ-1208&lt;/a&gt;) now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_postgres_datatypes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_postgres_datatypes&quot; /&gt;New Postgres Datatypes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector now allows to capture changes to columns of the &lt;code&gt;CIDR&lt;/code&gt; and &lt;code&gt;INET&lt;/code&gt; types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1189&quot;&gt;DBZ-1189&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot; /&gt;Bug Fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The fixed bugs include the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Closing connection after snapshotting (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1218&quot;&gt;DBZ-1218&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Can parse ALTER statement affecting enum column with character set options (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1203&quot;&gt;DBZ-1203&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Avoiding timeout after bootstrapping a new table (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1207&quot;&gt;DBZ-1207&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-4-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.4.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jordanbragg&quot;&gt;Jordan Bragg&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/preethi29&quot;&gt;Preethi Sadagopan&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sashakovryga&quot;&gt;Sasha Kovryga&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt; and
&lt;a href=&quot;https://github.com/Crim&quot;&gt;Stephen Powis&lt;/a&gt;
for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/</id>
<title>Debezium 0.9.3.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-03-26T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/03/26/debezium-0-9-3-final-released/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="docker"></category>
<summary>



The Debezium team is happy to announce the release of Debezium 0.9.3.Final!


This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
Overall, 17 issues were resolved.








Container images will be released with a small delay due to some Docker Hub configuration issues.







New Features


The 0.9.3 release comes with two larger new features:




A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (DBZ-1082).
Instead of adding yet another snapshotting mode,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.3.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions, but there are few significant new features too.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.3.Final&quot;&gt;17 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
Container images will be released with a small delay due to some Docker Hub configuration issues.
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot; /&gt;New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 0.9.3 release comes with two larger new features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A feature request was made to execute a partial recovery of the replication process after losing the replication slot with the PostgreSQL database, e.g. after failing over to a secondary database host (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1082&quot;&gt;DBZ-1082&lt;/a&gt;).
Instead of adding yet another snapshotting mode, we took a step back and decided to make the Postgres snapshotting process more customizable by introducing a service provider interface (SPI). This lets you implement and register your own Java class for controlling the snaphotting process.
See the issue description of DBZ-1082 for one possible custom implementation of this SPI, which is based on Postgres&#39; &lt;code&gt;catalog_xmin&lt;/code&gt; property and selects all records altered after the last known xmin position.
To learn more about the SPI, see the the &lt;a href=&quot;https://github.com/debezium/debezium/blob/master/debezium-connector-postgres/src/main/java/io/debezium/connector/postgresql/spi/Snapshotter.java&quot;&gt;Snapshotter&lt;/a&gt; contract.
Note that the feature is still in incubating phase and the SPI should be considered unstable for the time being.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Not long ago we published blogpost about implementing the &lt;a href=&quot;https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/&quot;&gt;outbox&lt;/a&gt; pattern with Debezium for propagating data changes between microservices.
Community member &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; expanded the idea and created a ready-made implementation of the single message transform (SMT) described in the post for routing events from the outbox table to specific topics.
This SMT is part of the Debezium core library now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1169&quot;&gt;DBZ-1169&lt;/a&gt;).
Its usage will be described in the documentation soon; for the time being please refer to the &lt;a href=&quot;https://github.com/debezium/debezium/tree/master/debezium-core/src/main/java/io/debezium/transforms/outbox/EventRouter.java&quot;&gt;EventRouter&lt;/a&gt; type and the accompanying configuration class.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot; /&gt;Bug fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We did a couple of fixes related to the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A regression that introduced a deadlock in snapshotting process has been fixed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1161&quot;&gt;DBZ-1161&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;hstore&lt;/code&gt; datatype works correctly in snapshot phase (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1162&quot;&gt;DBZ-1162&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;wal2json&lt;/code&gt; plug-in processes also empty events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1181&quot;&gt;DBZ-1181&lt;/a&gt;) as e.g. originating from materialize view updates; this should help to resolve some of the issues where log files in Postgres couldn’t be discarded due to Debezium’s replication slot not advancing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The commit time is propely converted to microseconds (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1174&quot;&gt;DBZ-1174&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes especially in SQL parser:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;SERIAL&lt;/code&gt; datatype and default value is now supported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1185&quot;&gt;DBZ-1185&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A specific detail in the MySQL grammar that allows to enumerate table options in &lt;code&gt;ALTER TABLE&lt;/code&gt; without comma works (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1186&quot;&gt;DBZ-1186&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A false alarm for empty MySQL password is no longer reported (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1188&quot;&gt;DBZ-1188&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It is no longer necessary to create history topic manually for broker without default topic replication value (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1179&quot;&gt;DBZ-1179&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is now possible to process multiple schemas with a single Oracle connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1166&quot;&gt;DBZ-1166&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-3-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/ShubhamRwt&quot;&gt;Shubham Rawat&lt;/a&gt;, &lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;, &lt;a href=&quot;https://github.com/jcasstevens&quot;&gt;Jon Casstevens&lt;/a&gt;, &lt;a href=&quot;https://github.com/hashhar&quot;&gt;Ashar Hassan&lt;/a&gt; and &lt;a href=&quot;https://github.com/p5k6&quot;&gt;Josh Stanfield&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/03/14/debezium-meets-quarkus/</id>
<title>Debezium meets Quarkus</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-03-14T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/03/14/debezium-meets-quarkus/" rel="alternate" type="text/html" />
<author>
<name>Jiri Pechanec</name>
</author>
<category term="quarkus"></category>
<category term="examples"></category>
<category term="microservices"></category>
<category term="apache-kafka"></category>
<summary>





Last week&#8217;s announcement of Quarkus sparked a great amount of interest in the Java community:
crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp; OpenJDK HotSpot.
In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium&#8217;s data change events via Apache Kafka.
For that purpose, we&#8217;ll see what it takes to convert the shipment microservice from our recent post about the outbox pattern into Quarkus-based service.




Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
It combines and tightly integrates...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Last week’s announcement of &lt;a href=&quot;https://quarkus.io/&quot;&gt;Quarkus&lt;/a&gt; sparked a great amount of interest in the Java community:
crafted from the best of breed Java libraries and standards, it allows to build Kubernetes-native applications based on GraalVM &amp;amp; OpenJDK HotSpot.
In this blog post we are going to demonstrate how a Quarkus-based microservice can consume Debezium’s data change events via Apache Kafka.
For that purpose, we’ll see what it takes to convert the shipment microservice from our recent post about the &lt;a href=&quot;2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern&quot;&gt;outbox pattern&lt;/a&gt; into Quarkus-based service.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Quarkus is a Java stack designed for the development of cloud-native applications based on the Java platform.
It combines and tightly integrates mature libraries such Hibernate ORM, Vert.x, Netty, RESTEasy and Apache Camel as well as the APIs from the &lt;a href=&quot;https://microprofile.io/&quot;&gt;Eclipse MicroProfile&lt;/a&gt; initiative,
such as &lt;a href=&quot;https://github.com/eclipse/microprofile-config&quot;&gt;Config&lt;/a&gt; or &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging&lt;/a&gt;.
Using Quarkus, you can develop applications using both imperative and reactive styles, also combining both approaches as needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is designed for significantly reduced memory consumption and improved startup time.
Last but not least, Quarkus supports both OpenJDK HotSpot and GraalVM virtual machines.
With GraalVM it is possible to compile the application into a native binary and thus reduce the resource consumption and startup time even more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To learn more about Quarkus itself, we recommend to take a look at its excellent &lt;a href=&quot;https://quarkus.io/get-started/&quot;&gt;Getting Started&lt;/a&gt; guide.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;consuming_kafka_messages_with_quarkus&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#consuming_kafka_messages_with_quarkus&quot; /&gt;Consuming Kafka Messages with Quarkus&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the original &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;example application&lt;/a&gt; demonstrating the outbox pattern,
there was a microservice (&quot;shipment&quot;) based on Thorntail that consumed the events produced by the Debezium connector.
We’ve extended the example with a new service named &quot;shipment-service-quarkus&quot;.
It provides the same functionality as the &quot;shipment-service&quot; but is implemented as a microservice based on Quarkus instead of Thorntail.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This makes the overall architecture look like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/outbox_pattern_quarkus.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To retrofit the original service into a Quarkus-based application, only a few changes were needed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Quarkus right now supports only MariaDB but not MySQL; hence we have included an instance of MariaDB to which the service is writing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;a href=&quot;https://javaee.github.io/jsonp/&quot;&gt;JSON-P API&lt;/a&gt; used do deserialize incoming JSON messages can currently not be used without RESTEasy (see &lt;a href=&quot;https://github.com/quarkusio/quarkus/issues/1480&quot;&gt;issue #1480&lt;/a&gt;, which should be fixed soon); so the code has been modified to use the Jackson API instead&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Instead of the Kafka consumer API, the &lt;a href=&quot;https://github.com/eclipse/microprofile-reactive-messaging&quot;&gt;Reactive Messaging API&lt;/a&gt; defined by MicroProfile is used to receive messages from Apache Kafka; as an implementation of that API, the one provided by the &lt;a href=&quot;https://github.com/smallrye/smallrye-reactive-messaging&quot;&gt;SmallRye project&lt;/a&gt; is used, which is bundled as a Quarkus extension&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the first two steps are mere technicalities,
the Reactive Messaging API is a nice simplification over the polling loop in the original consumer.
All that’s needed to consume messages from a Kafka topic is to annotate a method with &lt;code&gt;@Incoming&lt;/code&gt;,
and it will automatically be invoked when a new message arrives:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class KafkaEventConsumer {

    @Incoming(&quot;orders&quot;)
    public CompletionStage&amp;lt;Void&amp;gt; onMessage(KafkaMessage&amp;lt;String, String&amp;gt; message)
            throws IOException {
        // handle message...

        return message.ack();
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;orders&quot; message source is configured via the MicroProfile Config API,
which resolves it to the &quot;OrderEvents&quot; topic already known from the original outbox example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;build_process&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#build_process&quot; /&gt;Build Process&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The build process is mostly the same as it was before.
Instead of using the Thorntail Maven plug-in, the Quarkus Maven plug-in is used now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following Quarkus extensions are used:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-hibernate-orm&lt;/em&gt;: support for Hibernate ORM and JPA&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-jdbc-mariadb&lt;/em&gt;: support for accessing MariaDB through JDBC&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;io.quarkus:quarkus-smallrye-reactive-messaging-kafka&lt;/em&gt;: support for accessing Kafka through the MicroProfile Reactive Messaging API&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;They pull in some other extensions too, e.g. &lt;em&gt;quarkus-arc&lt;/em&gt; (the Quarkus CDI runtime) and &lt;em&gt;quarkus-vertx&lt;/em&gt; (used by the reactive messaging support).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, two more changes were needed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A new build profile named &lt;code&gt;native&lt;/code&gt; has been added; this is used to compile the service into a native binary image using the Quarkus Maven plug-in&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the &lt;code&gt;native-image.docker-build&lt;/code&gt; system property is enabled when running the build; this means that the native image build is done inside of a Docker container, so that GraalVM doesn’t have to be installed on the developer’s machine&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All the heavy-lifting is done by the Quarkus Maven plug-in which is configured in &lt;em&gt;pom.xml&lt;/em&gt; like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;  &amp;lt;build&amp;gt;
    &amp;lt;finalName&amp;gt;shipment&amp;lt;/finalName&amp;gt;
    &amp;lt;plugins&amp;gt;
      ...
      &amp;lt;plugin&amp;gt;
        &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
        &amp;lt;executions&amp;gt;
          &amp;lt;execution&amp;gt;
            &amp;lt;goals&amp;gt;
              &amp;lt;goal&amp;gt;build&amp;lt;/goal&amp;gt;
            &amp;lt;/goals&amp;gt;
          &amp;lt;/execution&amp;gt;
        &amp;lt;/executions&amp;gt;
      &amp;lt;/plugin&amp;gt;
    &amp;lt;/plugins&amp;gt;
  &amp;lt;/build&amp;gt;
  ...
    &amp;lt;profile&amp;gt;
      &amp;lt;id&amp;gt;native&amp;lt;/id&amp;gt;
      &amp;lt;build&amp;gt;
        &amp;lt;plugins&amp;gt;
          &amp;lt;plugin&amp;gt;
            &amp;lt;groupId&amp;gt;io.quarkus&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;quarkus-maven-plugin&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;${version.quarkus}&amp;lt;/version&amp;gt;
            &amp;lt;executions&amp;gt;
              &amp;lt;execution&amp;gt;
                &amp;lt;goals&amp;gt;
                  &amp;lt;goal&amp;gt;native-image&amp;lt;/goal&amp;gt;
                &amp;lt;/goals&amp;gt;
                &amp;lt;configuration&amp;gt;
                  &amp;lt;enableHttpUrlHandler&amp;gt;true&amp;lt;/enableHttpUrlHandler&amp;gt;
                  &amp;lt;autoServiceLoaderRegistration&amp;gt;false&amp;lt;/autoServiceLoaderRegistration&amp;gt;
                &amp;lt;/configuration&amp;gt;
              &amp;lt;/execution&amp;gt;
            &amp;lt;/executions&amp;gt;
          &amp;lt;/plugin&amp;gt;
        &amp;lt;/plugins&amp;gt;
      &amp;lt;/build&amp;gt;
    &amp;lt;/profile&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;configuration&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuration&quot; /&gt;Configuration&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As any Quarkus application, the shipment service is configured via the &lt;em&gt;application.properties&lt;/em&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;quarkus.datasource.url: jdbc:mariadb://shipment-db-quarkus:3306/shipmentdb
quarkus.datasource.driver: org.mariadb.jdbc.Driver
quarkus.datasource.username: mariadbuser
quarkus.datasource.password: mariadbpw
quarkus.hibernate-orm.database.generation=drop-and-create
quarkus.hibernate-orm.log.sql=true

smallrye.messaging.source.orders.type=io.smallrye.reactive.messaging.kafka.Kafka
smallrye.messaging.source.orders.topic=OrderEvents
smallrye.messaging.source.orders.bootstrap.servers=kafka:9092
smallrye.messaging.source.orders.key.deserializer=org.apache.kafka.common.serialization.StringDeserializer
smallrye.messaging.source.orders.value.deserializer=org.apache.kafka.common.serialization.StringDeserializer
smallrye.messaging.source.orders.group.id=shipment-service-quarkus&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In our case it contains&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;the definition of a datasource (based on MariaDB) to which the shipment service writes its data,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;the definition of a messaging source, which is backed by the &quot;OrderEvents&quot; Kafka topic, using the given bootstrap server, deserializers and Kafka consumer group id.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;execution&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#execution&quot; /&gt;Execution&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Docker Compose config file has been enriched with two services, MariaDB and the new Quarkus-based shipment service.
So when &lt;code&gt;docker-compose up&lt;/code&gt; is executed, two shipment services are started side-by-side: the original Thorntail-based one and the new one using Quarkus.
When the order services receives a new purchase order and exports a corresponding event to Apache Kafka via the outbox table,
that message is processed by both shipment services, as they are using distinct consumer group ids.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;performance_numbers&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#performance_numbers&quot; /&gt;Performance Numbers&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The numbers are definitely not scientific, but provide a good indication of the order-of-magnitude difference between the native Quarkus-based application and the Thorntail service running on the JVM:&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-all grid-all stretch table table-bordered table-striped&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 30%;&quot;&gt;
&lt;col style=&quot;width: 35%;&quot;&gt;
&lt;col style=&quot;width: 35%;&quot;&gt;
&lt;/col&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot; /&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Quarkus service&lt;/th&gt;
&lt;th class=&quot;tableblock halign-left valign-top&quot;&gt;Thorntail service&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;memory [MB]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;33.8&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;1257&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;start time [ms]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;260&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;5746&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;application package size [MB]&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;54&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;div class=&quot;content&quot;&gt;&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;131&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/col&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The memory data were obtained via &lt;code&gt;htop&lt;/code&gt; utility.
The startup time was measured till the message about application readiness was printed.
As with all performance measurements, you should run your own comparisons based on your set-up and workload to gain insight into the actual differences for your specific use cases.&lt;/p&gt;
&lt;/div&gt;
&lt;/col&gt;
&lt;/colgroup&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this post we have successfully demonstrated that it is possible to consume Debezium-generated events in a Java application written with the Quarkus Java stack.
We have also shown that it is possible to provide such application as a binary image and provided back-of-the-envelope performance numbers demonstrating significant savings in resources.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’d like to see the awesomeness of deploying Java microservices as native images by yourself,
you can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service-quarkus&quot;&gt;source code&lt;/a&gt; of the implementation in the Debezium examples repo.
If you got any questions or feedback, please let us know in the comments below;
looking forward to hearing from you!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to Guillaume Smet for reviewing an earlier version of this post!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/</id>
<title>Debezium 0.9.2.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-02-25T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/25/debezium-0-9-2-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="docker"></category>
<summary>



The Debezium team is happy to announce the release of Debezium 0.9.2.Final!


This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
Overall, 18 issues were resolved.


A couple of fixes relate to the Debezium Postgres connector:




When not using REPLICA IDENTITY FULL, certain data types could trigger exceptions for update or delete events; those are fixed now
(DBZ-1141, DBZ-1149)


The connector won&#8217;t fail any longer when encountering a change to a row with an unaltered TOAST column value
(DBZ-1146)




Also the Debezium MySQL connector saw a number of fixes:




The connector works correctly now when using GTIDs and ANSI_QUOTES SQL mode (DBZ-1147)


The new...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team is happy to announce the release of Debezium &lt;strong&gt;0.9.2.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is mostly a bug-fix release and a drop-in replacement for earlier Debezium 0.9.x versions.
Overall, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.2.Final&quot;&gt;18 issues&lt;/a&gt; were resolved.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A couple of fixes relate to the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Debezium Postgres connector&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When not using &lt;code&gt;REPLICA IDENTITY FULL&lt;/code&gt;, certain data types could trigger exceptions for update or delete events; those are fixed now
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1141&quot;&gt;DBZ-1141&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1149&quot;&gt;DBZ-1149&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The connector won’t fail any longer when encountering a change to a row with an unaltered TOAST column value
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1146&quot;&gt;DBZ-1146&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; saw a number of fixes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The connector works correctly now when using GTIDs and &lt;code&gt;ANSI_QUOTES&lt;/code&gt; SQL mode (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1147&quot;&gt;DBZ-1147&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The new Antlr-based DDL parsers can handle column names that are key words such as &lt;code&gt;MEDIUM&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1150&quot;&gt;DBZ-1150&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;TIME&lt;/code&gt; columns with a default value larger than 23:59:59 can be exported now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1137&quot;&gt;DBZ-1137&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another important fix was done in the &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
where the connector archive deployed to Maven Central accidentally contained all test-scoped and provided-scoped dependencies.
This has been resolved now, so the connector archive only contains the actually needed JARs and thus is much smaller (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1138&quot;&gt;DBZ-1138&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_features&quot; /&gt;New Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 0.9.2 release also comes with two small new features:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You can pass arbitrary parameters to the logical decoding plug-in used by the Postgres connector;
this can for instance be used with wal2json to limit the number of tables to capture on the server side
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1130&quot;&gt;DBZ-1130&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The MongoDB connector now has the long-awaited snapshotting mode &lt;code&gt;NEVER&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-867&quot;&gt;DBZ-867&lt;/a&gt;),
i.e. you can set up a new connector without taking an initial snapshot and instantly beginning streaming changes from the oplog&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;version_updates&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#version_updates&quot; /&gt;Version Updates&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As of this release, Debezium has been upgraded to Apache Kafka 2.1.1.
Amongst others, this release fixes an issue where the Kafka Connect REST API would expose connector credentials also when those were configured via secrets (&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-5117&quot;&gt;KAFKA-5117&lt;/a&gt;).
We’ve also upgraded the binlog client used by the MySQL connector to version 0.19.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1140&quot;&gt;DBZ-1140&lt;/a&gt;),
which fixes a bug that had caused exceptions during rebalancing the connector before (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1132&quot;&gt;DBZ-1132&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-2-final&quot;&gt;release notes&lt;/a&gt; for the complete list of issues fixed in Debezium 0.9.2.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Debezium community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/kbarber2&quot;&gt;Keith Barber&lt;/a&gt;, &lt;a href=&quot;https://github.com/krizhan&quot;&gt;Krizhan Mariampillai&lt;/a&gt; and &lt;a href=&quot;https://github.com/taylor-rolison&quot;&gt;Taylor Rolison&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/</id>
<title>Reliable Microservices Data Exchange With the Outbox Pattern</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-02-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/19/reliable-microservices-data-exchange-with-the-outbox-pattern/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="microservices"></category>
<category term="apache-kafka"></category>
<category term="featured"></category>
<summary>





As part of their business logic, microservices often do not only have to update their own local data store,
but they also need to notify other services about data changes that happened.
The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
it provides source services with instant "read your own writes" semantics,
while offering reliable, eventually consistent data exchange across service boundaries.




Update (13 Sept. 2019): To simplify usage of the outbox pattern, Debezium now provides a ready-to-use SMT for routing outbox events. The custom SMT discussed in this blog post is not needed any...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;openblock teaser&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As part of their business logic, microservices often do not only have to update their own local data store,
but they also need to notify other services about data changes that happened.
The outbox pattern describes an approach for letting services execute these two tasks in a safe and consistent manner;
it provides source services with instant &quot;read your own writes&quot; semantics,
while offering reliable, eventually consistent data exchange across service boundaries.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Update (13 Sept. 2019):&lt;/em&gt; To simplify usage of the outbox pattern, Debezium now provides a ready-to-use &lt;a href=&quot;https://debezium.io/documentation/reference/0.9/configuration/outbox-event-router.html&quot;&gt;SMT for routing outbox events&lt;/a&gt;. The custom SMT discussed in this blog post is not needed any longer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’ve built a couple of microservices,
you’ll probably agree that the &lt;a href=&quot;https://blog.christianposta.com/microservices/the-hardest-part-about-microservices-data/&quot;&gt;hardest part about them is data&lt;/a&gt;:
microservices don’t exist in isolation and very often they need to propagate data and data changes amongst each other.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For instance consider a microservice that manages purchase orders:
when a new order is placed, information about that order may have to be relayed to a shipment service
(so it can assemble shipments of one or more orders) and a customer service
(so it can update things like the customer’s total credit balance based on the new order).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are different approaches for letting the order service know the other two about new purchase orders;
e.g. it could invoke some &lt;a href=&quot;https://en.wikipedia.org/wiki/Representational_state_transfer&quot;&gt;REST&lt;/a&gt;, &lt;a href=&quot;https://grpc.io/&quot;&gt;grpc&lt;/a&gt; or other (synchronous) API provided by these services.
This might create some undesired coupling, though: the sending service must know which other services to invoke and where to find them.
It also must be prepared for these services temporarily not being available.
Service meshes such as &lt;a href=&quot;https://istio.io/&quot;&gt;Istio&lt;/a&gt; can come in helpful here, by providing capabilities like request routing, retries, circuit breakers and much more.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The general issue of any synchronous approach is that one service cannot really function without the other services which it invokes.
While buffering and retrying might help in cases where other services only need to be &lt;em&gt;notified&lt;/em&gt; of certain events,
this is not the case if a service actually needs to &lt;em&gt;query&lt;/em&gt; other services for information.
For instance, when a purchase order is placed, the order service might need to obtain the information how many times the purchased item is on stock from an inventory service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another downside of such a synchronous approach is that it lacks re-playability,
i.e. the possibility for new consumers to arrive after events have been sent and still be able to consume the entire event stream from the beginning.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Both problems can be addressed by using an asynchronous data exchange approach instead:
i.e having the order, inventory and other services propagate events through a durable message log such as &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
By subscribing to these event streams, each service will be notified about the data change of other services.
It can react to these events and, if needed, create a local representation of that data in its own data store,
using a representation tailored towards its own needs.
For instance, such view might be denormalized to efficiently support specific access patterns, or it may only contain a subset of the original data that’s relevant to the consuming service.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Durable logs also support re-playability,
i.e. new consumers can be added as needed, enabling use cases you might not have had in mind originally,
and without touching the source service.
E.g. consider a data warehouse which should keep information about all the orders ever placed, or some full-text search functionality on purchase orders based on &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;.
Once the purchase order events are in a Kafka topic
(Kafka’s topic’s retention policy settings can be used to ensure that events remain in a topic as long as its needed for the given use cases and business requirements),
new consumers can subscribe, process the topic from the very beginning and materialize a view of all the data in a microservice’s database, search index, data warehouse etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Dealing with Topic Growth&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Depending on the amount of data (number and size of records, frequency of changes),
it may or may not be feasible to keep events in topics for a long or even indefinite time.
Very often, some or even all events pertaining to a given data item
(e.g. a specific purchase order) might be eligible for deletion from a business point of view after a given point in time.
See the box &quot;Deletion of Events from Kafka Topics&quot; further below for some more thoughts on the deletion of events from Kafka topics in order to keep their size within bounds.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_issue_of_dual_writes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_issue_of_dual_writes&quot; /&gt;The Issue of Dual Writes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to provide their functionality, microservices will typically have their own local data store.
For instance, the order service may use a relational database to persist the information about purchase orders.
When a new order is placed, this may result in an &lt;code&gt;INSERT&lt;/code&gt; operation in a table &lt;code&gt;PurchaseOrder&lt;/code&gt; in the service’s database.
At the same time, the service may wish to send an event about the new order to Apache Kafka,
so to propagate that information to other interested services.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Simply issuing these two requests may lead to potential inconsistencies, though.
The reason being that we cannot have one shared transaction that would span the service’s database as well as Apache Kafka,
as the latter doesn’t support to be enlisted in distributed (XA) transactions.
So in unfortunate circumstances it might happen that we end up with having the new purchase order persisted in the local database,
but not having sent the corresponding message to Kafka
(e.g. due to some networking issue).
Or, the other way around, we might have sent the message to Kafka but failed to persist the purchase order in the local database.
Both situations are undesirable;
this may cause no shipment to be created for a seemingly successfully placed order.
Or a shipment gets created, but then there’d be no trace about the corresponding purchase order in the order service itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So how can this situation be avoided?
The answer is to only modify &lt;em&gt;one&lt;/em&gt; of the two resources (the database &lt;em&gt;or&lt;/em&gt; Apache Kafka) and drive the update of the second one based on that, in an eventually consistent manner.
Let’s first consider the case of only writing to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When receiving a new purchase order, the order service would not do the &lt;code&gt;INSERT&lt;/code&gt; into its database synchronously;
instead, it would only send an event describing the new order to a Kafka topic.
So only one resource gets modified at a time, and if something goes wrong with that,
we’ll find out about it instantly and report back to the caller of the order service that the request failed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time, the service itself would subscribe to that Kafka topic.
That way, it will be notified when a new message arrives in the topic and it can persist the new purchase order in its database.
There’s one subtle challenge here, though, and that is the lack of &quot;read your own write&quot; semantics.
E.g. let’s assume the order service also has an API for searching for all the purchase orders of a given customer.
When invoking that API right after placing a new order, due to the asynchronous nature of processing messages from the Kafka topic,
it might happen that the purchase order has not yet been persisted in the service’s database and thus will not be returned by that query.
That can lead to a very confusing user experience, as users for instance may miss newly placed orders in their shopping history.
There are ways to deal with this situation, e.g. the service could keep newly placed purchase orders in memory and answer subsequent queries based on that.
This gets quickly non-trivial though when implementing more complex queries or considering that the order service might also comprise multiple nodes in a clustered set-up,
which would require propagation of that data within the cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now how would things look like when only writing to the database synchronously and driving the export of a message to Apache Kafka based on that?
This is where the outbox pattern comes in.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_outbox_pattern&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_pattern&quot; /&gt;The Outbox Pattern&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea of this approach is to have an &quot;outbox&quot; table in the service’s database.
When receiving a request for placing a purchase order, not only an &lt;code&gt;INSERT&lt;/code&gt; into the &lt;code&gt;PurchaseOrder&lt;/code&gt; table is done,
but, as part of the same transaction,
also a record representing the event to be sent is inserted into that outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The record describes an event that happened in the service,
for instance it could be a JSON structure representing the fact that a new purchase order has been placed,
comprising data on the order itself, its order lines as well as contextual information such as a use case identifier.
By explicitly emitting events via records in the outbox table,
it can be ensured that events are structured in a way suitable for external consumers.
This also helps to make sure that event consumers won’t break
when for instance altering the internal domain model or the &lt;code&gt;PurchaseOrder&lt;/code&gt; table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;An asynchronous process monitors that table for new entries.
If there are any, it propagates the events as messages to Apache Kafka.
This gives us a very nice balance of characteristics:
By synchronously writing to the &lt;code&gt;PurchaseOrder&lt;/code&gt; table, the source service benefits from &quot;read your own writes&quot; semantics.
A subsequent query for purchase orders will return the newly persisted order, as soon as that first transaction has been committed.
At the same time, we get reliable, asynchronous, eventually consistent data propagation to other services via Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, the outbox pattern isn’t actually a new idea.
It has been in use for quite some time.
In fact, even when using JMS-style message brokers, which actually could participate in distributed transactions,
it can be a preferable option to avoid any coupling and potential impact by downtimes of remote resources such as a message broker.
You can also find a description of the pattern on Chris Richardson’s excellent &lt;a href=&quot;https://microservices.io/patterns/data/application-events.html&quot;&gt;microservices.io&lt;/a&gt; site.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Nevertheless, the pattern gets much less attention than it deserves and it is especially useful in the context of microservices.
As we’ll see, the outbox pattern can be implemented in a very elegant and efficient way using change data capture and Debezium.
In the following, let’s explore how.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;an_implementation_based_on_change_data_capture&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#an_implementation_based_on_change_data_capture&quot; /&gt;An Implementation Based on Change Data Capture&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;Log-based Change Data Capture&lt;/a&gt; (CDC) is a great fit for capturing new entries in the outbox table and stream them to Apache Kafka.
As opposed to any polling-based approach, event capture happens with a very low overhead in near-realtime.
Debezium comes with &lt;a href=&quot;https://debezium.io/docs/connectors/&quot;&gt;CDC connectors&lt;/a&gt; for several databases such as MySQL, Postgres and SQL Server.
The following example will use the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql&quot;&gt;Debezium connector for Postgres&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find the complete &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox&quot;&gt;source code of the example&lt;/a&gt; on GitHub.
Refer to the &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/README.md&quot;&gt;README.md&lt;/a&gt; for details on building and running the example code.
The example is centered around two microservices,
&lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/order-service&quot;&gt;order-service&lt;/a&gt; and &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/outbox/shipment-service&quot;&gt;shipment-service&lt;/a&gt;.
Both are implemented in Java, using &lt;a href=&quot;http://cdi-spec.org/&quot;&gt;CDI&lt;/a&gt; as the component model and JPA/Hibernate for accessing their respective databases.
The order service runs on &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; and exposes a simple REST API for placing purchase orders and canceling specific order lines.
It uses a Postgres database as its local data store.
The shipment service is based on &lt;a href=&quot;http://thorntail.io/&quot;&gt;Thorntail&lt;/a&gt;; via Apache Kafka, it receives events exported by the order service and creates corresponding shipment entries in its own MySQL database.
Debezium tails the transaction log (&quot;write-ahead log&quot;, WAL) of the order service’s Postgres database in order to capture any new events in the outbox table and propagates them to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The overall architecture of the solution can be seen in the following picture:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/outbox_pattern.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Outbox Pattern Overview&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that the pattern is in no way tied to these specific implementation choices.
It could equally well be realized using alternative technologies such as Spring Boot
(e.g. leveraging Spring Data’s &lt;a href=&quot;https://docs.spring.io/spring-data/commons/docs/current/api/index.html?org/springframework/data/domain/DomainEvents.html&quot;&gt;support for domain events&lt;/a&gt;),
plain JDBC or other programming languages than Java altogether.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a closer look at some of the relevant components of the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;the_outbox_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_outbox_table&quot; /&gt;The Outbox Table&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;outbox&lt;/code&gt; table resides in the database of the order service and has the following structure:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Column        |          Type          | Modifiers
--------------+------------------------+-----------
id            | uuid                   | not null
aggregatetype | character varying(255) | not null
aggregateid   | character varying(255) | not null
type          | character varying(255) | not null
payload       | jsonb                  | not null&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Its columns are these:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;id&lt;/code&gt;: unique id of each message; can be used by consumers to detect any duplicate events, e.g. when restarting to read messages after a failure.
Generated when creating a new event.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;aggregatetype&lt;/code&gt;: the type of the &lt;em&gt;aggregate root&lt;/em&gt; to which a given event is related;
the idea being, leaning on the same concept of domain-driven design,
that exported events should refer to an aggregate
(&lt;a href=&quot;https://martinfowler.com/bliki/DDD_Aggregate.html&quot;&gt;&quot;a cluster of domain objects that can be treated as a single unit&quot;&lt;/a&gt;),
where the aggregate root provides the sole entry point for accessing any of the entities within the aggregate.
This could for instance be &quot;purchase order&quot; or &quot;customer&quot;.&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This value will be used to route events to corresponding topics in Kafka,
so there’d be a topic for all events related to purchase orders,
one topic for all customer-related events etc.
Note that also events pertaining to a child entity contained within one such aggregate should use that same type.
So e.g. an event representing the cancelation of an individual order line
(which is part of the purchase order aggregate)
should also use the type of its aggregate root, &quot;order&quot;,
ensuring that also this event will go into the &quot;order&quot; Kafka topic.&lt;/p&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;aggregateid&lt;/code&gt;: the id of the aggregate root that is affected by a given event; this could for instance be the id of a purchase order or a customer id;
Similar to the aggregate type, events pertaining to a sub-entity contained within an aggregate should use the id of the containing aggregate root,
e.g. the purchase order id for an order line cancelation event.
This id will be used as the key for Kafka messages later on.
That way, all events pertaining to one aggregate root or any of its contained sub-entities will go into the same partition of that Kafka topic,
which ensures that consumers of that topic will consume all the events related to one and the same aggregate in the exact order as they were produced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;type&lt;/code&gt;: the type of event, e.g. &quot;Order Created&quot; or &quot;Order Line Canceled&quot;. Allows consumers to trigger suitable event handlers.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;payload&lt;/code&gt;: a JSON structure with the actual event contents, e.g. containing a purchase order, information about the purchaser, contained order lines, their price etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;sending_events_to_the_outbox&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_events_to_the_outbox&quot; /&gt;Sending Events to the Outbox&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to &quot;send&quot; events to the outbox, code in the order service could in general just do an &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.
However, it’s a good idea to go for a slightly more abstract API, allowing to adjust implementation details of the outbox later on more easily, if needed.
&lt;a href=&quot;https://docs.jboss.org/weld/reference/latest/en-US/html/events.html&quot;&gt;CDI events&lt;/a&gt; come in very handy for this.
They can be raised in the application code and will be processed &lt;em&gt;synchronously&lt;/em&gt; by the outbox event sender,
which will do the required &lt;code&gt;INSERT&lt;/code&gt; into the outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All outbox event types should implement the following contract, resembling the structure of the outbox table shown before:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public interface ExportedEvent {

    String getAggregateId();
    String getAggregateType();
    JsonNode getPayload();
    String getType();
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To produce such event, application code uses an injected &lt;code&gt;Event&lt;/code&gt; instance, as e.g. here in the &lt;code&gt;OrderService&lt;/code&gt; class:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderService {

    @PersistenceContext
    private EntityManager entityManager;

    @Inject
    private Event&amp;lt;ExportedEvent&amp;gt; event;

    @Transactional
    public PurchaseOrder addOrder(PurchaseOrder order) {
        order = entityManager.merge(order);

        event.fire(OrderCreatedEvent.of(order));
        event.fire(InvoiceCreatedEvent.of(order));

        return order;
    }

    @Transactional
    public PurchaseOrder updateOrderLine(long orderId, long orderLineId,
            OrderLineStatus newStatus) {
        // ...
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the &lt;code&gt;addOrder()&lt;/code&gt; method, the JPA entity manager is used to persist the incoming order in the database
and the injected &lt;code&gt;event&lt;/code&gt; is used to fire a corresponding &lt;code&gt;OrderCreatedEvent&lt;/code&gt; and an &lt;code&gt;InvoiceCreatedEvent&lt;/code&gt;.
Again, keep in mind that, despite the notion of &quot;event&quot;, these two things happen within one and the same transaction.
i.e. within this transaction, three records will be inserted into the database:
one in the table with purchase orders and two in the outbox table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Actual event implementations are straight-forward;
as an example, here’s the &lt;code&gt;OrderCreatedEvent&lt;/code&gt; class:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class OrderCreatedEvent implements ExportedEvent {

    private static ObjectMapper mapper = new ObjectMapper();

    private final long id;
    private final JsonNode order;

    private OrderCreatedEvent(long id, JsonNode order) {
        this.id = id;
        this.order = order;
    }

    public static OrderCreatedEvent of(PurchaseOrder order) {
        ObjectNode asJson = mapper.createObjectNode()
                .put(&quot;id&quot;, order.getId())
                .put(&quot;customerId&quot;, order.getCustomerId())
                .put(&quot;orderDate&quot;, order.getOrderDate().toString());

        ArrayNode items = asJson.putArray(&quot;lineItems&quot;);

        for (OrderLine orderLine : order.getLineItems()) {
        items.add(
                mapper.createObjectNode()
                .put(&quot;id&quot;, orderLine.getId())
                .put(&quot;item&quot;, orderLine.getItem())
                .put(&quot;quantity&quot;, orderLine.getQuantity())
                .put(&quot;totalPrice&quot;, orderLine.getTotalPrice())
                .put(&quot;status&quot;, orderLine.getStatus().name())
            );
        }

        return new OrderCreatedEvent(order.getId(), asJson);
    }

    @Override
    public String getAggregateId() {
        return String.valueOf(id);
    }

    @Override
    public String getAggregateType() {
        return &quot;Order&quot;;
    }

    @Override
    public String getType() {
        return &quot;OrderCreated&quot;;
    }

    @Override
    public JsonNode getPayload() {
        return order;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson’s&lt;/a&gt; &lt;code&gt;ObjectMapper&lt;/code&gt; is used to create a JSON representation of the event’s payload.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a look at the code that consumes any fired &lt;code&gt;ExportedEvent&lt;/code&gt; and does the corresponding write to the outbox table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class EventSender {

    @PersistenceContext
    private EntityManager entityManager;

    public void onExportedEvent(@Observes ExportedEvent event) {
        OutboxEvent outboxEvent = new OutboxEvent(
                event.getAggregateType(),
                event.getAggregateId(),
                event.getType(),
                event.getPayload()
        );

        entityManager.persist(outboxEvent);
        entityManager.remove(outboxEvent);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s rather simple: for each event the CDI runtime will invoke the &lt;code&gt;onExportedEvent()&lt;/code&gt; method.
An instance of the &lt;code&gt;OutboxEvent&lt;/code&gt; entity is persisted in the database — and removed right away!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This might be surprising at first.
But it makes sense when remembering how log-based CDC works:
it doesn’t examine the actual contents of the table in the database, but instead it tails the append-only transaction log.
The calls to &lt;code&gt;persist()&lt;/code&gt; and &lt;code&gt;remove()&lt;/code&gt; will create an &lt;code&gt;INSERT&lt;/code&gt; and a &lt;code&gt;DELETE&lt;/code&gt; entry in the log once the transaction commits.
After that, Debezium will process these events:
for any &lt;code&gt;INSERT&lt;/code&gt;, a message with the event’s payload will be sent to Apache Kafka.
&lt;code&gt;DELETE&lt;/code&gt; events on the other hand can be ignored,
as the removal from the outbox table is a mere technicality that doesn’t require any propagation to the message broker.
So we are able to capture the event added to the outbox table by means of CDC,
but when looking at the contents of the table itself, it will always be empty.
This means that no additional disk space is needed for the table
(apart from the log file elements which will automatically be discarded at some point)
and also no separate house-keeping process is required to stop it from growing indefinitely.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;registering_the_debezium_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#registering_the_debezium_connector&quot; /&gt;Registering the Debezium Connector&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the outbox implementation in place, it’s time to register the Debezium Postgres connector,
so it can capture any new events in the outbox table and relay them to Apache Kafka.
That can be done by POST-ing the following JSON request to the REST API of Kafka Connect:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;name&quot;: &quot;outbox-connector&quot;,
    &quot;config&quot;: {
        &quot;connector.class&quot; : &quot;io.debezium.connector.postgresql.PostgresConnector&quot;,
        &quot;tasks.max&quot; : &quot;1&quot;,
        &quot;database.hostname&quot; : &quot;order-db&quot;,
        &quot;database.port&quot; : &quot;5432&quot;,
        &quot;database.user&quot; : &quot;postgresuser&quot;,
        &quot;database.password&quot; : &quot;postgrespw&quot;,
        &quot;database.dbname&quot; : &quot;orderdb&quot;,
        &quot;database.server.name&quot; : &quot;dbserver1&quot;,
        &quot;schema.whitelist&quot; : &quot;inventory&quot;,
        &quot;table.whitelist&quot; : &quot;inventory.outboxevent&quot;,
        &quot;tombstones.on.delete&quot; : &quot;false&quot;,
        &quot;transforms&quot; : &quot;router&quot;,
        &quot;transforms.router.type&quot; : &quot;io.debezium.examples.outbox.routingsmt.EventRouter&quot;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This sets up an instance of &lt;code&gt;io.debezium.connector.postgresql.PostgresConnector&lt;/code&gt;,
capturing changes from the specified Postgres instance.
Note that by means of a table whitelist, only changes from the &lt;code&gt;outboxevent&lt;/code&gt; table are captured.
It also applies a single message transform (SMT) named &lt;code&gt;EventRouter&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Deletion of Events from Kafka Topics&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By setting the &lt;code&gt;tombstones.on.delete&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;, no deletion markers (&quot;tombstones&quot;) will be emitted by the connector when an event record gets deleted from the outbox table.
That makes sense, as the deletion from the outbox table shouldn’t affect the retention of events in the corresponding Kafka topics.
Instead, a specific retention time for the event topics may be configured in Kafka,
e.g. to retain all purchase order events for 30 days.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Alternatively, one could work with &lt;a href=&quot;https://kafka.apache.org/documentation/#compaction&quot;&gt;compacted topics&lt;/a&gt;.
This would require some changes to the design of events in the outbox table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;they must describe the entire aggregate;
so for instance also an event representing the cancelation of a single order line should describe the complete current state of the containing purchase order;
that way consumers will be able to obtain the entire state of the purchase order also when only seeing the last event pertaining to a given order, after log compaction ran.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;they must have one more &lt;code&gt;boolean&lt;/code&gt; attribute indicating whether a particular event represents the deletion of the event’s aggregate root.
Such an event (e.g. of type &lt;code&gt;OrderDeleted&lt;/code&gt;) could then be used by the event routing SMT described in the next section to produce a deletion marker for that aggregate root.
Log compaction would then remove all events pertaining to the given purchase order when its &lt;code&gt;OrderDeleted&lt;/code&gt; event has been written to the topic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Naturally, when deleting events, the event stream will not be re-playable from its very beginning any longer.
Depending on the specific business requirements, it might be sufficient to just keep the final state of a given purchase order, customer etc.
This could be achieved using compacted topics and a sufficiently value for the topic’s &lt;code&gt;delete.retention.ms&lt;/code&gt; setting.
Another option could be to move historic events to some sort of cold storage (e.g. an Amazon S3 bucket),
from where they can be retrieved if needed, followed by reading the latest events from the Kafka topics.
Which approach to follow depends on the specific requirements, expected amount of data and expertise in the team developing and operating the solution.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;topic_routing&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#topic_routing&quot; /&gt;Topic Routing&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By default, the Debezium connectors will send all change events originating from one given table to the same topic,
i.e. we’d end up with a single Kafka topic named &lt;code&gt;dbserver1.inventory.outboxevent&lt;/code&gt; which would contain all events,
be it order events, customer events etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To simplify the implementation of consumers which are only interested in specific event types it makes more sense, though,
to have multiple topics, e.g. &lt;code&gt;OrderEvents&lt;/code&gt;, &lt;code&gt;CustomerEvents&lt;/code&gt; and so on.
For instance the shipment service might not be interested in any customer events.
By only subscribing to the &lt;code&gt;OrderEvents&lt;/code&gt; topic, it will be sure to never receive any customer events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to route the change events captured from the outbox table to different topics, that custom SMT &lt;code&gt;EventRouter&lt;/code&gt; is used.
Here is the code of its &lt;code&gt;apply()&lt;/code&gt; method, which will be invoked by Kafka Connect for each record emitted by the Debezium connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Override
public R apply(R record) {
    // Ignoring tombstones just in case
    if (record.value() == null) {
        return record;
    }

    Struct struct = (Struct) record.value();
    String op = struct.getString(&quot;op&quot;);

    // ignoring deletions in the outbox table
    if (op.equals(&quot;d&quot;)) {
        return null;
    }
    else if (op.equals(&quot;c&quot;)) {
        Long timestamp = struct.getInt64(&quot;ts_ms&quot;);
        Struct after = struct.getStruct(&quot;after&quot;);

        String key = after.getString(&quot;aggregateid&quot;);
        String topic = after.getString(&quot;aggregatetype&quot;) + &quot;Events&quot;;

        String eventId = after.getString(&quot;id&quot;);
        String eventType = after.getString(&quot;type&quot;);
        String payload = after.getString(&quot;payload&quot;);

        Schema valueSchema = SchemaBuilder.struct()
            .field(&quot;eventType&quot;, after.schema().field(&quot;type&quot;).schema())
            .field(&quot;ts_ms&quot;, struct.schema().field(&quot;ts_ms&quot;).schema())
            .field(&quot;payload&quot;, after.schema().field(&quot;payload&quot;).schema())
            .build();

        Struct value = new Struct(valueSchema)
            .put(&quot;eventType&quot;, eventType)
            .put(&quot;ts_ms&quot;, timestamp)
            .put(&quot;payload&quot;, payload);

        Headers headers = record.headers();
        headers.addString(&quot;eventId&quot;, eventId);

        return record.newRecord(topic, null, Schema.STRING_SCHEMA, key, valueSchema, value,
                record.timestamp(), headers);
    }
    // not expecting update events, as the outbox table is &quot;append only&quot;,
    // i.e. event records will never be updated
    else {
        throw new IllegalArgumentException(&quot;Record of unexpected op type: &quot; + record);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When receiving a delete event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;d&lt;/code&gt;), it will discard that event,
as that deletion of event records from the outbox table is not relevant to downstream consumers.
Things get more interesting, when receiving a create event (&lt;code&gt;op&lt;/code&gt; = &lt;code&gt;c&lt;/code&gt;).
Such record will be propagated to Apache Kafka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s change events have a complex structure, that contain the old (&lt;code&gt;before&lt;/code&gt;) and new (&lt;code&gt;after&lt;/code&gt;) state of the represented row.
The event structure to propagate is obtained from the &lt;code&gt;after&lt;/code&gt; state.
The &lt;code&gt;aggregatetype&lt;/code&gt; value from the captured event record is used to build the name of the topic to send the event to.
For instance, events with &lt;code&gt;aggregatetype&lt;/code&gt; set to &lt;code&gt;Order&lt;/code&gt; will be sent to the &lt;code&gt;OrderEvents&lt;/code&gt; topic.
&lt;code&gt;aggregateid&lt;/code&gt; is used as the message key, making sure all messages of that aggregate will go into the same partition of that topic.
The message value is a structure comprising the original event payload (encoded as JSON),
the timestamp indicating when the event was produced and the event type.
Finally, the event UUID is propagated as a Kafka header field.
This allows for efficient duplicate detection by consumers, without having to examine the actual message contents.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;events_in_apache_kafka&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#events_in_apache_kafka&quot; /&gt;Events in Apache Kafka&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s take a look into the &lt;code&gt;OrderEvents&lt;/code&gt; and &lt;code&gt;CustomerEvents&lt;/code&gt; topics.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have checked out the example sources and started all the components via Docker Compose
(see the &lt;em&gt;README.md&lt;/em&gt; file in the example project for more details),
you can place purchase orders via the order service’s REST API like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/create-order-request.json | http POST http://localhost:8080/order-service/rest/orders&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Similarly, specific order lines can be canceled:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat resources/data/cancel-order-line-request.json | http PUT http://localhost:8080/order-service/rest/orders/1/lines/2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using a tool such as the very practical &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt; utility,
you should now see messages like these in the &lt;code&gt;OrderEvents&lt;/code&gt; topic:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -f &#39;Headers: %h\nKey: %k\nValue: %s\n&#39; -q -t OrderEvents&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Headers: eventId=d03dfb18-8af8-464d-890b-09eb8b2dbbdd
Key: &quot;4&quot;
Value: {&quot;eventType&quot;:&quot;OrderCreated&quot;,&quot;ts_ms&quot;:1550307598558,&quot;payload&quot;:&quot;{\&quot;id\&quot;: 4, \&quot;lineItems\&quot;: [{\&quot;id\&quot;: 7, \&quot;item\&quot;: \&quot;Debezium in Action\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 2, \&quot;totalPrice\&quot;: 39.98}, {\&quot;id\&quot;: 8, \&quot;item\&quot;: \&quot;Debezium for Dummies\&quot;, \&quot;status\&quot;: \&quot;ENTERED\&quot;, \&quot;quantity\&quot;: 1, \&quot;totalPrice\&quot;: 29.99}], \&quot;orderDate\&quot;: \&quot;2019-01-31T12:13:01\&quot;, \&quot;customerId\&quot;: 123}&quot;}
Headers: eventId=49f89ea0-b344-421f-b66f-c635d212f72c
Key: &quot;4&quot;
Value: {&quot;eventType&quot;:&quot;OrderLineUpdated&quot;,&quot;ts_ms&quot;:1550308226963,&quot;payload&quot;:&quot;{\&quot;orderId\&quot;: 4, \&quot;newStatus\&quot;: \&quot;CANCELLED\&quot;, \&quot;oldStatus\&quot;: \&quot;ENTERED\&quot;, \&quot;orderLineId\&quot;: 7}&quot;}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;payload&lt;/code&gt; field with the message values is the string-ified JSON representation of the original events.
The Debezium Postgres connector emits &lt;code&gt;JSONB&lt;/code&gt; columns as a string
(using the &lt;code&gt;io.debezium.data.Json&lt;/code&gt; logical type name),
which is why the quotes are escaped.
The &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility, and more specifically,
its &lt;code&gt;fromjson&lt;/code&gt; operator, come in handy for displaying the event payload in a more readable way:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kafkacat -b kafka:9092 -C -o beginning -t Order | jq &#39;.payload | fromjson&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;id&quot;: 4,
  &quot;lineItems&quot;: [
    {
      &quot;id&quot;: 7,
      &quot;item&quot;: &quot;Debezium in Action&quot;,
      &quot;status&quot;: &quot;ENTERED&quot;,
      &quot;quantity&quot;: 2,
      &quot;totalPrice&quot;: 39.98
    },
    {
      &quot;id&quot;: 8,
      &quot;item&quot;: &quot;Debezium for Dummies&quot;,
      &quot;status&quot;: &quot;ENTERED&quot;,
      &quot;quantity&quot;: 1,
      &quot;totalPrice&quot;: 29.99
    }
  ],
  &quot;orderDate&quot;: &quot;2019-01-31T12:13:01&quot;,
  &quot;customerId&quot;: 123
}
{
  &quot;orderId&quot;: 4,
  &quot;newStatus&quot;: &quot;CANCELLED&quot;,
  &quot;oldStatus&quot;: &quot;ENTERED&quot;,
  &quot;orderLineId&quot;: 7
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also take a look at the &lt;code&gt;CustomerEvents&lt;/code&gt; topic to inspect the events representing the creation of an invoice when a purchase order is added.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;duplicate_detection_in_the_consuming_service&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#duplicate_detection_in_the_consuming_service&quot; /&gt;Duplicate Detection in the Consuming Service&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At this point, our implementation of the outbox pattern is fully functional;
when the order service receives a request to place an order
(or cancel an order line),
it will persist the corresponding state in the &lt;code&gt;purchaseorder&lt;/code&gt; and &lt;code&gt;orderline&lt;/code&gt; tables of its database.
At the same time, within the same transaction, corresponding event entries will be added to the outbox table in the same database.
The Debezium Postgres connector captures any insertions into that table
and routes the events into the Kafka topic corresponding to the aggregate type represented by a given event.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To wrap things up, let’s explore how another microservice such as the shipment service can consume these messages.
The entry point into that service is a regular Kafka consumer implementation,
which is not too exciting and hence omitted here for the sake of brevity.
You can find its &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/outbox/shipment-service/src/main/java/io/debezium/examples/outbox/shipment/facade/KafkaEventConsumer.java&quot;&gt;source code&lt;/a&gt; in the example repository.
For each incoming message on the &lt;code&gt;Order&lt;/code&gt; topic, the consumer calls the &lt;code&gt;OrderEventHandler&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class OrderEventHandler {

    private static final Logger LOGGER = LoggerFactory.getLogger(OrderEventHandler.class);

    @Inject
    private MessageLog log;

    @Inject
    private ShipmentService shipmentService;

    @Transactional
    public void onOrderEvent(UUID eventId, String key, String event) {
        if (log.alreadyProcessed(eventId)) {
            LOGGER.info(&quot;Event with UUID {} was already retrieved, ignoring it&quot;, eventId);
            return;
        }

        JsonObject json = Json.createReader(new StringReader(event)).readObject();
        JsonObject payload = json.containsKey(&quot;schema&quot;) ? json.getJsonObject(&quot;payload&quot;) :json;

        String eventType = payload.getString(&quot;eventType&quot;);
        Long ts = payload.getJsonNumber(&quot;ts_ms&quot;).longValue();
        String eventPayload = payload.getString(&quot;payload&quot;);

        JsonReader payloadReader = Json.createReader(new StringReader(eventPayload));
        JsonObject payloadObject = payloadReader.readObject();

        if (eventType.equals(&quot;OrderCreated&quot;)) {
            shipmentService.orderCreated(payloadObject);
        }
        else if (eventType.equals(&quot;OrderLineUpdated&quot;)) {
            shipmentService.orderLineUpdated(payloadObject);
        }
        else {
            LOGGER.warn(&quot;Unkown event type&quot;);
        }

        log.processed(eventId);
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first thing done by &lt;code&gt;onOrderEvent()&lt;/code&gt; is to check whether the event with the given UUID has been processed before.
If so, any further calls for that same event will be ignored.
This is to prevent any duplicate processing of events caused by the &quot;at least once&quot; semantics of this data pipeline.
For instance it could happen that the Debezium connector or the consuming service fail
before acknowledging the retrieval of a specific event with the source database or the messaging broker, respectively.
In that case, after a restart of Debezium or the consuming service,
a few events may be processed a second time.
Propagating the event UUID as a Kafka message header allows for an efficient detection and exclusion of duplicates in the consumer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If a message is received for the first time, the message value is parsed and the business method of the &lt;code&gt;ShippingService&lt;/code&gt; method corresponding to the specific event type is invoked with the event payload.
Finally, the message is marked as processed with the message log.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This &lt;code&gt;MessageLog&lt;/code&gt; simply keeps track of all consumed events in a table within the service’s local database:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class MessageLog {

    @PersistenceContext
    private EntityManager entityManager;

    @Transactional(value=TxType.MANDATORY)
    public void processed(UUID eventId) {
        entityManager.persist(new ConsumedMessage(eventId, Instant.now()));
    }

    @Transactional(value=TxType.MANDATORY)
    public boolean alreadyProcessed(UUID eventId) {
        return entityManager.find(ConsumedMessage.class, eventId) != null;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;That way, should the transaction be rolled back for some reason, also the original message will not be marked as processed and an exception would bubble up to the Kafka event consumer loop.
This allows for re-trying to process the message later on.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that a more complete implementation should take care of re-trying given messages only for a certain number of times,
before re-routing any unprocessable messages to a dead-letter queue or similar.
Also there should be some house-keeping on the message log table;
periodically, all events older than the consumer’s current offset committed with the broker may be deleted,
as it’s ensured that such messages won’t be propagated to the consumer another time.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The outbox pattern is a great way for propagating data amongst different microservices.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By only modifying a single resource - the source service’s own database -
it avoids any potential inconsistencies of altering multiple resources at the same time which don’t share one common transactional context
(the database and Apache Kafka).
By writing to the database first, the source service has instant &quot;read your own writes&quot; semantics,
which is important for a consistent user experience, allowing query methods invoked following to a write to instantly reflect any data changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;At the same time, the pattern enables asynchronous event propagation to other microservices.
Apache Kafka acts as a highly scalable and reliable backbone for the messaging amongst the services.
Given the right topic retention settings, new consumers may come up long after an event has been originally produced,
and build up their own local state based on the event history.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Putting Apache Kafka into the center of the overall architecture also ensures a decoupling of involved services.
If for instance single components of the solution fail or are not available for some time, e.g. during an update,
events will simply be processed later on: after a restart,
the Debezium connector will continue to tail the outbox table from the point where it left off before.
Similarly, any consumer will continue to process topics from its previous offset.
By keeping track of already successfully processed messages, duplicates can be detected and excluded from repeated handling.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Naturally, such event pipeline between different services is eventually consistent,
i.e. consumers such as the shipping service may lag a bit behind producers such as the order service.
Usually, that’s just fine, though, and can be handled in terms of the application’s business logic.
For instance there’ll typically be no need to create a shipment within the very same second as an order has been placed.
Also, end-to-end delays of the overall solution are typically low (seconds or even sub-second range),
thanks to log-based change data capture which allows for emission of events in near-realtime.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One last thing to keep in mind is that the structure of the events exposed via the outbox should be considered a part of the emitting service’s API.
I.e. when needed, their structure should be adjusted carefully and with compatibility considerations in mind.
This is to ensure to not accidentally break any consumers when upgrading the producing service.
At the same time, consumers should be lenient when handling messages and for instance not fail when encountering unknown attributes within received events.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Many thanks to Hans-Peter Grahsl, Jiri Pechanec, Justin Holmes and René Kerner for their feedback while writing this post!&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/</id>
<title>Debezium at DevNation Live</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-02-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/13/debezium-webinar-at-devnation-live/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="introduction"></category>
<category term="presentation"></category>
<summary>



Last week I had the pleasure to do a webinar on change data streaming patterns for microservices with the fabulous Burr Sutter at DevNation Live.


The recording of that 30 min session is available on YouTube now.
It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
running on OpenShift.
The demo begins at 12 min 40 into the recording.


Enjoy!



-->


&#160;
You can also find the slide deck (in a slightly extended version) on Speaker Deck:
&#160;
&#160;







About Debezium


Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Last week I had the pleasure to do a &lt;a href=&quot;https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8/&quot;&gt;webinar on change data streaming patterns for microservices&lt;/a&gt; with the fabulous Burr Sutter at DevNation Live.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The recording of that 30 min session is available on YouTube now.
It also contains a demo that shows how to set-up a data streaming pipeline with Debezium and Apache Kafka,
running on OpenShift.
The demo begins at 12 min 40 into the recording.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Enjoy!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;responsive-video&quot;&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QYbXDp4Vu-8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot; /&gt;
&lt;!--&lt;iframe width=&quot;1600&quot; height=&quot;900&quot; src=&quot;https://www.youtube.com/embed/IOZ2Um6e430?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;--&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt; &lt;br&gt;
You can also find the &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/change-data-streaming-patterns-for-microservices-with-debezium-apache-kafka-meetup-hamburg&quot;&gt;slide deck&lt;/a&gt; (in a slightly extended version) on Speaker Deck:
 &lt;br&gt;
 &lt;br /&gt;
&lt;/br&gt;
&lt;div style=&quot;text-align-center&quot;&gt;
&lt;script async=&quot;&quot; class=&quot;speakerdeck-embed&quot; data-id=&quot;c390d77e50464c99916ede7368a279c2&quot; data-ratio=&quot;1.77777777777778&quot; src=&quot;//speakerdeck.com/assets/embed.js&quot; /&gt;
&lt;/div&gt;
&lt;/br&gt;
&lt;/p&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/</id>
<title>Debezium 0.9.1.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-02-13T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/13/debezium-0-9-1-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Quickly following up to last week&#8217;s release of Debezium 0.9, it&#8217;s my pleasure today to announce the release of Debezium 0.9.1.Final!


This release fixes a couple of bugs which were reported after the 0.9 release.
Most importantly, there are two fixes to the new Debezium connector for SQL Server,
which deal with correct handling of LSNs after connector restarts (DBZ-1128, DBZ-1131).
The connector also uses more reasonable defaults for the selectMethod and fetchSize options of the SQL Server JDBC driver (DBZ-1065),
which can help to significantly increase through-put and reduce memory consumption of the connector.


The MySQL connector supports GENERATED columns now with the new Antlr-based...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Quickly following up to last week’s release of Debezium 0.9, it’s my pleasure today to announce the release of Debezium &lt;strong&gt;0.9.1.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release fixes a couple of bugs which were reported after the 0.9 release.
Most importantly, there are two fixes to the new &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;Debezium connector for SQL Server&lt;/a&gt;,
which deal with correct handling of LSNs after connector restarts (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1128&quot;&gt;DBZ-1128&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1131&quot;&gt;DBZ-1131&lt;/a&gt;).
The connector also uses more reasonable defaults for the &lt;code&gt;selectMethod&lt;/code&gt; and &lt;code&gt;fetchSize&lt;/code&gt; options of the SQL Server JDBC driver (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1065&quot;&gt;DBZ-1065&lt;/a&gt;),
which can help to significantly increase through-put and reduce memory consumption of the connector.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The MySQL connector supports &lt;code&gt;GENERATED&lt;/code&gt; columns now with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1123&quot;&gt;DBZ-1123&lt;/a&gt;),
and for the Postgres connector the handling of primary key column definition changes was improved (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-997&quot;&gt;DBZ-997&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In terms of new features, there is a new container image provided on Docker Hub now:
the &lt;a href=&quot;https://hub.docker.com/r/debezium/tooling&quot;&gt;debezium/tooling&lt;/a&gt; image contains a couple of open-source CLI tools
(currently &lt;a href=&quot;https://github.com/edenhill/kafkacat&quot;&gt;kafkacat&lt;/a&gt;, &lt;a href=&quot;https://github.com/jakubroztocil/httpie&quot;&gt;httpie&lt;/a&gt;, &lt;a href=&quot;https://github.com/stedolan/jq&quot;&gt;jq&lt;/a&gt;, &lt;a href=&quot;https://github.com/dbcli/mycli&quot;&gt;mycli&lt;/a&gt; and &lt;a href=&quot;https://github.com/dbcli/pgcli&quot;&gt;pqcli&lt;/a&gt;)
which greatly help when working with Debezium connectors, Apache Kafka and Kafka Connect on the command line
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1125&quot;&gt;DBZ-1125&lt;/a&gt;).
A big thank you to the respective authors these fantastic tools!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/debezium_shell.gif&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;CLI tools for working with Debezium&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Altogether, &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20%3D%200.9.1.Final&quot;&gt;12 issues&lt;/a&gt; were resolved in this release.
Please refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-1-final&quot;&gt;release notes&lt;/a&gt; to learn more about all fixed bugs, update procedures etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/ivan-lorenz&quot;&gt;Ivan Lorenz&lt;/a&gt; and &lt;a href=&quot;https://github.com/tomazlemos&quot;&gt;Tomaz Lemos Fernandes&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/</id>
<title>Debezium 0.9.0.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-02-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/02/05/debezium-0-9-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



I&#8217;m delighted to announce the release of Debezium 0.9 Final!


This release only adds a small number of changes since last week&#8217;s CR1 release;
most prominently there&#8217;s some more metrics for the SQL Server connector
(lag behind master, number of transactions etc.)
and two bug fixes related to the handling of partitioned tables in MySQL (DBZ-1113) and Postgres (DBZ-1118).


Having been in the works for six months after the initial Alpha release,
Debezium 0.9 comes with a brand new connector for SQL Server,
lots of new features and improvements for the existing connectors,
updates to the latest versions of Apache Kafka and the supported databases
as well as a...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m delighted to announce the release of Debezium &lt;strong&gt;0.9 Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release only adds a small number of changes since last week’s CR1 release;
most prominently there’s some more metrics for the SQL Server connector
(lag behind master, number of transactions etc.)
and two bug fixes related to the handling of partitioned tables in MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1113&quot;&gt;DBZ-1113&lt;/a&gt;) and Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1118&quot;&gt;DBZ-1118&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having been in the works for six months after the initial Alpha release,
Debezium 0.9 comes with a brand new &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;connector for SQL Server&lt;/a&gt;,
lots of new features and improvements for the existing connectors,
updates to the latest versions of Apache Kafka and the supported databases
as well as a wide range of bug fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some key features of the release besides the aforementioned CDC connector for SQL Server are:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initial snapshotting for the Oracle connector (which remains to be a &quot;tech preview&quot; at this point)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Brand-new metrics for the SQL Server and Oracle connectors and extended metrics for the MySQL connector&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Field filtering and renaming for MongoDB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A new handler interface for the embedded engine&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lots of improvements around the &quot;event flattening&quot; SMT for MongoDB&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;More detailed source info in CDC events and optional metadata such as a column’s source type&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option to delay snapshots for a given time&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for HSTORE columns in Postgres&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Incubating support for picking up changes to the whitelist/blacklist configuration of the MySQL connector&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a teaser on the connector metrics support, here’s a screenshot of &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Java Mission Control&lt;/a&gt;
displaying the SQL Server connector metrics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/java_mission_control.png&quot; style=&quot;max-width:100%; margin-bottom:10px; margin-top:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The list above is far from being exhaustive; please take a look at the preview release announcements
(&lt;a href=&quot;https://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/&quot;&gt;Alpha1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/&quot;&gt;Alpha2&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/&quot;&gt;Beta1&lt;/a&gt;,
&lt;a href=&quot;https://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/&quot;&gt;Beta2&lt;/a&gt; and
&lt;a href=&quot;https://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
as well as the full list of a whopping &lt;a href=&quot;https://issues.redhat.com/issues/?jql=project%20%3D%20DBZ%20AND%20fixVersion%20in%20(0.9.0.Alpha1%2C%200.9.0.Alpha2%2C%200.9.0.Beta1%2C%200.9.0.Beta2%2C%200.9.0.CR1%2C%200.9.0.Final)%20ORDER%20BY%20issuetype%20ASC&amp;amp;startIndex=120&quot;&gt;176 fixed issues&lt;/a&gt; in JIRA.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s hard to say which of the changes and new features I’m most excited about,
but one thing surely sticking out is the tremendous amount of community work on this release.
Not less than 34 different members of Debezium’s outstanding community have contributed to this release.
A huge and massive &quot;Thank You!&quot; to all of you:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/amitsela&quot;&gt;Amit Sela&lt;/a&gt;,
&lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
&lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
&lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
&lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gaganpaytm&quot;&gt;Gagan Agrawal&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
&lt;a href=&quot;https://github.com/Ipshin&quot;&gt;Ilia Bogdanov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivankovbas&quot;&gt;Ivan Kovbas&lt;/a&gt;,
&lt;a href=&quot;https://github.com/jgao54&quot;&gt;Joy Gao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/kppullin&quot;&gt;Kevin Pullin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt;,
&lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
&lt;a href=&quot;https://github.com/olivierlemasle&quot;&gt;Olivier Lemasle&lt;/a&gt;,
&lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
&lt;a href=&quot;https://github.com/plarsson&quot;&gt;Peter Larsson&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt;,
&lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;,
&lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;,
&lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
&lt;a href=&quot;https://github.com/Tapppi&quot;&gt;Tapani Moilanen&lt;/a&gt;,
&lt;a href=&quot;https://github.com/trizko&quot;&gt;Tony Rizko&lt;/a&gt;
&lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and
&lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When upgrading from earlier Debezium releases,
please make sure to read the information regarding update procedures and breaking changes in the &lt;a href=&quot;https://debezium.io/docs/releases/&quot;&gt;release notes&lt;/a&gt;.
One relevant change to the users of the Debezium connector for MySQL is that our new Antlr-based DDL parser is used by default now.
After lots of honing we felt it’s time for using the new parser by default now.
While the existing parser can still be used as a fallback as of Debezium 0.9,
it will be phased out in 0.10.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot; /&gt;Next Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After some drinks to celebrate this release, the plan is to do a 0.9.1 release rather quickly
(probably in two weeks from now),
providing improvements and potential bug fixes to the features and changes done in 0.9.
We’ll also begin the work on Debezium 0.10,
stay tuned for the details on that!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For further plans beyond that, take a look at our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;road map&lt;/a&gt;.
Any suggestions and ideas are very welcomed on &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’re just about to begin using Debezium for streaming changes out of your database,
you might be interested in join us for the &lt;a href=&quot;https://www.redhat.com/en/events/webinar/change-data-streaming-patterns-microservices-kafka-and-debezium&quot;&gt;upcoming webinar&lt;/a&gt; on February 7th.
After a quick overview, you’ll see Debezium in action, as it streams changes to a browser-based dashboard and more.
You can also find lots of resources around Debezium and change data capture such as blog posts and presentations in our curated &lt;a href=&quot;https://debezium.io/docs/online-resources/&quot;&gt;list of online resources&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/</id>
<title>Debezium 0.9.0.CR1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2019-01-28T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2019/01/28/debezium-0-9-0-cr1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



Reaching the home stretch towards Debezium 0.9, it&#8217;s with great pleasure that I&#8217;m announcing the first release of Debezium in 2019, 0.9.0.CR1!


For this release we&#8217;ve mainly focused on sorting out remaining issues in the Debezium connector for SQL Server;
the connector comes with greatly improved performance and has received a fair number of bug fixes.


Other changes include a new interface for event handlers of Debezium&#8217;s embedded engine,
which allows for bulk handling of change events, an option to export the scale of numeric columns as schema parameter,
as well as a wide range of bug fixes for the Debezium connectors for MySQL, Postgres...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Reaching the home stretch towards Debezium 0.9, it’s with great pleasure that I’m announcing the first release of Debezium in 2019, &lt;strong&gt;0.9.0.CR1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For this release we’ve mainly focused on sorting out remaining issues in the Debezium &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;connector for SQL Server&lt;/a&gt;;
the connector comes with greatly improved performance and has received a fair number of bug fixes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Other changes include a new interface for event handlers of Debezium’s &lt;a href=&quot;https://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;,
which allows for bulk handling of change events, an option to export the scale of numeric columns as schema parameter,
as well as a wide range of bug fixes for the Debezium connectors for MySQL, Postgres and Oracle.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sql_server_connector_improvements&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sql_server_connector_improvements&quot; /&gt;SQL Server Connector Improvements&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The SQL Server connector supports blacklisting of specific columns now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1067&quot;&gt;DBZ-1067&lt;/a&gt;).
That’s useful in cases where you’d like to exclude specific columns from emitted change data messages, e.g. to data protection considerations.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;snapshot locking mode&quot; option has been reworked (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-947&quot;&gt;DBZ-947&lt;/a&gt;) and is named &quot;snapshot isolation mode&quot; now,
better reflecting its semantics.
A new mode &quot;repeatable_read&quot; has been added, and &quot;none&quot; has been renamed to &quot;read_uncommitted&quot;.
Please see the connector documentation and the &lt;a href=&quot;https://debezium.io/docs/releases/#breaking_changes&quot;&gt;migration notes&lt;/a&gt; for more details.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The connector allows for a much higher through-put now, thanks to caching of timestamps for the same LSN (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1078&quot;&gt;DBZ-1078&lt;/a&gt;).
Please refer to the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-cr1&quot;&gt;change log&lt;/a&gt; for details on bugs fixed in this connector.
A massive &quot;Thank You&quot; is in order to Grzegorz Kołakowski, for his tireless work on and testing of this connector!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;new_embedded_engine_handler_interface&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#new_embedded_engine_handler_interface&quot; /&gt;New Embedded Engine Handler Interface&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s embedded engine now comes with a new interface &lt;code&gt;ChangeConsumer&lt;/code&gt;,
which event handlers can implement if they’d like to process change events in bulks (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1080&quot;&gt;DBZ-1080&lt;/a&gt;).
That can result in substantial performance improvements when pushing change events to APIs that apply batch semantics themselves,
such as the Kinesis Producer Library.
You can learn more in the &lt;a href=&quot;https://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt; docs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;misc_changes_and_bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#misc_changes_and_bug_fixes&quot; /&gt;Misc. Changes and Bug Fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All the relational connectors allow now to propagate the scale of numeric columns as a schema parameter
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1073&quot;&gt;DBZ-1073&lt;/a&gt;).
This is controlled via the &lt;code&gt;column.propagate.source.type&lt;/code&gt; option and builds on the exposure of type name and width added in Debezium 0.8.
All these schema parameters can be used when creating the schema of corresponding tables in sink databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium’s container image for Apache Kafka allows to create and watch topics now
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1057&quot;&gt;DBZ-1057&lt;/a&gt;).
You also can specify a clean-up policy when creating a topic
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1038&quot;&gt;DBZ-1038&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium MySQL connector handles unsigned &lt;code&gt;SMALLINT&lt;/code&gt; columns as expected now.
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1063&quot;&gt;DBZ-1063&lt;/a&gt;).
For nullable columns with a default value, &lt;code&gt;NULL&lt;/code&gt; values are correctly exported
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1064&quot;&gt;DBZ-1064&lt;/a&gt;; previously, the default value would have been exported in that case).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Postgres connector handles tables without a primary key correctly now
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1029&quot;&gt;DBZ-1029&lt;/a&gt;).
We’ve also applied a fix to make sure that the connector works with Postgres on Amazon RDS,
which recently was broken due to an update of wal2json in RDS
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1083&quot;&gt;DBZ-1083&lt;/a&gt;).
Going forward, we’re planning to set-up CI jobs to test against Postgres on RDS in all the versions supported by the Debezium connector.
This will help us to spot similar issues early on and react quickly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-cr1&quot;&gt;change log&lt;/a&gt; for the complete list of all addressed issues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release wouldn’t have been possible without all the contributions by the following members of the Debezium community:
&lt;a href=&quot;https://github.com/addisonj&quot;&gt;Addison Higham&lt;/a&gt;,
&lt;a href=&quot;https://github.com/amitsela&quot;&gt;Amit Sela&lt;/a&gt;,
&lt;a href=&quot;https://github.com/gaganpaytm&quot;&gt;Gagan Agrawal&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/Ipshin&quot;&gt;Ilia Bogdanov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ivankovbas&quot;&gt;Ivan Kovbas&lt;/a&gt;,
&lt;a href=&quot;https://github.com/mtagle&quot;&gt;Moira Tagle&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt; and
&lt;a href=&quot;https://github.com/trizko&quot;&gt;Tony Rizko&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot; /&gt;Next Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The CR1 release took us a bit longer than anticipated.
The release of Debezium 0.9.0.Final will therefore be moved to early February.
Rather quickly thereafter we’re planning to release Debezium 0.9.1,
which will provide improvements and potential bugfixes to the features added in 0.9.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For further plans beyond that, check out our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;road map&lt;/a&gt;.
If you got any feedback or suggestions for future additions, please get in touch via the &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/</id>
<title>Debezium 0.9.0.Beta2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-12-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/12/19/debezium-0-9-0-beta2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



With only a few days left for the year, it&#8217;s about time for another Debezium release;
so it&#8217;s with great pleasure that I&#8217;m announcing Debezium 0.9.0.Beta2!


This release comes with support for MySQL 8 and Oracle 11g;
it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
Overall, not less than 42 issues were addressed;
very clearly, there has to be some deeper sense in that ;)


A big shout out goes to the following members Debezium&#8217;s amazing community, who contributed to this release:
Eero Koplimets,...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With only a few days left for the year, it’s about time for another Debezium release;
so it’s with great pleasure that I’m announcing Debezium &lt;strong&gt;0.9.0.Beta2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release comes with support for MySQL 8 and Oracle 11g;
it includes a first cut of metrics for monitoring the SQL Server and Oracle connectors,
several improvements to the MongoDB event flattening SMT as well as a wide range of bug fixes.
Overall, not less than &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;42 issues&lt;/a&gt; were addressed;
very clearly, there has to be &lt;a href=&quot;https://en.wikipedia.org/wiki/Phrases_from_The_Hitchhiker%27s_Guide_to_the_Galaxy#Answer_to_the_Ultimate_Question_of_Life%2C_the_Universe%2C_and_Everything_%2842%29&quot;&gt;some deeper sense&lt;/a&gt; in that ;)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big shout out goes to the following members Debezium’s amazing community, who contributed to this release:
&lt;a href=&quot;https://github.com/pimpelsang&quot;&gt;Eero Koplimets&lt;/a&gt;, &lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Hanlin Liu&lt;/a&gt;, &lt;a href=&quot;https://github.com/sweat123&quot;&gt;Lao Mei&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;, &lt;a href=&quot;https://github.com/tautautau&quot;&gt;Tautvydas Januskevicius&lt;/a&gt;,
&lt;a href=&quot;https://github.com/wscheep&quot;&gt;Wout Scheepers&lt;/a&gt; and &lt;a href=&quot;https://github.com/wangzheng422&quot;&gt;Zheng Wang&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the following, let’s take a closer look at some of the changes coming with the 0.9 Beta2 release.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#monitoring_and_metrics_for_the_sql_server_and_oracle_connectors&quot; /&gt;Monitoring and Metrics for the SQL Server and Oracle Connectors&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Following the example of the MySQL connector, the connectors for &lt;a href=&quot;https://debezium.io/docs/connectors/sqlserver/&quot;&gt;SQL Server&lt;/a&gt; and &lt;a href=&quot;https://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle&lt;/a&gt; now expose a range of metrics for monitoring purposes via JMX (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-978&quot;&gt;DBZ-978&lt;/a&gt;).
This includes values like the time since the last CDC event, offset of the last event, the total number of events, remaining and already scanned tables while doing a snapshot and much more.
Please see &lt;a href=&quot;https://debezium.io/docs/monitoring/&quot;&gt;the monitoring documentation&lt;/a&gt; for details on how to enable JMX.
The following image shows an example of displaying the values in OpenJDK’s &lt;a href=&quot;https://openjdk.java.net/projects/jmc/&quot;&gt;Mission Control&lt;/a&gt; tool:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/monitoring_mission_control.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Monitoring the Debezium SQL Server connector&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re planning to expand the set of exposed metrics in future versions and also make them available for Postgres and MongoDB.
Please let us know about the metrics you’d like to see by commenting on JIRA issue &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1040&quot;&gt;DBZ-1040&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As a bonus, we’ve also created a Grafana dashboard for visualizing all the relevant metrics:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/monitoring_dashboard.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Connector metrics in Grafana&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ll blog about monitoring and the dashboard in more detail soon;
but if you are interested, you already can take a look at &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/monitoring&quot;&gt;this demo&lt;/a&gt; in our examples repository.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;misc_features&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#misc_features&quot; /&gt;Misc. Features&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;snapshot.delay.ms&quot; option already known from the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;Debezium MySQL connector&lt;/a&gt; is now available for all other Debezium connectors, too (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-966&quot;&gt;DBZ-966&lt;/a&gt;).
This comes in handy when deploying multiple connectors to a Kafka Connect cluster,
which may cause rebalancing the connectors in the cluster,
interrupting and restarting running snapshots of already deployed connector instances.
This can be avoided by specifying a delay which allows to wait with the snapshotting until the rebalancing phase is completed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://debezium.io/docs/configuration/mongodb-event-flattening/&quot;&gt;MongoDB CDC Event Flattening&lt;/a&gt; transformation received a number of improvements:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Support for MongoDB’s &lt;code&gt;$unset&lt;/code&gt; operator (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-612&quot;&gt;DBZ-612&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Support for full document updates (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-987&quot;&gt;DBZ-987&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New option for dropping delete and tombstone messages (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-563&quot;&gt;DBZ-563&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Option to convey the original type of operation as a header parameter (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-971&quot;&gt;DBZ-971&lt;/a&gt;);
that option is also available for the &lt;a href=&quot;https://debezium.io/docs/configuration/event-flattening/&quot;&gt;Flattening SMT&lt;/a&gt; for the relational connectors and can be useful in case sink connectors need to differentiate between inserts and updates&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes&quot; /&gt;Bug fixes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As always, we’ve also fixed a good number of bugs reported by Debezium users.
The set of fixed issues includes:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Several bugs related to streaming changes from MySQL in GTID mode (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-923&quot;&gt;DBZ-923&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1005&quot;&gt;DBZ-1005&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1008&quot;&gt;DBZ-1008&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Handling of tables with reserved names in the SQL Server connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1031&quot;&gt;DBZ-1031&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Potential event loss after MySQL connector restart (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-1033&quot;&gt;DBZ-1033&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unchanged values of TOASTed columns caused the Postgres connector to fail (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-842&quot;&gt;DBZ-842&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-beta2&quot;&gt;change log&lt;/a&gt; for the complete list of addressed issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;next_steps&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#next_steps&quot; /&gt;Next Steps&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re planning to do a candidate release of Debezium 0.9 in early January.
Provided no critical issues show up, Debezium 0.9.0.Final should be out by the end of January.
For the CR we’ve mostly scheduled a number of further bug fixes, improvements to the SQL Server connector and the addition of further metrics.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In parallel, we’ll focus our attention on the Oracle connector again, finally getting back to the long-awaited LogMiner-based capture implementation (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-137&quot;&gt;DBZ-137&lt;/a&gt;).
This will be a primary feature of Debezium 0.10.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, we’ll spend some cycles on the blogging and demo side of things;
namely we’re thinking about writing on and demoing the new monitoring and metrics support,
HA architectures including failover with MySQL, HAProxy and Debezium,
as well as enriching CDC events with contextual information such as the current user or use case identifiers.
Stay tuned!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also going beyond 0.10, we got some &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;great plans&lt;/a&gt; for Debezium in the coming year.
If you’d like to bring in your ideas, too, please let us know on the &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; or in the comments below,
we’re looking forward to hearing from you.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And with that, all that remains to be said, is &lt;a href=&quot;https://en.wikipedia.org/wiki/Festivus&quot;&gt;&quot;Happy Festivus for the rest of us!&quot;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Happy change data streaming and see you in 2019!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/</id>
<title>Automating Cache Invalidation With Change Data Capture</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-12-05T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/12/05/automating-cache-invalidation-with-change-data-capture/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="featured"></category>
<summary>



The second-level cache of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
caching read-only or rarely modified entities avoids roundtrips to the database,
resulting in improved response times of the application.


Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
so its contents are shared across transactions and concurrent sessions.
Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://docs.jboss.org/hibernate/stable/orm/userguide/html_single/Hibernate_User_Guide.html#caching-config&quot;&gt;second-level cache&lt;/a&gt; of Hibernate ORM / JPA is a proven and efficient way to increase application performance:
caching read-only or rarely modified entities avoids roundtrips to the database,
resulting in improved response times of the application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unlike the first-level cache, the second-level cache is associated with the session factory (or entity manager factory in JPA terms),
so its contents are shared across transactions and concurrent sessions.
Naturally, if a cached entity gets modified, the corresponding cache entry must be updated (or purged from the cache), too.
As long as the data changes are done through Hibernate ORM, this is nothing to worry about: the ORM will update the cache automatically.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Things get tricky, though, when bypassing the application, e.g. when modifying records directly in the database.
Hibernate ORM then has no way of knowing that the cached data has become stale, and it’s necessary to invalidate the affected items explicitly.
A common way for doing so is to foresee some admin functionality that allows to clear  an application’s caches.
For this to work, it’s vital to not forget about calling that invalidation functionality, or the application will keep working with outdated cached data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In the following we’re going to explore an alternative approach for cache invalidation, which works in a reliable and fully automated way:
by employing Debezium and its &lt;a href=&quot;https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/&quot;&gt;change data capture&lt;/a&gt; (CDC) capabilities, you can track data changes in the database itself and react to any applied change.
This allows to invalidate affected cache entries in near-realtime,
without the risk of stale data due to missed changes.
If an entry has been evicted from the cache, Hibernate ORM will load the latest version of the entity from the database the next time is requested.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_example_application&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_example_application&quot; /&gt;The Example Application&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As an example, consider this simple model of two entities, &lt;code&gt;PurchaseOrder&lt;/code&gt; and &lt;code&gt;Item&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cache_invalidation_class_diagram.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Example domain model&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A purchase order represents the order of an item, where its total price is the ordered quantity times the item’s base price.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Source Code&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/&quot;&gt;source code&lt;/a&gt; of this example is provided on GitHub.
If you want to follow along and try out all the steps described in the following,
clone the repo and follow the instructions in &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/cache-invalidation/_README.md&quot;&gt;README.md&lt;/a&gt; for building the project.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Modelling order and item as JPA entities is straight-forward:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
public class PurchaseOrder {

    @Id
    @GeneratedValue(generator = &quot;sequence&quot;)
    @SequenceGenerator(
        name = &quot;sequence&quot;, sequenceName = &quot;seq_po&quot;, initialValue = 1001, allocationSize = 50
    )
    private long id;
    private String customer;
    @ManyToOne private Item item;
    private int quantity;
    private BigDecimal totalPrice;

    // ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As changes to items are rare, the &lt;code&gt;Item&lt;/code&gt; entity should be cached.
This can be done by simply specifying JPA’s &lt;a href=&quot;https://docs.oracle.com/javaee/7/api/javax/persistence/Cacheable.html&quot;&gt;@Cacheable&lt;/a&gt; annotation:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
@Cacheable
public class Item {

    @Id
    private long id;
    private String description;
    private BigDecimal price;

    // ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You also need to enable the second-level cache in the &lt;em&gt;META-INF/persistence.xml&lt;/em&gt; file.
The property &lt;code&gt;hibernate.cache.use_second_level_cache&lt;/code&gt; activates the cache itself, and the &lt;code&gt;ENABLE_SELECTIVE&lt;/code&gt; cache mode
causes only those entities to be put into the cache which are annotated with &lt;code&gt;@Cacheable&lt;/code&gt;.
It’s also a good idea to enable SQL query logging and cache access statistics.
That way you’ll be able to verify whether things work as expected by examining the application log:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&amp;gt;
&amp;lt;persistence xmlns=&quot;http://xmlns.jcp.org/xml/ns/persistence&quot;
    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
    xsi:schemaLocation=&quot;...&quot;
    version=&quot;2.2&quot;&amp;gt;

    &amp;lt;persistence-unit name=&quot;orders-PU-JTA&quot; transaction-type=&quot;JTA&quot;&amp;gt;
        &amp;lt;jta-data-source&amp;gt;java:jboss/datasources/OrderDS&amp;lt;/jta-data-source&amp;gt;
        &amp;lt;shared-cache-mode&amp;gt;ENABLE_SELECTIVE&amp;lt;/shared-cache-mode&amp;gt;
        &amp;lt;properties&amp;gt;
            &amp;lt;property name=&quot;hibernate.cache.use_second_level_cache&quot; value=&quot;true&quot; /&amp;gt;

            &amp;lt;property name=&quot;hibernate.show_sql&quot; value=&quot;true&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.format_sql&quot; value=&quot;true&quot; /&amp;gt;
            &amp;lt;property name=&quot;hibernate.generate_statistics&quot; value=&quot;true&quot; /&amp;gt;

            &amp;lt;!-- dialect etc. ... --&amp;gt;
        &amp;lt;/properties&amp;gt;
    &amp;lt;/persistence-unit&amp;gt;
&amp;lt;/persistence&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When running on a &lt;a href=&quot;https://www.oracle.com/technetwork/java/javaee/overview/index.html&quot;&gt;Java EE&lt;/a&gt; application server
(or &lt;a href=&quot;https://jakarta.ee/&quot;&gt;Jakarta EE&lt;/a&gt; how the stack is called after it has been donated to the Eclipse Foundation),
that’s all you need to enable second-level caching.
In the case of &lt;a href=&quot;http://wildfly.org/&quot;&gt;WildFly&lt;/a&gt; (which is what’s used in the example project), the &lt;a href=&quot;http://infinispan.org/&quot;&gt;Infinispan&lt;/a&gt; key/value store is used as the cache provider by default.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now try and see what happens when modifying an item’s price by running some SQL in the database,
bypassing the application layer.
If you’ve checked out the example source code, comment out the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class and start the application as described in the &lt;em&gt;README.md&lt;/em&gt;.
You then can place purchase orders using curl like this
(a couple of example items have been persisted at application start-up):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
  -X POST \
  --data &#39;{ &quot;customer&quot; : &quot;Billy-Bob&quot;, &quot;itemId&quot; : 10003, &quot;quantity&quot; : 2 }&#39; \
  http://localhost:8080/cache-invalidation/rest/orders&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;{
    &quot;id&quot; : 1002,
    &quot;customer&quot; : &quot;Billy-Bob&quot;,
    &quot;item&quot; : {
        &quot;id&quot; :10003,
        &quot;description&quot; : &quot;North By Northwest&quot;,
        &quot;price&quot; : 14.99
    },
    &quot;quantity&quot; : 2,
    &quot;totalPrice&quot; : 29.98
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The response is the expected one, as the item price is 14.99.
Now update the item’s price directly in the database.
The example uses Postgres, so you can use the &lt;em&gt;psql&lt;/em&gt; CLI utility to do so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;docker-compose exec postgres bash -c &#39;psql -U $POSTGRES_USER $POSTGRES_DB -c &quot;UPDATE item SET price = 20.99 where id = 10003&quot;&#39;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Placing another purchase order for the same item using curl,
you’ll see that the calculated total price doesn’t reflect the update.
Not good!
But it’s not too surprising, given that the price update was applied completely bypassing the application layer and Hibernate ORM.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;the_change_event_handler&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#the_change_event_handler&quot; /&gt;The Change Event Handler&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s explore how to use Debezium and CDC to react to changes in the &lt;code&gt;item&lt;/code&gt; table and invalidate corresponding cache entries.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While Debezium most of the times is deployed into &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt; (thus streaming change events into Apache Kafka topics),
it has another mode of operation that comes in very handy for the use case at hand.
Using the &lt;a href=&quot;https://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;, you can run the Debezium connectors as a library directly within your application.
For each change event received from the database, a configured callback method will be invoked, which in the case at hand will evict the affected item from the second-level cache.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following picture shows the design of this approach:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cache_invalidation_architecture.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While this doesn’t come with the scalability and fault tolerance provided by Apache Kafka,
it nicely fits the given requirements.
As the second-level cache is bound to the application lifecycle, there is for instance no need for the offset management and restarting capabilities provided by the Kafka Connect framework.
For the given use case it is enough to receive data change events while the application is running, and using the embedded engine enables exactly that.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Clustered Applications&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that it still might make sense to use Apache Kafka and the regular deployment of Debezium into Kafka Connect when running a clustered application where each node has a local cache.
Instead of registering a connector on each node, Kafka and Connect would allow you to deploy a single connector instance and have the application nodes listen to the topic(s) with the change events.
This would result in less resource utilization in the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Having added the dependencies of the Debezium embedded engine (&lt;em&gt;io.debezium:debezium-embedded:0.9.0.Beta1&lt;/em&gt;) and the Debezium Postgres connector (&lt;em&gt;io.debezium:debezium-connector-postgres:0.9.0.Beta1&lt;/em&gt;) to your project,
a class &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; for listening to any changes in the database can be implemented like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class DatabaseChangeEventListener {

    @Resource
    private ManagedExecutorService executorService;

    @PersistenceUnit private EntityManagerFactory emf;

    @PersistenceContext
    private EntityManager em;

    private EmbeddedEngine engine;

    public void startEmbeddedEngine(@Observes @Initialized(ApplicationScoped.class) Object init) {
        Configuration config = Configuration.empty()
                .withSystemProperties(Function.identity()).edit()
                .with(EmbeddedEngine.CONNECTOR_CLASS, PostgresConnector.class)
                .with(EmbeddedEngine.ENGINE_NAME, &quot;cache-invalidation-engine&quot;)
                .with(EmbeddedEngine.OFFSET_STORAGE, MemoryOffsetBackingStore.class)
                .with(&quot;name&quot;, &quot;cache-invalidation-connector&quot;)
                .with(&quot;database.hostname&quot;, &quot;postgres&quot;)
                .with(&quot;database.port&quot;, 5432)
                .with(&quot;database.user&quot;, &quot;postgresuser&quot;)
                .with(&quot;database.password&quot;, &quot;postgrespw&quot;)
                .with(&quot;database.server.name&quot;, &quot;dbserver1&quot;)
                .with(&quot;database.dbname&quot;, &quot;inventory&quot;)
                .with(&quot;database.whitelist&quot;, &quot;public&quot;)
                .with(&quot;snapshot.mode&quot;, &quot;never&quot;)
                .build();

        this.engine = EmbeddedEngine.create()
                .using(config)
                .notifying(this::handleDbChangeEvent)
                .build();

        executorService.execute(engine);
    }

    @PreDestroy
    public void shutdownEngine() {
        engine.stop();
    }

    private void handleDbChangeEvent(SourceRecord record) {
        if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
            Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
            Struct payload = (Struct) record.value();
            Operation op = Operation.forCode(payload.getString(&quot;op&quot;));

            if (op == Operation.UPDATE || op == Operation.DELETE) {
                emf.getCache().evict(Item.class, itemId);
            }
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Upon application start-up, this configures an instance of the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql/&quot;&gt;Debezium Postgres connector&lt;/a&gt; and sets up the embedded engine for running the connector.
The &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql/#connector-properties&quot;&gt;connector options&lt;/a&gt; (host name, credentials etc.) are mostly the same as when deploying the connector into Kafka Connect.
There is no need for doing an initial snapshot of the existing data, hence the &lt;a href=&quot;https://debezium.io/docs/connectors/postgresql/#snapshots&quot;&gt;snapshot mode&lt;/a&gt; is set to &quot;never&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The offset storage option is used for controlling how connector offsets should be persisted.
As it’s not necessary to process any change events occurring while the connector is not running
(instead you’d just begin to read the log from the current location after the restart),
the in-memory implementation provided by Kafka Connect is used.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once configured, the embedded engine must be run via an &lt;code&gt;Executor&lt;/code&gt; instance.
As the example runs in WildFly, a managed executor can simply be obtained through &lt;code&gt;@Resource&lt;/code&gt; injection for that purpose (see &lt;a href=&quot;https://www.jcp.org/en/jsr/detail?id=236&quot;&gt;JSR 236&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The embedded engine is configured to invoke the &lt;code&gt;handleDbChangeEvent()&lt;/code&gt; method for each received data change event.
In this method it first is checked whether the incoming event originates from the &lt;code&gt;item&lt;/code&gt; table.
If that’s the case, and if the change event represents an &lt;code&gt;UPDATE&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt; statement,
the affected &lt;code&gt;Item&lt;/code&gt; instance is evicted from the second-level cache.
JPA 2.0 provides a &lt;a href=&quot;https://javaee.github.io/javaee-spec/javadocs/index.html?javax/persistence/Cache.html&quot;&gt;simple API&lt;/a&gt; for this purpose which is accessible via the &lt;code&gt;EntityManagerFactory&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With the &lt;code&gt;DatabaseChangeEventListener&lt;/code&gt; class in place, the cache entry will now automatically be evicted when doing another item update via &lt;em&gt;psql&lt;/em&gt;.
When placing the first purchase order for that item after the update, you’ll see in the application log how Hibernate ORM executes a query &lt;code&gt;SELECT ... FROM item ...&lt;/code&gt; in order to load the item referenced by the order.
Also the cache statistics will report one &quot;L2C miss&quot;.
Upon subsequent orders of that same item it will be obtained from the cache again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Eventual Consistency&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the event handling happens in near-realtime, it’s important to point out that it still applies eventual consistency semantics.
This means that there is a very short time window between the point in time where a transaction is committed
and the point in time where the change event is streamed from the log to the event handler and the cache entry is invalidated.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;avoiding_cache_invalidations_after_application_triggered_data_changes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#avoiding_cache_invalidations_after_application_triggered_data_changes&quot; /&gt;Avoiding Cache Invalidations After Application-triggered Data Changes&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The change event listener shown above satisfies the requirement of invalidating cached items after external data changes.
But in its current form it is evicting cache items a bit too aggressively:
cached items will also be purged when updating an &lt;code&gt;Item&lt;/code&gt; instance through the application itself.
This is not only not needed (as the cached item already is the current version), but it’s even counter-productive:
the superfluous cache evictions will cause additional database roundtrips, resulting in longer response times.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is therefore necessary to distinguish between data changes performed by the application itself and external data changes.
Only in the latter case the affected items should be evicted from the cache.
In order to do so, you can leverage the fact that each Debezium data change event contains the id of the originating transaction.
Keeping track of all transactions run by the application itself allows to trigger the cache eviction only for those items altered by external transactions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Accounting for this change, the overall architecture looks like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock centered-image&quot;&gt;
    &lt;img src=&quot;https://debezium.io/images/cache_invalidation_architecture_tx_registry.png&quot; style=&quot;max-width:100%; margin-bottom:20px; margin-top:20px;&quot; class=&quot;responsive-image&quot; alt=&quot;Architecture Overview with Transaction Registry&quot;&gt;
&lt;/img&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first thing to implement is the transaction registry, i.e. a class for the transaction book keeping:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class KnownTransactions {

    private final DefaultCacheManager cacheManager;
    private final Cache&amp;lt;Long, Boolean&amp;gt; applicationTransactions;

    public KnownTransactions() {
        cacheManager = new DefaultCacheManager();
        cacheManager.defineConfiguration(
                &quot;tx-id-cache&quot;,
                new ConfigurationBuilder()
                    .expiration()
                        .lifespan(60, TimeUnit.SECONDS)
                    .build()
                );

        applicationTransactions = cacheManager.getCache(&quot;tx-id-cache&quot;);
    }

    @PreDestroy
    public void stopCacheManager() {
        cacheManager.stop();
    }

    public void register(long txId) {
        applicationTransactions.put(txId, true);
    }

    public boolean isKnown(long txId) {
        return Boolean.TRUE.equals(applicationTransactions.get(txId));
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This uses the Infinispan &lt;code&gt;DefaultCacheManager&lt;/code&gt; for creating and maintaining an in-memory cache of transaction ids encountered by the application.
As data change events arrive in near-realtime, the TTL of the cache entries can be rather short
(in fact, the value of one minute shown in the example is chosen very conservatively, usually events should be received within seconds).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next step is to retrieve the current transaction id whenever a request is processed by the application and register it within &lt;code&gt;KnownTransactions&lt;/code&gt;.
This should happen once per transaction.
There are multiple ways for implementing this logic; in the following a Hibernate ORM &lt;code&gt;FlushEventListener&lt;/code&gt; is used for this purpose:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;class TransactionRegistrationListener implements FlushEventListener {

    private volatile KnownTransactions knownTransactions;

    public TransactionRegistrationListener() {
    }

    @Override
    public void onFlush(FlushEvent event) throws HibernateException {
        event.getSession().getActionQueue().registerProcess( session -&amp;gt; {
            Number txId = (Number) event.getSession().createNativeQuery(&quot;SELECT txid_current()&quot;)
                    .setFlushMode(FlushMode.MANUAL)
                    .getSingleResult();

            getKnownTransactions().register(txId.longValue());
        } );
    }

    private  KnownTransactions getKnownTransactions() {
        KnownTransactions value = knownTransactions;

        if (value == null) {
            knownTransactions = value = CDI.current().select(KnownTransactions.class).get();
        }

        return value;
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As there’s no portable way to obtain the transaction id, this is done using a native SQL query.
In the case of Postgres, the &lt;code&gt;txid_current()&lt;/code&gt; function can be called for that.
Hibernate ORM event listeners are not subject to dependency injection via CDI.
Hence the static &lt;code&gt;current()&lt;/code&gt; method is used to obtain a handle to the application’s CDI container and get a reference to the &lt;code&gt;KnownTransactions&lt;/code&gt; bean.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This listener will be invoked whenever Hibernate ORM is synchronizing its persistence context with the database (&quot;flushing&quot;),
which usually happens exactly once when the transaction is committed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;admonitionblock note&quot;&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td class=&quot;icon&quot;&gt;
&lt;i class=&quot;fa icon-note&quot; title=&quot;Note&quot; /&gt;
&lt;/td&gt;
&lt;td class=&quot;content&quot;&gt;
&lt;div class=&quot;title&quot;&gt;Manual Flushes&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The session / entity manager can also be flushed manually, in which case the &lt;code&gt;txid_current()&lt;/code&gt; function would be invoked multiple times.
That’s neglected here for the sake of simplicity.
The actual code in the example repo contains a slightly extended version of this class which makes sure that the transaction id is obtained only once.&lt;/p&gt;
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To register the flush listener with Hibernate ORM, an &lt;code&gt;Integrator&lt;/code&gt; implementation must be created and declared in the &lt;em&gt;META-INF/services/org.hibernate.integrator.spi.Integrator&lt;/em&gt; file:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;public class TransactionRegistrationIntegrator implements Integrator {

    @Override
    public void integrate(Metadata metadata, SessionFactoryImplementor sessionFactory,
            SessionFactoryServiceRegistry serviceRegistry) {
        serviceRegistry.getService(EventListenerRegistry.class)
            .appendListeners(EventType.FLUSH, new TransactionRegistrationListener());
    }

    @Override
    public void disintegrate(SessionFactoryImplementor sessionFactory,
            SessionFactoryServiceRegistry serviceRegistry) {
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io.debezium.examples.cacheinvalidation.persistence.TransactionRegistrationIntegrator&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;During bootstrap, Hibernate ORM will detect the integrator class (by means of the &lt;a href=&quot;https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/util/ServiceLoader.html&quot;&gt;Java service loader&lt;/a&gt;),
invoke its &lt;code&gt;integrate()&lt;/code&gt; method which in turn will register the listener class for the &lt;code&gt;FLUSH&lt;/code&gt; event.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The last step is to exclude any events stemming from transactions run by the application itself in the database change event handler:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@ApplicationScoped
public class DatabaseChangeEventListener {

    // ...

    @Inject
    private KnownTransactions knownTransactions;

    private void handleDbChangeEvent(SourceRecord record) {
        if (record.topic().equals(&quot;dbserver1.public.item&quot;)) {
            Long itemId = ((Struct) record.key()).getInt64(&quot;id&quot;);
            Struct payload = (Struct) record.value();
            Operation op = Operation.forCode(payload.getString(&quot;op&quot;));
            Long txId = ((Struct) payload.get(&quot;source&quot;)).getInt64(&quot;txId&quot;);

            if (!knownTransactions.isKnown(txId) &amp;amp;&amp;amp;
                    (op == Operation.UPDATE || op == Operation.DELETE)) {
                emf.getCache().evict(Item.class, itemId);
            }
        }
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And with that, you got all the pieces in place: cached &lt;code&gt;Item&lt;/code&gt;s will only be evicted after external data changes, but not after changes done by the application itself.
To confirm, you can invoke the example’s &lt;code&gt;items&lt;/code&gt; resource using curl:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&amp;gt; curl -H &quot;Content-Type: application/json&quot; \
  -X PUT \
  --data &#39;{ &quot;description&quot; : &quot;North by Northwest&quot;, &quot;price&quot; : 20.99}&#39; \
  http://localhost:8080/cache-invalidation/rest/items/10003&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When placing the next order for the item after this update, you should see that the &lt;code&gt;Item&lt;/code&gt; entity is obtained from the cache,
i.e. the change event will not have caused the item’s cache entry to be evicted.
In contrast, if you update the item’s price via &lt;em&gt;psql&lt;/em&gt; another time,
the item should be removed from the cache and the order request will produce a cache miss, followed by a &lt;code&gt;SELECT&lt;/code&gt; against the &lt;code&gt;item&lt;/code&gt; table in the database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this blog post we’ve explored how Debezium and change data capture can be employed to invalidate application-level caches after external data changes.
Compared to manual cache invalidation, this approach works very reliably
(by capturing changes directly from the database log, no events will be missed) and fast
(cache eviction happens in near-realtime after the data changes).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As you have seen, not too much glue code is needed in order to implement this.
While the shown implementation is somewhat specific to the entities of the example,
it should be possible to implement the change event handler in a more generic fashion,
so that it can handle a set of configured entity types
(essentially, the database change listener would have to convert the primary key field(s) from the change events into the primary key type of the corresponding entities in a generic way).
Also such generic implementation would have to provide the logic for obtaining the current transaction id for the most commonly used databases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please let us know whether you think this would be an interesting extension to have for Debezium and Hibernate ORM.
For instance this could be a new module under the Debezium umbrella,
and it could also be a very great project to work on, should you be interested in contributing to Debezium.
If you got any thoughts on this idea, please post a comment below or come to our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Many thanks to Guillaume Smet, Hans-Peter Grahsl and Jiri Pechanec for their feedback while writing this post!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/</id>
<title>Debezium 0.9.0.Beta1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-11-22T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/11/22/debezium-0-9-0-beta1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Beta1!
Oh, and to those of you who are celebrating it&#8201;&#8212;&#8201;Happy Thanksgiving!


This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:




Initial snapshots can be done using the snapshot isolation level if enabled in the DB (DBZ-941)


Changes to the structures of captured tables after the connector has been set up are supported now (DBZ-812)


New connector option decimal.handling.mode (DBZ-953) and pass-through of any database.* option to the JDBC driver (DBZ-964)




Besides that, we spent some time on supporting the latest versions of the different databases.
The Debezium connectors now support Postgres...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Beta1&lt;/strong&gt;!
Oh, and to those of you who are celebrating it — Happy Thanksgiving!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This new Debezium release comes with several great improvements to our work-in-progress SQL Server connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Initial snapshots can be done using the &lt;code&gt;snapshot&lt;/code&gt; isolation level if enabled in the DB (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-941&quot;&gt;DBZ-941&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Changes to the structures of captured tables after the connector has been set up are supported now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;New connector option &lt;code&gt;decimal.handling.mode&lt;/code&gt; (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-953&quot;&gt;DBZ-953&lt;/a&gt;) and pass-through of any &lt;code&gt;database.*&lt;/code&gt; option to the JDBC driver (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-964&quot;&gt;DBZ-964&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides that, we spent some time on supporting the latest versions of the different databases.
The Debezium connectors now support Postgres 11 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-955&quot;&gt;DBZ-955&lt;/a&gt;) and MongoDB 4.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-974&quot;&gt;DBZ-974&lt;/a&gt;).
We are also working on supporting MySQL 8.0, which should be completed in the next 0.9.x release.
The Debezium container images have been updated to Kafka 2.0.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-979&quot;&gt;DBZ-979&lt;/a&gt;)
and the Kafka Connect image now supports the &lt;code&gt;STATUS_STORAGE_TOPIC&lt;/code&gt; environment variable,
bringing consistency with &lt;code&gt;CONFIG_STORAGE_TOPIC&lt;/code&gt; and &lt;code&gt;OFFSET_STORAGE_TOPIC&lt;/code&gt; that already were supported before (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-893&quot;&gt;DBZ-893&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As usual, several bugs were fixed, too.
Several of them dealt with the new Antlr-based DDL parser for the MySQL connector.
By now we feel confident about its implementation, so it’s the default DDL parser as of this release (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-757&quot;&gt;DBZ-757&lt;/a&gt;).
If you would like to continue to use the legacy parser for some reason, you can do so by setting the &lt;code&gt;ddl.parser.mode&lt;/code&gt; connector option to &quot;legacy&quot;.
This implementation will remain available in the lifetime of Debezium 0.9.x and is scheduled for removal after that.
So please make sure to log issues in JIRA should you run into any problems with the Antlr parser.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Overall, this release contains &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-beta1&quot;&gt;21 fixes&lt;/a&gt;.
Thanks a lot to all the community members who helped with making this happen:
&lt;a href=&quot;https://github.com/anton-martynov&quot;&gt;Anton Martynov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
&lt;a href=&quot;https://github.com/grzegorz8&quot;&gt;Grzegorz Kołakowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/olavim&quot;&gt;Olavi Mustanoja&lt;/a&gt;,
&lt;a href=&quot;https://github.com/renatomefi&quot;&gt;Renato Mefi&lt;/a&gt;,
&lt;a href=&quot;https://github.com/vamossagar12&quot;&gt;Sagar Rao&lt;/a&gt; and
&lt;a href=&quot;https://github.com/shivamsharma&quot;&gt;Shivam Sharma&lt;/a&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;what_else&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#what_else&quot; /&gt;What else?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the work towards Debezium 0.9 continues, we’ve lately been quite busy with presenting Debezium at multiple conferences.
You can find the slides and recordings from &lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt; San Francisco and &lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;Voxxed Days Microservices&lt;/a&gt; on our list of &lt;a href=&quot;https://debezium.io/docs/online-resources/&quot;&gt;online resources&lt;/a&gt; around Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There you also can find the links to the slides of the great talk &quot;The Why’s and How’s of Database Streaming&quot; by Joy Gao of WePay, a Debezium user of the first hour,
as well as the link to a blog post by Hans-Peter Grahsl about setting up a CDC pipeline from MySQL into Cosmos DB running on Azure.
If you know about other great articles, session recordings or similar on Debezium and change data capture which should be added there, please let us know.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/</id>
<title>Debezium 0.9.0.Alpha2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-10-04T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/10/04/debezium-0-9-0-alpha2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="sqlserver"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



It&#8217;s my pleasure to announce the release of Debezium 0.9.0.Alpha2!


While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
as lots of fixes and new features - many of them contributed by community members - have piled up,
which we wanted to get into your hands as quickly as possible.


This release supports Apache Kafka 2.0, comes with support for Postgres' HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
and contains multiple bug fixes and performance improvements.
Overall, this release contains 55 fixes
(note that a few of these have...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It’s my pleasure to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While the work on the connectors for SQL Server and Oracle continues, we decided to do another Alpha release,
as lots of fixes and new features - many of them contributed by community members - have piled up,
which we wanted to get into your hands as quickly as possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This release supports Apache Kafka 2.0, comes with support for Postgres&#39; HSTORE column type, allows to rename and filter fields from change data messages for MongoDB
and contains multiple bug fixes and performance improvements.
Overall, this release contains &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-alpha2&quot;&gt;55 fixes&lt;/a&gt;
(note that a few of these have been merged back to 0.8.x and are contained in earlier 0.8 releases, too).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A big &quot;Thank You&quot; is in order to community members
&lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;,
&lt;a href=&quot;https://github.com/artiship&quot;&gt;Artiship Artiship&lt;/a&gt;,
&lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt;,
&lt;a href=&quot;https://github.com/deepakbarr&quot;&gt;Deepak Barr&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ian-axelrod&quot;&gt;Ian Axelrod&lt;/a&gt;,
&lt;a href=&quot;https://github.com/ooooorz&quot;&gt;Liu Hanlin&lt;/a&gt;,
&lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt;,
&lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt;,
&lt;a href=&quot;https://github.com/PSanetra&quot;&gt;Philip Sanetra&lt;/a&gt;,
&lt;a href=&quot;https://github.com/sagarrao&quot;&gt;Sagar Rao&lt;/a&gt; and
&lt;a href=&quot;https://github.com/SyedMuhammadSufyian&quot;&gt;Syed Muhammad Sufyian&lt;/a&gt;
for their contributions to this release.
We salute you!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;kafka_upgrade&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#kafka_upgrade&quot; /&gt;Kafka Upgrade&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium runs with and has been tested on top of the recently released Apache Kafka 2.0 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-858&quot;&gt;DBZ-858&lt;/a&gt;).
The widely used version Kafka 1.x continues to be supported as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that 0.10.x is not supported due to Debezium’s usage of the admin client API which is only available in later versions.
It shouldn’t be too hard to work around this, so if someone is interested in helping out with this,
this would be a great contribution (see &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-883&quot;&gt;DBZ-883&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;support_for_hstore_columns_in_postgres&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#support_for_hstore_columns_in_postgres&quot; /&gt;Support for HSTORE columns in Postgres&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Postgres is an amazingly powerful and flexible RDBMS, not the least due to its wide range of column types which go far beyond what’s defined by the SQL standard.
One of these types being &lt;a href=&quot;https://www.postgresql.org/docs/current/static/hstore.html&quot;&gt;HSTORE&lt;/a&gt;, which is a string-to-string map essentially.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium can capture changes to columns of this type now (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-898&quot;&gt;DBZ-898&lt;/a&gt;).
By default, the field values will be represented using Kafka Connect’s map data type.
As this may not be supported by all sink connectors,
you might alternatively represent them as a string-ified JSON by setting the new &lt;code&gt;hstore.handling.mode&lt;/code&gt; connector option to &lt;code&gt;json&lt;/code&gt;.
In this case, you’d see HSTORE columns represented as values in change messages like so: &lt;code&gt;{ &quot;key1&quot; : &quot;val1&quot;, &quot;key2&quot; : &quot;val2&quot; }&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;field_filtering_and_renaming_for_mongodb&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#field_filtering_and_renaming_for_mongodb&quot; /&gt;Field filtering and renaming for MongoDB&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unlike the connectors for MySQL and Postgres, the Debezium MongoDB connector so far didn’t allow to exclude single fields of captured collections from CDC messages.
Also renaming them wasn’t supported e.g. by means of Kafka’s &lt;code&gt;ReplaceField&lt;/code&gt; SMT.
The reason being that MongoDB doesn’t mandate a fixed schema for the documents of a given collection,
and documents therefore are represented in change messages using a single string-ified JSON field.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks to the fantastic work of community member Andrey Pustovetov,
this finally has changed, i.e. you can remove given fields (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-633&quot;&gt;DBZ-633&lt;/a&gt;) now from the CDC messages of given collections or have them renamed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-881&quot;&gt;DBZ-881&lt;/a&gt;).
Please refer to the description of the new connector options &lt;code&gt;field.blacklist&lt;/code&gt; and &lt;code&gt;field.renames&lt;/code&gt; in the &lt;a href=&quot;https://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector documentation&lt;/a&gt; to learn more.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;extended_source_info&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#extended_source_info&quot; /&gt;Extended source info&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another contribution by Andrey is the new optional &lt;code&gt;connector&lt;/code&gt; field within the source info block of CDC messages
(&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-918&quot;&gt;DBZ-918&lt;/a&gt;).
This tells the type of source connector that produced the messages (&quot;mysql&quot;, &quot;postgres&quot; etc.),
which can come in handy in cases where specific semantics need to be applied on the consumer side depending on the type of source database.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;bug_fixes_and_version_upgrades&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#bug_fixes_and_version_upgrades&quot; /&gt;Bug fixes and version upgrades&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The new release contains a good number of bug fixes and other smaller improvements.
Amongst them are&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;correct handling of invalid temporal default values with MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-927&quot;&gt;DBZ-927&lt;/a&gt;),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;support for table/collection names with special characters for MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-878&quot;&gt;DBZ-878&lt;/a&gt;) and MongoDB (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-865&quot;&gt;DBZ-865&lt;/a&gt;) and&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;fixed handling of blacklisted tables with the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-872&quot;&gt;DBZ-872&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Community member Ian Axelrod provided a fix for a potential performance issue,
where changes to tables with TOAST columns in Postgres would cause repeated updates to the connector’s internal schema metadata,
which can be a costly operation (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-911&quot;&gt;DBZ-911&lt;/a&gt;).
Please refer to the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector documentation&lt;/a&gt; for details on the new &lt;code&gt;schema.refresh.mode&lt;/code&gt; option,
which deals with this issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In terms of version upgrades we migrated to the latest releases of the MySQL (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-763&quot;&gt;DBZ-763&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-764&quot;&gt;DBZ-764&lt;/a&gt;) and Postgres drivers (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-912&quot;&gt;DBZ-912&lt;/a&gt;).
The former is part of a longer stream of work leading towards support of MySQL 8 which should be finished in one of the next Debezium releases.
For Postgres we provide a Docker image with Debezium’s supported logical decoding plug-ins based on Alpine now,
which might be interesting to those concerned about container size (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-705&quot;&gt;DBZ-705&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the change log for the complete list of fixed issues.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The work towards Debezium 0.9 continues, and we’ll focus mostly on improvements to the SQL Server and Oracle connectors.
Other potential topics include support for MySQL 8 and native logical decoding as introduced with Postgres 10,
which should greatly help with using the Debezium Postgres connectors in cloud environments such as Amazon RDS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ll also be talking about Debezium at the following conferences:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Already last week I had the opportunity to present Debezium at &lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;.
If you are interested, you can find the (German) &lt;a href=&quot;https://speakerdeck.com/gunnarmorling/streaming-von-datenbankanderungen-mit-debezium-jug-saxony-day&quot;&gt;slideset of that talk&lt;/a&gt; on Speaker Deck.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/</id>
<title>Materializing Aggregate Views With Hibernate and Debezium</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-09-20T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/09/20/materializing-aggregate-views-with-hibernate-and-debezium/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<category term="featured"></category>
<summary>



Updating external full text search indexes (e.g. Elasticsearch) after data changes is a very popular use case for change data capture (CDC).


As we&#8217;ve discussed in a blog post a while ago,
the combination of Debezium&#8217;s CDC source connectors and Confluent&#8217;s sink connector for Elasticsearch makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
which is perfectly fine for many use cases.


It gets more challenging though if you&#8217;d like to put entire aggregates into...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Updating external full text search indexes (e.g. &lt;a href=&quot;https://www.elastic.co/products/elasticsearch&quot;&gt;Elasticsearch&lt;/a&gt;) after data changes is a very popular use case for change data capture (CDC).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As we’ve discussed in a &lt;a href=&quot;https://debezium.io/blog/2018/01/17/streaming-to-elasticsearch/&quot;&gt;blog post&lt;/a&gt; a while ago,
the combination of Debezium’s CDC source connectors and Confluent’s &lt;a href=&quot;https://docs.confluent.io/current/connect/connect-elasticsearch/docs/index.html&quot;&gt;sink connector for Elasticsearch&lt;/a&gt; makes it straight forward to capture data changes in MySQL, Postgres etc. and push them towards Elastisearch in near real-time.
This results in a 1:1 relationship between tables in the source database and a corresponding search index in Elasticsearch,
which is perfectly fine for many use cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It gets more challenging though if you’d like to put entire aggregates into a single index.
An example could be a customer and all their addresses;
those would typically be stored in two separate tables in an RDBMS, linked by a foreign key,
whereas you’d like to have just one index in Elasticsearch,
containing documents of customers with their addresses embedded,
allowing you to efficiently search for customers based on their address.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Following up to the &lt;a href=&quot;https://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based solution&lt;/a&gt; to this we described recently,
we’d like to present in this post an alternative for materializing such aggregate views driven by the application layer.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;overview&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#overview&quot; /&gt;Overview&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is to materialize views in a separate table in the source database,
right in the moment the original data is altered.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Aggregates are serialized as JSON structures (which naturally can represent any nested object structure) and stored in a specific table.
This is done within the actual transaction altering the data,
which means the aggregate view is always consistent with the primary data.
In particular this approach isn’t prone to exposing intermediary aggregations as the KStreams-based solution discussed in the post linked above.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The following picture shows the overall architecture:&lt;/p&gt;
&lt;/div&gt;
&lt;img src=&quot;https://debezium.io/images/jpa_aggregations.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Streaming Materialized Aggregate Views to Elastisearch&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here the aggregate views are materialized by means of a small extension to &lt;a href=&quot;http://hibernate.org/orm/&quot;&gt;Hibernate ORM&lt;/a&gt;,
which stores the JSON aggregates within the source database
(note &quot;aggregate views&quot; can be considered conceptually the same as &quot;materialized views&quot; as known from different RDBMS,
as in that they materialize the result of a &quot;join&quot; operation,
but technically we’re not using the latter to store aggregate views, but a regular table).
Changes to that aggregate table are then captured by Debezium and streamed to one topic per aggregate type.
The Elasticsearch sink connector can subscribe to these topics and update corresponding full-text indexes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find a proof-of-concept implementation (said Hibernate extension and related code) of this idea in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
Of course the general idea isn’t limited to Hibernate ORM or JPA,
you could implement something similar with any other API you’re using to access your data.&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;creating_aggregate_views_via_hibernate_orm&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#creating_aggregate_views_via_hibernate_orm&quot; /&gt;Creating Aggregate Views via Hibernate ORM&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For the following let’s assume we’re persisting a simple domain model
(comprising a &lt;code&gt;Customer&lt;/code&gt; entity and a few related ones like &lt;code&gt;Address&lt;/code&gt;, (customer) &lt;code&gt;Category&lt;/code&gt; etc.) in a database.
Using Hibernate for that allows us to make the creation of aggregates fully transparent to the actual application code using a &lt;a href=&quot;http://docs.jboss.org/hibernate/orm/current/userguide/html_single/Hibernate_User_Guide.html#events-events&quot;&gt;Hibernate event listener&lt;/a&gt;.
Thanks to its extensible architecture, we can plug such listener into Hibernate just by adding it to the classpath,
from where it will be picked up automatically when bootstrapping the entity manager / session factory.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Our example listener reacts to an annotation, &lt;code&gt;@MaterializeAggregate&lt;/code&gt;,
which marks those entity types that should be the roots of materialized aggregates.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
@MaterializeAggregate(aggregateName=&quot;customers-complete&quot;)
public class Customer {

    @Id
    private long id;

    private String firstName;

    @OneToMany(mappedBy = &quot;customer&quot;, fetch = FetchType.EAGER, cascade = CascadeType.ALL)
    private Set&amp;lt;Address&amp;gt; addresses;

    @ManyToOne
    private Category category;

    ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now if any entity annotated with &lt;code&gt;@MaterializeAggregate&lt;/code&gt; is inserted, updated or deleted via Hibernate,
the listener will kick in and materialize a JSON view of the aggregate root (customer) and its associated entities (addresses, category).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Under the hood the &lt;a href=&quot;https://github.com/FasterXML/jackson&quot;&gt;Jackson API&lt;/a&gt; is used for serializing the model into JSON.
This means you can use any of its annotations to customize the JSON output, e.g. &lt;code&gt;@JsonIgnore&lt;/code&gt;  to exclude the inverse relationship from &lt;code&gt;Address&lt;/code&gt; to &lt;code&gt;Customer&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;@Entity
public class Address {

    @Id
    private long id;

    @ManyToOne
    @JoinColumn(name = &quot;customer_id&quot;)
    @JsonIgnore
    private Customer customer;

    private String street;

    private String city;

    ...
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that &lt;code&gt;Address&lt;/code&gt; itself isn’t marked with &lt;code&gt;@MaterializeAggregate&lt;/code&gt;, i.e. it won’t be materialized into an aggregate view by itself.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After using JPA’s &lt;code&gt;EntityManager&lt;/code&gt; to insert or update a few customers,
let’s take a look at the &lt;code&gt;aggregates&lt;/code&gt; table which has been populated by the listener
(value schema omitted for the sake of brevity):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&amp;gt; select * from aggregates;

| rootType | keySchema | rootId | materialization | valueSchema |

| customers-complete

| {
  &quot;schema&quot; : {
    &quot;type&quot; : &quot;struct&quot;,
    &quot;fields&quot; : [ {
      &quot;type&quot; : &quot;int64&quot;,
      &quot;optional&quot; : false,
      &quot;field&quot; : &quot;id&quot;
    } ],
    &quot;optional&quot; : false,
    &quot;name&quot; : &quot;customers-complete.Key&quot;
  }
}

| { &quot;id&quot; : 1004 }

| { &quot;schema&quot; : { ... } }

| {
  &quot;id&quot; : 1004,
  &quot;firstName&quot; : &quot;Anne&quot;,
  &quot;lastName&quot; : &quot;Kretchmar&quot;,
  &quot;email&quot; : &quot;annek@noanswer.org&quot;,
  &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
  &quot;birthday&quot; : 5098,
  &quot;category&quot; : {
    &quot;id&quot; : 100001,
    &quot;name&quot; : &quot;Retail&quot;
  },
  &quot;addresses&quot; : [ {
    &quot;id&quot; : 16,
    &quot;street&quot; : &quot;1289 University Hill Road&quot;,
    &quot;city&quot; : &quot;Canehill&quot;,
    &quot;state&quot; : &quot;Arkansas&quot;,
    &quot;zip&quot; : &quot;72717&quot;,
    &quot;type&quot; : &quot;SHIPPING&quot;
  } ]
} |&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The table contains these columns:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;rootType&lt;/code&gt;: The name of the aggregate as given in the &lt;code&gt;@MaterializeAggregate&lt;/code&gt; annotation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;rootId&lt;/code&gt;: The aggregate’s id as serialized JSON&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;materialization&lt;/code&gt;: The aggregate itself as serialized JSON; in this case a customer and their addresses, category etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;keySchema&lt;/code&gt;: The Kafka Connect schema of the row’s key&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;valueSchema&lt;/code&gt;: The Kafka Connect schema of the materialization&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let’s talk about the two schema columns for a bit.
JSON itself is quite limited as far as its supported data types are concerned.
So for instance we’d loose information about a numeric field’s value range (int vs. long etc.) without any additional information.
Therefore the listener derives the corresponding schema information for key and aggregate view from the entity model and stores it within the aggregate records.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now Jackson itself only supports JSON Schema, which would be a bit too limited for our purposes.
Hence the example implementation provides custom serializers for Jackson’s schema system,
which allow us to emit Kafka Connect’s schema representation (with more precise type information) instead of plain JSON Schema.
This will come in handy in the following when we’d like to expand the string-based JSON representations of key and value into properly typed Kafka Connect records.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;capturing_changes_to_the_aggregate_table&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#capturing_changes_to_the_aggregate_table&quot; /&gt;Capturing Changes to the Aggregate Table&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We now have a mechanism in place which transparently persists aggregates into a separate table within the source database,
whenever the application data is changed through Hibernate.
Note that this happens within the boundaries of the source transaction,
so if the same would be rolled back for some reason, also the aggregate view would not be updated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Hibernate listener uses insert-or-update semantics when writing an aggregate view,
i.e. for a given aggregate root there’ll always be exactly one corresponding entry in the aggregate table which reflects its current state.
If an aggregate root entity is deleted, the listener will also drop the entry from the aggregate table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So let’s set up Debezium now to capture any changes to the &lt;code&gt;aggregates&lt;/code&gt; table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
  -H &quot;Accept:application/json&quot; \
  -H &quot;Content-Type:application/json&quot; \
  http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
  {
      &quot;name&quot;: &quot;inventory-connector&quot;,
      &quot;config&quot;: {
          &quot;connector.class&quot;: &quot;io.debezium.connector.mysql.MySqlConnector&quot;,
          &quot;tasks.max&quot;: &quot;1&quot;,
          &quot;database.hostname&quot;: &quot;mysql&quot;,
          &quot;database.port&quot;: &quot;3306&quot;,
          &quot;database.user&quot;: &quot;debezium&quot;,
          &quot;database.password&quot;: &quot;dbz&quot;,
          &quot;database.server.id&quot;: &quot;184054&quot;,
          &quot;database.server.name&quot;: &quot;dbserver1&quot;,
          &quot;database.whitelist&quot;: &quot;inventory&quot;,
          &quot;table.whitelist&quot;: &quot;.*aggregates&quot;,
          &quot;database.history.kafka.bootstrap.servers&quot;: &quot;kafka:9092&quot;,
          &quot;database.history.kafka.topic&quot;: &quot;schema-changes.inventory&quot;
      }
  }
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This registers the MySQL connector with the &quot;inventory&quot; database
(we’re using an expanded version of the schema from the &lt;a href=&quot;https://debezium.io/docs/tutorial/&quot;&gt;Debezium tutorial&lt;/a&gt;),
capturing any changes to the &quot;aggregates&quot; table.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;expanding_json&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#expanding_json&quot; /&gt;Expanding JSON&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If we now were to browse the corresponding Kafka topic, we’d see data change events in the known Debezium format for all the changes to the &lt;code&gt;aggregates&lt;/code&gt; table.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &quot;materialization&quot; field with the records&#39; &quot;after&quot; state still is a single field containing a JSON string, though.
What we’d rather like to have is a strongly typed Kafka Connect record, whose schema exactly describes the aggregate structure and the types of its fields.
For that purpose the example project provides an SMT (single message transform) which takes the JSON materialization and the corresponding &lt;code&gt;valueSchema&lt;/code&gt; and converts this into a full-blown Kafka Connect record.
The same is done for keys.
DELETE events are rewritten into tombstone events.
Finally, the SMT re-routes every record to a topic named after the aggregate root,
allowing consumers to subscribe just to changes to specific aggregate types.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So let’s add that SMT when registering the Debezium CDC connector:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
&quot;transforms&quot;:&quot;expandjson&quot;,
&quot;transforms.expandjson.type&quot;:&quot;io.debezium.aggregation.smt.ExpandJsonSmt&quot;,
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When now browsing the &quot;customers-complete&quot; topic, we’ll see the strongly typed Kafka Connect records we’d expect:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;schema&quot;: {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [
            {
                &quot;type&quot;: &quot;int64&quot;,
                &quot;optional&quot;: false,
                &quot;field&quot;: &quot;id&quot;
            }
        ],
        &quot;optional&quot;: false,
        &quot;name&quot;: &quot;customers-complete.Key&quot;
    },
    &quot;payload&quot;: {
        &quot;id&quot;: 1004
    }
}
{
    &quot;schema&quot;: {
        &quot;type&quot;: &quot;struct&quot;,
        &quot;fields&quot;: [ ... ],
        &quot;optional&quot;: true,
        &quot;name&quot;: &quot;urn:jsonschema:com:example:domain:Customer&quot;
    },
    &quot;payload&quot;: {
        &quot;id&quot;: 1004,
        &quot;firstName&quot;: &quot;Anne&quot;,
        &quot;lastName&quot;: &quot;Kretchmar&quot;,
        &quot;email&quot;: &quot;annek@noanswer.org&quot;,
        &quot;active&quot;: true,
        &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
        &quot;birthday&quot; : 5098,
        &quot;category&quot;: {
            &quot;id&quot;: 100001,
            &quot;name&quot;: &quot;Retail&quot;
        },
        &quot;addresses&quot;: [
            {
                &quot;id&quot;: 16,
                &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                &quot;city&quot;: &quot;Canehill&quot;,
                &quot;state&quot;: &quot;Arkansas&quot;,
                &quot;zip&quot;: &quot;72717&quot;,
                &quot;type&quot;: &quot;LIVING&quot;
            }
        ]
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To confirm that these are actual typed Kafka Connect records and not just a single JSON string field,
you could for instance use the &lt;a href=&quot;https://debezium.io/docs/configuration/avro/&quot;&gt;Avro message converter&lt;/a&gt; and examine the message schemas in the schema registry.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sinking_aggregate_messages_into_elasticsearch&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sinking_aggregate_messages_into_elasticsearch&quot; /&gt;Sinking Aggregate Messages Into Elasticsearch&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The last missing step is to register the Confluent Elasticsearch sink connector, hooking it up with the &quot;customers-complete&quot; topic and letting it push any changes to the corresponding index:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;curl -i -X POST \
  -H &quot;Accept:application/json&quot; \
  -H &quot;Content-Type:application/json&quot; \
  http://localhost:8083/connectors/ -d @- &amp;lt;&amp;lt;-EOF
  {
      &quot;name&quot;: &quot;es-customers&quot;,
      &quot;config&quot;: {
          &quot;connector.class&quot;: &quot;io.confluent.connect.elasticsearch.ElasticsearchSinkConnector&quot;,
          &quot;tasks.max&quot;: &quot;1&quot;,
          &quot;topics&quot;: &quot;customers-complete&quot;,
          &quot;connection.url&quot;: &quot;http://elastic:9200&quot;,
          &quot;key.ignore&quot;: &quot;false&quot;,
          &quot;schema.ignore&quot; : &quot;false&quot;,
          &quot;behavior.on.null.values&quot; : &quot;delete&quot;,
          &quot;type.name&quot;: &quot;customer-with-addresses&quot;,
          &quot;transforms&quot; : &quot;key&quot;,
          &quot;transforms.key.type&quot;: &quot;org.apache.kafka.connect.transforms.ExtractField$Key&quot;,
          &quot;transforms.key.field&quot;: &quot;id&quot;
      }
  }
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This uses Connect’s &lt;code&gt;ExtractField&lt;/code&gt; transformation to obtain just the actual id value from the key struct and use it as key for the corresponding Elasticsearch documents.
Specifying the &quot;behavior.on.null.values&quot; option will let the connector delete the corresponding document from the index when encountering a tombstone message (i.e. a message with a key but without value).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, we can use the Elasticsearch REST API to browse the index and of course use its powerful full-text query language to find customers by the address or any other property embedded into the aggregate structure:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&amp;gt; curl -X GET -H &quot;Accept:application/json&quot; \
  http://localhost:9200/customers-complete/_search?pretty

  {
      &quot;_shards&quot;: {
          &quot;failed&quot;: 0,
          &quot;successful&quot;: 5,
          &quot;total&quot;: 5
      },
      &quot;hits&quot;: {
          &quot;hits&quot;: [
              {
                  &quot;_id&quot;: &quot;1004&quot;,
                  &quot;_index&quot;: &quot;customers-complete&quot;,
                  &quot;_score&quot;: 1.0,
                  &quot;_source&quot;: {
                      &quot;active&quot;: true,
                      &quot;addresses&quot;: [
                          {
                              &quot;city&quot;: &quot;Canehill&quot;,
                              &quot;id&quot;: 16,
                              &quot;state&quot;: &quot;Arkansas&quot;,
                              &quot;street&quot;: &quot;1289 University Hill Road&quot;,
                              &quot;type&quot;: &quot;LIVING&quot;,
                              &quot;zip&quot;: &quot;72717&quot;
                          }
                      ],
                      &quot;tags&quot; : [ &quot;long-term&quot;, &quot;vip&quot; ],
                      &quot;birthday&quot; : 5098,
                      &quot;category&quot;: {
                          &quot;id&quot;: 100001,
                          &quot;name&quot;: &quot;Retail&quot;
                      },
                      &quot;email&quot;: &quot;annek@noanswer.org&quot;,
                      &quot;firstName&quot;: &quot;Anne&quot;,
                      &quot;id&quot;: 1004,
                      &quot;lastName&quot;: &quot;Kretchmar&quot;,
                      &quot;scores&quot;: [],
                      &quot;someBlob&quot;: null,
                      &quot;tags&quot;: []
                  },
                  &quot;_type&quot;: &quot;customer-with-addresses&quot;
              }
          ],
          &quot;max_score&quot;: 1.0,
          &quot;total&quot;: 1
      },
      &quot;timed_out&quot;: false,
      &quot;took&quot;: 11
  }&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And there you have it: a customer’s complete data, including their addresses, categories, tags etc., materialized into a single document within Elasticsearch.
If you’re using JPA to update the customer, you’ll see the data in the index being updated accordingly in near-realtime.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;pros_and_cons&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#pros_and_cons&quot; /&gt;Pros and Cons&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So what are the advantages and disadvantages of this approach for materializing aggregates from multiple source tables compared to the &lt;a href=&quot;https://debezium.io/blog/2018/03/08/creating-ddd-aggregates-with-debezium-and-kafka-streams/&quot;&gt;KStreams-based approach&lt;/a&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The big advantage is consistency and awareness of transactional boundaries,
whereas the KStreams-based solution in its suggested form was prone to exposing intermediary aggregates.
For instance, if you’re storing a customer and three addresses, it might happen that the streaming query first creates an aggregation of the customer and the two addresses inserted first, and shortly thereafter the complete aggregate with all three addresses.
This not the case for the approach discussed here, as you’ll only ever stream complete aggregates to Kafka.
Also this approach feels a bit more &quot;light-weight&quot;, i.e. a simple marker annotation (together with some Jackson annotations for fine-tuning the emitted JSON structures) is enough in order to materialize aggregates from your domain model,
whereas some more effort was needed to set up the required streams, temporary tables etc. with the KStreams solution.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The downside of driving aggregations through the application layer is that it’s not fully agnostic to the way you access the primary data.
If you bypass the application, e.g. by patching data directly in the database, naturally these updates would be missed, requiring a refresh of affected aggregates.
Although this again could be done through change data capture and Debezium:
change events to source tables could be captured and consumed by the application itself, allowing it to re-materialize aggregates after external data changes.
You also might argue that running JSON serializations within source transactions and storing aggregates within the source database represents some overhead.
This often may be acceptable, though.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Another question to ask is what’s the advantage of using change data capture on an intermediary aggregate table over simply posting REST requests to Elasticsearch.
The answer is the highly increased robustness and fault tolerance.
If the Elasticsearch cluster can’t be accessed for some reason, the machinery of Kafka and Kafka Connect will ensure that any change events will be propagated eventually, once the sink is up again.
Also other consumers than Elasticsearch can subscribe to the aggregate topic, the log can be replayed from the beginning etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note that while we’ve been talking primarily about using Elasticsearch as a data sink, there are also other datastores and connectors that support complexly structured records.
One example would be MongoDB and the &lt;a href=&quot;https://github.com/hpgrahsl/kafka-connect-mongodb&quot;&gt;sink connector&lt;/a&gt; maintained by Hans-Peter Grahsl,
which one could use to sink customer aggregates into MongoDB, for instance enabling efficient retrieval of a customer and all their associated data with a single primary key look-up.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#outlook&quot; /&gt;Outlook&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Hibernate ORM extension as well as the SMT discussed in this post can be found in our &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/jpa-aggregations&quot;&gt;examples repository&lt;/a&gt;.
They should be considered to be at &quot;proof-of-concept&quot; level currently.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;That being said, we’re considering to make this a Debezium component proper,
allowing you to employ this aggregation approach within your Hibernate-based applications just by pulling in this new component.
For that we’d have to improve a few things first, though.
Most importantly, an API is needed which will let you (re-)create aggregates on demand,
e.g. for existing data or for data updated by bulk updates via the Criteria API / JPQL (which will be missed by listeners).
Also aggregates should be re-created automatically, if any of the referenced entities change
(with the current PoC, only a change to the customer instance itself will trigger its aggregate view to be rebuilt, but not a change to one of its addresses).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you like this idea, then let us know about it,
so we can gauge the general interest in this.
Also, this would be a great item to work on, if you’re interested in contributing to the Debezium project.
Looking forward to hearing from you, e.g. in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to Hans-Peter Grahsl for his feedback on an earlier version of this post!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/</id>
<title>Debezium 0.8.3.Final Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-09-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/09/19/debezium-0-8-3-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="docker"></category>
<summary>



As temperatures are cooling off, the Debezium team is getting into full swing again and we&#8217;re happy to announce the release of Debezium 0.8.3.Final!


This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
There are 14 fixes in this release.
As in earlier 0.8.x releases, we&#8217;ve further improved the new Antlr-based DDL parser used by the MySQL connector (see DBZ-901, DBZ-903 and DBZ-910).


The Postgres connector saw a huge improvement to its start-up time for databases with lots of custom types (DBZ-899).
The user reporting this issue had nearly 200K...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As temperatures are cooling off, the Debezium team is getting into full swing again and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.3.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 goes on in parallel.
There are &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-8-3-final&quot;&gt;14 fixes&lt;/a&gt; in this release.
As in earlier 0.8.x releases, we’ve further improved the new Antlr-based DDL parser used by the &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt; (see &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-901&quot;&gt;DBZ-901&lt;/a&gt;, &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-903&quot;&gt;DBZ-903&lt;/a&gt; and &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-910&quot;&gt;DBZ-910&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector&lt;/a&gt; saw a huge improvement to its start-up time for databases with lots of custom types (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-899&quot;&gt;DBZ-899&lt;/a&gt;).
The user reporting this issue had nearly 200K entries in pg_catalog.pg_type, and due to an N + 1 SELECT issue within the Postgres driver itself, this caused the connector to take 24 minutes to start.
By using a custom query for obtaining the type metadata, we were able to cut down this time to 5 seconds!
Right now we’re working with the maintainers of the Postgres driver to get this issue fixed upstream, too.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;more_flexible_propagation_of_deletes&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#more_flexible_propagation_of_deletes&quot; /&gt;More Flexible Propagation of DELETEs&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides those bug fixes we decided to also merge one new feature from the 0.9.x branch into the 0.8.3.Final release,
which those of you may find useful who are using the &lt;a href=&quot;https://debezium.io/docs/configuration/event-flattening/&quot;&gt;SMT for extracting the &quot;after&quot; state&lt;/a&gt; from change events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-857&quot;&gt;DBZ-857&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This SMT can be employed to stream changes to sink connectors which expect just a &quot;flat&quot; row representation of data instead of Debezium’s complex event structure.
Not all sink connectors support the handling of deletions, though.
E.g. some connectors will fail when encountering tombstone events.
Therefore the SMT can now optionally rewrite delete events into updates of a special &quot;deleted&quot; marker field.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For that, set the &lt;code&gt;delete.handling.mode&lt;/code&gt; option of the SMT to &quot;rewrite&quot;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...
&quot;transforms&quot; : &quot;unwrap&quot;,
&quot;transforms.unwrap.type&quot;: &quot;io.debezium.transforms.UnwrapFromEnvelope&quot;,
&quot;transforms.unwrap.delete.handling.mode&quot; : &quot;rewrite&quot;,
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a DELETE event is propagated, the &quot;__deleted&quot; field of outgoing records will be set to true.
So when for instance consuming the events with the JDBC sink connector, you’d see this being reflected in a corresponding column in the sink tables:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;__deleted | last_name |  id  | first_name |         email
-----------+-----------+------+------------+-----------------------
false     | Thomas    | 1001 | Sally      | sally.thomas@acme.com
false     | Bailey    | 1002 | George     | gbailey@foobar.com
false     | Kretchmar | 1004 | Anne       | annek@noanswer.org
true      | Walker    | 1003 | Edward     | ed@walker.com&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You then for instance can use a batch job running on your sink to remove all records flagged as deleted.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
The current plan is to do the next 0.9 release (either Alpha2 or Beta1) in two weeks from now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Also it’s the beginning of the conference season, so we’ll spend some time with preparing demos and presenting Debezium at multiple locations.
There will be sessions on change data capture with Debezium a these conferences:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://jug-saxony-day.org/programm/#!/P31&quot;&gt;JUG Saxony Day&lt;/a&gt;; Dresden, Germany; Sept. 28&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://kafka-summit.org/sessions/change-data-streaming-patterns-microservices-debezium/&quot;&gt;Kafka Summit&lt;/a&gt;; San Francisco, Cal.; Oct. 17&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://vxdms2018.confinabox.com/talk/INI-9172/Data_Streaming_for_Microservices_using_Debezium&quot;&gt;VoxxedDays Microservices&lt;/a&gt;; Paris, France; Oct. 29 - 31&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https://cfp.devoxx.ma/2018/talk/AEY-4477/Change_Data_Streaming_Patterns_for_Microservices_With_Debezium&quot;&gt;Devoxx Morocco&lt;/a&gt;; Marrakesh, Morocco; Nov. 27 - 29&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you are at any of these conferences, come and say Hi;
we’d love to exchange with you about your use cases, feature requests, feedback on our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and any other ideas around Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, a big &quot;Thank You&quot; goes to our fantastic community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/maver1ck&quot;&gt;Maciej Bryński&lt;/a&gt; and &lt;a href=&quot;https://github.com/PengLyu&quot;&gt;Peng Lyu&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/</id>
<title>Streaming MySQL Data Changes to Amazon Kinesis</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-08-30T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/08/30/streaming-mysql-data-changes-into-kinesis/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="examples"></category>
<summary>



Most of the times Debezium is used to stream data changes into Apache Kafka.
What though if you&#8217;re using another streaming platform such as Apache Pulsar or a cloud-based solution such as Amazon Kinesis, Azure Event Hubs and the like?
Can you still benefit from Debezium&#8217;s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?


Turns out, with just a bit of glue code, you can!
In the following we&#8217;ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
a fully-managed data streaming service available...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Most of the times Debezium is used to stream data changes into &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt;.
What though if you’re using another streaming platform such as &lt;a href=&quot;https://pulsar.incubator.apache.org/&quot;&gt;Apache Pulsar&lt;/a&gt; or a cloud-based solution such as &lt;a href=&quot;https://aws.amazon.com/kinesis/&quot;&gt;Amazon Kinesis&lt;/a&gt;, &lt;a href=&quot;https://azure.microsoft.com/services/event-hubs/&quot;&gt;Azure Event Hubs&lt;/a&gt; and the like?
Can you still benefit from Debezium’s powerful change data capture (CDC) capabilities  and ingest changes from databases such as MySQL, Postgres, SQL Server etc.?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Turns out, with just a bit of glue code, you can!
In the following we’ll discuss how to use Debezium to capture changes in a MySQL database and stream the change events into Kinesis,
a fully-managed data streaming service available on the Amazon cloud.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;introducing_the_debezium_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#introducing_the_debezium_embedded_engine&quot; /&gt;Introducing the Debezium Embedded Engine&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is implemented as a set of connectors for Kafka and thus usually is run via &lt;a href=&quot;https://kafka.apache.org/documentation/#connect&quot;&gt;Kafka Connect&lt;/a&gt;.
But there’s one little gem in Debezium which isn’t as widely known yet, which is the &lt;a href=&quot;https://debezium.io/docs/embedded/&quot;&gt;embedded engine&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When using this engine, the Debezium connectors are not executed within Kafka Connect, but as a library embedded into your own Java application.
For this purpose, the &lt;em&gt;debezium-embedded&lt;/em&gt; module provides a small runtime environment which performs the tasks that’d otherwise be handled by the Kafka Connect framework:
requesting change records from the connector, committing offsets etc.
Each change record produced by the connector is passed to a configured event handler method,
which in our case will convert the record into its JSON representation and submit it to a Kinesis stream, using the Kinesis Java API.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The overall architecture looks like so:&lt;/p&gt;
&lt;/div&gt;
&lt;img src=&quot;https://debezium.io/images/debezium-embedded.png&quot; style=&quot;max-width:100%; margin-bottom:10px;&quot; class=&quot;responsive-image&quot; alt=&quot;Debezium Embedded Engine Streaming to Amazon Kinesis&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now let’s walk through the relevant parts of the code required for that.
A complete executable example can be found in the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis&quot;&gt;debezium-examples&lt;/a&gt; repo on GitHub.&lt;/p&gt;
&lt;/div&gt;
&lt;/img&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;set_up&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#set_up&quot; /&gt;Set-Up&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to use Debezium’s embedded engine, add the &lt;em&gt;debezium-embedded&lt;/em&gt; dependency as well as the Debezium connector of your choice to your project’s &lt;em&gt;pom.xml&lt;/em&gt;.
In the following we’re going to use the connector for MySQL.
We also need to add a dependency to the &lt;a href=&quot;https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/kinesis/package-summary.html&quot;&gt;Kinesis Client API&lt;/a&gt;, so these are the dependencies needed:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-xml&quot; data-lang=&quot;xml&quot;&gt;...
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;debezium-embedded&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;io.debezium&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;debezium-connector-mysql&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;0.8.3.Final&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&amp;lt;dependency&amp;gt;
    &amp;lt;groupId&amp;gt;com.amazonaws&amp;lt;/groupId&amp;gt;
    &amp;lt;artifactId&amp;gt;amazon-kinesis-client&amp;lt;/artifactId&amp;gt;
    &amp;lt;version&amp;gt;1.9.0&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;configuring_the_embedded_engine&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#configuring_the_embedded_engine&quot; /&gt;Configuring the Embedded Engine&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium embedded engine is configured through an instance of &lt;code&gt;io.debezium.config.Configuration&lt;/code&gt;.
This class can obtain values from system properties or from a given config file,
but for the sake of the example we’ll simply pass all required values via its fluent builder API:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;Configuration config = Configuration.create()
    .with(EmbeddedEngine.CONNECTOR_CLASS, &quot;io.debezium.connector.mysql.MySqlConnector&quot;)
    .with(EmbeddedEngine.ENGINE_NAME, &quot;kinesis&quot;)
    .with(MySqlConnectorConfig.SERVER_NAME, &quot;kinesis&quot;)
    .with(MySqlConnectorConfig.SERVER_ID, 8192)
    .with(MySqlConnectorConfig.HOSTNAME, &quot;localhost&quot;)
    .with(MySqlConnectorConfig.PORT, 3306)
    .with(MySqlConnectorConfig.USER, &quot;debezium&quot;)
    .with(MySqlConnectorConfig.PASSWORD, &quot;dbz&quot;)
    .with(MySqlConnectorConfig.DATABASE_WHITELIST, &quot;inventory&quot;)
    .with(MySqlConnectorConfig.TABLE_WHITELIST, &quot;inventory.customers&quot;)
    .with(EmbeddedEngine.OFFSET_STORAGE,
        &quot;org.apache.kafka.connect.storage.MemoryOffsetBackingStore&quot;)
    .with(MySqlConnectorConfig.DATABASE_HISTORY,
        MemoryDatabaseHistory.class.getName())
    .with(&quot;schemas.enable&quot;, false)
    .build();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’ve ever set up the Debezium MySQL connector in Kafka Connect, most of the properties will look familiar to you.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But let’s talk about the &lt;code&gt;OFFSET_STORAGE&lt;/code&gt; and &lt;code&gt;DATABASE_HISTORY&lt;/code&gt; options in a bit more detail.
They deal with how connector offsets and the database history should be persisted.
When running the connector via Kafka Connect, both would typically be stored in specific Kafka topics.
But that’s not an option here, so an alternative is needed.
For this example we’re simply going to keep the offsets and database history in memory.
I.e. if the engine is restarted, this information will be lost and the connector will start from scratch, e.g. with a new initial snapshot.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While out of scope for this blog post, it wouldn’t be too difficult to create alternative implementations of the &lt;code&gt;OffsetBackingStore&lt;/code&gt; and &lt;code&gt;DatabaseHistory&lt;/code&gt; contracts, respectively.
For instance if you’re fully committed into the AWS cloud services, you could think of storing offsets and database history in the DynamoDB NoSQL store.
Note that, different from Kafka, a Kinesis stream wouldn’t be suitable for storing the database history.
The reason being, that the maximum retention period for Kinesis data streams is seven days, whereas the database history must be kept for the entire lifetime of the connector.
Another alternative could be to use the existing filesystem based implementations &lt;code&gt;FileOffsetBackingStore&lt;/code&gt; and &lt;code&gt;FileDatabaseHistory&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next step is to build an &lt;code&gt;EmbeddedEngine&lt;/code&gt; instance from the configuration.
Again this is done using a fluent API:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;EmbeddedEngine engine = EmbeddedEngine.create()
    .using(config)
    .using(this.getClass().getClassLoader())
    .using(Clock.SYSTEM)
    .notifying(this::sendRecord)
    .build();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The most interesting part here is the &lt;code&gt;notifying&lt;/code&gt; call.
The method passed here is the one which will be invoked by the engine for each emitted data change record.
So let’s take a look at the implementation of this method.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sending_change_records_to_kinesis&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sending_change_records_to_kinesis&quot; /&gt;Sending Change Records to Kinesis&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;sendRecord()&lt;/code&gt; method is where the magic happens.
We’ll convert the incoming &lt;code&gt;SourceRecord&lt;/code&gt; into an equivalent JSON representation and propagate it to a Kinesis stream.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For that, it’s important to understand some conceptual differences between Apache Kafka and Kinesis.
Specifically, messages in Kafka have a &lt;em&gt;key&lt;/em&gt; and a &lt;em&gt;value&lt;/em&gt; (which both are arbitrary byte arrays).
In case of Debezium, the key of data change events represents the primary key of the affected record and the value is a structure comprising of old and new row state as well as some additional metadata.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In Kinesis on the other hand a message contains a &lt;em&gt;data blob&lt;/em&gt; (again an arbitrary byte sequence) and a &lt;em&gt;partition key&lt;/em&gt;.
Kinesis streams can be split up into multiple shards and the partition key is used to determine into which shard a given message should go.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now one could think of mapping the key from Debezium’s change data events to the Kinesis partition key,
but partition keys are limited to a length of 256 bytes.
Depending on the length of primary key column(s) in the captured tables, this might not be enough.
So a safer option is to create a hash value from the change message key and use that as the partition key.
This in turn means that the change message key structure should be added next to the actual value to the Kinesis message’s data blob.
While the key column values themselves are part of the value structure, too, a consumer otherwise wouldn’t know which column(s) make up the primary key.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With that in mind, let’s take a look at the &lt;code&gt;sendRecord()&lt;/code&gt; implementation:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;private void sendRecord(SourceRecord record) {
    // We are interested only in data events not schema change events
    if (record.topic().equals(&quot;kinesis&quot;)) {
        return;
    }

    // create schema for container with key *and* value
    Schema schema = SchemaBuilder.struct()
        .field(&quot;key&quot;, record.keySchema())
        .field(&quot;value&quot;, record.valueSchema())
        .build();

    Struct message = new Struct(schema);
    message.put(&quot;key&quot;, record.key());
    message.put(&quot;value&quot;, record.value());

    // create partition key by hashing the record&#39;s key
    String partitionKey = String.valueOf(
        record.key() != null ? record.key().hashCode() : -1);

    // create data blob representing the container by using Kafka Connect&#39;s
    // JSON converter
    final byte[] payload = valueConverter.fromConnectData(
        &quot;dummy&quot;, schema, message);

    // Assemble the put-record request ...
    PutRecordRequest putRecord = new PutRecordRequest();

    putRecord.setStreamName(record.topic());
    putRecord.setPartitionKey(partitionKey);
    putRecord.setData(ByteBuffer.wrap(payload));

    // ... and execute it
    kinesisClient.putRecord(putRecord);
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The code is quite straight-forward; as discussed above it’s first creating a container structure containing key &lt;em&gt;and&lt;/em&gt; value of the incoming source record.
This structure then is converted into a binary representation using the JSON converter provided by Kafka Connect (an instance of &lt;code&gt;JsonConverter&lt;/code&gt;).
Then a &lt;code&gt;PutRecordRequest&lt;/code&gt; is assembled from that blob, the partition key and the change record’s topic name, which finally is sent to Kinesis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Kinesis client object can be re-used and is set up once like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;// Uses the credentials from the local &quot;default&quot; AWS profile
AWSCredentialsProvider credentialsProvider =
    new ProfileCredentialsProvider(&quot;default&quot;);

this.kinesisClient = AmazonKinesisClientBuilder.standard()
    .withCredentials(credentialsProvider)
    .withRegion(&quot;eu-central-1&quot;) // use your AWS region here
    .build();&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;With that, we’ve set up an instance of Debezium’s &lt;code&gt;EmbeddedEngine&lt;/code&gt; which runs the configured MySQL connector and passes each emitted change event to Amazon Kinesis.
The last missing step is to actually run the engine.
This is done on a separate thread using an &lt;code&gt;Executor&lt;/code&gt;, e.g. like so:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-java&quot; data-lang=&quot;java&quot;&gt;ExecutorService executor = Executors.newSingleThreadExecutor();
executor.execute(engine);&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note you also should make sure to properly shut down the engine eventually.
How that can be done &lt;a href=&quot;https://github.com/debezium/debezium-examples/blob/master/kinesis/src/main/java/io/debezium/examples/kinesis/ChangeDataSender.java#L83-L88&quot;&gt;is shown&lt;/a&gt; in the accompanying example in the &lt;em&gt;debezium-examples&lt;/em&gt; repo.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;running_the_example&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#running_the_example&quot; /&gt;Running the Example&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally let’s take a look at running the complete example and consuming the Debezium CDC events from the Kinesis stream.
Start by cloning the examples repository and go to the &lt;em&gt;kinesis&lt;/em&gt; directory:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;git clone https://github.com/debezium/debezium-examples.git
cd debezium-examples/kinesis&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Make sure you’ve met the &lt;a href=&quot;https://github.com/debezium/debezium-examples/tree/master/kinesis#prerequisites&quot;&gt;prerequisites&lt;/a&gt; described in the example’s &lt;em&gt;README.md&lt;/em&gt;;
most notably you should have a local Docker installation and you’ll need to have set up an AWS account as well as have the AWS client tools installed.
Note that Kinesis isn’t part of the free tier when registering with AWS, i.e. you’ll pay a (small) amount of money when executing the example.
Don’t forget to delete the streams you’ve set up once done, we won’t pay your AWS bills :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now run Debezium’s MySQL example database to have some data to play with:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;docker run -it --rm --name mysql -p 3306:3306 \
  -e MYSQL_ROOT_PASSWORD=debezium \
  -e MYSQL_USER=mysqluser \
  -e MYSQL_PASSWORD=mysqlpw \
  debezium/example-mysql:0.8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Create a Kinesis stream for change events from the &lt;code&gt;customers&lt;/code&gt; table:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis create-stream --stream-name kinesis.inventory.customers \
  --shard-count 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Execute the Java application that runs the Debezium embedded engine
(if needed, adjust the value of the &lt;code&gt;kinesis.region&lt;/code&gt; property in &lt;em&gt;pom.xml&lt;/em&gt; to your own region first):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;mvn exec:java&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will start up the engine and the MySQL connector, which takes an initial snapshot of the captured database.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In order to take a look at the CDC events in the Kinesis stream, the AWS CLI can be used
(usually, you’d implement a Kinesis Streams application for consuming the events).
To do so, set up a &lt;a href=&quot;https://docs.aws.amazon.com/streams/latest/dev/developing-consumers-with-sdk.html#kinesis-using-sdk-java-get-data-shard-iterators&quot;&gt;shard iterator&lt;/a&gt; first:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;ITERATOR=$(aws kinesis get-shard-iterator --stream-name kinesis.inventory.customers --shard-id 0 --shard-iterator-type TRIM_HORIZON | jq &#39;.ShardIterator&#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Note how the &lt;a href=&quot;https://stedolan.github.io/jq/&quot;&gt;jq&lt;/a&gt; utility is used to obtain the generated id of the iterator from the JSON structure returned by the Kinesis API.
Next that iterator can be used to examine the stream:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You should receive an array of records like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
    &quot;Records&quot;: [
        {
            &quot;SequenceNumber&quot;:
                &quot;49587760482547027816046765529422807492446419903410339842&quot;,
            &quot;ApproximateArrivalTimestamp&quot;: 1535551896.475,
            &quot;Data&quot;: &quot;eyJiZWZvcm...4OTI3MzN9&quot;,
            &quot;PartitionKey&quot;: &quot;eyJpZCI6MTAwMX0=&quot;
        },
        ...
    ]
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;Data&lt;/code&gt; element is a Base64-encoded representation of the message’s data blob.
Again &lt;em&gt;jq&lt;/em&gt; comes in handy: we can use it to just extract the &lt;code&gt;Data&lt;/code&gt; part of each record and decode the Base64 representation
(make sure to use jq 1.6 or newer):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;aws kinesis get-records --shard-iterator $ITERATOR | \
  jq -r &#39;.Records[].Data | @base64d&#39; | jq .&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now you should see the change events as JSON, each one with key and value:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;{
  &quot;key&quot;: {
    &quot;id&quot;: 1001
  },
  &quot;value&quot;: {
    &quot;before&quot;: null,
    &quot;after&quot;: {
      &quot;id&quot;: 1001,
      &quot;first_name&quot;: &quot;Sally&quot;,
      &quot;last_name&quot;: &quot;Thomas&quot;,
      &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
    },
    &quot;source&quot;: {
      &quot;version&quot;: &quot;0.8.1.Final&quot;,
      &quot;name&quot;: &quot;kinesis&quot;,
      &quot;server_id&quot;: 0,
      &quot;ts_sec&quot;: 0,
      &quot;gtid&quot;: null,
      &quot;file&quot;: &quot;mysql-bin.000003&quot;,
      &quot;pos&quot;: 154,
      &quot;row&quot;: 0,
      &quot;snapshot&quot;: true,
      &quot;thread&quot;: null,
      &quot;db&quot;: &quot;inventory&quot;,
      &quot;table&quot;: &quot;customers&quot;,
      &quot;query&quot;: null
    },
    &quot;op&quot;: &quot;c&quot;,
    &quot;ts_ms&quot;: 1535555325628
  }
}
...&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next let’s try and update a record in MySQL:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;# Start MySQL CLI client
docker run -it --rm --name mysqlterm --link mysql --rm mysql:5.7 \
  sh -c &#39;exec mysql -h&quot;$MYSQL_PORT_3306_TCP_ADDR&quot; \
  -P&quot;$MYSQL_PORT_3306_TCP_PORT&quot; -uroot -p&quot;$MYSQL_ENV_MYSQL_ROOT_PASSWORD&quot;&#39;

# In the MySQL client
use inventory;
update customers set first_name = &#39;Trudy&#39; where id = 1001;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you now fetch the iterator again, you should see one more data change event representing that update:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;highlight&quot;&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;...

{
  &quot;key&quot;: {
    &quot;id&quot;: 1001
  },
  &quot;value&quot;: {
    &quot;before&quot;: {
      &quot;id&quot;: 1001,
      &quot;first_name&quot;: &quot;Sally&quot;,
      &quot;last_name&quot;: &quot;Thomas&quot;,
      &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
    },
    &quot;after&quot;: {
      &quot;id&quot;: 1001,
      &quot;first_name&quot;: &quot;Trudy&quot;,
      &quot;last_name&quot;: &quot;Thomas&quot;,
      &quot;email&quot;: &quot;sally.thomas@acme.com&quot;
    },
    &quot;source&quot;: {
      &quot;version&quot;: &quot;0.8.1.Final&quot;,
      &quot;name&quot;: &quot;kinesis&quot;,
      &quot;server_id&quot;: 223344,
      &quot;ts_sec&quot;: 1535627629,
      &quot;gtid&quot;: null,
      &quot;file&quot;: &quot;mysql-bin.000003&quot;,
      &quot;pos&quot;: 364,
      &quot;row&quot;: 0,
      &quot;snapshot&quot;: false,
      &quot;thread&quot;: 10,
      &quot;db&quot;: &quot;inventory&quot;,
      &quot;table&quot;: &quot;customers&quot;,
      &quot;query&quot;: null
    },
    &quot;op&quot;: &quot;u&quot;,
    &quot;ts_ms&quot;: 1535627622546
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once you’re done, stop the embedded engine application by hitting Ctrl + C,
stop the MySQL server by running &lt;code&gt;docker stop mysql&lt;/code&gt; and delete the &lt;em&gt;kinesis.inventory.customers&lt;/em&gt; stream in Kinesis.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary_and_outlook&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary_and_outlook&quot; /&gt;Summary and Outlook&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In this blog post we’ve demonstrated that Debezium cannot only be used to stream data changes into Apache Kafka, but also into other streaming platforms such as Amazon Kinesis.
Leveraging its embedded engine and by implementing a bit of glue code, you can benefit from &lt;a href=&quot;https://debezium.io/docs/connectors/&quot;&gt;all the CDC connectors&lt;/a&gt; provided by Debezium and their capabilities and connect them to the streaming solution of your choice.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And we’re thinking about even further simplifying this usage of Debezium.
Instead of requiring you to implement your own application that invokes the embedded engine API,
we’re considering to provide a small self-contained Debezium runtime which you can simply execute.
It’d be configured with the source connector to run and make use of an outbound plug-in SPI with ready-to-use implementations for Kinesis, Apache Pulsar and others.
Of course such runtime would also provide suitable implementations for safely persisting offsets and database history,
and it’d offer means of monitoring, health checks etc.
Meaning you could connect the Debezium source connectors with your preferred streaming platform in a robust and reliable way, without any manual coding required!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you like this idea, then please check out JIRA issue &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-651&quot;&gt;DBZ-651&lt;/a&gt; and let us know about your thoughts,
e.g. by leaving a comment on the issue, in the comment section below or on our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/08/30/debezium-0-8-2-released/</id>
<title>Debezium 0.8.2 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-08-30T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/08/30/debezium-0-8-2-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="mongodb"></category>
<category term="postgres"></category>
<category term="docker"></category>
<summary>



The Debezium team is back from summer holidays and we&#8217;re happy to announce the release of Debezium 0.8.2!


This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.


Note: By accident the version of the release artifacts is 0.8.2 instead of 0.8.2.Final.
This is not in line with our recently established convention of always letting release versions end with qualifiers such as Alpha1, Beta1, CR1 or Final.
The next version in the 0.8 line will be 0.8.3.Final and we&#8217;ll improve our release pipeline to make sure that this situation doesn&#8217;t occur again.


The...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium team is back from summer holidays and we’re happy to announce the release of Debezium &lt;strong&gt;0.8.2&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is a bugfix release to the current stable release line of Debezium, 0.8.x, while the work on Debezium 0.9 is continuing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; By accident the version of the release artifacts is &lt;em&gt;0.8.2&lt;/em&gt; instead of &lt;em&gt;0.8.2.Final&lt;/em&gt;.
This is not in line with our recently established convention of always letting release versions end with qualifiers such as &lt;em&gt;Alpha1&lt;/em&gt;, &lt;em&gt;Beta1&lt;/em&gt;, &lt;em&gt;CR1&lt;/em&gt; or &lt;em&gt;Final&lt;/em&gt;.
The next version in the 0.8 line will be &lt;em&gt;0.8.3.Final&lt;/em&gt; and we’ll improve our release pipeline to make sure that this situation doesn’t occur again.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The 0.8.2 release contains &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-8-2&quot;&gt;10 fixes&lt;/a&gt; overall, most of them dealing with issues related to DDL parsing as done by the Debezium &lt;a href=&quot;https://debezium.io/docs/connectors/mysql/&quot;&gt;MySQL connector&lt;/a&gt;.
For instance, implicit non-nullable primary key columns will be handled correctly now using the new Antlr-based DDL parser (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-860&quot;&gt;DBZ-860&lt;/a&gt;).
Also the &lt;a href=&quot;https://debezium.io/docs/connectors/mongodb/&quot;&gt;MongoDB connector&lt;/a&gt; saw a bug fix (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-838&quot;&gt;DBZ-838&lt;/a&gt;): initial snapshots will be interrupted now if the connector is requested to stop
(e.g. when shutting down Kafka Connect).
More a useful improvement rather than a bug fix is the &lt;a href=&quot;https://debezium.io/docs/connectors/postgres/&quot;&gt;Postgres connector’s&lt;/a&gt; capability to add the table, schema and database names to the &lt;code&gt;source&lt;/code&gt; block of emitted CDC events (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-866&quot;&gt;DBZ-866&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Thanks a lot to community members &lt;a href=&quot;https://github.com/jchipmunk&quot;&gt;Andrey Pustovetov&lt;/a&gt;, &lt;a href=&quot;https://github.com/CliffWheadon&quot;&gt;Cliff Wheadon&lt;/a&gt; and &lt;a href=&quot;https://github.com/oripwk&quot;&gt;Ori Popowski&lt;/a&gt; for their contributions to this release!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re continuing the work on Debezium 0.9, which will mostly be about improvements to the SQL Server and Oracle connectors.
Both will get support for handling structural changes to captured tables while the connectors are running.
Also the exploration of alternatives to using the XStream API for the Oracle connector continues.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Finally, a recurring theme of our work is to further consolidate the code bases of the different connectors,
which will allow us to roll out new and improved features more quickly across all the Debezium connectors.
The recently added Oracle and SQL Server connectors already share a lot of code,
and in the next step we’ve planned to move the existing Postgres connector to the new basis established for these two connectors.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/</id>
<title>Debezium 0.9 Alpha1 and 0.8.1 Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-07-26T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/07/26/debezium-0-9-0-alpha1-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="postgres"></category>
<category term="oracle"></category>
<category term="docker"></category>
<category term="sqlserver"></category>
<summary>



Just two weeks after the Debezium 0.8 release, I&#8217;m very happy to announce the release of Debezium 0.9.0.Alpha1!


The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for MS SQL Server.
Based on the CDC functionality available in the Enterprise and Standard editions,
the new connector lets you stream data changes out of Microsoft&#8217;s popular RDBMS.


Besides that we&#8217;ve continued the work on the Debezium Oracle connector.
Most notably, it supports initial snapshots of captured tables now.
We&#8217;ve also upgraded Apache Kafka in our Docker images to 1.1.1 (DBZ-829).


Please take a look at the change log for the...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Just two weeks after the Debezium 0.8 release, I’m very happy to announce the release of Debezium &lt;strong&gt;0.9.0.Alpha1&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The main feature of the new version is a first work-in-progress version of the long-awaited Debezium connector for &lt;a href=&quot;https://www.microsoft.com/en-us/sql-server&quot;&gt;MS SQL Server&lt;/a&gt;.
Based on the &lt;a href=&quot;https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/about-change-data-capture-sql-server?view=sql-server-2017&quot;&gt;CDC functionality&lt;/a&gt; available in the Enterprise and Standard editions,
the new connector lets you stream data changes out of Microsoft’s popular RDBMS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Besides that we’ve continued the work on the Debezium &lt;a href=&quot;https://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;.
Most notably, it supports initial snapshots of captured tables now.
We’ve also upgraded Apache Kafka in our Docker images to 1.1.1 (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-829&quot;&gt;DBZ-829&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please take a look at the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-9-0-alpha1&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.9.0.Alpha1 and general upgrade notes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; &lt;span class=&quot;line-through&quot;&gt;At the time of writing (2018-07-26), the release artifacts (connector archives) are available on &lt;a href=&quot;http://central.maven.org/maven2/io/debezium/&quot;&gt;Maven Central&lt;/a&gt;.
We’ll upload the Docker images for 0.9.0.Alpha1 to &lt;a href=&quot;https://hub.docker.com/u/debezium/&quot;&gt;Docker Hub&lt;/a&gt; as soon as possible.&lt;/span&gt;
The Docker images are already uplodaded and ready for use under tags &lt;code&gt;0.9.0.Alpha1&lt;/code&gt; and rolling &lt;code&gt;0.9&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;sql_server_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#sql_server_connector&quot; /&gt;SQL Server Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Support for SQL Server had been on the wish list of Debezium users for a long time (the original issue was &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
Thanks to lots of basic infrastructure created while working on the Oracle connector,
we were finally able to come up with a first preview of this new connector in comparatively short time of development.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Just as the Oracle connector, the one for SQL Server is under active development and should be considered an incubating feature at this point.
So for instance the structure of emitted change messages may change in upcoming releases.
In terms of features, it supports initial snapshotting and capturing changes via SQL Server’s CDC functionality.
There’s support for the most common column types, table whitelisting/blacklisting and more.
The most significant feature missing is support for structural changes of tables while the connector is running.
This is the next feature we’ll work on and it’s planned to be delivered as part of the next 0.9 release (see &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-812&quot;&gt;DBZ-812&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’d be very happy to learn about any feedback you may have on this newest connector of the Debezium family.
If you spot any bugs or have feature requests for it, please create a report in our &lt;a href=&quot;https://issues.redhat.com/browse/DBZ&quot;&gt;JIRA tracker&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;oracle_connector&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#oracle_connector&quot; /&gt;Oracle Connector&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The Debezium connector for Oracle is able to take initial snapshots now.
By means of the new connector option &lt;code&gt;snapshot.mode&lt;/code&gt; you can control whether &lt;em&gt;read&lt;/em&gt; events for all the records of all the captured tables should be emitted.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition the support for numeric data types has been honed (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-804&quot;&gt;DBZ-804&lt;/a&gt;);
any integer columns (i.e. &lt;code&gt;NUMBER&lt;/code&gt; with a scale &amp;lt;\= 0) will be emitted using the corresponding &lt;code&gt;int8&lt;/code&gt;/&lt;code&gt;int16&lt;/code&gt;/&lt;code&gt;int32&lt;/code&gt;/&lt;code&gt;int64&lt;/code&gt; field type,
if the columns precision allows for that.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve also spent some time on expanding the Oracle &lt;a href=&quot;https://debezium.io/docs/connectors/oracle/&quot;&gt;connector documentation&lt;/a&gt;,
which covers the structure of emitted change events and all the data type mappings in detail now.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;debezium_0_8_1_final&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#debezium_0_8_1_final&quot; /&gt;Debezium 0.8.1.Final&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Together with Debezium 0.9.0.Alpha1 we also did another release of the current stable Debezium version 0.8.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;While 0.9 at this point is more interesting to those eager to try out the latest developments in the Oracle and SQL Server connectors,
0.8.1.Final is a recommended upgrade especially to the users of the Postgres connector.
This release fixes an issue where it could happen that WAL segments on the server were retained longer than necessary,
in case only records of non-whitelisted tables changed for a while.
This has been addressed by means of supporting heartbeat messages (as already known from the MySQL connector) also for Postgres (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-800&quot;&gt;DBZ-800&lt;/a&gt;).
This lets the connector regularly commit offsets to Kafka Connect which also serves as the hook to acknowledge processed LSNs with the Postgres server.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can find the list of all changes done in Debezium 0.8.1.Final in the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-8-1-final&quot;&gt;change log&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As discussed above, we’ll work on supporting structural changes to captured tables while the SQL Server connector is running.
The same applies to the Oracle connector.
This will require some work on our DDL parsers, but thanks to the foundations provided by our recent migration of the MySQL DDL parser to Antlr, this should be manageable.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other big focus of work with be to provide an alternative implementation for getting changes from Oracle which isn’t based on the XStream API.
We’ve done some experiments with LogMiner and are also actively exploring further alternatives.
While some details are still unclear, we are optimistic to have something to release in this area soon.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you’d like to learn more about some middle and long term ideas, please check out our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt;.
Also please get in touch with us if you got any ideas or suggestions for future development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/</id>
<title>Five Advantages of Log-Based Change Data Capture</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-07-19T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/07/19/advantages-of-log-based-change-data-capture/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="discussion"></category>
<category term="featured"></category>
<summary>



Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the Darmstadt Java User Group.
It was a great evening with lots of interesting discussions and questions.
One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?


So first of all, what&#8217;s the difference between the two approaches?
With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
Log-based CDC in contrast works by...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Yesterday I had the opportunity to present Debezium and the idea of change data capture (CDC) to the &lt;a href=&quot;https://twitter.com/JUG_DA/status/1019634941020332032&quot;&gt;Darmstadt Java User Group&lt;/a&gt;.
It was a great evening with lots of interesting discussions and questions.
One of the questions being the following: what is the advantage of using a log-based change data capturing tool such as Debezium over simply polling for updated records?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So first of all, what’s the difference between the two approaches?
With polling-based (or query-based) CDC you repeatedly run queries (e.g. via JDBC) for retrieving any newly inserted or updated rows from the tables to be captured.
Log-based CDC in contrast works by reacting to any changes to the database’s log files (e.g. MySQL’s binlog or MongoDB’s op log).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As this wasn’t the first time this question came up, I thought I could provide a more extensive answer also here on the blog.
That way I’ll be able to refer to this post in the future, should the question come up again :)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So without further ado, here’s my list of five advantages of log-based CDC over polling-based approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;dlist&quot;&gt;
&lt;dl&gt;
&lt;dt class=&quot;hdlist1&quot;&gt;All Data Changes Are Captured&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;By reading the database’s log, you get the complete list of all data changes in their exact order of application.
This is vital for many use cases where you are interested in the complete history of record changes.
In contrast, with a polling-based approach you might miss intermediary data changes that happen between two runs of the poll loop.
For instance it could happen that a record is inserted and deleted between two polls,
in which case this record would never be captured by poll-based CDC.&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Related to this is the aspect of downtimes, e.g. when updating the CDC tool.
With poll-based CDC, only the latest state of a given record would be captured once the CDC tool is back online,
missing any earlier changes to the record that occurred during the downtime.
A log-based CDC tool will be able to resume reading the database log from the point where it left off before it was shut down,
causing the complete history of data changes to be captured.&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;dt class=&quot;hdlist1&quot;&gt;Low Delays of Events While Avoiding Increased CPU Load&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;With polling, you might be tempted to increase the frequency of polling attempts in order to reduce the chances of missing intermediary updates.
While this works to some degree, polling too frequently may cause performance issues (as the queries used for polling cause load on the source database).
On the other hand, expanding the polling interval will reduce the CPU load but may not only result in missed change events but also in a longer delay for propagating data changes.
Log-based CDC allows you to react to data changes in near real-time without paying the price of spending CPU time on running polling queries repeatedly.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;hdlist1&quot;&gt;No Impact on Data Model&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Polling requires some indicator to identify those records that have been changed since the last poll.
So all the captured tables need to have some column like &lt;code&gt;LAST_UPDATE_TIMESTAMP&lt;/code&gt; which can be used to find changed rows.
This can be fine in some cases, but in others such requirement might not be desirable.
Specifically, you’ll need to make sure that the update timestamps are maintained correctly on all tables to be captured by the writing applications or e.g. through triggers.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Deletes&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Naturally, polling will not allow you to identify any records that have been deleted since the last poll.
Often times that’s a problem for replication-like use cases where you’d like to have an identical data set on the source database and the replication targets,
meaning you’d also like to delete records on the sink side if they have been removed in the source database.&lt;/p&gt;
&lt;/dd&gt;
&lt;dt class=&quot;hdlist1&quot;&gt;Can Capture Old Record State And Further Meta Data&lt;/dt&gt;
&lt;dd&gt;
&lt;p&gt;Depending on the source database’s capabilities, log-based CDC can provide the old record state for update and delete events.
Whereas with polling, you’ll only get the current row state.
Having the old row state handy in a single change event can be interesting for many use cases, e.g. if you’d like to display the complete data change with old and new column values to an application user for auditing purposes.&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In addition, log-based approaches often can provide streams of schema changes (e.g. in form of applied DDL statements) and expose additional metadata such as transaction ids or the user applying a certain change.
These things may generally be doable with query-based approaches, too (depending on the capabilities of the database), I haven’t really seen it being done in practice, though.&lt;/p&gt;
&lt;/div&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;summary&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#summary&quot; /&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And that’s it, five advantages of log-based change data capture.
Note that this is not to say that polling-based CDC doesn’t have its applications.
If for instance your use case can be satisfied by propagating changes once per hour and it’s not a problem to miss intermediary versions of records that were valid in between, it can be perfectly fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But if you’re interested in capturing data changes in near real-time, making sure you don’t miss any change events (including deletions), then I’d recommend very much to explore the possibilities of log-based CDC as enabled by Debezium.
The Debezium connectors do all the heavy-lifting for you, i.e. you don’t have to deal with all the low-level specifics of the individual databases and the means of getting changes from their logs.
Instead, you can consume the generic and largely unified change data events produced by Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
<entry>
<id>https://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/</id>
<title>Debezium 0.8 Final Is Released</title>
<updated>2020-01-31T16:30:41+00:00</updated>
<published>2018-07-12T00:00:00+00:00</published>
<link href="https://debezium.io/blog/2018/07/12/debezium-0-8-0-final-released/" rel="alternate" type="text/html" />
<author>
<name>Gunnar Morling</name>
</author>
<category term="releases"></category>
<category term="mysql"></category>
<category term="postgres"></category>
<category term="mongodb"></category>
<category term="oracle"></category>
<category term="docker"></category>
<summary>



I&#8217;m very happy to announce the release of Debezium 0.8.0.Final!


The key features of Debezium 0.8 are the first work-in-progress version of our Oracle connector
(based on the XStream API) and a brand-new parser for MySQL DDL statements.
Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).


Please see the previous announcements (Beta 1, CR 1)
to learn about all...
</summary>
<content type="html">
&lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I’m very happy to announce the release of Debezium &lt;strong&gt;0.8.0.Final&lt;/strong&gt;!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The key features of Debezium 0.8 are the first work-in-progress version of our &lt;a href=&quot;https://debezium.io/docs/connectors/oracle/&quot;&gt;Oracle connector&lt;/a&gt;
(based on the XStream API) and a brand-new parser for MySQL DDL statements.
Besides that, there are plenty of smaller new features (e.g. propagation of default values to corresponding Connect schemas,
optional propagation of source queries in CDC messages and a largely improved SMT for sinking changes from MongoDB into RDBMS)
as well as lots of bug fixes (e.g. around temporal and numeric column types, large transactions with Postgres).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please see the previous announcements (&lt;a href=&quot;https://debezium.io/blog/2018/06/21/debezium-0-8-0-beta1-released/&quot;&gt;Beta 1&lt;/a&gt;, &lt;a href=&quot;https://debezium.io/blog/2018/07/04/debezium-0-8-0-cr1-released/&quot;&gt;CR 1&lt;/a&gt;)
to learn about all the changes in more depth.
The Final release largely resembles CR1;
apart from further improvements to the Oracle connector (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-762&quot;&gt;DBZ-792&lt;/a&gt;) there’s one nice addition to the MySQL connector contributed by &lt;a href=&quot;https://github.com/pgoranss&quot;&gt;Peter Goransson&lt;/a&gt;:
when doing a snapshot, it will now expose information about the processed rows via JMX (&lt;a href=&quot;https://issues.redhat.com/browse/DBZ-789&quot;&gt;DBZ-789&lt;/a&gt;), which is very handy when snapshotting larger tables.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Please take a look at the &lt;a href=&quot;https://debezium.io/docs/releases/#release-0-8-0-final&quot;&gt;change log&lt;/a&gt; for the complete list of changes in 0.8.0.Final and general upgrade notes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;whats_next&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#whats_next&quot; /&gt;What’s next?&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’re continuing our work on the Oracle connector.
The work on initial snapshotting is well progressing and it should be part of the next release.
Other improvements will be support for structural changes to captured tables after the initial snapshot has been made,
more extensive source info metadata and more.
Please track &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-716&quot;&gt;DBZ-716&lt;/a&gt; for this work; the improvements are planned to be released incrementally in the upcoming versions of Debezium.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We’ve also started to explore ingesting changes via LogMiner.
This is more involved in terms of engineering efforts than using XStream, but it comes with the huge advantage of not requiring a separate license
(LogMiner comes with the Oracle database itself).
It’s not quite clear yet when we can release something on this front, and we’re also actively exploring further alternatives.
But we are quite optimistic and hope to have something some time soon.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The other focus of work is a connector for SQL Server (see &lt;a href=&quot;https://issues.redhat.com/browse/DBZ-40&quot;&gt;DBZ-40&lt;/a&gt;).
Work on this has started as well, and there should be an Alpha1 release of Debezium 0.9 with a first drop of that connector within the next few weeks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To find out about some more long term ideas, please check out our &lt;a href=&quot;https://debezium.io/docs/roadmap/&quot;&gt;roadmap&lt;/a&gt; and get in touch with us, if you got any ideas or suggestions for future development.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;about_debezium&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#about_debezium&quot; /&gt;About Debezium&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Debezium is an open source distributed platform that turns your existing databases into event streams,
so applications can see and respond almost instantly to each committed row-level change in the databases.
Debezium is built on top of &lt;a href=&quot;http://kafka.apache.org/&quot;&gt;Kafka&lt;/a&gt; and provides &lt;a href=&quot;http://kafka.apache.org/documentation.html#connect&quot;&gt;Kafka Connect&lt;/a&gt; compatible connectors that monitor specific database management systems.
Debezium records the history of data changes in Kafka logs, so your application can be stopped and restarted at any time and can easily consume all of the events it missed while it was not running,
ensuring that all events are processed correctly and completely.
Debezium is &lt;a href=&quot;https://debezium.io/license/&quot;&gt;open source&lt;/a&gt; under the &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0.html&quot;&gt;Apache License, Version 2.0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;get_involved&quot;&gt;&lt;a class=&quot;anchor&quot; href=&quot;#get_involved&quot; /&gt;Get involved&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We hope you find Debezium interesting and useful, and want to give it a try.
Follow us on Twitter &lt;a href=&quot;https://twitter.com/debezium&quot;&gt;@debezium&lt;/a&gt;, &lt;a href=&quot;https://gitter.im/debezium/user&quot;&gt;chat with us on Gitter&lt;/a&gt;,
or join our &lt;a href=&quot;https://groups.google.com/forum/#!forum/debezium&quot;&gt;mailing list&lt;/a&gt; to talk with the community.
All of the code is open source &lt;a href=&quot;https://github.com/debezium/&quot;&gt;on GitHub&lt;/a&gt;,
so build the code locally and help us improve ours existing connectors and add even more connectors.
If you find problems or have ideas how we can improve Debezium, please let us know or &lt;a href=&quot;https://issues.redhat.com/projects/DBZ/issues/&quot;&gt;log an issue&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</content>
</entry>
</feed>
